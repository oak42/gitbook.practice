<html><head><meta http-equiv="Content-Type" content="text/html;charset=utf-8"><link href="style.css" rel="stylesheet" type="text/css"><title>Hadoop：The Definitive Guide    4th    Tom White    2015</title></head><body contenteditable="true"><div class="calibre1" id="calibre_link-707"><div class="book" id="calibre_link-3957" title="Hadoop:&nbsp;The Definitive Guide"><div class="book"><div class="book"><div class="book"><h2 class="part" id="calibre_link-2036" style="font-family: &quot;Microsoft YaHei UI&quot;; -webkit-font-smoothing: antialiased; -webkit-tap-highlight-color: transparent; background-color: rgba(255, 218, 185, 0.1); box-sizing: border-box; line-height: 1.2; margin: 0.83em 0px 0px; padding: 0px; word-break: break-word; overflow-wrap: break-word;"><span style="font-size: 1em;">------------------------------------------------------------------</span>------------------------------</h2><div style="
    font-size: 1.38em;
    text-shadow: 0 0 .1em;
">https://www.oreilly.com/catalog/<font color="#ff0000">errata</font>unconfirmed.csp?isbn=0636920033448</div><h1 class="title">Hadoop:&nbsp;The Definitive Guide</h1></div><div class="book"><div class="book"><h3 class="author"><span class="firstname">Tom</span> <span class="firstname">White</span></h3></div></div></div></div></div></div>

<div class="calibre1" id="calibre_link-453"><section type="preface" id="calibre_link-3958"><div class="titlepage"></div><p class="calibre2">For Eliane, Emilia, and Lottie</p></section></div>

<div class="calibre1" id="calibre_link-567"><section type="preface" id="calibre_link-3959" title="Foreword"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Foreword</h2></div><div class="book"><div class="book"><h3 class="author1"><span class="firstname">Doug</span> <span class="firstname">Cutting, April 2009</span></h3><div class="book"><span class="calibre">Shed in the Yard, California<br class="calibre3"></span></div></div></div></div></div><p class="calibre2">Hadoop got its start in Nutch. A few of us were attempting to build an
  open source web search engine and having trouble managing computations
  running on even a handful of computers. Once Google published its GFS and
  MapReduce papers, the route became clear. They’d devised systems to solve
  precisely the problems we were having with Nutch. So we started, two of us,
  half-time, to try to re-create these systems as a part of Nutch.</p><p class="calibre2">We managed to get Nutch limping along on 20 machines, but it soon
  became clear that to handle the Web’s massive scale, we’d need to run it on
  thousands of machines, and moreover, that the job was bigger than two
  half-time developers could handle.</p><p class="calibre2">Around that time, Yahoo! got interested, and quickly put together a
  team that I joined. We split off the distributed computing part of Nutch,
  naming it Hadoop. With the help of Yahoo!, Hadoop soon grew into a
  technology that could truly scale to the Web.</p><p class="calibre2">In 2006, Tom White started contributing to Hadoop. I already knew Tom
  through an excellent article he’d written about Nutch, so I knew he could
  present complex ideas in clear prose. I soon learned that he could also
  develop software that was as pleasant to read as his prose.</p><p class="calibre2">From the beginning, Tom’s contributions to Hadoop showed his concern
  for users and for the project. Unlike most open source contributors, Tom is
  not primarily interested in tweaking the system to better meet his own
  needs, but rather in making it easier for anyone to use.</p><p class="calibre2">Initially, Tom specialized in making Hadoop run well on Amazon’s EC2
  and S3 services. Then he moved on to tackle a wide variety of problems,
  including improving the MapReduce APIs, enhancing the website, and devising
  an object serialization framework. In all cases, Tom presented his ideas
  precisely. In short order, Tom earned the role of Hadoop committer and soon
  thereafter became a member of the Hadoop Project Management
  Committee.</p><p class="calibre2">Tom is now a respected senior member of the Hadoop developer
  community. Though he’s an expert in many technical corners of the project,
  his specialty is making Hadoop easier to use and understand.</p><p class="calibre2">Given this, I was very pleased when I learned that Tom intended to
  write a book about Hadoop. Who could be better qualified? Now you have the
  opportunity to learn about Hadoop from a master—not only of the technology,
  but also of common sense and plain talk.</p></section></div>

<div class="calibre1" id="calibre_link-455"><section type="preface" id="calibre_link-3960" title="Preface"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Preface</h2></div></div></div><p class="calibre2">Martin Gardner, the mathematics and science writer, once said in an
  interview:</p><div class="blockquote"><blockquote class="blockquote1"><p class="calibre4">Beyond calculus, I am lost. <br>That was the secret of my column’s
    success. <br>It took me so long to understand what I was writing about that I
    knew how to write in a way most readers would understand.<sup class="calibre5">[<a class="firstname" href="#calibre_link-456" id="calibre_link-459">1</a>]</sup></p></blockquote></div><p class="calibre2">In many ways, this is how I feel about Hadoop. <br>Its inner workings are
  complex, resting as they do on a mixture of distributed systems theory,
  practical engineering, and common sense. <br>And to the uninitiated, Hadoop can
  appear alien.&nbsp; &nbsp;&nbsp;<font color="#800080" style="
    font-size: .6em;
">//&nbsp;[ˌʌnɪˈnɪʃiˌeɪtəd]&nbsp; 无专门知识（或经验）的人；门外汉；外行 people who have no special knowledge or experience of sth</font></p><p class="calibre2">But it doesn’t need to be like this. <br>Stripped to its core, the tools
  that Hadoop provides for working with big data are simple. <br>If there’s a
  common theme, <br>it is about raising the level of abstraction<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;—to create
  building blocks for programmers who have lots of data to store and analyze,
  and <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; who don’t have the time, the skill, or the inclination <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;to become
  distributed systems experts <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; to build the infrastructure to handle it.</p><p class="calibre2">With such a simple and generally applicable feature set, it seemed
  obvious to me when I started using it that Hadoop deserved to be widely
  used. However, at the time (in early 2006), setting up, configuring, and
  writing programs to use Hadoop was an art. Things have certainly improved
  since then: there is more documentation, there are more examples, and there
  are thriving mailing lists to go to when you have questions. And yet the
  biggest hurdle for newcomers is understanding what this technology is
  capable of, where it excels, and how to use it. That is why I wrote this
  book.</p><p class="calibre2">The Apache Hadoop community has come a long way. <br>Since the publication
  of the first edition of this book, the Hadoop project has blossomed. <br>“Big
  data” has become a household term.<sup class="calibre6">[<a class="firstname" href="#calibre_link-457" id="calibre_link-460">2</a>]</sup>&nbsp; &nbsp; <font color="#800080">// household&nbsp;&nbsp;家庭的；家常的；王室的

</font><br>In this time, the software has made great leaps in adoption,
  performance, reliability, scalability, and manageability. <br>The number of
  things being built and run on the Hadoop platform has grown enormously. <br>In
  fact, it’s difficult for one person to keep track. <br>To gain even wider
  adoption, I believe we need to make Hadoop even easier to use. <br>This will
  involve writing more tools; integrating with even more systems; and writing
  new, improved APIs. <br>I’m looking forward to being a part of this, and I hope
  this book will encourage and enable others to do so, too.</p><div class="book" title="Administrative Notes"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-3961">Administrative Notes</h2></div></div></div><p class="calibre2">During discussion of a particular Java class in the text, I often
    omit its package name to reduce clutter. If you need to know which package
    a class is in, you can easily look it up in the Java API documentation for
    Hadoop (linked to from the <a class="ulink" href="http://hadoop.apache.org/" target="_top">Apache
    Hadoop home page</a>), or the relevant project. Or if you’re using an
    integrated development environment (IDE), its auto-complete mechanism can
    help find what you’re looking for.</p><p class="calibre2">Similarly, although it deviates from usual style guidelines, program
    listings that import multiple classes from the same package may use the
    asterisk wildcard character to save space (for example, <code class="literal">import org.apache.hadoop.io.*</code>).</p><p class="calibre2">The sample programs in this book are available for download from the
    book’s <a class="ulink" href="http://www.hadoopbook.com/" target="_top">website</a>. You will
    also find instructions there for obtaining the datasets that are used in
    examples throughout the book, as well as further notes for running the
    programs in the book and links to updates, additional resources, and my
    blog.</p></div><div class="book" title="What’s New in the Fourth Edition?"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-3962">What’s New in the Fourth Edition?</h2></div></div></div><p class="calibre2">The fourth edition covers Hadoop 2 exclusively. <br>The Hadoop 2 release
    series is the current active release series and contains the most stable
    versions of Hadoop.</p><p class="calibre2">There are new chapters covering YARN (<a class="ulink" href="#calibre_link-318" title="Chapter&nbsp;4.&nbsp;YARN">Chapter&nbsp;4</a>),
    Parquet (<a class="ulink" href="#calibre_link-208" title="Chapter&nbsp;13.&nbsp;Parquet">Chapter&nbsp;13</a>), Flume (<a class="ulink" href="#calibre_link-276" title="Chapter&nbsp;14.&nbsp;Flume">Chapter&nbsp;14</a>), Crunch (<a class="ulink" href="#calibre_link-283" title="Chapter&nbsp;18.&nbsp;Crunch">Chapter&nbsp;18</a>), and
    Spark (<a class="ulink" href="#calibre_link-352" title="Chapter&nbsp;19.&nbsp;Spark">Chapter&nbsp;19</a>). There’s also a new section to help
    readers navigate different pathways through the book (<a class="ulink" href="#calibre_link-458" title="What’s in This Book?">What’s in This Book?</a>).</p><p class="calibre2">This edition includes two new case studies (Chapters <a class="ulink" href="#calibre_link-282" title="Chapter&nbsp;22.&nbsp;Composable Data at Cerner">22</a> and <a class="ulink" href="#calibre_link-349" title="Chapter&nbsp;23.&nbsp;Biological Data Science: Saving Lives with Software">23</a>): one on how Hadoop is used in healthcare
    systems, and another on using Hadoop technologies for genomics data
    processing. Case studies from the previous editions can now be found <a class="ulink" href="http://bit.ly/hadoop_tdg_prev" target="_top">online</a>.</p><p class="calibre2">Many corrections, updates, and improvements have been made to
    existing chapters to bring them up to date with the latest releases of
    Hadoop and its related projects.</p></div><div class="book" title="What’s New in the Third Edition?"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-3963">What’s New in the Third Edition?</h2></div></div></div><p class="calibre2">The third edition covers the 1.x (formerly 0.20) release series of
    Apache Hadoop, as well as the newer 0.22 and 2.x (formerly 0.23) series.
    With a few exceptions, which are noted in the text, all the examples in
    this book run against these versions.</p><p class="calibre2">This edition uses the new MapReduce API for most of the examples.
    Because the old API is still in widespread use, it continues to be
    discussed in the text alongside the new API, and the equivalent code using
    the old API can be found on the book’s website.</p><p class="calibre2">The major change in Hadoop 2.0 is the new MapReduce runtime,
    MapReduce 2, which is built on a new distributed resource management
    system called YARN. This edition includes new sections covering MapReduce
    on YARN: how it works (<a class="ulink" href="#calibre_link-331" title="Chapter&nbsp;7.&nbsp;How MapReduce Works">Chapter&nbsp;7</a>) and how to run it
    (<a class="ulink" href="#calibre_link-1" title="Chapter&nbsp;10.&nbsp;Setting Up a Hadoop Cluster">Chapter&nbsp;10</a>).</p><p class="calibre2">There is more MapReduce material, too, including development
    practices such as packaging MapReduce jobs with Maven, setting the user’s
    Java classpath, and writing tests with MRUnit (all in <a class="ulink" href="#calibre_link-79" title="Chapter&nbsp;6.&nbsp;Developing a MapReduce Application">Chapter&nbsp;6</a>). In addition, there is more depth on features
    such as output committers and the distributed cache (both in <a class="ulink" href="#calibre_link-261" title="Chapter&nbsp;9.&nbsp;MapReduce Features">Chapter&nbsp;9</a>), as well as task memory monitoring (<a class="ulink" href="#calibre_link-1" title="Chapter&nbsp;10.&nbsp;Setting Up a Hadoop Cluster">Chapter&nbsp;10</a>). There is a new section on writing MapReduce jobs
    to process Avro data (<a class="ulink" href="#calibre_link-133" title="Chapter&nbsp;12.&nbsp;Avro">Chapter&nbsp;12</a>), and one on running a
    simple MapReduce workflow in Oozie (<a class="ulink" href="#calibre_link-79" title="Chapter&nbsp;6.&nbsp;Developing a MapReduce Application">Chapter&nbsp;6</a>).</p><p class="calibre2">The chapter on HDFS (<a class="ulink" href="#calibre_link-161" title="Chapter&nbsp;3.&nbsp;The Hadoop Distributed Filesystem">Chapter&nbsp;3</a>) now has
    introductions to high availability, federation, and the new WebHDFS and
    HttpFS filesystems.</p><p class="calibre2">The chapters on Pig, Hive, Sqoop, and ZooKeeper have all been
    expanded to cover the new features and changes in their latest
    releases.</p><p class="calibre2">In addition, numerous corrections and improvements have been made
    throughout the book.</p></div><div class="book" title="What’s New in the Second Edition?"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-3964">What’s New in the Second Edition?</h2></div></div></div><p class="calibre2">The second edition has two new chapters on Sqoop and Hive (Chapters
    <a class="ulink" href="#calibre_link-391" title="Chapter&nbsp;15.&nbsp;Sqoop">15</a> and <a class="ulink" href="#calibre_link-402" title="Chapter&nbsp;17.&nbsp;Hive">17</a>, respectively), a
    new section covering Avro (in <a class="ulink" href="#calibre_link-133" title="Chapter&nbsp;12.&nbsp;Avro">Chapter&nbsp;12</a>), an
    introduction to the new security features in Hadoop (in <a class="ulink" href="#calibre_link-1" title="Chapter&nbsp;10.&nbsp;Setting Up a Hadoop Cluster">Chapter&nbsp;10</a>), and a new case study on analyzing massive
    network graphs using Hadoop.</p><p class="calibre2">This edition continues to describe the 0.20 release series of Apache
    Hadoop, because this was the latest stable release at the time of writing.
    New features from later releases are occasionally mentioned in the text,
    however, with reference to the version that they were introduced
    in.</p></div><div class="book" title="Conventions Used in This Book"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-3965">Conventions Used in This Book</h2></div></div></div><p class="calibre2">The following typographical conventions are used in this
    book:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Italic</span></dt><dd class="calibre8"><p class="calibre2">Indicates new terms, URLs, email addresses, filenames, and
          file extensions.</p></dd><dt class="calibre7"><span class="term"><code class="literal1">Constant width</code></span></dt><dd class="calibre8"><p class="calibre2">Used for program listings, as well as within paragraphs to
          refer to commands and command-line options and to program elements
          such as variable or function names, databases, data types,
          environment variables, statements, and keywords.</p></dd><dt class="calibre7"><span class="term"><strong class="userinput"><code class="calibre9">Constant width
        bold</code></strong></span></dt><dd class="calibre8"><p class="calibre2">Shows commands or other text that should be typed literally by
          the user.</p></dd><dt class="calibre7"><span class="term"><em class="replaceable"><code class="replaceable">Constant width italic</code></em></span></dt><dd class="calibre8"><p class="calibre2">Shows text that should be replaced with user-supplied values
          or by values determined by context.</p></dd></dl></div><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">This icon signifies a general note.</p></div><div class="note" title="Tip"><h3 class="title3">Tip</h3><p class="calibre2">This icon signifies a tip or suggestion.</p></div><div class="caution" title="Caution"><h3 class="title4">Caution</h3><p class="calibre2">This icon indicates a warning or caution.</p></div></div><div class="book" title="Using Code Examples"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-3966">Using Code Examples</h2></div></div></div><p class="calibre2">Supplemental material (code, examples, exercise, etc.) is available
    for download at this book’s <a class="ulink" href="http://hadoopbook.com" target="_top">website</a> and on <a class="ulink" href="https://github.com/tomwhite/hadoop-book/" target="_top">GitHub</a>.</p><p class="calibre2">This book is here to help you get your job done. In general, you may
    use the code in this book in your programs and documentation. You do not
    need to contact us for permission unless you’re reproducing a significant
    portion of the code. For example, writing a program that uses several
    chunks of code from this book does not require permission. Selling or
    distributing a CD-ROM of examples from O’Reilly books does require
    permission. Answering a question by citing this book and quoting example
    code does not require permission. Incorporating a significant amount of
    example code from this book into your product’s documentation does require
    permission.</p><p class="calibre2">We appreciate, but do not require, attribution. An attribution
    usually includes the title, author, publisher, and ISBN. For example:
    “<span class="calibre"><em class="calibre10">Hadoop: The Definitive Guide</em></span>, Fourth Edition, by Tom
    White (O’Reilly). Copyright 2015 Tom White, 978-1-491-90163-2.”</p><p class="calibre2">If you feel your use of code examples falls outside fair use or the
    permission given here, feel free to contact us at
    <span class="calibre"><a class="ulink" href="mailto:permissions@oreilly.com">permissions@oreilly.com</a></span>.</p></div><div class="book" title="Safari® Books Online"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-3967">Safari® Books Online</h2></div></div></div><div class="note1" title="Note"><h3 class="title5">Note</h3><p class="calibre2"><a class="ulink" href="http://safaribooksonline.com" target="_top">Safari Books Online</a>
        is an on-demand digital library that delivers expert <a class="ulink" href="https://www.safaribooksonline.com/explore/" target="_top">content</a> in both
      book and video form from the world’s leading authors in technology and
      business.</p></div><p class="calibre2">Technology professionals, software developers, web designers, and
    business and creative professionals use Safari Books Online as their
    primary resource for research, problem solving, learning, and
    certification training.</p><p class="calibre2">Safari Books Online offers a range of <a class="ulink" href="https://www.safaribooksonline.com/pricing/" target="_top">plans and pricing</a> for <a class="ulink" href="https://www.safaribooksonline.com/enterprise/" target="_top">enterprise</a>,
      <a class="ulink" href="https://www.safaribooksonline.com/government/" target="_top">government</a>, <a class="ulink" href="https://www.safaribooksonline.com/academic-public-library/" target="_top">education</a>, and individuals.</p><p class="calibre2">Members have access to thousands of books, training videos, and prepublication manuscripts in one fully searchable database from publishers like O’Reilly Media, Prentice Hall Professional, Addison-Wesley Professional, Microsoft Press, Sams, Que, Peachpit Press, Focal Press, Cisco Press, John Wiley &amp; Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones &amp; Bartlett, Course Technology, and hundreds <a class="ulink" href="https://www.safaribooksonline.com/our-library/" target="_top">more</a>. For more information about Safari Books Online, please visit us <a class="ulink" href="http://safaribooksonline.com/" target="_top">online</a>.</p></div><div class="book" title="How to Contact Us"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-3968">How to Contact Us</h2></div></div></div><p class="calibre2">Please address comments and questions concerning this book to the
    publisher:</p><table class="simplelist"><tbody><tr class="calibre11"><td class="calibre12">O’Reilly Media, Inc.</td></tr><tr class="calibre13"><td class="calibre12">1005 Gravenstein Highway North</td></tr><tr class="calibre11"><td class="calibre12">Sebastopol, CA 95472</td></tr><tr class="calibre13"><td class="calibre12">800-998-9938 (in the United States or Canada)</td></tr><tr class="calibre11"><td class="calibre12">707-829-0515 (international or local)</td></tr><tr class="calibre13"><td class="calibre12">707-829-0104 (fax)</td></tr></tbody></table><p class="calibre2">We have a web page for this book, where we list errata, examples,
    and any additional information. You can access this page at <a class="ulink" href="http://bit.ly/hadoop_tdg_4e" target="_top">http://bit.ly/hadoop_tdg_4e</a>.</p><p class="calibre2">To comment or ask technical questions about this book, send email to
      <span class="calibre"><a class="ulink" href="mailto:bookquestions@oreilly.com">bookquestions@oreilly.com</a></span>.</p><p class="calibre2">For more information about our books, courses, conferences, and
    news, see our website at <a class="ulink" href="http://www.oreilly.com" target="_top">http://www.oreilly.com</a>.</p><p class="calibre2">Find us on Facebook: <a class="ulink" href="http://facebook.com/oreilly" target="_top">http://facebook.com/oreilly</a></p><p class="calibre2">Follow us on Twitter: <a class="ulink" href="http://twitter.com/oreillymedia" target="_top">http://twitter.com/oreillymedia</a></p><p class="calibre2">Watch us on YouTube: <a class="ulink" href="http://www.youtube.com/oreillymedia" target="_top">http://www.youtube.com/oreillymedia</a></p></div><div class="book" title="Acknowledgments"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-3969">Acknowledgments</h2></div></div></div><p class="calibre2">I have relied on many people, both directly and indirectly, in
    writing this book. I would like to thank the Hadoop community, from whom I
    have learned, and continue to learn, a great deal.</p><p class="calibre2">In particular, I would like to thank Michael Stack and Jonathan Gray
    for writing the chapter on HBase. Thanks also go to Adrian Woodhead, Marc
    de Palol, Joydeep Sen Sarma, Ashish Thusoo, Andrzej Białecki, Stu Hood,
    Chris K. Wensel, and Owen O’Malley
    for contributing case studies.</p><p class="calibre2">I would like to thank the following reviewers who contributed many
    helpful suggestions and improvements to my drafts: Raghu Angadi, Matt
    Biddulph, Christophe Bisciglia, Ryan Cox, Devaraj Das, Alex Dorman, Chris
    Douglas, Alan Gates, Lars George, Patrick Hunt, Aaron Kimball, Peter Krey,
    Hairong Kuang, Simon Maxen, Olga Natkovich, Benjamin Reed, Konstantin
    Shvachko, Allen Wittenauer, Matei Zaharia, and Philip Zeyliger. Ajay Anand
    kept the review process flowing smoothly. Philip (“flip”) Kromer kindly
    helped me with the NCDC weather dataset featured in the examples in this
    book. Special thanks to Owen O’Malley and Arun C. Murthy for explaining
    the intricacies of the MapReduce shuffle to me. Any errors that remain
    are, of course, to be laid at my door.</p><p class="calibre2">For the second edition, I owe a debt of gratitude for the detailed
    reviews and feedback from Jeff Bean, Doug Cutting, Glynn Durham, Alan
    Gates, Jeff Hammerbacher, Alex Kozlov, Ken Krugler, Jimmy Lin, Todd
    Lipcon, Sarah Sproehnle, Vinithra Varadharajan, and Ian Wrigley, as well
    as all the readers who submitted errata for the first edition. I would
    also like to thank Aaron Kimball for contributing the chapter on Sqoop,
    and Philip (“flip”) Kromer for the case study on graph processing.</p><p class="calibre2">For the third edition, thanks go to Alejandro Abdelnur, Eva
    Andreasson, Eli Collins, Doug Cutting, Patrick Hunt, Aaron Kimball, Aaron
    T. Myers, Brock Noland, Arvind Prabhakar, Ahmed Radwan, and Tom Wheeler
    for their feedback and suggestions. Rob Weltman kindly gave very detailed
    feedback for the whole book, which greatly improved the final manuscript.
    Thanks also go to all the readers who submitted errata for the second
    edition.</p><p class="calibre2">For the fourth edition, I would like to thank Jodok Batlogg, Meghan
    Blanchette, Ryan Blue, Jarek Jarcec Cecho, Jules Damji, Dennis Dawson,
    Matthew Gast, Karthik Kambatla, Julien Le Dem, Brock Noland, Sandy Ryza,
    Akshai Sarma, Ben Spivey, Michael Stack, Kate Ting, Josh Walter, Josh
    Wills, and Adrian Woodhead for all of their invaluable review feedback.
    Ryan Brush, Micah Whitacre, and Matt Massie kindly contributed new case
    studies for this edition. Thanks again to all the readers who submitted
    errata.</p><p class="calibre2">I am particularly grateful to Doug Cutting for his encouragement,
    support, and friendship, and for contributing the Foreword.</p><p class="calibre2">Thanks also go to the many others with whom I have had conversations
    or email discussions over the course
    of writing the book.</p><p class="calibre2">Halfway through writing the first edition of this book, I joined
    Cloudera, and I want to thank my colleagues for being incredibly supportive
    in allowing me the time to write and to get it finished promptly.</p><p class="calibre2">I am grateful to my editors, Mike Loukides and Meghan Blanchette,
    and their colleagues at O’Reilly for their help in the preparation of this
    book. Mike and Meghan have been there throughout to answer my questions,
    to read my first drafts, and to keep me on schedule.</p><p class="calibre2">Finally, the writing of this book has been a great deal of work, and
    I couldn’t have done it without the constant support of my family. My
    wife, Eliane, not only kept the home going, but also stepped in to help
    review, edit, and chase case studies. My daughters, Emilia and Lottie,
    have been very understanding, and I’m looking forward to spending lots
    more time with all of them.</p></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" id="calibre_link-456"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-459">1</a>] </sup>Alex Bellos, <a class="ulink" href="http://bit.ly/science_of_fun" target="_top">“The
        science of fun,”</a> <span class="calibre"><em class="calibre10">The Guardian</em></span>, May 31,
        2008.</p></div><div class="footnote" id="calibre_link-457"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-460">2</a>] </sup>It was added to the <span class="calibre"><em class="calibre10"><a class="ulink" href="http://bit.ly/6_13_oed_update" target="_top">Oxford
      English Dictionary</a></em></span> in 2013.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-390"><div class="book" type="part" id="calibre_link-3970" title="Part&nbsp;I.&nbsp;Hadoop Fundamentals"><div class="book"><div class="book"><div class="book"><h1 class="title6">Part&nbsp;I.&nbsp;Hadoop Fundamentals</h1></div></div></div></div></div>

<div class="calibre1" id="calibre_link-790"><section type="chapter" id="calibre_link-3971" title="Chapter&nbsp;1.&nbsp;Meet Hadoop"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;1.&nbsp;Meet Hadoop</h2></div></div></div><div class="blockquote"><blockquote class="blockquote1"><p class="calibre4">In pioneer days they used oxen for heavy pulling, and when one ox
    couldn’t budge a log, they didn’t try to grow a larger ox. <br>We shouldn’t be
    trying for bigger computers, but for more systems of computers.</p><div class="attribution"><p class="calibre15">—<span class="calibre">Grace Hopper</span></p></div></blockquote></div><div class="book" title="Data!"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-3972">Data!</h2></div></div></div><p class="calibre2">We live in the data <a class="calibre" id="calibre_link-2055"></a>age. It’s not easy to measure the total volume of data
    stored electronically, but an IDC estimate put the size of <a class="calibre" id="calibre_link-1493"></a>the “digital universe” at 4.4 zettabytes in 2013 and is
    forecasting a tenfold growth by 2020 to 44 zettabytes.<sup class="calibre6">[<a class="firstname" href="#calibre_link-791" id="calibre_link-811">3</a>]</sup> A zettabyte is 10<sup class="calibre6">21</sup> bytes, or
    <a class="calibre" id="calibre_link-3911"></a>equivalently one thousand exabytes, one million petabytes,
    or one billion terabytes. That’s
    more than one disk drive for every person in the world.</p><p class="calibre2">This flood of data is coming from many sources. Consider the
    following:<sup class="calibre6">[<a class="firstname" href="#calibre_link-792" id="calibre_link-812">4</a>]</sup></p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">The New York Stock Exchange generates about 4−5 terabytes of
        data per day.</p></li><li class="listitem"><p class="calibre2">Facebook hosts more than 240 billion photos, growing at 7
        petabytes per month.</p></li><li class="listitem"><p class="calibre2">Ancestry.com, the genealogy site, stores around 10 petabytes of
        data.</p></li><li class="listitem"><p class="calibre2">The Internet Archive stores around 18.5 petabytes of
        data.</p></li><li class="listitem"><p class="calibre2">The Large Hadron Collider near Geneva, Switzerland, produces
        about 30 petabytes of data per
        year.</p></li></ul></div><p class="calibre2">So there’s a lot of data out there. But you are probably wondering
    how it affects you. Most of the data is locked up in the largest web
    properties (like search engines) or in scientific or financial
    institutions, isn’t it? Does the advent of big data affect smaller
    organizations or individuals?</p><p class="calibre2">I argue that it does. Take photos, for example. My wife’s
    grandfather was an avid photographer and took photographs throughout his
    adult life. His entire corpus of medium-format, slide, and 35mm film, when
    scanned in at high resolution, occupies around 10 gigabytes. Compare this
    to the digital photos my family took in 2008, which take up about 5
    gigabytes of space. My family is producing photographic data at 35 times
    the rate my wife’s grandfather’s did, and the rate is increasing every
    year as it becomes easier to take more and more photos.</p><p class="calibre2">More generally, the digital streams that individuals are producing
    are growing apace. <a class="ulink" href="http://bit.ly/ms_mylifebits" target="_top">Microsoft
    Research’s MyLifeBits project</a> gives a <a class="calibre" id="calibre_link-2733"></a><a class="calibre" id="calibre_link-2695"></a>glimpse of the archiving of personal information that may
    become commonplace in the near future. MyLifeBits was an experiment where
    an individual’s interactions—phone calls, emails, documents—were captured
    electronically and stored for later access. The data gathered included a
    photo taken every minute, which resulted in an overall data volume of 1
    gigabyte per month. When
    storage costs come down enough to make it feasible to store continuous
    audio and video, the data volume for a future MyLifeBits service will be
    many times that.</p><p class="calibre2">The trend is for every individual’s data footprint to grow, but
    perhaps more significantly, the amount of data generated by machines as a
    part of the Internet of Things will be even greater than that generated by
    people. Machine logs, RFID readers, sensor networks, vehicle GPS traces,
    retail transactions—all of these
    contribute to the growing mountain of data.</p><p class="calibre2">The volume of data being made publicly available increases every
    year, too. Organizations no longer have to merely manage their own data;
    success in the future will be dictated to a large extent by their ability
    to extract value from other organizations’ data.</p><p class="calibre2">Initiatives such <a class="calibre" id="calibre_link-3093"></a><a class="calibre" id="calibre_link-884"></a>as <a class="ulink" href="http://aws.amazon.com/public-data-sets/" target="_top">Public Data Sets on Amazon
    Web Services</a> and <a class="ulink" href="http://infochimps.org/" target="_top">Infochimps.org</a> exist <a class="calibre" id="calibre_link-2097"></a>to foster <a class="calibre" id="calibre_link-2098"></a>the “information commons,” where data can be freely (or for
    a modest price) shared for anyone to download and analyze. Mashups between
    <a class="calibre" id="calibre_link-2648"></a>different information sources make for unexpected and
    hitherto unimaginable applications.</p><p class="calibre2">Take, for example, the <a class="ulink" href="http://astrometry.net/" target="_top">Astrometry.net project</a>, which
    <a class="calibre" id="calibre_link-922"></a>watches the Astrometry group on Flickr for new photos of the
    night sky. It analyzes each image and identifies which part of the sky it
    is from, as well as any interesting celestial bodies, such as stars or
    galaxies. This project shows the kinds of things that are possible when
    data (in this case, tagged photographic images) is made available and used
    for something (image analysis) that was not anticipated by the creator.</p><p class="calibre2">It has been said that “more data usually beats better algorithms,”
    <a class="calibre" id="calibre_link-3132"></a>which is to say that for some problems (such as recommending
    movies or music based on past preferences), however fiendish your
    algorithms, often they can be beaten simply by having more data (and a
    less sophisticated algorithm).<sup class="calibre6">[<a class="firstname" href="#calibre_link-793" id="calibre_link-813">5</a>]</sup></p><p class="calibre2">The good news is that big data is here. The bad news is that we are
    struggling to store and <a class="calibre" id="calibre_link-1494"></a>analyze it.</p></div><div class="book" title="Data Storage and Analysis"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-3973">Data Storage and Analysis</h2></div></div></div><p class="calibre2">The problem is <a class="calibre" id="calibre_link-1344"></a>simple: although the storage capacities of hard drives have
    increased massively over the years, access speeds—the rate at which data
    can be read from drives—have not kept up. One typical drive from 1990
    could store 1,370 MB of data and had a transfer speed of 4.4
    MB/s,<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-794" id="calibre_link-814">6</a>]</sup> so you could read all the data from a full drive in around
    five minutes. Over 20 years later, 1-terabyte drives are the norm, but the
    transfer speed is around 100 MB/s, so
    it takes more than two and a half hours to read all the data off the
    disk.</p><p class="calibre2">This is a long time to read all data on a single drive—and writing
    is even slower. The obvious way to reduce the time is to read from
    multiple disks at once. Imagine if we had 100 drives, each holding one
    hundredth of the data. Working in parallel, we could read the data in
    under two minutes.</p><p class="calibre2">Using only one hundredth of a disk may seem wasteful. But we can
    store 100 datasets, each of which is 1 terabyte, and provide shared access
    to them. We can imagine that the users of such a system would be happy to
    share access in return for shorter analysis times, and statistically, that
    their analysis jobs would be likely to be spread over time, so they
    wouldn’t interfere with each other too much.</p><p class="calibre2">There’s more to being able to read and write data in parallel to or
    from multiple disks, though.</p><p class="calibre2">The first problem to solve is hardware failure: as soon as you start
    using many pieces of hardware, the chance that one will fail is fairly
    high. <br>A common way of avoiding data loss is through replication: redundant
    copies of the data are kept by the system <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;so that in the event of failure,
    there is another copy available. <br>This is how RAID works, for instance,
<br>&nbsp;although Hadoop’s filesystem, the <a class="calibre" id="calibre_link-3974"></a><a class="calibre" id="calibre_link-1922"></a>Hadoop Distributed Filesystem (HDFS), takes a slightly
    different approach, as you shall see later.</p><p class="calibre2">The second problem is that most analysis tasks need to be able to
    combine the data in some way, <br>and data read from one disk may need to be
    combined with data from any of the other 99 disks. <br>Various distributed
    systems allow data to be combined from multiple sources, but doing this
    correctly is notoriously challenging. <br>MapReduce provides a <a class="calibre" id="calibre_link-2438"></a>programming model that abstracts the problem from disk reads
    and writes, <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;transforming it into a computation over sets of keys and
    values. <br>We look at the details of this model in later chapters, <br>but the
    important point for the present discussion is that <span style="
    text-shadow: 0 0 .38em;
">there are two parts to
    the computation—the map and the reduce</span>—and <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span style="
    text-shadow: 0 0 0.38em;
">it’s the interface between the
    two where the “mixing” occurs</span>. <br>Like HDFS, MapReduce has built-in
    reliability.</p><p class="calibre2">In a nutshell, this is what Hadoop provides: a reliable, scalable
    platform for storage and analysis. <br>What’s more, because it runs on
    commodity hardware and is open source, Hadoop is affordable.</p></div><div class="book" title="Querying All Your Data"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-3975">Querying All Your Data</h2></div></div></div><p class="calibre2">The approach taken by <a class="calibre" id="calibre_link-2494"></a><a class="calibre" id="calibre_link-3103"></a>MapReduce may seem like a brute-force approach. <br>The premise
    is that the entire dataset—or at least a good portion of it—can be
    processed for each query. <br>But this is its power. <br>MapReduce is a
    <em class="calibre10">batch</em> query processor, <br>and the ability to run an ad
    hoc query against your whole dataset and get the results in a reasonable
    time is transformative. <br>It changes the way you think about data and
<br>&nbsp; &nbsp; unlocks data that was previously archived on tape or disk. <br>It gives people
    the opportunity to innovate with data. <br>Questions that took too long to get
    answered before can now be answered, which in turn leads to new questions
    and new insights.</p><p class="calibre2">For example, Mailtrust, <a class="calibre" id="calibre_link-2383"></a><a class="calibre" id="calibre_link-3129"></a>Rackspace’s mail division, used Hadoop for processing email
    logs. <br>One ad hoc query they wrote was to find the geographic distribution
    of their users. In their words:</p><div class="blockquote"><blockquote class="blockquote1"><p class="calibre4">This data was so useful that we’ve scheduled the MapReduce job to
      run monthly and we will be using this data to help us decide which
      Rackspace data centers to place new mail servers in as we grow.</p></blockquote></div><p class="calibre2">By bringing several hundred gigabytes of data together and having
    the tools to analyze it, the Rackspace engineers were able to gain an
    understanding of the data that they otherwise would never have had, and
    furthermore, they were able to use what they had learned to improve the
    service for their customers.</p></div><div class="book" title="Beyond Batch"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-3976">Beyond Batch</h2></div></div></div><p class="calibre2">For all its strengths, MapReduce is fundamentally a <a class="calibre" id="calibre_link-2445"></a><a class="calibre" id="calibre_link-3105"></a><a class="calibre" id="calibre_link-980"></a>batch processing system, and is not suitable for interactive
    analysis.&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // 🚀<br>You can’t run a query and get results back in a few seconds or
    less. <br>Queries typically take minutes or more, so it’s best for offline
    use, where there isn’t a human sitting in the processing loop waiting for
    results.</p><p class="calibre2">However, since its original incarnation, <a class="calibre" id="calibre_link-1834"></a>Hadoop has evolved beyond batch processing.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // 🚀<br>Indeed, the term
    “Hadoop” is sometimes used to refer to a larger ecosystem of projects, not
    just HDFS and MapReduce, <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; that fall under the umbrella of infrastructure
    for distributed computing and <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;large-scale data processing. <br>Many of these
    are hosted by the <a class="ulink" href="http://www.apache.org/" target="_top">Apache Software
    Foundation</a>, <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; which <a class="calibre" id="calibre_link-898"></a>provides support for a community of open source software projects,
<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;including the original HTTP Server from which it gets its name.</p><p class="calibre2">The first component to provide online access <a class="calibre" id="calibre_link-1889"></a>was HBase, a key-value store that uses HDFS for its
    underlying storage.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // 🚀<br>HBase provides both online read/write access of
    individual rows <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;and batch operations for reading and writing data in bulk,
<br>making it a good solution for building applications on.</p><p class="calibre2">The real enabler for <span style="
    text-shadow: 0 0 1em cyan;
">new</span> <u style="
    text-decoration: underline .14em green;
">processing model</u>s in Hadoop <br>was the
    introduction <a class="calibre" id="calibre_link-3824"></a>of YARN (which stands for <em class="calibre10">Yet Another Resource
    Negotiator</em>) in Hadoop 2.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // 🚀<br>YARN is a <span style="
    text-shadow: 0 0 .38em;
"><u style="
    text-decoration: underline .1em;
">cluster resource management
    system</u></span>,<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;which allows any distributed program (not just MapReduce) to run
    on data in a Hadoop cluster.</p><p class="calibre2">In the last few years, there has been a flowering of different
    processing patterns that work with Hadoop. <br>Here is a sample:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Interactive SQL</span></dt><dd class="calibre8"><p class="calibre2">By dispensing with <a class="calibre" id="calibre_link-2132"></a>MapReduce and <br>&nbsp; &nbsp; &nbsp; using a distributed query engine that
          uses dedicated “always on” daemons (like Impala) or container reuse
          (like Hive on Tez), <br>it’s possible to achieve low-latency responses
          for SQL queries on Hadoop while still scaling up to large dataset
          sizes.</p></dd><dt class="calibre7"><span class="term">Iterative processing</span></dt><dd class="calibre8"><p class="calibre2">Many <a class="calibre" id="calibre_link-2156"></a>algorithms—such as those in machine learning—are
          iterative in nature, <br>so it’s much more efficient to hold each
          intermediate working set in memory, compared to loading from disk on
          each iteration. <br>The architecture of MapReduce does not allow this,
<br>but it’s straightforward with Spark, for example, <br>and it enables a
          highly exploratory style of working with datasets.</p></dd><dt class="calibre7"><span class="term">Stream processing</span></dt><dd class="calibre8"><p class="calibre2">Streaming <a class="calibre" id="calibre_link-3547"></a>systems like Storm, Spark Streaming, or Samza make it
          possible to run real-time, distributed computations on unbounded
          streams of data and emit results to Hadoop storage or external
          systems.</p></dd><dt class="calibre7"><span class="term">Search</span></dt><dd class="calibre8"><p class="calibre2">The Solr search <a class="calibre" id="calibre_link-3299"></a>platform can run on a Hadoop cluster, indexing
          documents as they are added to HDFS, and serving search queries from
          indexes stored in HDFS.</p></dd></dl></div><p class="calibre2">Despite the emergence of different processing frameworks on Hadoop,
<br>MapReduce still has a place for batch processing, and it is useful to
    understand how it works since it introduces several concepts that apply
    more generally (like the idea of input formats, or how a dataset is split
    into pieces).</p></div><div class="book" title="Comparison with Other Systems"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-3977">Comparison with Other Systems</h2></div></div></div><p class="calibre2">Hadoop isn’t the first <a class="calibre" id="calibre_link-1356"></a>distributed system for data storage and analysis, <br>but it has
    some unique properties that set it apart from other systems that may seem
    similar. <br>Here we look at some of them.</p><div class="book" title="Relational Database Management Systems"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-3978">Relational Database Management Systems</h3></div></div></div><p class="calibre2">Why can’t we use databases with <a class="calibre" id="calibre_link-3979"></a><a class="calibre" id="calibre_link-3139"></a>lots of disks to do large-scale analysis? <br>Why is Hadoop
      needed?</p><p class="calibre2">The answer to these questions comes from another <a class="calibre" id="calibre_link-3314"></a><a class="calibre" id="calibre_link-3713"></a>trend in disk drives: <span style="
    text-shadow: 0 0 0.38em darkcyan;
"><u style="">seek time</u> is improving more slowly
      than <u style="">transfer rate</u></span>. <br>Seeking is the process of moving the disk’s head to
      a particular place on the disk to read or write data. <br>It characterizes
      the latency of a disk operation, whereas the transfer rate corresponds
      to a disk’s bandwidth.</p><p class="calibre2">If the <u>data</u> <u style="
    text-decoration: underline 0.14em;
">access pattern</u> is dominated by seeks, <br>it will take
      longer to read or write large portions of the dataset than streaming
      through it, which operates at the transfer rate. <br>On the other hand, <br>for
      updating a small proportion of records in a database, <br>a traditional
      <a class="calibre" id="calibre_link-971"></a>B-Tree (the data structure used in relational databases,
      which is limited by the rate at which it can perform seeks) works well.
<br>For updating the majority of a database, a B-Tree is less efficient than
      <a class="calibre" id="calibre_link-2510"></a>MapReduce, which uses Sort/Merge to rebuild the
      database.</p><p class="calibre2">In many ways, MapReduce can be seen as a <b style="
    text-shadow: 0 0 .38em purple;
">complement</b> to a
      Relational Database Management System (RDBMS). <br>(The differences between
      the two systems are shown in <a class="ulink" href="#calibre_link-795" title="Table&nbsp;1-1.&nbsp;RDBMS compared to MapReduce">Table&nbsp;1-1</a>.) <br>MapReduce is
      a good fit for problems that need to analyze the <span style="
    text-shadow: 0 0 1em;
">whole</span> dataset in a
      batch fashion, particularly for ad hoc analysis. <br>An RDBMS is good for
      point queries or updates, <br>where the dataset has been indexed to deliver
      low-latency retrieval and update times of <span style="
    text-shadow: 0 0 1em;
">a relatively small amount</span> of
      data. <br>MapReduce suits applications where the data is written once and
      read many times, <br>whereas a relational database is good for datasets that
      are continually updated.<sup class="calibre6">[<a class="firstname" href="#calibre_link-796" id="calibre_link-815">7</a>]</sup></p><div class="table"><a id="calibre_link-795" class="calibre"></a><div class="table-title">Table&nbsp;1-1.&nbsp;RDBMS compared to MapReduce</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="c"><col class="newcol"><col class="newcol"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">&nbsp;</td><td class="calibre20">Traditional RDBMS</td><td class="calibre21">MapReduce</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><span class="calibre"><strong class="calibre24">Data size</strong></span></td><td class="calibre23">Gigabytes</td><td class="calibre25">Petabytes</td></tr><tr class="calibre26"><td class="calibre23"><span class="calibre"><strong class="calibre24">Access</strong></span></td><td class="calibre23">Interactive and batch</td><td class="calibre25">Batch</td></tr><tr class="calibre19"><td class="calibre23"><span class="calibre"><strong class="calibre24">Updates</strong></span></td><td class="calibre23">Read and write many times</td><td class="calibre25">Write once, read many times</td></tr><tr class="calibre26"><td class="calibre23"><span class="calibre"><strong class="calibre24">Transactions</strong></span></td><td class="calibre23">ACID</td><td class="calibre25">None</td></tr><tr class="calibre19"><td class="calibre23"><span class="calibre"><strong class="calibre24">Structure</strong></span></td><td class="calibre23">Schema-on-write</td><td class="calibre25">Schema-on-read</td></tr><tr class="calibre26"><td class="calibre23"><span class="calibre"><strong class="calibre24">Integrity</strong></span></td><td class="calibre23">High</td><td class="calibre25">Low</td></tr><tr class="calibre19"><td class="calibre27"><span class="calibre"><strong class="calibre24">Scaling</strong></span></td><td class="calibre27">Nonlinear</td><td class="calibre28">Linear</td></tr></tbody></table></div></div><p class="calibre2"><span style="
    text-shadow: 0 0 .38em red;
">However, the differences between relational databases and Hadoop
      systems are blurring. <br></span>Relational databases have started incorporating
      some of the ideas from Hadoop, and from the other direction, Hadoop
      systems such as Hive are becoming more interactive (by moving away from
      MapReduce) and adding features like indexes and transactions that make
      them look more and more like traditional RDBMSs.</p><p class="calibre2">Another difference between Hadoop and an RDBMS is the amount of
      structure in the datasets on which they operate. <br><em class="calibre10">Structured
      data</em> is <a class="calibre" id="calibre_link-3562"></a>organized into entities that have a defined format, such
      as XML documents or database tables that conform to a particular
      predefined schema. <br>This is the realm of the RDBMS.
      <br><em class="calibre10">Semi-structured data</em>, <a class="calibre" id="calibre_link-3324"></a>on the other hand, is looser, and though there may be a
      schema, it is often ignored, so it may be used only as a guide to the
      structure of the data: for example, a spreadsheet, in which the
      structure is the grid of cells, although the cells themselves may hold
      any form of data. <br><em class="calibre10">Unstructured data</em> does
      <a class="calibre" id="calibre_link-3746"></a>not have any particular internal structure: for example,
      plain text or image data. <br>Hadoop works well on unstructured or
      semi-structured data because it is designed to interpret the data at
      processing time (so <a class="calibre" id="calibre_link-3282"></a>called <em class="calibre10">schema-on-read</em>). <br>This
      provides flexibility and avoids the costly data loading phase of an
      RDBMS, since in Hadoop it is just a file copy.</p><p class="calibre2">Relational data is often <em class="calibre10">normalized</em> to
      <a class="calibre" id="calibre_link-2808"></a>retain its integrity and remove redundancy. Normalization poses problems for Hadoop
      processing because it makes reading a record a nonlocal operation, and
      one of the central assumptions that Hadoop makes is that it is possible
      to perform (high-speed) streaming reads and writes.</p><p class="calibre2">A web server log is a good example of a set of records that is
      <span class="calibre">not</span> normalized (for example, the
      client hostnames are specified in full each time, even though the same
      client may appear many times), and this is one reason that logfiles of
      all kinds are particularly well suited to analysis with Hadoop. Note
      that Hadoop can perform joins; it’s just that they are not used as much
      as in the relational world.</p><p class="calibre2">MapReduce—and the other <u style="
    text-decoration: underline 0.14em green;
">processing model</u>s in Hadoop—scales
      linearly with the size of the data.
<br>Data is partitioned, and the <a class="calibre" id="calibre_link-2946"></a>functional primitives (like map and reduce) can work in
      parallel on separate partitions. <br>This means that if you double the size
      of the input data, a job will run twice as slowly. <br>But if you also
      double the size of the cluster, a job will run as fast as the original
      one. <br>This is not generally true of SQL <a class="calibre" id="calibre_link-3140"></a>queries.</p></div><div class="book" title="Grid Computing"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-3980">Grid Computing</h3></div></div></div><p class="calibre2">The high-performance computing (HPC)<a class="calibre" id="calibre_link-1996"></a><a class="calibre" id="calibre_link-2056"></a><a class="calibre" id="calibre_link-1810"></a> and grid computing communities have been doing
      large-scale data processing for years, using such application program
      interfaces (APIs) as the <a class="calibre" id="calibre_link-2672"></a><a class="calibre" id="calibre_link-2701"></a>Message Passing Interface (MPI). <br>Broadly, the approach in
      HPC is to distribute the work across a cluster of machines, which access
      a shared filesystem, hosted by a storage area network (SAN). This works
      well for predominantly compute-intensive jobs, but it becomes a problem
      when nodes need to access larger data volumes (hundreds of gigabytes,
      the point at which Hadoop really starts to shine), since the network
      bandwidth is the bottleneck and compute nodes become idle.</p><p class="calibre2">Hadoop tries to co-locate the <a class="calibre" id="calibre_link-1805"></a>data with the compute nodes, so data access is fast
      because it is local.<sup class="calibre6">[<a class="firstname" href="#calibre_link-797" id="calibre_link-816">8</a>]</sup> This feature, known <a class="calibre" id="calibre_link-1341"></a>as <em class="calibre10">data locality</em>, is at the heart
      of data processing in Hadoop and is the reason for its good performance.
      Recognizing that network bandwidth is the most precious resource in a
      data center environment (it is easy to saturate network links by copying
      data around), Hadoop goes to great lengths to conserve it by explicitly
      modeling network topology. Notice that this arrangement does not
      preclude high-CPU analyses in Hadoop.</p><p class="calibre2">MPI gives great control to programmers, but it requires that they
      explicitly handle the mechanics of the data flow, exposed via low-level
      C routines and constructs such as sockets, as well as the higher-level
      algorithms for the analyses. Processing in Hadoop operates only at the
      higher level: the programmer thinks in terms of the data model (such as
      key-value pairs for MapReduce), while the data flow remains
      implicit.</p><p class="calibre2">Coordinating the processes in a large-scale distributed
      computation is a challenge. The hardest aspect is gracefully handling
      partial failure—when you don’t know whether or not a remote process has
      failed—and still making progress with the overall computation.
      Distributed processing frameworks like MapReduce spare the programmer
      from having to think about failure, since the implementation detects failed tasks and
      reschedules replacements on machines that are healthy. MapReduce is able
      to do this because it is <a class="calibre" id="calibre_link-3386"></a>a <em class="calibre10">shared-nothing</em> architecture,
      meaning that tasks have no dependence on one other. (This is a slight
      oversimplification, since the output from mappers is fed to the
      reducers, but this is under the control of the MapReduce system; in this
      case, it needs to take more care rerunning a failed reducer than
      rerunning a failed map, because it has to make sure it can retrieve the
      necessary map outputs and, if not, regenerate them by running the
      relevant maps again.) So from the programmer’s point of view, the order
      in which the tasks run doesn’t matter. By contrast, MPI programs have to
      explicitly manage their own checkpointing and recovery, which gives more
      control to the programmer but makes them more difficult to write.</p></div><div class="book" title="Volunteer Computing"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-3981">Volunteer Computing</h3></div></div></div><p class="calibre2">When people first hear <a class="calibre" id="calibre_link-3769"></a>about Hadoop and MapReduce they often ask, “How is it
      different from SETI@home?” SETI, the Search for Extra-Terrestrial
      Intelligence, runs a project <a class="calibre" id="calibre_link-3383"></a>called <a class="ulink" href="http://setiathome.berkeley.edu/" target="_top">SETI@home</a> in which
      volunteers donate CPU time from their otherwise idle computers to
      analyze radio telescope data for signs of intelligent life outside
      Earth. SETI@home is the most well known of many <em class="calibre10">volunteer
      computing</em> projects; others include <a class="calibre" id="calibre_link-1809"></a>the Great Internet Mersenne Prime Search (to search for
      large prime numbers) <a class="calibre" id="calibre_link-1725"></a>and Folding@home (to understand protein folding and how it
      relates to disease).</p><p class="calibre2">Volunteer computing projects work by breaking the problems they
      are trying to solve into chunks
      called <em class="calibre10">work units</em>, <a class="calibre" id="calibre_link-3787"></a>which are sent to computers around the world to be
      analyzed. For example, a SETI@home work unit is about 0.35 MB of radio
      telescope data, and takes hours or days to analyze on a typical home
      computer. When the analysis is completed, the results are sent back to
      the server, and the client gets another work unit. As a precaution to
      combat cheating, each work unit is sent to three different machines and
      needs at least two results to agree to be accepted.</p><p class="calibre2">Although SETI@home may be superficially similar to MapReduce
      (breaking a problem into independent pieces to be worked on in
      parallel), there are some significant differences. The SETI@home problem
      is very CPU-intensive, which makes it suitable for running on hundreds
      of thousands of computers across the world<sup class="calibre6">[<a class="firstname" href="#calibre_link-798" id="calibre_link-817">9</a>]</sup> because the time to transfer the work unit is dwarfed by
      the time to run the computation on it. Volunteers are donating CPU
      cycles, not bandwidth.</p><p class="calibre2">MapReduce is designed to run jobs that last minutes or hours on
      trusted, dedicated hardware running in a single data center with very
      high aggregate bandwidth interconnects. By contrast, SETI@home runs a
      perpetual computation on untrusted machines on the Internet with highly
      variable connection speeds and no data <a class="calibre" id="calibre_link-1357"></a><a class="calibre" id="calibre_link-2511"></a>locality.</p></div></div><div class="book" title="A Brief History of Apache Hadoop"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-67">A Brief History of Apache Hadoop</h2></div></div></div><p class="calibre2">Hadoop was created by <a class="calibre" id="calibre_link-1835"></a><a class="calibre" id="calibre_link-1316"></a>Doug Cutting, the creator of Apache Lucene, the widely used
    text search library. Hadoop has its origins in Apache Nutch, an <a class="calibre" id="calibre_link-2821"></a>open source web search engine, itself a part of the Lucene
    project.</p><div class="sidebar"><a id="calibre_link-3982" class="calibre"></a><div class="sidebar-title">The Origin of the Name “Hadoop”</div><p class="calibre2">The name Hadoop is not an acronym; it’s a made-up name. The
      project’s creator, Doug Cutting, explains how the name came
      about:</p><div class="blockquote"><blockquote class="blockquote"><p class="calibre4">The name my kid gave a stuffed yellow elephant. Short,
        relatively easy to spell and pronounce, meaningless, and not used
        elsewhere: those are my naming criteria. Kids are good at generating
        such. Googol is a kid’s term.</p></blockquote></div><p class="calibre2">Projects in the Hadoop ecosystem also tend to have names that are
      unrelated to their function, often with an elephant or other animal
      theme (“Pig,” for example).
      Smaller components are given more descriptive (and therefore more
      mundane) names. This is a good principle, as it means you can generally
      work out what something does from its name. For example, the <a class="calibre" id="calibre_link-2740"></a>namenode<sup class="calibre5">[<a class="firstname" type="noteref" href="#calibre_link-799" id="calibre_link-818">10</a>]</sup> manages the filesystem namespace.</p></div><p class="calibre2">Building a web search engine from scratch was an ambitious goal, for
    not only is the software required to crawl and index websites complex to
    write, but it is also a challenge to run without a dedicated operations
    team, since there are so many moving parts. It’s expensive, too:
    <a class="calibre" id="calibre_link-1044"></a>Mike Cafarella and Doug Cutting estimated a system
    supporting a one-billion-page index
    would cost around $500,000 in hardware, with a monthly running cost of
    $30,000.<sup class="calibre6">[<a class="firstname" href="#calibre_link-800" id="calibre_link-819">11</a>]</sup> Nevertheless, they believed it was a worthy goal, as it
    would open up and ultimately democratize search engine algorithms.</p><p class="calibre2">Nutch was started in 2002, and a working crawler and search system
    quickly emerged. However, its creators realized that their architecture
    wouldn’t scale to the billions of pages on the Web. Help was at hand with
    the publication of a paper in 2003 that described the architecture of
    Google’s <a class="calibre" id="calibre_link-1799"></a><a class="calibre" id="calibre_link-1801"></a>distributed filesystem, called GFS, which was being used in
    production at Google.<sup class="calibre6">[<a class="firstname" href="#calibre_link-801" id="calibre_link-820">12</a>]</sup> GFS, or something like it, would solve their storage needs
    for the very large files generated as a part of the web crawl and indexing
    process. In particular, GFS would free up time being spent on
    administrative tasks such as managing storage nodes. In 2004, Nutch’s
    developers set about writing an open source implementation, the<a class="calibre" id="calibre_link-2820"></a><a class="calibre" id="calibre_link-2781"></a> Nutch Distributed Filesystem (NDFS).</p><p class="calibre2">In 2004, Google published the paper that introduced MapReduce to the
    world.<sup class="calibre6">[<a class="firstname" href="#calibre_link-802" id="calibre_link-821">13</a>]</sup> Early in 2005, the Nutch developers had a working MapReduce
    implementation in Nutch, and by the middle of that year all the major
    Nutch algorithms had been ported to run using MapReduce and NDFS.</p><p class="calibre2">NDFS and the MapReduce implementation in Nutch were applicable
    beyond the realm of search, and in February 2006 they moved out of Nutch
    to form an independent subproject of Lucene called Hadoop.
    At around the same time, Doug Cutting joined Yahoo!, which provided a
    dedicated team and the resources to turn Hadoop into a system that ran at
    web scale (see the following sidebar). This was demonstrated in February
    2008 when Yahoo! announced that its production search index was being
    generated by a 10,000-core Hadoop <a class="calibre" id="calibre_link-2822"></a><a class="calibre" id="calibre_link-1317"></a>cluster.<sup class="calibre6">[<a class="firstname" href="#calibre_link-803" id="calibre_link-822">14</a>]</sup></p><div class="sidebar"><a id="calibre_link-3983" class="calibre"></a><div class="sidebar-title">Hadoop at Yahoo!</div><p class="calibre2">Building Internet-scale <a class="calibre" id="calibre_link-3823"></a>search engines requires huge amounts of data and therefore
      large numbers of machines to process it. Yahoo! Search consists of four
      primary components: the <em class="calibre10">Crawler</em>, which downloads
      pages from web servers; the <em class="calibre10">WebMap</em>, which builds
      a graph of the known Web; the <em class="calibre10">Indexer</em>, which
      builds a reverse index to the best pages; and the
      <em class="calibre10">Runtime</em>, which answers users’ queries. The WebMap
      is a graph that consists of roughly 1 trillion
      (10<sup class="calibre5">12</sup>) edges, each representing a web link,
      and 100 billion (10<sup class="calibre5">11</sup>) nodes, each
      representing distinct URLs. Creating and analyzing such a large graph
      requires a large number of computers running for many days. In early
      2005, the infrastructure for the WebMap, <a class="calibre" id="calibre_link-1542"></a>named <em class="calibre10">Dreadnaught</em>, needed to be
      redesigned to scale up to more nodes. Dreadnaught had successfully
      scaled from 20 to 600 nodes, but required a complete redesign to scale
      out further. Dreadnaught is similar to MapReduce in many ways, but
      provides more flexibility and less structure. In particular, each
      fragment in a Dreadnaught job could send output to each of the fragments
      in the next stage of the job, but the sort was all done in library code.
      In practice, most of the WebMap phases were pairs that corresponded to
      MapReduce. Therefore, the WebMap applications would not require
      extensive refactoring to fit into MapReduce.</p><p class="calibre2">Eric Baldeschwieler (aka Eric14) <a class="calibre" id="calibre_link-977"></a>created a small team, and we started designing and prototyping a new framework, written in
      C++ modeled and after GFS and MapReduce, to replace Dreadnaught.
      Although the immediate need was for a new framework for WebMap, it was
      clear that standardization of the batch platform across Yahoo! Search
      was critical and that by making the framework general enough to support
      other users, we could better leverage investment in the new
      platform.</p><p class="calibre2">At the same time, we were watching Hadoop, which was part of
      Nutch, and its progress. In January 2006, Yahoo! hired Doug Cutting, and
      a month later we decided to abandon our prototype and adopt Hadoop. The
      advantage of Hadoop over our prototype and design was that it was
      already working with a real application (Nutch) on 20 nodes. That
      allowed us to bring up a research cluster two months later and start
      helping real customers use the new framework much sooner than we could
      have otherwise. Another advantage, of course, was that since Hadoop was
      already open source, it was easier (although far from easy!) to get
      permission from Yahoo!’s legal department to work in open source. So, we
      set up a 200-node cluster for the researchers in early 2006 and put the
      WebMap conversion plans on hold while <a class="calibre" id="calibre_link-2890"></a>we supported and improved Hadoop for the research
      users.</p><p class="right">—Owen O’Malley, 2009</p></div><p class="calibre2">In January 2008, Hadoop was made its own top-level project at
    Apache, confirming its success and its diverse, active community. By this
    time, Hadoop was being used by many other companies besides Yahoo!, such
    as Last.fm, Facebook, and the <em class="calibre10">New York
    Times</em>.</p><p class="calibre2">In one well-publicized feat, the <em class="calibre10">New York
    Times</em> used Amazon’s <a class="calibre" id="calibre_link-885"></a><a class="calibre" id="calibre_link-1558"></a>EC2 compute cloud to crunch through 4 terabytes of scanned
    archives from the paper, converting them to PDFs for the Web.<sup class="calibre6">[<a class="firstname" href="#calibre_link-804" id="calibre_link-823">15</a>]</sup> The processing took less than 24 hours to run using 100
    machines, and the project probably wouldn’t have been embarked upon
    without the combination of Amazon’s pay-by-the-hour model (which allowed
    the <span class="calibre"><em class="calibre10">NYT</em></span> to access a large number of machines for a
    short period) and Hadoop’s easy-to-use parallel programming model.</p><p class="calibre2">In April 2008, Hadoop broke a world record to become the fastest
    system to sort an entire terabyte of data. Running on a 910-node cluster,
    Hadoop sorted 1 terabyte in 209 seconds (just under 3.5 minutes), beating
    the previous year’s winner of 297 seconds.<sup class="calibre6">[<a class="firstname" href="#calibre_link-805" id="calibre_link-824">16</a>]</sup> In November of the same year, Google reported that its
    MapReduce implementation sorted 1 terabyte in 68 seconds.<sup class="calibre6">[<a class="firstname" href="#calibre_link-806" id="calibre_link-825">17</a>]</sup> Then, in April 2009, it was announced that a team at Yahoo!
    had used Hadoop to sort 1 terabyte in 62 seconds.<sup class="calibre6">[<a class="firstname" href="#calibre_link-807" id="calibre_link-826">18</a>]</sup></p><p class="calibre2">The trend since then has been to sort even larger volumes of data at
    ever faster rates. In the 2014 competition, a team from Databricks were
    joint winners of the Gray Sort benchmark. They used a 207-node Spark
    cluster to sort 100 terabytes of data in 1,406 seconds, a rate of 4.27
    terabytes per minute.<sup class="calibre6">[<a class="firstname" href="#calibre_link-808" id="calibre_link-827">19</a>]</sup></p><p class="calibre2">Today, Hadoop is widely used in mainstream enterprises. Hadoop’s
    role as a general-purpose storage and analysis platform for big data has
    been recognized by the industry, and this fact is reflected in the number
    of products that use or incorporate Hadoop in some way. Commercial Hadoop
    support is available from large, established enterprise vendors, including
    EMC, IBM, Microsoft, and Oracle, as well as from specialist Hadoop
    companies such as Cloudera, Hortonworks, and <a class="calibre" id="calibre_link-1836"></a>MapR.</p></div><div class="book" title="What’s in This Book?"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-458">What’s in This Book?</h2></div></div></div><p class="calibre2">The book is divided into five main parts: <br>Parts <a class="ulink" href="#calibre_link-390" title="Part&nbsp;I.&nbsp;Hadoop Fundamentals">I</a> to <a class="ulink" href="#calibre_link-809" title="Part&nbsp;III.&nbsp;Hadoop Operations">III</a> are about
    core Hadoop, <br><a class="ulink" href="#calibre_link-88" title="Part&nbsp;IV.&nbsp;Related Projects">Part&nbsp;IV</a> covers related projects
      in the Hadoop ecosystem, and <br><a class="ulink" href="#calibre_link-207" title="Part&nbsp;V.&nbsp;Case Studies">Part&nbsp;V</a> contains Hadoop case studies. <br>You can read the book from cover to cover, <br>but
    there are alternative pathways through the book that allow you to skip
    chapters that aren’t needed to read later ones. <br>See <a class="ulink" href="#calibre_link-810" title="Figure&nbsp;1-1.&nbsp;Structure of the book: there are various pathways through the content">Figure&nbsp;1-1</a>.</p><p class="calibre2"><a class="ulink" href="#calibre_link-390" title="Part&nbsp;I.&nbsp;Hadoop Fundamentals">Part&nbsp;I</a> is made up of five chapters
    that cover the fundamental components in Hadoop and should be read before
    tackling later chapters. <br><a class="ulink" href="#calibre_link-790" title="Chapter&nbsp;1.&nbsp;Meet Hadoop">Chapter&nbsp;1</a> (this chapter) is a high-level introduction to Hadoop. <br><a class="ulink" href="#calibre_link-462" title="Chapter&nbsp;2.&nbsp;MapReduce">Chapter&nbsp;2</a> provides an
    introduction to MapReduce. <br><a class="ulink" href="#calibre_link-161" title="Chapter&nbsp;3.&nbsp;The Hadoop Distributed Filesystem">Chapter&nbsp;3</a> looks at Hadoop filesystems, and in particular
    HDFS, in depth. <br><a class="ulink" href="#calibre_link-318" title="Chapter&nbsp;4.&nbsp;YARN">Chapter&nbsp;4</a> discusses YARN, Hadoop’s
    cluster resource management system. <br><a class="ulink" href="#calibre_link-226" title="Chapter&nbsp;5.&nbsp;Hadoop I/O">Chapter&nbsp;5</a> covers the
    I/O building blocks in Hadoop: data integrity, compression, serialization,
    and file-based data structures.</p><p class="calibre2"><a class="ulink" href="#calibre_link-409" title="Part&nbsp;II.&nbsp;MapReduce">Part&nbsp;II</a> has four chapters that cover MapReduce
    in depth. <br>They provide useful understanding for later chapters (such as
    the data processing chapters in <a class="ulink" href="#calibre_link-88" title="Part&nbsp;IV.&nbsp;Related Projects">Part&nbsp;IV</a>), <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; but
    could be skipped on a first reading. <br><a class="ulink" href="#calibre_link-79" title="Chapter&nbsp;6.&nbsp;Developing a MapReduce Application">Chapter&nbsp;6</a> goes
    through the practical steps needed to develop a MapReduce application.
    <br><a class="ulink" href="#calibre_link-331" title="Chapter&nbsp;7.&nbsp;How MapReduce Works">Chapter&nbsp;7</a> looks at how MapReduce is implemented in
    Hadoop, from the point of view of a user. <br><a class="ulink" href="#calibre_link-568" title="Chapter&nbsp;8.&nbsp;MapReduce Types and Formats">Chapter&nbsp;8</a>
    is about the MapReduce programming model and the various data formats that
    MapReduce can work with. <br><a class="ulink" href="#calibre_link-261" title="Chapter&nbsp;9.&nbsp;MapReduce Features">Chapter&nbsp;9</a> is on
    advanced MapReduce topics, including sorting and joining data.</p><p class="calibre2"><a class="ulink" href="#calibre_link-809" title="Part&nbsp;III.&nbsp;Hadoop Operations">Part&nbsp;III</a> concerns the administration of
    Hadoop: <br>Chapters <a class="ulink" href="#calibre_link-1" title="Chapter&nbsp;10.&nbsp;Setting Up a Hadoop Cluster">10</a> and <a class="ulink" href="#calibre_link-0" title="Chapter&nbsp;11.&nbsp;Administering Hadoop">11</a> describe how to set up and maintain a
    Hadoop cluster running HDFS and MapReduce on YARN.</p><p class="calibre2"><a class="ulink" href="#calibre_link-88" title="Part&nbsp;IV.&nbsp;Related Projects">Part&nbsp;IV</a> of the book is dedicated to
    projects that build on Hadoop or are closely related to it. <br>Each chapter
    covers one project and is largely independent of the other chapters in
    this part, so they can be read in any order.</p><p class="calibre2">The first two chapters in this part are about data formats. <br><a class="ulink" href="#calibre_link-133" title="Chapter&nbsp;12.&nbsp;Avro">Chapter&nbsp;12</a> looks at Avro, a cross-language data serialization
    library for Hadoop, and <br><a class="ulink" href="#calibre_link-208" title="Chapter&nbsp;13.&nbsp;Parquet">Chapter&nbsp;13</a> covers Parquet, an
    efficient columnar storage format for nested data.</p><p class="calibre2">The next two chapters look at data ingestion, or how to get your
    data into Hadoop. <br><a class="ulink" href="#calibre_link-276" title="Chapter&nbsp;14.&nbsp;Flume">Chapter&nbsp;14</a> is about Flume, for
    high-volume ingestion of streaming data. <br><a class="ulink" href="#calibre_link-391" title="Chapter&nbsp;15.&nbsp;Sqoop">Chapter&nbsp;15</a> is
    about Sqoop, for efficient bulk transfer of data between structured data
    stores (like relational databases) and HDFS.</p><p class="calibre2">The common theme of the next four chapters is data processing, and
    in particular using higher-level abstractions than MapReduce. <br>Pig (<a class="ulink" href="#calibre_link-520" title="Chapter&nbsp;16.&nbsp;Pig">Chapter&nbsp;16</a>) is a data flow language for exploring very large
    datasets. <br>Hive (<a class="ulink" href="#calibre_link-402" title="Chapter&nbsp;17.&nbsp;Hive">Chapter&nbsp;17</a>) is a data warehouse for
    managing data stored in HDFS and provides a query language based on SQL.
<br>Crunch (<a class="ulink" href="#calibre_link-283" title="Chapter&nbsp;18.&nbsp;Crunch">Chapter&nbsp;18</a>) is a high-level Java API for
    writing data processing pipelines that can run on MapReduce or Spark.
<br>Spark (<a class="ulink" href="#calibre_link-352" title="Chapter&nbsp;19.&nbsp;Spark">Chapter&nbsp;19</a>) is a cluster computing framework for
    large-scale data processing; <br>it provides a <em class="calibre10">directed acyclic
    graph</em> (DAG) engine, and APIs in Scala, Java, and
    Python.</p><p class="calibre2"><a class="ulink" href="#calibre_link-68" title="Chapter&nbsp;20.&nbsp;HBase">Chapter&nbsp;20</a> is an introduction to HBase, a
    distributed column-oriented real-time database that uses HDFS for its
    underlying storage. <br>And <a class="ulink" href="#calibre_link-74" title="Chapter&nbsp;21.&nbsp;ZooKeeper">Chapter&nbsp;21</a> is about ZooKeeper, a
    distributed, highly available coordination service <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;that provides useful
    primitives for building distributed applications.</p><p class="calibre2">Finally, <a class="ulink" href="#calibre_link-207" title="Part&nbsp;V.&nbsp;Case Studies">Part&nbsp;V</a> is a collection of
    case studies contributed by people using Hadoop in interesting
    ways.</p><p class="calibre2">Supplementary information about Hadoop, such as how to install it on
    your machine, can be found in the appendixes.</p><div class="figure"><a id="calibre_link-810" class="calibre"></a><div class="book"><div class="book"><img alt="Structure of the book: there are various pathways through the content" src="images/000012.png" class="calibre29" style="
    width: 40em;
"></div></div><div class="figure-title">Figure&nbsp;1-1.&nbsp;Structure of the book: there are various pathways through the
      content</div></div></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" id="calibre_link-791"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-811">3</a>] </sup>These statistics were reported in a study entitled <a class="ulink" href="http://bit.ly/digital_universe" target="_top">“The
        Digital Universe of Opportunities: Rich Data and the Increasing Value
        of the Internet of Things.”</a></p></div><div class="footnote" id="calibre_link-792"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-812">4</a>] </sup>All figures are from 2013 or 2014. For more information, see Tom
        Groenfeldt, <a class="ulink" href="http://bit.ly/nyse_data_deluge" target="_top">“At
        NYSE, The Data Deluge Overwhelms Traditional Databases”</a>; Rich
        Miller, <a class="ulink" href="http://bit.ly/facebook_exabyte" target="_top">“Facebook
        Builds Exabyte Data Centers for Cold Storage”</a>; Ancestry.com’s
        <a class="ulink" href="http://corporate.ancestry.com/press/company-facts/" target="_top">“Company
        Facts”</a>; Archive.org’s <a class="ulink" href="https://archive.org/web/petabox.php" target="_top">“Petabox”</a>; and the
        <a class="ulink" href="http://wlcg.web.cern.ch/" target="_top">Worldwide LHC Computing Grid
        project’s welcome page</a>.</p></div><div class="footnote" id="calibre_link-793"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-813">5</a>] </sup>The quote is from Anand Rajaraman’s blog post <a class="ulink" href="http://bit.ly/more_data" target="_top">“More
        data usually beats better algorithms,”</a> in which he writes
        about the Netflix Challenge. Alon Halevy, Peter Norvig, and Fernando
        Pereira make the same point in <a class="ulink" href="http://bit.ly/unreasonable_effect" target="_top">“The
        Unreasonable Effectiveness of Data,”</a> <span class="calibre"><em class="calibre10">IEEE
        Intelligent Systems</em></span>, March/April 2009.</p></div><div class="footnote" type="footnote" id="calibre_link-794"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-814">6</a>] </sup>These specifications are for the Seagate ST-41600n.</p></div><div class="footnote" id="calibre_link-796"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-815">7</a>] </sup>In January 2007, David J. DeWitt and Michael Stonebraker
          caused a stir by publishing <a class="ulink" href="http://bit.ly/step_backwards" target="_top">“MapReduce: A major step
          backwards,”</a> in
          which they criticized MapReduce for being a poor substitute for
          relational databases. Many commentators argued that it was a false
          comparison (see, for example, Mark C. Chu-Carroll’s <a class="ulink" href="http://bit.ly/dbs_are_hammers" target="_top">“Databases are hammers; MapReduce is a
          screwdriver”</a>), and
          DeWitt and Stonebraker followed up with “MapReduce II,” where
          they addressed the main topics brought up by others.</p></div><div class="footnote" id="calibre_link-797"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-816">8</a>] </sup>Jim Gray was an early advocate of putting the computation near
          the data. See <a class="ulink" href="http://bit.ly/dist_comp_econ" target="_top">“Distributed
          Computing Economics,”</a> March 2003.</p></div><div class="footnote" id="calibre_link-798"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-817">9</a>] </sup>In January 2008, <a class="ulink" href="http://bit.ly/new_seti_at_home_data" target="_top">SETI@home
          was reported</a> to be processing 300 gigabytes a day, using
          320,000 computers (most of which are not dedicated to SETI@home;
          they are used for other things, too).</p></div><div class="footnote" type="footnote" id="calibre_link-799"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-818">10</a>] </sup>In this book, we use the lowercase form, “namenode,” to denote
          the entity when it’s being referred to generally, and the CamelCase
          form <code class="literal">NameNode</code> to denote the Java class that
          implements it.</p></div><div class="footnote" id="calibre_link-800"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-819">11</a>] </sup>See Mike Cafarella and Doug Cutting, <a class="ulink" href="http://bit.ly/building_nutch" target="_top">“Building Nutch: Open
        Source Search,”</a> <em class="calibre10">ACM Queue</em>, April
        2004.</p></div><div class="footnote" id="calibre_link-801"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-820">12</a>] </sup>Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung, <a class="ulink" href="http://research.google.com/archive/gfs.html" target="_top">“The Google File
        System,”</a> October 2003.</p></div><div class="footnote" id="calibre_link-802"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-821">13</a>] </sup>Jeffrey Dean and Sanjay Ghemawat, <a class="ulink" href="http://research.google.com/archive/mapreduce.html" target="_top">“MapReduce:
        Simplified Data Processing on Large Clusters,”</a> December
        2004.</p></div><div class="footnote" id="calibre_link-803"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-822">14</a>] </sup><a class="ulink" href="http://bit.ly/yahoo_hadoop" target="_top">“Yahoo!
        Launches World’s Largest Hadoop Production Application,”</a>
        February 19, 2008.</p></div><div class="footnote" id="calibre_link-804"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-823">15</a>] </sup>Derek Gottfrid, <a class="ulink" href="http://bit.ly/supercomputing_fun" target="_top">“Self-Service,
        Prorated Super Computing Fun!”</a> November 1, 2007.</p></div><div class="footnote" id="calibre_link-805"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-824">16</a>] </sup>Owen O’Malley, <a class="ulink" href="http://sortbenchmark.org/YahooHadoop.pdf" target="_top">“TeraByte Sort on
        Apache Hadoop,”</a> May 2008.</p></div><div class="footnote" id="calibre_link-806"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-825">17</a>] </sup>Grzegorz Czajkowski, <a class="ulink" href="http://bit.ly/sorting_1pb" target="_top">“Sorting
        1PB with MapReduce,”</a> November 21, 2008.</p></div><div class="footnote" id="calibre_link-807"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-826">18</a>] </sup>Owen O’Malley and Arun C. Murthy, <a class="ulink" href="http://sortbenchmark.org/Yahoo2009.pdf" target="_top">“Winning a 60 Second Dash
        with a Yellow Elephant,”</a> April 2009.</p></div><div class="footnote" id="calibre_link-808"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-827">19</a>] </sup>Reynold Xin et al., <a class="ulink" href="http://sortbenchmark.org/ApacheSpark2014.pdf" target="_top">“GraySort on Apache
        Spark by Databricks,”</a> November 2014.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-462"><section type="chapter" id="calibre_link-3984" title="Chapter&nbsp;2.&nbsp;MapReduce"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;2.&nbsp;MapReduce</h2></div></div></div><p class="calibre2">MapReduce is a <a class="calibre" id="calibre_link-2439"></a>programming model for data processing. The model is simple,
  yet not too simple to express useful programs in. Hadoop can run MapReduce
  programs written in various languages; in this chapter, we look at the same
  program expressed in Java, Ruby, and Python. Most importantly, MapReduce
  programs are inherently parallel, thus putting very large-scale data
  analysis into the hands of anyone with enough machines at their disposal.
  MapReduce comes into its own for large datasets, so let’s start by looking
  at one.</p><div class="book" title="A Weather Dataset"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-3985">A Weather Dataset</h2></div></div></div><p class="calibre2">For our <a class="calibre" id="calibre_link-2517"></a>example, we will write a program that mines weather data.
    Weather sensors collect data every hour at many locations across the globe
    and gather a large volume of log data, which is a good candidate for
    analysis with MapReduce because we want to process all the data, and the
    data is semi-structured and record-oriented.</p><div class="book" title="Data Format"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-3986">Data Format</h3></div></div></div><p class="calibre2">The data we will use is from the <a class="ulink" href="http://www.ncdc.noaa.gov/" target="_top">National Climatic Data Center</a>,
      or NCDC. The data is <a class="calibre" id="calibre_link-2770"></a><a class="calibre" id="calibre_link-2776"></a>stored using a line-oriented ASCII format, in which each
      line is a record. The format supports a rich set of meteorological
      elements, many of which are optional or with variable data lengths. For
      simplicity, we focus on the basic elements, such as temperature, which
      are always present and are of fixed width.</p><p class="calibre2"><a class="ulink" href="#calibre_link-463" title="Example&nbsp;2-1.&nbsp;Format of a National Climatic Data Center record">Example&nbsp;2-1</a> shows a sample line with some of the
      salient fields annotated. The line has been split into multiple lines to
      show each field; in the real file, fields are packed into one line with
      no delimiters.</p><div class="example"><a id="calibre_link-463" class="calibre"></a><div class="example-title">Example&nbsp;2-1.&nbsp;Format of a National Climatic Data Center record</div><div class="book"><pre class="screen">0057
332130   # USAF weather station identifier
99999    # WBAN weather station identifier
19500101 # observation date
0300     # observation time
4
+51317   # latitude (degrees x 1000)
+028783  # longitude (degrees x 1000)
FM-12
+0171    # elevation (meters)
99999
V020
320      # wind direction (degrees)
1        # quality code
N
0072
1
00450    # sky ceiling height (meters)
1        # quality code
C
N
010000   # visibility distance (meters)
1        # quality code
N
9
-0128    # air temperature (degrees Celsius x 10)
1        # quality code
-0139    # dew point temperature (degrees Celsius x 10)
1        # quality code
10268    # atmospheric pressure (hectopascals x 10)
1        # quality code</pre></div></div><p class="calibre2">Datafiles are organized by date and weather station. There is a
      directory for each year from 1901 to 2001, each containing a gzipped
      file for each weather station with its readings for that year. For
      example, here are the first entries for 1990:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">ls raw/1990 | head</code></strong>
010010-99999-1990.gz
010014-99999-1990.gz
010015-99999-1990.gz
010016-99999-1990.gz
010017-99999-1990.gz
010030-99999-1990.gz
010040-99999-1990.gz
010080-99999-1990.gz
010100-99999-1990.gz
010150-99999-1990.gz</pre><p class="calibre2">There are tens of thousands of weather stations, so the whole
      dataset is made up of a large number of relatively small files. It’s
      generally easier and more efficient to process a smaller number of
      relatively large files, so the data was preprocessed so that each year’s
      readings were concatenated into a single file. (The means by which this
      was carried out is described in <a class="ulink" href="#calibre_link-464" title="Appendix&nbsp;C.&nbsp;Preparing the NCDC Weather Data">Appendix&nbsp;C</a>.)</p></div></div><div class="book" title="Analyzing the Data with Unix Tools"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-3987">Analyzing the Data with Unix Tools</h2></div></div></div><p class="calibre2">What’s the <a class="calibre" id="calibre_link-1347"></a>highest recorded global temperature for each year in the
    dataset? We will answer this first without using Hadoop, as this
    information will provide a performance baseline and a useful means to
    check our results.</p><p class="calibre2">The classic tool for processing line-oriented data <a class="calibre" id="calibre_link-968"></a>is <em class="calibre10">awk</em>. <a class="ulink" href="#calibre_link-465" title="Example&nbsp;2-2.&nbsp;A program for finding the maximum recorded temperature by year from NCDC weather records">Example&nbsp;2-2</a> is a small script to calculate the maximum
    temperature for each year.</p><div class="example"><a id="calibre_link-465" class="calibre"></a><div class="example-title">Example&nbsp;2-2.&nbsp;A program for finding the maximum recorded temperature by year
      from NCDC weather records</div><div class="book"><pre class="screen"><code class="c1">#!/usr/bin/env bash</code>
<code class="k">for</code> year in all/*
<code class="k">do</code>
  <code class="nb">echo</code> -ne <code class="sb">`</code>basename <code class="nv">$year</code> .gz<code class="sb">`</code><code class="sb">"\t"</code>
  gunzip -c <code class="nv">$year</code> <code class="p">|</code> <code class="se">\</code>
    awk <code class="sb">'{ temp = substr($0, 88, 5) + 0;</code>
<code class="sb">           q = substr($0, 93, 1);</code>
<code class="sb">           if (temp !=9999 &amp;&amp; q ~ /[01459]/ &amp;&amp; temp &gt; max) max = temp }</code>
<code class="sb">         END { print max }'</code>
<code class="k">done</code></pre></div></div><p class="calibre2">The script loops through the compressed year files, first printing
    the year, and then processing each file using <em class="calibre10">awk</em>. The <em class="calibre10">awk</em> script extracts two fields from the data:
    the air temperature and the quality code. The air temperature value is
    turned into an integer by adding 0. Next, a test is applied to see whether
    the temperature is valid (the value 9999 signifies a missing value in the
    NCDC dataset) and whether the quality code indicates that the reading is
    not suspect or erroneous. If the reading is OK, the value is compared with
    the maximum value seen so far, which is updated if a new maximum is found.
    The <code class="literal">END</code> block is executed after all the
    lines in the file have been processed, and it prints the maximum
    value.</p><p class="calibre2">Here is the beginning of a run:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">./max_temperature.sh</code></strong>
1901	317
1902	244
1903	289
1904	256
1905	283
...</pre><p class="calibre2">The temperature values in the source file are scaled by a factor of
    10, so this works out as a maximum temperature of 31.7°C for 1901 (there
    were very few readings at the beginning of the century, so this is
    plausible). The complete run for the century took 42 minutes in one run on
    a single EC2 High-CPU Extra Large instance.</p><p class="calibre2">To speed up the processing, we need to run parts of the program in
    parallel. In theory, this is straightforward: we could process different
    years in different processes, using all the available hardware threads on
    a machine. There are a few problems with this, however.</p><p class="calibre2">First, dividing the work into equal-size pieces isn’t always easy or
    obvious. In this case, the file size for different years varies widely, so
    some processes will finish much earlier than others. Even if they pick up
    further work, the whole run is dominated by the longest file. A better
    approach, although one that requires more work, is to split the input into
    fixed-size chunks and assign each chunk to a process.</p><p class="calibre2">Second, combining the results from independent processes may require
    further processing. In this case, the result for each year is independent
    of other years, and they may be combined by concatenating all the results
    and sorting by year. If using the fixed-size chunk approach, the
    combination is more delicate. For this example, data for a particular year
    will typically be split into several chunks, each processed independently.
    We’ll end up with the maximum temperature for each chunk, so the final
    step is to look for the highest of these maximums for each year.</p><p class="calibre2">Third, you are still limited by the processing capacity of a single
    machine. If the best time you can achieve is 20 minutes with the number of
    processors you have, then that’s it. You can’t make it go faster. Also,
    some datasets grow beyond the capacity of a single machine. When we start
    using multiple machines, a whole host of other factors come into play,
    mainly falling into the categories of coordination and reliability. Who
    runs the overall job? How do we deal with failed processes?</p><p class="calibre2">So, although it’s feasible to parallelize the processing, in
    practice it’s messy. Using a framework like Hadoop to take care of these
    issues is a great <a class="calibre" id="calibre_link-1348"></a><a class="calibre" id="calibre_link-969"></a>help.</p></div><div class="book" title="Analyzing the Data with Hadoop"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-3988">Analyzing the Data with Hadoop</h2></div></div></div><p class="calibre2">To take <a class="calibre" id="calibre_link-1345"></a>advantage of the parallel processing that Hadoop provides,
    we need to express our query as a MapReduce job. After some local,
    small-scale testing, we will be able to run it on a cluster of
    machines.</p><div class="book" title="Map and Reduce"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-3989">Map and Reduce</h3></div></div></div><p class="calibre2">MapReduce works by breaking the processing into two phases: the
      map phase and the reduce phase. Each phase has key-value pairs as input
      and output, the types of which may be chosen by the programmer. The
      programmer also specifies two functions: the map function and the reduce
      function.</p><p class="calibre2">The input to our map phase is the raw NCDC data. We choose a text
      input format that gives us each line in the dataset as a text value. The
      key is the offset of the beginning of the line from the beginning of the
      file, but as we have no need for this, we ignore it.</p><p class="calibre2">Our map function <a class="calibre" id="calibre_link-2387"></a>is simple. We pull out the year and the air temperature,
      because these are the only fields we are interested in. In this case,
      the map function is just a data preparation phase, setting up the data
      in such a way that the reduce function can do its work on it: finding
      the maximum temperature for each year. The map function is also a good
      place to drop bad records: here we filter out temperatures that are
      missing, suspect, or erroneous.</p><p class="calibre2">To visualize the way the map works, consider the following sample
      lines of input data (some unused columns have been dropped to fit the
      page, indicated by ellipses):</p><pre class="screen1">0067011990999991950051507004<span class="calibre">...</span>9999999N9+00001+99999999999<span class="calibre">...</span>
0043011990999991950051512004<span class="calibre">...</span>9999999N9+00221+99999999999<span class="calibre">...</span>
0043011990999991950051518004<span class="calibre">...</span>9999999N9-00111+99999999999<span class="calibre">...</span>
0043012650999991949032412004<span class="calibre">...</span>0500001N9+01111+99999999999<span class="calibre">...</span>
0043012650999991949032418004<span class="calibre">...</span>0500001N9+00781+99999999999<span class="calibre">...</span></pre><p class="calibre2">These lines are presented to the map function as the key-value
      pairs:</p><a id="calibre_link-3990" class="calibre"></a><pre class="screen1">(0, 006701199099999<strong class="userinput"><code class="calibre9">1950</code></strong>051507004...9999999N9+<strong class="userinput"><code class="calibre9">0000</code></strong>1+99999999999...)
(106, 004301199099999<strong class="userinput"><code class="calibre9">1950</code></strong>051512004...9999999N9+<strong class="userinput"><code class="calibre9">0022</code></strong>1+99999999999...)
(212, 004301199099999<strong class="userinput"><code class="calibre9">1950</code></strong>051518004...9999999N9-<strong class="userinput"><code class="calibre9">0011</code></strong>1+99999999999...)
(318, 004301265099999<strong class="userinput"><code class="calibre9">1949</code></strong>032412004...0500001N9+<strong class="userinput"><code class="calibre9">0111</code></strong>1+99999999999...)
(424, 004301265099999<strong class="userinput"><code class="calibre9">1949</code></strong>032418004...0500001N9+<strong class="userinput"><code class="calibre9">0078</code></strong>1+99999999999...)</pre><p class="calibre2">The keys are the line offsets within the file, which we ignore in
      our map function. The map function merely extracts the year and the air
      temperature (indicated in bold text), and emits them as its output (the
      temperature values have been interpreted as integers):</p><a id="calibre_link-3991" class="calibre"></a><pre class="screen1">(1950, 0)
(1950, 22)
(1950, −11)
(1949, 111)
(1949, 78)</pre><p class="calibre2">The output from the map function is processed by the MapReduce
      framework before being sent to the reduce function. This processing
      sorts and groups the key-value pairs by key. So, continuing the example,
      our reduce function sees the following input:</p><a id="calibre_link-3992" class="calibre"></a><pre class="screen1">(1949, [111, 78])
(1950, [0, 22, −11])</pre><p class="calibre2">Each year appears with a list of all its air temperature readings.
      All the <a class="calibre" id="calibre_link-3177"></a>reduce function has to do now is iterate through the list
      and pick up the maximum reading:</p><a id="calibre_link-3993" class="calibre"></a><pre class="screen1">(1949, 111)
(1950, 22)</pre><p class="calibre2">This is the final output: the maximum global temperature recorded
      in each year.</p><p class="calibre2">The whole data flow is illustrated in <a class="ulink" href="#calibre_link-466" title="Figure&nbsp;2-1.&nbsp;MapReduce logical data flow">Figure&nbsp;2-1</a>. At the bottom of the diagram is a Unix
      pipeline, which mimics the whole MapReduce flow and which we will see
      again later in this chapter when we look at Hadoop Streaming.</p><div class="figure"><a id="calibre_link-466" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-3994" class="calibre"></a><img alt="MapReduce logical data flow" src="images/000020.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;2-1.&nbsp;MapReduce logical data flow</div></div></div><div class="book" title="Java MapReduce"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-756">Java MapReduce</h3></div></div></div><p class="calibre2">Having run through how the <a class="calibre" id="calibre_link-2180"></a>MapReduce program works, the next step is to express it in
      code. We need three things: a map function, a reduce function, and some
      code to run the job. The <a class="calibre" id="calibre_link-2394"></a>map function is represented by the <code class="literal">Mapper</code> class, which declares an abstract
      <code class="literal">map</code><code class="literal">()</code> method. <a class="ulink" href="#calibre_link-467" title="Example&nbsp;2-3.&nbsp;Mapper for the maximum temperature example">Example&nbsp;2-3</a> shows the implementation of our map
      function.</p><div class="example"><a id="calibre_link-467" class="calibre"></a><div class="example-title">Example&nbsp;2-3.&nbsp;Mapper for the maximum temperature example</div><div class="book"><pre class="screen"><code class="k">import</code> <code class="nn">java.io.IOException</code><code class="o">;</code>

<code class="k">import</code> <code class="nn">org.apache.hadoop.io.IntWritable</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.io.LongWritable</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.io.Text</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.mapreduce.Mapper</code><code class="o">;</code>

<code class="k">public</code> <code class="k">class</code> <code class="nc">MaxTemperatureMapper</code>
    <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">&gt;</code> <code class="o">{</code>

  <code class="k">private</code> <code class="k">static</code> <code class="k">final</code> <code class="kt">int</code> <code class="n">MISSING</code> <code class="o">=</code> <code class="mi">9999</code><code class="o">;</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">LongWritable</code> <code class="n">key</code><code class="o">,</code> <code class="n">Text</code> <code class="n">value</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    
    <code class="n">String</code> <code class="n">line</code> <code class="o">=</code> <code class="n">value</code><code class="o">.</code><code class="na">toString</code><code class="o">();</code>
    <code class="n">String</code> <code class="n">year</code> <code class="o">=</code> <code class="n">line</code><code class="o">.</code><code class="na">substring</code><code class="o">(</code><code class="mi">15</code><code class="o">,</code> <code class="mi">19</code><code class="o">);</code>
    <code class="kt">int</code> <code class="n">airTemperature</code><code class="o">;</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">line</code><code class="o">.</code><code class="na">charAt</code><code class="o">(</code><code class="mi">87</code><code class="o">)</code> <code class="o">==</code> <code class="sb">'+'</code><code class="o">)</code> <code class="o">{</code> <code class="c2">// parseInt doesn't like leading plus signs</code>
      <code class="n">airTemperature</code> <code class="o">=</code> <code class="n">Integer</code><code class="o">.</code><code class="na">parseInt</code><code class="o">(</code><code class="n">line</code><code class="o">.</code><code class="na">substring</code><code class="o">(</code><code class="mi">88</code><code class="o">,</code> <code class="mi">92</code><code class="o">));</code>
    <code class="o">}</code> <code class="k">else</code> <code class="o">{</code>
      <code class="n">airTemperature</code> <code class="o">=</code> <code class="n">Integer</code><code class="o">.</code><code class="na">parseInt</code><code class="o">(</code><code class="n">line</code><code class="o">.</code><code class="na">substring</code><code class="o">(</code><code class="mi">87</code><code class="o">,</code> <code class="mi">92</code><code class="o">));</code>
    <code class="o">}</code>
    <code class="n">String</code> <code class="n">quality</code> <code class="o">=</code> <code class="n">line</code><code class="o">.</code><code class="na">substring</code><code class="o">(</code><code class="mi">92</code><code class="o">,</code> <code class="mi">93</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">airTemperature</code> <code class="o">!=</code> <code class="n">MISSING</code> <code class="o">&amp;&amp;</code> <code class="n">quality</code><code class="o">.</code><code class="na">matches</code><code class="o">(</code><code class="sb">"[01459]"</code><code class="o">))</code> <code class="o">{</code>
      <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="n">year</code><code class="o">),</code> <code class="k">new</code> <code class="n">IntWritable</code><code class="o">(</code><code class="n">airTemperature</code><code class="o">));</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The <code class="literal">Mapper</code> class is a generic
      type, with four formal type parameters that specify the input key, input
      value, output key, and output value types of the map function. For the
      present example, the input key is a long integer offset, the input value
      is a line of text, the output key is a year, and the output value is an
      air temperature (an integer). Rather than using built-in Java types,
      Hadoop provides its own set of basic types that are optimized for
      network serialization. These are found in <a class="calibre" id="calibre_link-2863"></a>the <code class="literal">org.apache.hadoop.io</code> package. Here we
      <a class="calibre" id="calibre_link-2366"></a>use <code class="literal">LongWritable</code>, which
      corresponds to a Java <code class="literal">Long</code>, <code class="literal">Text</code> (like Java <code class="literal">String</code>), and <code class="literal">IntWritable</code> (like Java <code class="literal">Integer</code>).</p><p class="calibre2">The <code class="literal">map()</code> method is passed a
      key and a value. We convert the <code class="literal">Text</code>
      value containing the line of input into a Java <code class="literal">String</code>, then use its <code class="literal">substring()</code> method to extract the columns we
      are interested in.</p><p class="calibre2">The <code class="literal">map()</code> method also provides
      an instance of <code class="literal">Context</code> to write the
      output to. In this case, we write the year as a <code class="literal">Text</code> object (since we are just using it as a
      key), and the temperature is wrapped in <a class="calibre" id="calibre_link-2137"></a>an <code class="literal">IntWritable</code>. We
      write an output record only if the temperature is present and the
      quality code indicates the temperature reading is OK.</p><p class="calibre2">The reduce function is <a class="calibre" id="calibre_link-3182"></a>similarly defined using a <code class="literal">Reducer</code>, as illustrated in <a class="ulink" href="#calibre_link-468" title="Example&nbsp;2-4.&nbsp;Reducer for the maximum temperature example">Example&nbsp;2-4</a>.</p><div class="example"><a id="calibre_link-468" class="calibre"></a><div class="example-title">Example&nbsp;2-4.&nbsp;Reducer for the maximum temperature example</div><div class="book"><pre class="screen"><code class="k">import</code> <code class="nn">java.io.IOException</code><code class="o">;</code>

<code class="k">import</code> <code class="nn">org.apache.hadoop.io.IntWritable</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.io.Text</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.mapreduce.Reducer</code><code class="o">;</code>

<code class="k">public</code> <code class="k">class</code> <code class="nc">MaxTemperatureReducer</code>
    <code class="k">extends</code> <code class="n">Reducer</code><code class="o">&lt;</code><code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">&gt;</code> <code class="o">{</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">reduce</code><code class="o">(</code><code class="n">Text</code> <code class="n">key</code><code class="o">,</code> <code class="n">Iterable</code><code class="o">&lt;</code><code class="n">IntWritable</code><code class="o">&gt;</code> <code class="n">values</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    
    <code class="kt">int</code> <code class="n">maxValue</code> <code class="o">=</code> <code class="n">Integer</code><code class="o">.</code><code class="na">MIN_VALUE</code><code class="o">;</code>
    <code class="k">for</code> <code class="o">(</code><code class="n">IntWritable</code> <code class="n">value</code> <code class="o">:</code> <code class="n">values</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">maxValue</code> <code class="o">=</code> <code class="n">Math</code><code class="o">.</code><code class="na">max</code><code class="o">(</code><code class="n">maxValue</code><code class="o">,</code> <code class="n">value</code><code class="o">.</code><code class="na">get</code><code class="o">());</code>
    <code class="o">}</code>
    <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">key</code><code class="o">,</code> <code class="k">new</code> <code class="n">IntWritable</code><code class="o">(</code><code class="n">maxValue</code><code class="o">));</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">Again, four formal type parameters are used to specify the input
      and output types, this time for the reduce function. The input types of
      the reduce function must match the output types of the map function:
      <code class="literal">Text</code> and <code class="literal">IntWritable</code>. And in this case, the output
      types of the reduce function are <code class="literal">Text</code>
      and <code class="literal">IntWritable</code>, for a year and its
      maximum temperature, which we find by iterating through the temperatures
      and comparing each with a record of the highest found so far.</p><p class="calibre2">The third piece of code runs the MapReduce job (see <a class="ulink" href="#calibre_link-469" title="Example&nbsp;2-5.&nbsp;Application to find the maximum temperature in the weather dataset">Example&nbsp;2-5</a>).</p><div class="example"><a id="calibre_link-469" class="calibre"></a><div class="example-title">Example&nbsp;2-5.&nbsp;Application to find the maximum temperature in the weather
        dataset</div><div class="book"><pre class="screen"><code class="k">import</code> <code class="nn">org.apache.hadoop.fs.Path</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.io.IntWritable</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.io.Text</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.mapreduce.Job</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.mapreduce.lib.input.FileInputFormat</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.mapreduce.lib.output.FileOutputFormat</code><code class="o">;</code>

<code class="k">public</code> <code class="k">class</code> <code class="nc">MaxTemperature</code> <code class="o">{</code>

  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="na">length</code> <code class="o">!=</code> <code class="mi">2</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="sb">"Usage: MaxTemperature &lt;input path&gt; &lt;output path&gt;"</code><code class="o">);</code>
      <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(-</code><code class="mi">1</code><code class="o">);</code>
    <code class="o">}</code>
    
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Job</code><code class="o">();</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setJarByClass</code><code class="o">(</code><code class="n">MaxTemperature</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setJobName</code><code class="o">(</code><code class="sb">"Max temperature"</code><code class="o">);</code>

    <code class="n">FileInputFormat</code><code class="o">.</code><code class="na">addInputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]));</code>
    <code class="n">FileOutputFormat</code><code class="o">.</code><code class="na">setOutputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">]));</code>
    
    <code class="n">job</code><code class="o">.</code><code class="na">setMapperClass</code><code class="o">(</code><code class="n">MaxTemperatureMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setReducerClass</code><code class="o">(</code><code class="n">MaxTemperatureReducer</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="n">job</code><code class="o">.</code><code class="na">setOutputKeyClass</code><code class="o">(</code><code class="n">Text</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputValueClass</code><code class="o">(</code><code class="n">IntWritable</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">A <code class="literal">Job</code> object forms the
      specification of the job and gives you control over how the job is run.
      When we run this job on a Hadoop cluster, we will package the code into
      a JAR file (which Hadoop will distribute around the cluster). Rather
      than explicitly specifying the name of the JAR file, we can pass a class
      in the <code class="literal">Job</code>’s <code class="literal">setJarByClass()</code> method, which Hadoop will use
      to locate the relevant JAR file by looking for the JAR file containing
      this class.</p><p class="calibre2">Having constructed a <code class="literal">Job</code>
      object, we specify the input and output paths. An input path is
      specified by calling the static <code class="literal">addInputPath()</code> method on <code class="literal">FileInputFormat</code>, and it can be a single file,
      a directory (in which case, the input forms all the files in that
      directory), or a file pattern. As the name suggests, <code class="literal">addInputPath()</code> can be called more than once to
      use input from multiple paths.</p><p class="calibre2">The output path (of which there is only one) is specified by the
      static <code class="literal">setOutputPath()</code> method on
      <code class="literal">FileOutputFormat</code>. It specifies a
      directory where the output files from the reduce function are written.
      The directory shouldn’t exist before running the job because Hadoop will
      complain and not run the job. This precaution is to prevent data loss
      (it can be very annoying to accidentally overwrite the output of a long
      job with that of another).</p><p class="calibre2">Next, we specify the map and reduce types to use via the
      <code class="literal">setMapperClass()</code> and <code class="literal">setReducerClass()</code>
      methods.</p><p class="calibre2">The <code class="literal">setOutputKeyClass()</code> and
      <code class="literal">setOutputValueClass()</code> methods control
      the output types for the reduce function, and must match what the
      <code class="literal">Reduce</code> class produces. The map output types default
      to the same types, so they do not need to be set if the mapper produces
      the same types as the reducer (as it does in our case). However, if they
      are different, the map output types must be set using the <code class="literal">setMapOutputKeyClass()</code>
      and <code class="literal">setMapOutputValueClass()</code>
      methods.</p><p class="calibre2">The input types are controlled via the input format, which we have
      not explicitly set because we are using the default <code class="literal">TextInputFormat</code>.</p><p class="calibre2">After setting the classes that define the map and reduce
      functions, we are ready to run the job. The <code class="literal">waitForCompletion()</code> method on <code class="literal">Job</code> submits the job and waits for it to
      finish. The single argument to the method is a flag indicating whether
      verbose output is generated. When <code class="literal">true</code>, the job writes information about its
      progress to the console.</p><p class="calibre2">The return value of the <code class="literal">waitForCompletion()</code> method is a Boolean
      indicating success (<code class="literal">true</code>) or failure
      (<code class="literal">false</code>), which we translate into the
      program’s exit code of <code class="literal">0</code> or <code class="literal">1</code>.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">The Java MapReduce API used in this section, and throughout the
        book, is called the “new API”; it replaces the older, functionally
        equivalent API. The differences between the two APIs are explained in
        <a class="ulink" href="#calibre_link-249" title="Appendix&nbsp;D.&nbsp;The Old and New Java MapReduce APIs">Appendix&nbsp;D</a>, along with tips on how to convert between the
        two APIs. You can also find the old API equivalent of the maximum
        temperature application there.</p></div><div class="book" title="A test run"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-499">A test run</h4></div></div></div><p class="calibre2">After writing a MapReduce job, <a class="calibre" id="calibre_link-3654"></a>it’s normal to try it out on a small dataset to flush
        out any immediate problems with the code. First, install Hadoop in
        standalone mode (there are instructions for how to do this in <a class="ulink" href="#calibre_link-28" title="Appendix&nbsp;A.&nbsp;Installing Apache Hadoop">Appendix&nbsp;A</a>). This is the mode in which Hadoop runs using the
        local filesystem with a local job runner. Then, install and compile
        the examples using the instructions on the book’s website.</p><p class="calibre2">Let’s test it on the five-line sample discussed earlier (the
        output has been slightly reformatted to fit the page, and some lines
        have been removed):</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">export HADOOP_CLASSPATH=hadoop-examples.jar</code></strong>
<code class="literal">% </code><strong class="userinput"><code class="calibre9">hadoop MaxTemperature input/ncdc/sample.txt output</code></strong>
14/09/16 09:48:39 WARN util.NativeCodeLoader: Unable to load native-hadoop 
library for your platform... using builtin-java classes where applicable
14/09/16 09:48:40 WARN mapreduce.JobSubmitter: Hadoop command-line option 
parsing not performed. Implement the Tool interface and execute your application 
with ToolRunner to remedy this.
14/09/16 09:48:40 INFO input.FileInputFormat: Total input paths to process : 1
14/09/16 09:48:40 INFO mapreduce.JobSubmitter: number of splits:1
14/09/16 09:48:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: 
job_local26392882_0001
14/09/16 09:48:40 INFO mapreduce.Job: The url to track the job: 
http://localhost:8080/
14/09/16 09:48:40 INFO mapreduce.Job: Running job: job_local26392882_0001
14/09/16 09:48:40 INFO mapred.LocalJobRunner: OutputCommitter set in config null
14/09/16 09:48:40 INFO mapred.LocalJobRunner: OutputCommitter is 
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
14/09/16 09:48:40 INFO mapred.LocalJobRunner: Waiting for map tasks
14/09/16 09:48:40 INFO mapred.LocalJobRunner: Starting task: 
attempt_local26392882_0001_m_000000_0
14/09/16 09:48:40 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null
14/09/16 09:48:40 INFO mapred.LocalJobRunner: 
14/09/16 09:48:40 INFO mapred.Task: Task:attempt_local26392882_0001_m_000000_0 
is done. And is in the process of committing
14/09/16 09:48:40 INFO mapred.LocalJobRunner: map
14/09/16 09:48:40 INFO mapred.Task: Task 'attempt_local26392882_0001_m_000000_0'
 done.
14/09/16 09:48:40 INFO mapred.LocalJobRunner: Finishing task: 
attempt_local26392882_0001_m_000000_0
14/09/16 09:48:40 INFO mapred.LocalJobRunner: map task executor complete.
14/09/16 09:48:40 INFO mapred.LocalJobRunner: Waiting for reduce tasks
14/09/16 09:48:40 INFO mapred.LocalJobRunner: Starting task: 
attempt_local26392882_0001_r_000000_0
14/09/16 09:48:40 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null
14/09/16 09:48:40 INFO mapred.LocalJobRunner: 1 / 1 copied.
14/09/16 09:48:40 INFO mapred.Merger: Merging 1 sorted segments
14/09/16 09:48:40 INFO mapred.Merger: Down to the last merge-pass, with 1 
segments left of total size: 50 bytes
14/09/16 09:48:40 INFO mapred.Merger: Merging 1 sorted segments
14/09/16 09:48:40 INFO mapred.Merger: Down to the last merge-pass, with 1 
segments left of total size: 50 bytes
14/09/16 09:48:40 INFO mapred.LocalJobRunner: 1 / 1 copied.
14/09/16 09:48:40 INFO mapred.Task: Task:attempt_local26392882_0001_r_000000_0 
is done. And is in the process of committing
14/09/16 09:48:40 INFO mapred.LocalJobRunner: 1 / 1 copied.
14/09/16 09:48:40 INFO mapred.Task: Task attempt_local26392882_0001_r_000000_0 
is allowed to commit now
14/09/16 09:48:40 INFO output.FileOutputCommitter: Saved output of task 
'attempt...local26392882_0001_r_000000_0' to file:/Users/tom/book-workspace/
hadoop-book/output/_temporary/0/task_local26392882_0001_r_000000
14/09/16 09:48:40 INFO mapred.LocalJobRunner: reduce &gt; reduce
14/09/16 09:48:40 INFO mapred.Task: Task 'attempt_local26392882_0001_r_000000_0'
 done.
14/09/16 09:48:40 INFO mapred.LocalJobRunner: Finishing task: 
attempt_local26392882_0001_r_000000_0
14/09/16 09:48:40 INFO mapred.LocalJobRunner: reduce task executor complete.
14/09/16 09:48:41 INFO mapreduce.Job: Job job_local26392882_0001 running in uber
 mode : false
14/09/16 09:48:41 INFO mapreduce.Job:  map 100% reduce 100%
14/09/16 09:48:41 INFO mapreduce.Job: Job job_local26392882_0001 completed 
successfully
14/09/16 09:48:41 INFO mapreduce.Job: Counters: 30
    File System Counters
        FILE: Number of bytes read=377168
        FILE: Number of bytes written=828464
        FILE: Number of read operations=0
        FILE: Number of large read operations=0
        FILE: Number of write operations=0
    Map-Reduce Framework
        Map input records=5
        Map output records=5
        Map output bytes=45
        Map output materialized bytes=61
        Input split bytes=129
        Combine input records=0
        Combine output records=0
        Reduce input groups=2
        Reduce shuffle bytes=61
        Reduce input records=5
        Reduce output records=2
        Spilled Records=10
        Shuffled Maps =1
        Failed Shuffles=0
        Merged Map outputs=1
        GC time elapsed (ms)=39
        Total committed heap usage (bytes)=226754560
    File Input Format Counters 
        Bytes Read=529
    File Output Format Counters 
        Bytes Written=29</pre><p class="calibre2">When the <code class="literal">hadoop</code> command is
        <a class="calibre" id="calibre_link-1845"></a><a class="calibre" id="calibre_link-2190"></a><a class="calibre" id="calibre_link-2294"></a>invoked with a classname as the first argument, it
        launches a Java virtual machine (JVM) to run the class. The <code class="literal">hadoop</code> command adds the Hadoop libraries
        (and their dependencies) to the classpath and picks up the Hadoop
        configuration, too. To add the application classes to the classpath,
        we’ve defined an environment variable <a class="calibre" id="calibre_link-1571"></a>called <code class="literal">HADOOP_CLASSPATH</code>, which the
        <span class="calibre"><em class="calibre10">hadoop</em></span> script picks up.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">When running in local (standalone) mode, the programs in this
          book all assume that you have set the <code class="literal">HADOOP_CLASSPATH</code> in this way. The commands
          should be run from the directory that the example code is installed in.</p></div><p class="calibre2">The output from running the job provides some useful
        information. For example, we can
        see that the job was given an ID of <code class="literal">job_local26392882_0001</code>, and it ran one map task and one reduce task (with
        the following IDs: <code class="literal">attempt_local26392882_0001_m_000000_0</code> and
        <code class="literal">attempt_local26392882_0001_r_000000_0</code>).
        Knowing the job and task IDs can be very useful when debugging
        MapReduce jobs.</p><p class="calibre2">The last section of the output, titled “Counters,” shows the
        statistics that Hadoop generates for each job it runs. These are very
        useful for checking whether the amount of data processed is what you
        expected. For example, we can follow the number of records that went
        through the system: five map input records produced five map output
        records (since the mapper emitted one output record for each valid
        input record), then five reduce input records in two groups (one for
        each unique key) produced two reduce output records.</p><p class="calibre2">The output was written to the <em class="calibre10">output</em> directory, which contains one output
        file per reducer. The job had a single reducer, so we find a single
        file, named <em class="calibre10">part-r-00000</em>:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">cat output/part-r-00000</code></strong>
1949	111
1950	22</pre><p class="calibre2">This result is the same as when we went through it by hand
        earlier. We interpret this as saying that the maximum temperature
        recorded in 1949 was 11.1°C, and in 1950 it <a class="calibre" id="calibre_link-1346"></a><a class="calibre" id="calibre_link-2181"></a><a class="calibre" id="calibre_link-3655"></a>was 2.2°C.</p></div></div></div><div class="book" title="Scaling Out"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-173">Scaling Out</h2></div></div></div><p class="calibre2">You’ve seen how MapReduce works for <a class="calibre" id="calibre_link-1354"></a><a class="calibre" id="calibre_link-3265"></a>small inputs; now it’s time to take a bird’s-eye view of the
    system and look at the data flow for large inputs. For simplicity, the
    examples so far have used files on
    the local filesystem. However, to scale out, we need to store the
    <a class="calibre" id="calibre_link-1958"></a>data in a distributed filesystem (typically HDFS, which
    you’ll learn about in the next chapter). This allows Hadoop to move the
    MapReduce computation to each machine hosting a part of the data, using
    Hadoop’s resource management system, called <a class="calibre" id="calibre_link-3840"></a>YARN (see <a class="ulink" href="#calibre_link-318" title="Chapter&nbsp;4.&nbsp;YARN">Chapter&nbsp;4</a>). Let’s see how this
    works.</p><div class="book" title="Data Flow"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-3995">Data Flow</h3></div></div></div><p class="calibre2">First, some terminology. <a class="calibre" id="calibre_link-3268"></a>A MapReduce <em class="calibre10">job</em> is a <a class="calibre" id="calibre_link-3788"></a>unit of work that the client wants to be performed: it
      consists of the input data, the MapReduce program, and configuration
      information. Hadoop runs the job by dividing it into
      <em class="calibre10">tasks</em>, of which there are two <a class="calibre" id="calibre_link-2389"></a><a class="calibre" id="calibre_link-3178"></a>types: <em class="calibre10">map tasks</em> and
      <em class="calibre10">reduce tasks</em>. The tasks are scheduled using YARN
      and run on nodes in the cluster. If a task fails, it will be
      automatically rescheduled to run on a different node.</p><p class="calibre2">Hadoop divides the input to a MapReduce job into fixed-size pieces
      <a class="calibre" id="calibre_link-2110"></a><a class="calibre" id="calibre_link-3996"></a><a class="calibre" id="calibre_link-2103"></a>called <em class="calibre10">input splits</em>, or just
      <em class="calibre10">splits</em>. Hadoop creates one map task for each
      split, which runs the user-defined map function for each
      <em class="calibre10">record</em> in the split.</p><p class="calibre2">Having many splits means the time taken to process each split is
      small compared to the time to process the whole input. So if we are
      processing the splits in parallel, the processing is better load
      balanced when the splits are small, since a faster machine will be able
      to process proportionally more splits over the course of the job than a
      slower machine. Even if the machines are identical, failed processes or
      other jobs running concurrently make load balancing desirable, and the
      quality of the load balancing increases as the splits become more fine
      grained.</p><p class="calibre2">On the other hand, if splits are too small, the overhead of
      managing the splits and map task creation begins to dominate the total
      job execution time. For most jobs, a good split size tends to be the
      size of an HDFS block, which is 128 MB by default, although this can be
      changed for the cluster (for all newly created files) or specified when
      each file is created.</p><p class="calibre2">Hadoop does its best to run the map task on a node where the input
      data resides in HDFS, because it doesn’t use valuable cluster bandwidth.
      This is called the <em class="calibre10">data locality
      optimization</em>.<a class="calibre" id="calibre_link-1342"></a> Sometimes, however, all the nodes hosting the HDFS block
      replicas for a map task’s input split are running other map tasks, so
      the job scheduler will look for a free map slot on a node in the same
      rack as one of the blocks. Very occasionally even this is not possible,
      so an off-rack node is used, which results in an inter-rack network
      transfer. The three possibilities are illustrated in <a class="ulink" href="#calibre_link-187" title="Figure&nbsp;2-2.&nbsp;Data-local (a), rack-local (b), and off-rack (c) map tasks">Figure&nbsp;2-2</a>.</p><p class="calibre2">It should now be clear why the optimal split size is the same as
      the block size: it is the largest size of input that can be guaranteed to be stored on a single
      node. If the split spanned two blocks, it would be unlikely that any
      HDFS node stored both blocks, so some of the split would have to be
      transferred across the network to the node running the map task, which
      is clearly less efficient than running the whole map task using local
      data.</p><p class="calibre2">Map tasks write their output to the local disk, not to HDFS. Why
      is this? Map output is intermediate output: it’s processed by reduce
      tasks to produce the final output, and once the job is complete, the map
      output can be thrown away. So, storing it in HDFS with replication would
      be overkill. If the node running the map task fails before the map
      output has been consumed by the reduce task, then Hadoop will
      automatically rerun the map task on another node to re-create the map
      output.</p><div class="book"><div class="figure"><a id="calibre_link-187" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-3997" class="calibre"></a><img alt="Data-local (a), rack-local (b), and off-rack (c) map tasks" src="images/000028.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;2-2.&nbsp;Data-local (a), rack-local (b), and off-rack (c) map
        tasks</div></div></div><p class="calibre2">Reduce tasks don’t have the advantage of data locality; the input
      to a single reduce task is normally the output from all mappers. In the
      present example, we have a single reduce task that is fed by all of the
      map tasks. Therefore, the sorted map outputs have to be transferred
      across the network to the node where the reduce task is running, where
      they are merged and then passed to the user-defined reduce function. The
      output of the reduce is normally stored in HDFS for reliability. As
      explained in <a class="ulink" href="#calibre_link-161" title="Chapter&nbsp;3.&nbsp;The Hadoop Distributed Filesystem">Chapter&nbsp;3</a>, for each HDFS block of the
      reduce output, the first replica is stored on the local node, with other
      replicas being stored on off-rack nodes for reliability. Thus, writing
      the reduce output does consume network bandwidth, but only as much as a
      normal HDFS write pipeline consumes.</p><p class="calibre2">The whole data flow with a single reduce task is illustrated in
      <a class="ulink" href="#calibre_link-470" title="Figure&nbsp;2-3.&nbsp;MapReduce data flow with a single reduce task">Figure&nbsp;2-3</a>. The dotted boxes
      indicate nodes, the dotted arrows show data transfers on a node, and the
      solid arrows show data transfers between nodes.</p><div class="book"><div class="figure"><a id="calibre_link-470" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-3998" class="calibre"></a><img alt="MapReduce data flow with a single reduce task" src="images/000037.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;2-3.&nbsp;MapReduce data flow with a single reduce task</div></div></div><p class="calibre2">The number of reduce tasks is not governed by the size of the
      input, but instead is specified independently. In <a class="ulink" href="#calibre_link-471" title="The Default MapReduce Job">The Default MapReduce Job</a>, you will see how to choose the
      number of reduce tasks for a given job.</p><p class="calibre2">When there are multiple reducers, the map tasks
      <em class="calibre10">partition</em> their output, each creating one
      partition for each reduce task. There can be many keys (and their
      associated values) in each partition, but the records for any given key
      are all in a single partition. The partitioning can be controlled by a
      user-defined partitioning function, but normally the default
      partitioner—which buckets keys using a hash function—works very
      well.</p><p class="calibre2">The data flow for the general case of multiple reduce tasks is
      illustrated in <a class="ulink" href="#calibre_link-472" title="Figure&nbsp;2-4.&nbsp;MapReduce data flow with multiple reduce tasks">Figure&nbsp;2-4</a>. This
      diagram makes it clear why the data flow between map and reduce tasks is
      colloquially known as “the shuffle,” as each reduce task is fed by many
      map tasks. The shuffle is more complicated than this diagram suggests,
      and tuning it can have a big impact on job execution time, as you will
      see in <a class="ulink" href="#calibre_link-46" title="Shuffle and Sort">Shuffle and Sort</a>.</p><div class="book"><div class="figure"><a id="calibre_link-472" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-3999" class="calibre"></a><img alt="MapReduce data flow with multiple reduce tasks" src="images/000046.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;2-4.&nbsp;MapReduce data flow with multiple reduce tasks</div></div></div><p class="calibre2">Finally, it’s also possible to have zero reduce tasks. This can be
      appropriate when you don’t need the shuffle because the processing can
      be carried out entirely in parallel (a few examples are discussed in
      <a class="ulink" href="#calibre_link-473" title="NLineInputFormat">NLineInputFormat</a>). In this case, the only off-node
      data transfer is when the map tasks write to <a class="calibre" id="calibre_link-3269"></a><a class="calibre" id="calibre_link-1959"></a>HDFS (see <a class="ulink" href="#calibre_link-474" title="Figure&nbsp;2-5.&nbsp;MapReduce data flow with no reduce tasks">Figure&nbsp;2-5</a>).</p></div><div class="book" title="Combiner Functions"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-539">Combiner Functions</h3></div></div></div><p class="calibre2">Many MapReduce jobs <a class="calibre" id="calibre_link-3266"></a>are limited by the bandwidth available on the cluster, so
      it pays to minimize the data transferred between map and reduce tasks.
      Hadoop allows the user to specify a <em class="calibre10">combiner
      function</em> to be <a class="calibre" id="calibre_link-1179"></a>run on the map output, and the combiner function’s output
      forms the input to the reduce function. Because the combiner function is
      an optimization, Hadoop does not provide a guarantee of how many times
      it will call it for a particular map output record, if at all. In other
      words, calling the combiner function zero, one, or many times should
      produce the same output from the reducer.</p><div class="book"><div class="figure"><a id="calibre_link-474" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4000" class="calibre"></a><img alt="MapReduce data flow with no reduce tasks" src="images/000054.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;2-5.&nbsp;MapReduce data flow with no reduce tasks</div></div></div><p class="calibre2">The contract for the combiner function constrains the type of
      function that may be used. This is best illustrated with an example.
      Suppose that for the <span class="calibre">maximum</span> temperature example,
      readings for the year 1950 were processed by two maps (because they were
      in different splits). Imagine the first map produced the output:</p><a id="calibre_link-4001" class="calibre"></a><pre class="screen1">(<code class="literal">1950</code>, 0)
(<code class="literal">1950</code>, 20)
(<code class="literal">1950</code>, 10)</pre><p class="calibre2">and the second produced:</p><a id="calibre_link-4002" class="calibre"></a><pre class="screen1">(<code class="literal">1950</code>, 25)
(<code class="literal">1950</code>, 15)</pre><p class="calibre2">The reduce function would be called with a list of all the
      values:</p><a id="calibre_link-4003" class="calibre"></a><pre class="screen1">(<code class="literal">1950</code>, [0, 20, 10, 25, 15])</pre><p class="calibre2">with output:</p><a id="calibre_link-4004" class="calibre"></a><pre class="screen1">(<code class="literal">1950</code>, 25)</pre><p class="calibre2">since 25 is the maximum value in the list. We could use a combiner
      function that, just like the reduce function, finds the maximum
      temperature for each map output. The reduce function would then be
      called with:</p><a id="calibre_link-4005" class="calibre"></a><pre class="screen1">(<code class="literal">1950</code>, [20, 25])</pre><p class="calibre2">and would produce the same output as before. More succinctly, we
      may express the function calls on the temperature values in this case as
      follows:</p><a id="calibre_link-4006" class="calibre"></a><pre class="screen1"><span class="calibre">max</span>(0, 20, 10, 25, 15) = <span class="calibre">max</span>(<span class="calibre">max</span>(0, 20, 10), <span class="calibre">max</span>(25, 15)) = <span class="calibre">max</span>(20, 25) = 25</pre><p class="calibre2">Not all functions possess this property.<sup class="calibre6">[<a class="firstname" href="#calibre_link-475" id="calibre_link-485">20</a>]</sup> For example, if we were calculating mean temperatures, we
      couldn’t use the mean as our combiner function, because:</p><a id="calibre_link-4007" class="calibre"></a><pre class="screen1"><span class="calibre">mean</span>(0, 20, 10, 25, 15) = 14</pre><p class="calibre2">but:</p><a id="calibre_link-4008" class="calibre"></a><pre class="screen1"><span class="calibre">mean</span>(<span class="calibre">mean</span>(0, 20, 10), <span class="calibre">mean</span>(25, 15)) = <span class="calibre">mean</span>(10, 20) = 15</pre><p class="calibre2">The combiner function doesn’t replace the reduce function. (How
      could it? The reduce function is still needed to process records with
      the same key from different maps.) But it can help cut down the amount
      of data shuffled between the mappers and the reducers, and for this
      reason alone it is always worth considering whether you can use a
      combiner function in your MapReduce job.</p><div class="book" title="Specifying a combiner function"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4009">Specifying a combiner function</h4></div></div></div><p class="calibre2">Going back to the Java MapReduce program, the combiner function
        is defined using the <code class="literal">Reducer</code> class,
        and for this application, it is the same implementation as the reduce
        function in <code class="literal">MaxTemperatureReducer</code>.
        The only change we need to make is to set the <a class="calibre" id="calibre_link-3267"></a><a class="calibre" id="calibre_link-1180"></a><a class="calibre" id="calibre_link-3179"></a><a class="calibre" id="calibre_link-2390"></a>combiner class on the <code class="literal">Job</code> (see <a class="ulink" href="#calibre_link-476" title="Example&nbsp;2-6.&nbsp;Application to find the maximum temperature, using a combiner function for efficiency">Example&nbsp;2-6</a>).</p><div class="example"><a id="calibre_link-476" class="calibre"></a><div class="example-title">Example&nbsp;2-6.&nbsp;Application to find the maximum temperature, using a combiner
          function for efficiency</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">MaxTemperatureWithCombiner</code> <code class="o">{</code>

  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="na">length</code> <code class="o">!=</code> <code class="mi">2</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="sb">"Usage: MaxTemperatureWithCombiner &lt;input path&gt; "</code> <code class="o">+</code>
          <code class="sb">"&lt;output path&gt;"</code><code class="o">);</code>
      <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(-</code><code class="mi">1</code><code class="o">);</code>
    <code class="o">}</code>
    
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Job</code><code class="o">();</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setJarByClass</code><code class="o">(</code><code class="n">MaxTemperatureWithCombiner</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setJobName</code><code class="o">(</code><code class="sb">"Max temperature"</code><code class="o">);</code>

    <code class="n">FileInputFormat</code><code class="o">.</code><code class="na">addInputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]));</code>
    <code class="n">FileOutputFormat</code><code class="o">.</code><code class="na">setOutputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">]));</code>
    
    <code class="n">job</code><code class="o">.</code><code class="na">setMapperClass</code><code class="o">(</code><code class="n">MaxTemperatureMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <span class="calibre24"><strong class="calibre9"><code class="n1">job</code><code class="o1">.</code><code class="na1">setCombinerClass</code><code class="o1">(</code><code class="n1">MaxTemperatureReducer</code><code class="o1">.</code><code class="na1">class</code><code class="o1">)</code></strong></span><code class="o">;</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setReducerClass</code><code class="o">(</code><code class="n">MaxTemperatureReducer</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="n">job</code><code class="o">.</code><code class="na">setOutputKeyClass</code><code class="o">(</code><code class="n">Text</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputValueClass</code><code class="o">(</code><code class="n">IntWritable</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div></div></div><div class="book" title="Running a Distributed MapReduce Job"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4010">Running a Distributed MapReduce Job</h3></div></div></div><p class="calibre2">The same <a class="calibre" id="calibre_link-3270"></a>program will run, without alteration, on a full dataset.
      This is the point of MapReduce: it scales to the size of your data and
      the size of your hardware. Here’s one data point: on a 10-node EC2
      cluster running High-CPU Extra Large instances, the program took six
      minutes to run.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-477" id="calibre_link-486">21</a>]</sup></p><p class="calibre2">We’ll go through the mechanics of running programs on a <a class="calibre" id="calibre_link-2518"></a><a class="calibre" id="calibre_link-1355"></a>cluster in <a class="ulink" href="#calibre_link-79" title="Chapter&nbsp;6.&nbsp;Developing a MapReduce Application">Chapter&nbsp;6</a>.</p></div></div><div class="book" title="Hadoop Streaming"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-550">Hadoop Streaming</h2></div></div></div><p class="calibre2">Hadoop provides an API to MapReduce that <a class="calibre" id="calibre_link-2476"></a><a class="calibre" id="calibre_link-1848"></a>allows you to write your map and <a class="calibre" id="calibre_link-2392"></a><a class="calibre" id="calibre_link-3181"></a>reduce functions in languages other than Java.
    <em class="calibre10">Hadoop Streaming</em> uses Unix standard streams as the
    interface between Hadoop and your program, so you can use any language
    that can read standard input and write to standard output to write your
    MapReduce program.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-478" id="calibre_link-487">22</a>]</sup></p><p class="calibre2">Streaming is naturally suited for text processing. Map input data is
    passed over standard input to your map function, which processes it line
    by line and writes lines to standard output. A map output key-value pair
    is written as a single tab-delimited line. Input to the reduce function is
    in the same format—a tab-separated key-value pair—passed over standard
    input. The reduce function reads lines from standard input, which the
    framework guarantees are sorted by key, and writes its results to standard
    output.</p><p class="calibre2">Let’s illustrate this by rewriting our MapReduce program for finding
    maximum temperatures by year in Streaming.</p><div class="book" title="Ruby"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4011">Ruby</h3></div></div></div><p class="calibre2">The map function can be <a class="calibre" id="calibre_link-1851"></a><a class="calibre" id="calibre_link-3251"></a>expressed in Ruby as shown in <a class="ulink" href="#calibre_link-479" title="Example&nbsp;2-7.&nbsp;Map function for maximum temperature in Ruby">Example&nbsp;2-7</a>.</p><div class="example"><a id="calibre_link-479" class="calibre"></a><div class="example-title">Example&nbsp;2-7.&nbsp;Map function for maximum temperature in Ruby</div><div class="book"><pre class="screen"><code class="c2">#!/usr/bin/env ruby</code>

<code class="no">STDIN</code><code class="o">.</code><code class="n">each_line</code> <code class="k">do</code> <code class="o">|</code><code class="n">line</code><code class="o">|</code>
  <code class="n">val</code> <code class="o">=</code> <code class="n">line</code>
  <code class="n">year</code><code class="p">,</code> <code class="n">temp</code><code class="p">,</code> <code class="n">q</code> <code class="o">=</code> <code class="n">val</code><code class="o">[</code><code class="mi">15</code><code class="p">,</code><code class="mi">4</code><code class="o">]</code><code class="p">,</code> <code class="n">val</code><code class="o">[</code><code class="mi">87</code><code class="p">,</code><code class="mi">5</code><code class="o">]</code><code class="p">,</code> <code class="n">val</code><code class="o">[</code><code class="mi">92</code><code class="p">,</code><code class="mi">1</code><code class="o">]</code>
  <code class="nb">puts</code> <code class="sb">"</code><code class="si">#{</code><code class="n">year</code><code class="si">}</code><code class="se">\t</code><code class="si">#{</code><code class="n">temp</code><code class="si">}</code><code class="sb">"</code> <code class="k">if</code> <code class="p">(</code><code class="n">temp</code> <code class="o">!=</code> <code class="sb">"+9999"</code> <code class="o">&amp;&amp;</code> <code class="n">q</code> <code class="o">=~</code> <code class="sr">/[01459]/</code><code class="p">)</code>
<code class="k">end</code></pre></div></div><p class="calibre2">The program iterates over lines from standard input by executing a
      block for each line from <code class="literal">STDIN</code> (a
      global constant of type <code class="literal">IO</code>). The
      block pulls out the relevant fields from each input line and, if the
      temperature is valid, writes the year and the temperature separated by a
      tab character, <code class="literal">\t</code>, to standard output
      (using <code class="literal">puts</code>).</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">It’s worth drawing out a design difference between Streaming and
        the <a class="calibre" id="calibre_link-2163"></a>Java MapReduce API. The Java API is geared toward
        processing your map function one record at a time. The framework calls
        the <code class="literal">map(</code><code class="literal">)</code> method on your <code class="literal">Mapper</code> for each record in the input, whereas
        with Streaming the map program can decide how to process the input—for
        example, it could easily read and process multiple lines at a time
        since it’s in control of the reading. The user’s Java map
        implementation is “pushed” records, but it’s still possible to
        consider multiple lines at a time by accumulating previous lines in an
        instance variable in the <code class="literal">Mapper</code>.<sup class="calibre5">[<a class="firstname" type="noteref" href="#calibre_link-480" id="calibre_link-488">23</a>]</sup> In this case, you need to implement the
        <code class="literal">cleanup()</code> method so that you know when the
        last record has been read, so you can finish processing the last group
        of lines.</p></div><p class="calibre2">Because the script just operates on standard input and output,
      it’s trivial to test the script without using Hadoop, simply by using
      Unix pipes:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">cat input/ncdc/sample.txt | ch02-mr-intro/src/main/ruby/max_temperature_map.rb</code></strong>
1950    +0000
1950    +0022
1950    -0011
1949    +0111
1949    +0078</pre><p class="calibre2">The reduce function shown in <a class="ulink" href="#calibre_link-481" title="Example&nbsp;2-8.&nbsp;Reduce function for maximum temperature in Ruby">Example&nbsp;2-8</a> is a little more complex.</p><div class="example"><a id="calibre_link-481" class="calibre"></a><div class="example-title">Example&nbsp;2-8.&nbsp;Reduce function for maximum temperature in Ruby</div><div class="book"><pre class="screen"><code class="c2">#!/usr/bin/env ruby</code>

<code class="n">last_key</code><code class="p">,</code> <code class="n">max_val</code> <code class="o">=</code> <code class="kp">nil</code><code class="p">,</code> <code class="o">-</code><code class="mi">1000000</code>
<code class="no">STDIN</code><code class="o">.</code><code class="n">each_line</code> <code class="k">do</code> <code class="o">|</code><code class="n">line</code><code class="o">|</code>
  <code class="n">key</code><code class="p">,</code> <code class="n">val</code> <code class="o">=</code> <code class="n">line</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="sb">"</code><code class="se">\t</code><code class="sb">"</code><code class="p">)</code>
  <code class="k">if</code> <code class="n">last_key</code> <code class="o">&amp;&amp;</code> <code class="n">last_key</code> <code class="o">!=</code> <code class="n">key</code>
    <code class="nb">puts</code> <code class="sb">"</code><code class="si">#{</code><code class="n">last_key</code><code class="si">}</code><code class="se">\t</code><code class="si">#{</code><code class="n">max_val</code><code class="si">}</code><code class="sb">"</code>
    <code class="n">last_key</code><code class="p">,</code> <code class="n">max_val</code> <code class="o">=</code> <code class="n">key</code><code class="p">,</code> <code class="n">val</code><code class="o">.</code><code class="n">to_i</code>
  <code class="k">else</code>
    <code class="n">last_key</code><code class="p">,</code> <code class="n">max_val</code> <code class="o">=</code> <code class="n">key</code><code class="p">,</code> <code class="o">[</code><code class="n">max_val</code><code class="p">,</code> <code class="n">val</code><code class="o">.</code><code class="n">to_i</code><code class="o">].</code><code class="n">max</code>
  <code class="k">end</code>
<code class="k">end</code>
<code class="nb">puts</code> <code class="sb">"</code><code class="si">#{</code><code class="n">last_key</code><code class="si">}</code><code class="se">\t</code><code class="si">#{</code><code class="n">max_val</code><code class="si">}</code><code class="sb">"</code> <code class="k">if</code> <code class="n">last_key</code></pre></div></div><p class="calibre2">Again, the program iterates over lines from standard input, but
      this time we have to store some state as we process each key group. In
      this case, the keys are the years, and we store the last key seen and
      the maximum temperature seen so far for that key. The MapReduce
      framework ensures that the keys are ordered, so we know that if a key is
      different from the previous one, we have moved into a new key group. In
      contrast to the Java API, where you are provided an iterator over each
      key group, in Streaming you have to find key group boundaries in your
      program.</p><p class="calibre2">For each line, we pull out the key and value. Then, if we’ve just
      finished a group (<code class="literal">last_key &amp;&amp; last_key
      != key</code>), we write the key and the maximum temperature for that
      group, separated by a tab character, before resetting the maximum
      temperature for the new key. If we haven’t just finished a group, we
      just update the maximum temperature for the current key.</p><p class="calibre2">The last line of the program ensures that a line is written for
      the last key group in the input.</p><p class="calibre2">We can now simulate the whole MapReduce pipeline with a Unix
      pipeline (which is equivalent to the Unix pipeline shown in <a class="ulink" href="#calibre_link-466" title="Figure&nbsp;2-1.&nbsp;MapReduce logical data flow">Figure&nbsp;2-1</a>):</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">cat input/ncdc/sample.txt | \</code></strong>
<strong class="userinput"><code class="calibre9">  ch02-mr-intro/src/main/ruby/max_temperature_map.rb | \</code></strong>
<strong class="userinput"><code class="calibre9">  sort | ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb</code></strong>
1949	111
1950	22</pre><p class="calibre2">The output is the same as that of the Java program, so the next
      step is to run it using Hadoop itself.</p><p class="calibre2">The <code class="literal">hadoop</code> command <a class="calibre" id="calibre_link-1844"></a>doesn’t support a Streaming option; instead, you specify
      the Streaming JAR file along with the <code class="literal">jar</code> option. Options to the Streaming program
      specify the input and output paths and the map and reduce scripts. This
      is what it looks like:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \</code></strong>
<strong class="userinput"><code class="calibre9">  -input input/ncdc/sample.txt \</code></strong>
<strong class="userinput"><code class="calibre9">  -output output \</code></strong>
<strong class="userinput"><code class="calibre9">  -mapper ch02-mr-intro/src/main/ruby/max_temperature_map.rb \</code></strong>
<strong class="userinput"><code class="calibre9">  -reducer ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb</code></strong></pre><p class="calibre2">When running on a large dataset on a cluster, we should use the
      <code class="literal">-combiner</code> option
      to set the combiner:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \</code></strong>
<strong class="userinput"><code class="calibre9">  -files ch02-mr-intro/src/main/ruby/max_temperature_map.rb,\</code></strong>
<strong class="userinput"><code class="calibre9">ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb \</code></strong>
<strong class="userinput"><code class="calibre9">  -input input/ncdc/all \</code></strong>
<strong class="userinput"><code class="calibre9">  -output output \</code></strong>
<strong class="userinput"><code class="calibre9">  -mapper ch02-mr-intro/src/main/ruby/max_temperature_map.rb \</code></strong>
<strong class="userinput"><code class="calibre9">  -combiner ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb \</code></strong>
<strong class="userinput"><code class="calibre9">  -reducer ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb</code></strong></pre><p class="calibre2">Note also the use of <code class="literal">-files</code>,
      which we use when running Streaming programs on the cluster to ship the
      scripts to the <a class="calibre" id="calibre_link-3252"></a><a class="calibre" id="calibre_link-1852"></a>cluster.</p></div><div class="book" title="Python"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4012">Python</h3></div></div></div><p class="calibre2">Streaming <a class="calibre" id="calibre_link-1850"></a><a class="calibre" id="calibre_link-3101"></a>supports any programming language that can read from
      standard input and write to standard output, so for readers more
      familiar with Python, here’s the same example again.<sup class="calibre6">[<a class="firstname" href="#calibre_link-482" id="calibre_link-489">24</a>]</sup> The map script is in <a class="ulink" href="#calibre_link-483" title="Example&nbsp;2-9.&nbsp;Map function for maximum temperature in Python">Example&nbsp;2-9</a>, and the reduce script is in <a class="ulink" href="#calibre_link-484" title="Example&nbsp;2-10.&nbsp;Reduce function for maximum temperature in Python">Example&nbsp;2-10</a>.</p><div class="example"><a id="calibre_link-483" class="calibre"></a><div class="example-title">Example&nbsp;2-9.&nbsp;Map function for maximum temperature in Python</div><div class="book"><pre class="screen"><code class="c1">#!/usr/bin/env python</code>

<code class="k">import</code> <code class="nn">re</code>
<code class="k">import</code> <code class="nn">sys</code>

<code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">sys</code><code class="o">.</code><code class="n">stdin</code><code class="p">:</code>
  <code class="n">val</code> <code class="o">=</code> <code class="n">line</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code>
  <code class="p">(</code><code class="n">year</code><code class="p">,</code> <code class="n">temp</code><code class="p">,</code> <code class="n">q</code><code class="p">)</code> <code class="o">=</code> <code class="p">(</code><code class="n">val</code><code class="p">[</code><code class="mi">15</code><code class="p">:</code><code class="mi">19</code><code class="p">],</code> <code class="n">val</code><code class="p">[</code><code class="mi">87</code><code class="p">:</code><code class="mi">92</code><code class="p">],</code> <code class="n">val</code><code class="p">[</code><code class="mi">92</code><code class="p">:</code><code class="mi">93</code><code class="p">])</code>
  <code class="k">if</code> <code class="p">(</code><code class="n">temp</code> <code class="o">!=</code> <code class="sb">"+9999"</code> <code class="ow">and</code> <code class="n">re</code><code class="o">.</code><code class="n">match</code><code class="p">(</code><code class="sb">"[01459]"</code><code class="p">,</code> <code class="n">q</code><code class="p">)):</code>
    <code class="k">print</code> <code class="sb">"</code><code class="si">%s</code><code class="se">\t</code><code class="si">%s</code><code class="sb">"</code> <code class="o">%</code> <code class="p">(</code><code class="n">year</code><code class="p">,</code> <code class="n">temp</code><code class="p">)</code></pre></div></div><div class="example"><a id="calibre_link-484" class="calibre"></a><div class="example-title">Example&nbsp;2-10.&nbsp;Reduce function for maximum temperature in Python</div><div class="book"><pre class="screen"><code class="c1">#!/usr/bin/env python</code>

<code class="k">import</code> <code class="nn">sys</code>

<code class="p">(</code><code class="n">last_key</code><code class="p">,</code> <code class="n">max_val</code><code class="p">)</code> <code class="o">=</code> <code class="p">(</code><code class="nb">None</code><code class="p">,</code> <code class="o">-</code><code class="n">sys</code><code class="o">.</code><code class="n">maxint</code><code class="p">)</code>
<code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">sys</code><code class="o">.</code><code class="n">stdin</code><code class="p">:</code>
  <code class="p">(</code><code class="n">key</code><code class="p">,</code> <code class="n">val</code><code class="p">)</code> <code class="o">=</code> <code class="n">line</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="sb">"</code><code class="se">\t</code><code class="sb">"</code><code class="p">)</code>
  <code class="k">if</code> <code class="n">last_key</code> <code class="ow">and</code> <code class="n">last_key</code> <code class="o">!=</code> <code class="n">key</code><code class="p">:</code>
    <code class="k">print</code> <code class="sb">"</code><code class="si">%s</code><code class="se">\t</code><code class="si">%s</code><code class="sb">"</code> <code class="o">%</code> <code class="p">(</code><code class="n">last_key</code><code class="p">,</code> <code class="n">max_val</code><code class="p">)</code>
    <code class="p">(</code><code class="n">last_key</code><code class="p">,</code> <code class="n">max_val</code><code class="p">)</code> <code class="o">=</code> <code class="p">(</code><code class="n">key</code><code class="p">,</code> <code class="nb">int</code><code class="p">(</code><code class="n">val</code><code class="p">))</code>
  <code class="k">else</code><code class="p">:</code>
    <code class="p">(</code><code class="n">last_key</code><code class="p">,</code> <code class="n">max_val</code><code class="p">)</code> <code class="o">=</code> <code class="p">(</code><code class="n">key</code><code class="p">,</code> <code class="nb">max</code><code class="p">(</code><code class="n">max_val</code><code class="p">,</code> <code class="nb">int</code><code class="p">(</code><code class="n">val</code><code class="p">)))</code>

<code class="k">if</code> <code class="n">last_key</code><code class="p">:</code>
  <code class="k">print</code> <code class="sb">"</code><code class="si">%s</code><code class="se">\t</code><code class="si">%s</code><code class="sb">"</code> <code class="o">%</code> <code class="p">(</code><code class="n">last_key</code><code class="p">,</code> <code class="n">max_val</code><code class="p">)</code></pre></div></div><p class="calibre2">We can test the programs and run the job in the same way we did in
      Ruby. For example, to run a <a class="calibre" id="calibre_link-2477"></a>test:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">cat input/ncdc/sample.txt | \</code></strong>
<strong class="userinput"><code class="calibre9">  ch02-mr-intro/src/main/python/max_temperature_map.py | \</code></strong>
<strong class="userinput"><code class="calibre9">  sort | ch02-mr-intro/src/main/python/max_temperature_reduce.py</code></strong>
1949    111
1950    22</pre></div></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" id="calibre_link-475"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-485">20</a>] </sup>Functions with this property are called
          <em class="calibre10">commutative</em> and
          <em class="calibre10">associative</em>. They are also sometimes referred
          to as <em class="calibre10">distributive</em>, such as by Jim Gray et
          al.’s <a class="ulink" href="http://bit.ly/data_cube" target="_top">“Data Cube: A Relational
          Aggregation Operator Generalizing Group-By, Cross-Tab, and
          Sub-Totals,”</a> February1995.</p></div><div class="footnote" type="footnote" id="calibre_link-477"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-486">21</a>] </sup>This is a factor of seven faster than the serial run on
          <a class="calibre" id="calibre_link-970"></a>one machine using <em class="calibre10">awk</em>. The main reason it wasn’t
          proportionately faster is because the input data wasn’t evenly
          partitioned. For convenience, the input files were gzipped by year,
          resulting in large files for later years in the dataset, when the
          number of weather records was much higher.</p></div><div class="footnote" type="footnote" id="calibre_link-478"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-487">22</a>] </sup>Hadoop Pipes is an alternative to Streaming for C++ programmers.
        It uses sockets to communicate with the process running the C++ map or
        reduce function.</p></div><div class="footnote" type="footnote" id="calibre_link-480"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-488">23</a>] </sup>Alternatively, you could use “pull”-style processing in the
            new MapReduce API; see <a class="ulink" href="#calibre_link-249" title="Appendix&nbsp;D.&nbsp;The Old and New Java MapReduce APIs">Appendix&nbsp;D</a>.</p></div><div class="footnote" id="calibre_link-482"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-489">24</a>] </sup>As an alternative to Streaming, Python programmers should
          <a class="calibre" id="calibre_link-1549"></a>consider <a class="ulink" href="http://klbostee.github.io/dumbo/" target="_top">Dumbo</a>, which makes
          the Streaming MapReduce interface more Pythonic and easier to
          use.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-161"><section type="chapter" id="calibre_link-4013" title="Chapter&nbsp;3.&nbsp;The Hadoop Distributed Filesystem"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;3.&nbsp;The Hadoop Distributed Filesystem</h2></div></div></div><p class="calibre2">When a dataset outgrows the storage capacity of a single physical
  machine, it becomes necessary to partition it across a number of separate
  machines. Filesystems that manage the storage across a network of machines
  are called <em class="calibre10">distributed filesystems</em>. Since they are
  network based, all the complications of network programming kick in, thus
  making distributed filesystems more complex than regular disk filesystems.
  For example, one of the biggest challenges is making the filesystem tolerate
  node failure without suffering data loss.</p><p class="calibre2">Hadoop comes with a distributed filesystem called HDFS, which stands
  <a class="calibre" id="calibre_link-1923"></a>for <em class="calibre10">Hadoop Distributed Filesystem</em>. (You
  may sometimes see references to “DFS”—informally or in older documentation
  or configurations—which is the same thing.) HDFS is Hadoop’s flagship
  filesystem and is the focus of this chapter, but Hadoop actually has a
  general-purpose filesystem abstraction, so we’ll see along the way how
  Hadoop integrates with other storage systems (such as the local filesystem
  and Amazon S3).</p><div class="book" title="The Design of HDFS"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4014">The Design of HDFS</h2></div></div></div><p class="calibre2">HDFS is a filesystem <a class="calibre" id="calibre_link-1940"></a>designed for storing very large files with <u style="
    text-decoration: underline wavy;
">streaming</u> <u>data</u>
    <u style="
    text-decoration: underline 0.14em;
">access pattern</u>s, <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;running on clusters of commodity hardware.<sup class="calibre6">[<a class="firstname" href="#calibre_link-162" id="calibre_link-196">25</a>]</sup> <br>Let’s examine this statement in more detail:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Very large files</span></dt><dd class="calibre8"><p class="calibre2">“Very large” in this context means files that are hundreds of
          megabytes, gigabytes, or terabytes in size. There are Hadoop
          clusters running today that store petabytes of data.<sup class="calibre6">[<a class="firstname" href="#calibre_link-163" id="calibre_link-197">26</a>]</sup></p></dd><dt class="calibre7"><span class="term">Streaming data access</span></dt><dd class="calibre8"><p class="calibre2">HDFS is built around the idea that the most efficient data
          processing pattern is a write-once, read-many-times pattern. A
          dataset is typically generated or copied from source, and then
          various analyses are performed on that dataset over time. Each
          analysis will involve a large proportion, if not all, of the
          dataset, so the time to read the whole dataset is more important
          than the latency in reading the first record.</p></dd><dt class="calibre7"><span class="term">Commodity hardware</span></dt><dd class="calibre8"><p class="calibre2">Hadoop doesn’t require expensive, highly reliable hardware.
          It’s designed to run on clusters of commodity hardware (commonly
          available hardware that can be obtained from multiple
          vendors)<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-164" id="calibre_link-198">27</a>]</sup> for which the chance of node failure across the
          cluster is high, at least for large clusters. HDFS is designed to
          carry on working without a noticeable interruption to the user in
          the face of such failure.</p></dd></dl></div><p class="calibre2">It is also worth examining the applications for which using HDFS
    does not work so well. Although this may change in the future, these are
    areas where HDFS is not a good fit today:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Low-latency data access</span></dt><dd class="calibre8"><p class="calibre2">Applications that require low-latency access to data, in the
          tens of milliseconds range, will not work well with HDFS. Remember,
          HDFS is optimized for delivering a high throughput of data, and this
          may be at the expense of latency. HBase (see <a class="ulink" href="#calibre_link-68" title="Chapter&nbsp;20.&nbsp;HBase">Chapter&nbsp;20</a>) is <a class="calibre" id="calibre_link-1890"></a>currently a better choice for low-latency
          access.</p></dd></dl></div><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Lots of small files</span></dt><dd class="calibre8"><p class="calibre2">Because the namenode holds filesystem metadata <a class="calibre" id="calibre_link-2681"></a>in memory, the limit to the number of files in a
          filesystem is governed by the amount of memory on the <a class="calibre" id="calibre_link-2756"></a><a class="calibre" id="calibre_link-1684"></a>namenode. As a rule of thumb, each file, <a class="calibre" id="calibre_link-1505"></a>directory, and block takes about 150 bytes. So, for
          example, if you had one
          million files, each taking one block, you would need at least 300 MB
          of memory. Although storing millions of files is feasible, billions
          is beyond the capability of current hardware.<sup class="calibre6">[<a class="firstname" href="#calibre_link-165" id="calibre_link-199">28</a>]</sup></p></dd></dl></div><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Multiple writers, arbitrary file modifications</span></dt><dd class="calibre8"><p class="calibre2">Files in HDFS may be written to by a single writer. Writes are
          always made at the end of the file, in append-only fashion. There is
          no support for multiple writers or for modifications at arbitrary
          offsets in the file. (These might be supported in the future, but
          they are likely to be relatively <a class="calibre" id="calibre_link-1941"></a>inefficient.)</p></dd></dl></div></div><div class="book" title="HDFS Concepts"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4015">HDFS Concepts</h2></div></div></div><div class="book" title="Blocks"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4016">Blocks</h3></div></div></div><p class="calibre2">A disk <a class="calibre" id="calibre_link-1010"></a>has a <a class="calibre" id="calibre_link-1926"></a><a class="calibre" id="calibre_link-1664"></a>block size, which is the minimum amount of data that it
      can read or write. Filesystems for a single disk build on this by
      dealing with data in blocks, which are an integral multiple of the disk
      block size. Filesystem blocks are typically a few kilobytes in size,
      whereas disk blocks are normally 512 bytes. This is generally
      transparent to the filesystem user who is simply reading or writing a
      file of whatever length. However, there are tools to perform filesystem
      maintenance, such <a class="calibre" id="calibre_link-1448"></a><a class="calibre" id="calibre_link-1744"></a>as <em class="calibre10">df</em> and <em class="calibre10">fsck</em>, that operate on the filesystem block
      level.</p><p class="calibre2">HDFS, too, has the concept of a block, but it is a much larger
      unit—128 MB by default. Like in a filesystem for a single disk, files in
      HDFS are broken into block-sized chunks, which are stored as independent
      units. Unlike a filesystem for a single disk, a file in HDFS that is
      smaller than a single block does not occupy a full block’s worth of
      underlying storage. (For example,
      a 1 MB file stored with a block size of 128 MB uses 1 MB of disk space,
      not 128 MB.) When unqualified, the term “block” in this book refers to a
      block in HDFS.</p><div class="sidebar"><a id="calibre_link-4017" class="calibre"></a><div class="sidebar-title">Why Is a Block in HDFS So Large?</div><p class="calibre2">HDFS blocks are large compared to disk blocks, and the reason is
        to minimize the cost of seeks. If the block is large enough, the time
        it takes to transfer the data from the disk can be significantly
        longer than the time to seek to the start of the block. Thus,
        transferring a large file made of multiple blocks operates at the disk
        transfer rate.</p><p class="calibre2">A quick calculation shows that if the seek time is around 10 ms
        and the transfer rate is 100 MB/s, to make the seek time 1% of the
        transfer time, we need to make the block size around 100 MB. The
        default is actually 128 MB, although many HDFS installations use
        larger block sizes. This figure will continue to be revised upward as
        transfer speeds grow with new generations of disk drives.</p><p class="calibre2">This argument shouldn’t be taken too far, however. Map tasks in
        MapReduce <a class="calibre" id="calibre_link-2393"></a>normally operate on one block at a time, so if you have
        too few tasks (fewer than nodes in the cluster), your jobs will run
        slower than they could otherwise.</p></div><p class="calibre2">Having a block abstraction for a distributed filesystem brings
      several benefits. The first benefit is the most obvious: a file can be
      larger than any single disk in the network. There’s nothing that
      requires the blocks from a file to be stored on the same disk, so they
      can take advantage of any of the disks in the cluster. In fact, it would
      be possible, if unusual, to store a single file on an HDFS cluster whose
      blocks filled all the disks in the cluster.</p><p class="calibre2">Second, making the unit of abstraction a block rather than a
      <a class="calibre" id="calibre_link-1351"></a>file simplifies the storage subsystem. Simplicity is
      something to strive for in all systems, but it is especially important for a distributed system in
      which the failure modes are so varied. The storage subsystem deals with
      blocks, simplifying storage management (because blocks are a fixed size,
      it is easy to calculate how many can be stored on a given disk) and
      eliminating <a class="calibre" id="calibre_link-2674"></a>metadata concerns (because blocks are just chunks of data
      to be stored, file metadata such as <a class="calibre" id="calibre_link-2977"></a>permissions information does not need to be stored with
      the blocks, so another system can handle metadata separately).</p><p class="calibre2">Furthermore, blocks fit well with replication for providing fault
      tolerance and availability. To insure against corrupted blocks and disk
      and machine failure, each block is replicated to a small number of
      physically separate machines (typically three). If a block becomes
      unavailable, a copy can be read from another location in a way that is
      transparent to the client. A block that is no longer available due to
      corruption or machine failure can be replicated from its alternative
      locations to other live machines to bring the replication factor back to
      the normal level. (See <a class="ulink" href="#calibre_link-166" title="Data Integrity">Data Integrity</a> for more on
      guarding against corrupt data.) Similarly, some applications may choose
      to set a high replication factor for the blocks in a popular file to
      spread the read load on the cluster.</p><p class="calibre2">Like its disk filesystem cousin, HDFS’s <code class="literal">fsck</code> command understands blocks. For example,
      <a class="calibre" id="calibre_link-1011"></a><a class="calibre" id="calibre_link-1745"></a>running:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hdfs fsck / -files -blocks</code></strong></pre><p class="calibre2">will list the blocks that make up each file in the filesystem.
      (See also <a class="ulink" href="#calibre_link-16" title="Filesystem check (fsck)">Filesystem check (fsck)</a>.)</p></div><div class="book" title="Namenodes and Datanodes"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4018">Namenodes and Datanodes</h3></div></div></div><p class="calibre2">An HDFS cluster has two types of nodes operating in <a class="calibre" id="calibre_link-2651"></a>a master−worker pattern: a <em class="calibre10">namenode</em>
      (the master) <a class="calibre" id="calibre_link-2741"></a>and a <a class="calibre" id="calibre_link-1388"></a>number of <em class="calibre10">datanodes</em> (workers). The
      namenode manages the <a class="calibre" id="calibre_link-1685"></a>filesystem namespace. It maintains the filesystem tree and
      the metadata for all the files and directories in the tree. This
      information is stored persistently on the local disk in the form of two
      files: the namespace image and the edit log. The namenode also knows the
      datanodes on which all the blocks for a given file are located; however,
      it does not store block locations
      persistently, because this information is reconstructed from datanodes when the system starts.</p><p class="calibre2">A <em class="calibre10">client</em> accesses the filesystem on behalf
      of the user by communicating with the namenode and datanodes. The client
      presents a filesystem interface similar to a Portable Operating System
      Interface (POSIX), so the user code does not need to know about the
      namenode and datanodes to function.</p><p class="calibre2">Datanodes are the workhorses of the filesystem. They store and
      retrieve blocks when they are told to (by clients or the namenode), and
      they report back to the namenode periodically with lists of blocks that
      they are storing.</p><p class="calibre2">Without the namenode, the filesystem cannot be used. In fact, if
      the machine running the namenode were obliterated, all the files on the
      filesystem would be lost since there would be no way of knowing how to
      reconstruct the files from the blocks on the datanodes. For this reason, it is
      important to make the namenode resilient to failure, and Hadoop provides
      two mechanisms for this.</p><p class="calibre2">The first way is to back up the files that make up the persistent
      state of the filesystem metadata. Hadoop can be configured so that the
      namenode writes its persistent state to multiple filesystems. These
      writes are synchronous and atomic. The usual configuration choice is to
      write to local disk as well as a remote NFS mount.</p><p class="calibre2">It is also possible to run a <em class="calibre10">secondary
      namenode</em>, which <a class="calibre" id="calibre_link-3300"></a><a class="calibre" id="calibre_link-2763"></a>despite its name does not act as a namenode. Its main role
      is to periodically merge the namespace image with the edit log to
      prevent the edit log from becoming too large. The secondary namenode
      usually runs on a separate physical machine because it requires plenty
      of CPU and as much memory as the namenode to perform the merge. It keeps
      a copy of the merged namespace image, which can be used in the event of
      the namenode failing. However, the state of the secondary namenode lags
      that of the primary, so in the event of total failure of the primary,
      data loss is almost certain. The usual course of action in this case is
      to copy the namenode’s metadata files that are on NFS to the secondary
      and run it as the new primary. (Note that it is possible to run a hot
      standby namenode instead of a secondary, as discussed in <a class="ulink" href="#calibre_link-21" title="HDFS High Availability">HDFS High Availability</a>.)</p><p class="calibre2">See <a class="ulink" href="#calibre_link-24" title="The filesystem image and edit log">The filesystem image and edit log</a> for more
      details.</p></div><div class="book" title="Block Caching"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4019">Block Caching</h3></div></div></div><p class="calibre2">Normally a datanode reads <a class="calibre" id="calibre_link-1012"></a>blocks from disk, but for frequently accessed files the
      blocks may be explicitly cached in the datanode’s memory, in an off-heap
        <em class="calibre10">block cache</em>. By default, a block is cached in
      only one datanode’s memory, although the number is configurable on a
      per-file basis. Job schedulers (for MapReduce, Spark, and other
      frameworks) can take advantage of cached blocks by running tasks on the
      datanode where a block is cached, for increased read performance. A
      small lookup table used in a join is a good candidate for caching, for
      example.</p><p class="calibre2">Users or applications instruct the <a class="calibre" id="calibre_link-2742"></a>namenode which files to cache (and for how long) by adding
      a <em class="calibre10">cache directive</em> to a <em class="calibre10">cache
      pool</em>. Cache pools are an administrative grouping for
      managing cache permissions and resource usage.</p></div><div class="book" title="HDFS Federation"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-3">HDFS Federation</h3></div></div></div><p class="calibre2">The namenode keeps a <a class="calibre" id="calibre_link-2757"></a>reference to every file and block in the filesystem in
      memory, which means that on very large clusters with many files, memory
      becomes the limiting factor for scaling (see <a class="ulink" href="#calibre_link-167" title="How Much Memory Does a Namenode Need?">How Much Memory Does a Namenode Need?</a>). HDFS
      federation, introduced <a class="calibre" id="calibre_link-1967"></a>in the 2.x release series, allows a cluster to scale by
      adding namenodes, each of which manages a portion of the filesystem
      namespace. For example, one namenode might manage all the files rooted
      under <em class="calibre10">/user</em>, say, and a second
      namenode might handle files under <em class="calibre10">/share</em>.</p><p class="calibre2">Under federation, each namenode manages a <em class="calibre10">namespace
      volume</em>, which is made up of the metadata for the namespace,
      and a <em class="calibre10">block pool</em> containing all the blocks for
      the files in the namespace. Namespace volumes are independent of each
      other, which means namenodes do not communicate with one another, and
      furthermore the failure of one namenode does not affect the availability
      of the namespaces managed by other namenodes. Block pool storage is
      <span class="calibre">not</span> partitioned, however, so
      datanodes register with each namenode in the cluster and store blocks
      from multiple block pools.</p><p class="calibre2">To access a federated HDFS cluster, clients use client-side mount
      tables to map file paths to namenodes. This is managed in configuration
      <a class="calibre" id="calibre_link-3762"></a>using <code class="literal">ViewFileSystem</code>
      and the <code class="literal">viewfs://</code> URIs.</p></div><div class="book" title="HDFS High Availability"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-21">HDFS High Availability</h3></div></div></div><p class="calibre2">The combination of <a class="calibre" id="calibre_link-1992"></a><a class="calibre" id="calibre_link-1678"></a><a class="calibre" id="calibre_link-1946"></a>replicating namenode metadata on multiple filesystems and
      using the secondary namenode to create checkpoints protects against data
      loss, but it does not provide high availability of the filesystem. The
      namenode is <a class="calibre" id="calibre_link-2766"></a><a class="calibre" id="calibre_link-3408"></a><a class="calibre" id="calibre_link-3482"></a>still a <em class="calibre10">single point of failure</em>
      (SPOF). If it did fail, all clients—including MapReduce jobs—would be
      unable to read, write, or list files, because the namenode is the sole
      repository of the metadata and the file-to-block mapping. In such an
      event, the whole Hadoop system would effectively be out of service until
      a new namenode could be brought online.</p><p class="calibre2">To recover from a failed namenode in this situation, an
      administrator starts a new primary namenode with one of the filesystem
      metadata replicas and configures datanodes and clients to use this new
      namenode. The new namenode is not able to serve requests until it has
      (i) loaded its namespace image into memory, (ii) replayed its edit log,
      and (iii) received enough block reports from the datanodes to leave safe
      mode. On large clusters with many files and blocks, the time it takes
      for a namenode to start from cold can be 30 minutes or more.</p><p class="calibre2">The long recovery time is a problem for routine maintenance, too.
      In fact, because unexpected failure of the namenode is so rare, the case
      for planned downtime is actually more important in practice.</p><p class="calibre2">Hadoop 2 remedied this situation by adding support for HDFS high
      availability (HA). In this implementation, there are a pair of namenodes
      in an active-standby configuration. In the event of the failure of the
      active namenode, the standby takes over its duties to continue servicing
      client requests without a significant interruption. A few architectural
      changes are needed to allow this to happen:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">The namenodes must use highly available shared storage to
          share the edit log. When a standby namenode comes up, it reads up to
          the end of the shared edit log to synchronize its state with the
          active namenode, and then continues to read new entries as they are
          written by the active namenode.</p></li><li class="listitem"><p class="calibre2">Datanodes must send block reports to both namenodes because
          the block mappings are stored in a namenode’s memory, and not on
          disk.</p></li><li class="listitem"><p class="calibre2">Clients must be configured to handle namenode failover, using
          a mechanism that is transparent to users.</p></li><li class="listitem"><p class="calibre2">The secondary namenode’s role is subsumed by the standby,
          which takes periodic checkpoints of the active namenode’s
          namespace.</p></li></ul></div><p class="calibre2">There are two choices for the highly available shared storage: an
      NFS filer, or <a class="calibre" id="calibre_link-3124"></a><a class="calibre" id="calibre_link-3102"></a>a <em class="calibre10">quorum journal manager</em> (QJM). The
      QJM is a dedicated HDFS implementation, designed for the sole purpose of
      providing a highly available edit log, and is the recommended choice for
      most HDFS installations. The QJM runs as a group <a class="calibre" id="calibre_link-2288"></a>of <em class="calibre10">journal nodes</em>, and each edit
      must be written to a majority of the journal nodes. Typically, there are
      three journal nodes, so the system can tolerate the loss of one of them.
      This arrangement is similar to the <a class="calibre" id="calibre_link-3943"></a>way ZooKeeper works, although it is important to realize
      that the QJM implementation does not use ZooKeeper. (Note, however, that
      HDFS HA <span class="calibre"><em class="calibre10">does</em></span> use ZooKeeper for electing the active
      namenode, as explained in the next section.)</p><p class="calibre2">If the active namenode fails, the standby can take over very
      quickly (in a few tens of seconds) because it has the latest state
      available in memory: both the latest edit log entries and an up-to-date
      block mapping. The actual observed failover time will be longer in
      practice (around a minute or so), because the system needs to be
      conservative in deciding that the active namenode has failed.</p><p class="calibre2">In the unlikely event of the standby being down when the active
      fails, the administrator can still start the standby from cold. This is
      no worse than the non-HA case, and from an operational point of view
      it’s an improvement, because the process is a standard operational
      procedure built into Hadoop.</p><div class="book" title="Failover and fencing"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4020">Failover and fencing</h4></div></div></div><p class="calibre2">The transition from the <a class="calibre" id="calibre_link-2755"></a><a class="calibre" id="calibre_link-1601"></a>active namenode to the standby is managed by a new
        entity in the system called the <em class="calibre10">failover
        controller</em>. There are various failover controllers, but
        the default implementation uses <a class="calibre" id="calibre_link-3941"></a>ZooKeeper to ensure that only one namenode is active.
        Each namenode runs a lightweight failover controller process whose job
        it is to monitor its namenode for failures (using a simple
        heartbeating mechanism) and trigger a failover should a namenode
        fail.</p><p class="calibre2">Failover may also be initiated manually by an administrator, for
        example, in the case of routine maintenance. This is known as
        <a class="calibre" id="calibre_link-1803"></a>a <em class="calibre10">graceful failover</em>, since the
        failover controller arranges an orderly transition for both namenodes
        to switch roles.</p><p class="calibre2">In the case of an ungraceful failover, however, it is impossible
        to be sure that the failed namenode has stopped running. For example,
        a slow network or a network partition can trigger a failover
        transition, even though the previously active namenode is still
        running and thinks it is still the active namenode. The HA
        implementation goes to great lengths to ensure that the previously
        active namenode is prevented from doing any damage and causing
        corruption—a method known as <em class="calibre10">fencing</em>.</p><p class="calibre2">The QJM only allows one namenode to write to the edit log at one
        time; however, it is still possible for the previously active namenode
        to serve stale read requests to clients, so setting up an SSH fencing
        command that will kill the namenode’s process is a good idea. Stronger
        fencing methods are required when using an NFS filer for the shared
        edit log, since it is not possible to only allow one namenode to write
        at a time (this is why QJM is recommended). The range of fencing
        mechanisms includes revoking the namenode’s access to the shared
        storage directory (typically by using a vendor-specific NFS command),
        and disabling its network port via a remote management command. As a
        last resort, the previously active namenode can be fenced with a
        technique rather graphically known as <em class="calibre10">STONITH</em>,
        or “shoot the other node in the head,” which uses a specialized power
        distribution unit to forcibly power down the host machine.</p><p class="calibre2">Client failover is handled transparently by the client library.
        The simplest
        implementation uses client-side configuration to control failover. The
        HDFS URI uses a logical hostname that is mapped to a pair of namenode
        addresses (in the configuration file), and the client library tries
        each namenode address until the operation <a class="calibre" id="calibre_link-1927"></a><a class="calibre" id="calibre_link-1665"></a><a class="calibre" id="calibre_link-1993"></a><a class="calibre" id="calibre_link-1679"></a><a class="calibre" id="calibre_link-1947"></a>succeeds.</p></div></div></div><div class="book" title="The Command-Line Interface"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-532">The Command-Line Interface</h2></div></div></div><p class="calibre2">We’re going to <a class="calibre" id="calibre_link-1936"></a><a class="calibre" id="calibre_link-1186"></a>have a look at HDFS by interacting with it from the command
    line. There are many other interfaces to HDFS, but the command line is one
    of the simplest and, to many developers, the most familiar.</p><p class="calibre2">We are going to run HDFS on one machine, so first follow the
    instructions for setting up Hadoop in pseudodistributed mode in <a class="ulink" href="#calibre_link-28" title="Appendix&nbsp;A.&nbsp;Installing Apache Hadoop">Appendix&nbsp;A</a>. Later we’ll see how to run HDFS on a cluster of
    machines to give us scalability and fault tolerance.</p><p class="calibre2">There are two properties that we set in the pseudodistributed
    configuration that deserve further explanation. The first is <code class="literal">fs.defaultFS</code>, set <a class="calibre" id="calibre_link-1734"></a>to <code class="literal">hdfs://localhost/</code>, which is used to set a
    default filesystem for Hadoop.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-168" id="calibre_link-200">29</a>]</sup> Filesystems are specified by a URI, and here we have used an
    <code class="literal">hdfs</code> URI to configure Hadoop to use
    HDFS by default. The HDFS daemons will use this property to determine the
    host and port for the HDFS namenode. We’ll be running it on localhost, on
    the default HDFS port, 8020. And HDFS clients will use this property to
    work out where the namenode is running so they can connect to it.</p><p class="calibre2">We set the second <a class="calibre" id="calibre_link-1483"></a>property, <code class="literal">dfs.replication</code>, to 1 so that HDFS doesn’t
    replicate filesystem blocks by the
    default factor of three. When running with a single datanode, HDFS can’t
    replicate blocks to three datanodes, so it would perpetually warn about
    blocks being under-replicated. This setting solves that problem.</p><div class="book" title="Basic Filesystem Operations"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4021">Basic Filesystem Operations</h3></div></div></div><p class="calibre2">The filesystem is <a class="calibre" id="calibre_link-1666"></a><a class="calibre" id="calibre_link-1928"></a><a class="calibre" id="calibre_link-1839"></a>ready to be used, and we can do all of the usual
      filesystem operations, such as reading files, creating directories,
      moving files, deleting data, and listing directories. You can type
      <code class="literal">hadoop fs -help</code> to get detailed help
      on every command.</p><p class="calibre2">Start by copying a file from the local filesystem to HDFS:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -copyFromLocal input/docs/quangle.txt \
  hdfs://localhost/user/tom/quangle.txt</code></strong></pre><p class="calibre2">This command invokes Hadoop’s filesystem shell command <code class="literal">fs</code>, which supports a number of subcommands—in
      this case, we are running <code class="literal">-copyFromLocal</code>. The local file <em class="calibre10">quangle.txt</em> is copied to the file <em class="calibre10">/user/tom/quangle.txt</em> on the HDFS instance
      running on localhost. In fact, we could have omitted the scheme and host
      of the URI and picked up the default, <code class="literal">hdfs://localhost</code>, as specified in <em class="calibre10">core-site.xml</em>:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -copyFromLocal input/docs/quangle.txt /user/tom/quangle.txt</code></strong></pre><p class="calibre2">We also could have used a relative path and copied the file to our
      home directory in HDFS, which in this case is <em class="calibre10">/user/tom</em>:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -copyFromLocal input/docs/quangle.txt quangle.txt</code></strong></pre><p class="calibre2">Let’s copy the file back to the local filesystem and check whether
      it’s the same:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -copyToLocal quangle.txt quangle.copy.txt</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">md5 input/docs/quangle.txt quangle.copy.txt</code></strong>
MD5 (input/docs/quangle.txt) = e7891a2627cf263a079fb0f18256ffb2
MD5 (quangle.copy.txt) = e7891a2627cf263a079fb0f18256ffb2</pre><p class="calibre2">The MD5 digests are the same, showing that the file survived its
      trip to HDFS and is back intact.</p><p class="calibre2">Finally, let’s look at an HDFS file listing. We create a directory
      first just to see how it is displayed in the listing:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -mkdir books</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -ls .</code></strong>
Found 2 items
drwxr-xr-x   - tom supergroup          0 2014-10-04 13:22 books
-rw-r--r--   1 tom supergroup        119 2014-10-04 13:21 quangle.txt</pre><p class="calibre2">The information returned is very similar to that returned by the
      Unix command <code class="literal">ls -l</code>, with a few minor
      differences. The first column shows the file mode. The second column is
      the replication factor of the file (something a traditional Unix
      filesystem does not have). Remember we set the default replication
      factor in the site-wide configuration to be 1, which is why we see the
      same value here. The entry in this column is empty for directories
      because the concept of replication does not apply to them—directories
      are treated as metadata and stored by the namenode, not the datanodes.
      The third and fourth columns show the file owner and group. The fifth
      column is the size of the file in bytes, or zero for directories. The
      sixth and seventh columns are the last modified date and time. Finally,
      the eighth column is the name of the file or <a class="calibre" id="calibre_link-1937"></a><a class="calibre" id="calibre_link-1187"></a><a class="calibre" id="calibre_link-1929"></a><a class="calibre" id="calibre_link-1667"></a><a class="calibre" id="calibre_link-1840"></a>directory.</p><div class="sidebar"><a id="calibre_link-4022" class="calibre"></a><div class="sidebar-title">File Permissions in HDFS</div><p class="calibre2">HDFS has a permissions <a class="calibre" id="calibre_link-1942"></a><a class="calibre" id="calibre_link-2976"></a>model for files and directories that is much like the
        POSIX model. There are three types of permission: the <a class="calibre" id="calibre_link-3156"></a><a class="calibre" id="calibre_link-3125"></a>read permission (<code class="literal">r</code>),
        the <a class="calibre" id="calibre_link-3808"></a><a class="calibre" id="calibre_link-3770"></a>write permission (<code class="literal">w</code>),
        and the <a class="calibre" id="calibre_link-3821"></a><a class="calibre" id="calibre_link-1586"></a>execute permission (<code class="literal">x</code>). The read permission is required to read
        files or list the contents of a directory. The write permission is
        required to write a file or, for a directory, to create or delete
        files or directories in it. The execute permission is ignored for a
        file because you can’t execute a file on HDFS (unlike POSIX), and for
        a directory this permission is required to access its children.</p><p class="calibre2">Each file and directory has an <em class="calibre10">owner</em>, a
        <em class="calibre10">group</em>, and a <em class="calibre10">mode</em>. The
        mode is <a class="calibre" id="calibre_link-1504"></a><a class="calibre" id="calibre_link-1626"></a>made up of the permissions for the user who is the
        owner, the permissions for the users who are members of the group, and the
        permissions for users who are neither the owners nor members of the
        group.</p><p class="calibre2">By default, Hadoop runs with security disabled, which means that
        a client’s identity is not authenticated. Because clients are remote,
        it is possible for a client to become an arbitrary user simply by
        creating an account of that name on the remote system. This is not
        possible if security is turned on; see <a class="ulink" href="#calibre_link-169" title="Security">Security</a>.
        Either way, it is worthwhile having permissions enabled (as they are
        by default; see <a class="calibre" id="calibre_link-1482"></a>the <code class="literal">dfs.permissions</code><code class="literal">.enabled</code> property) to avoid
        accidental modification or deletion of substantial parts of the
        filesystem, either by users or by automated tools or programs.</p><p class="calibre2">When permissions checking is enabled, the owner permissions are
        checked if the client’s username matches the owner, and the group
        permissions are checked if the client is a member of the group;
        otherwise, the other permissions are checked.</p><p class="calibre2">There is a concept of a superuser, which is the identity of the
        namenode process. Permissions checks are not performed for the
        superuser.</p></div></div></div><div class="book" title="Hadoop Filesystems"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-15">Hadoop Filesystems</h2></div></div></div><p class="calibre2">Hadoop has an abstract notion of <a class="calibre" id="calibre_link-1676"></a>filesystems, of which HDFS is just one implementation. The
    Java abstract <a class="calibre" id="calibre_link-1650"></a>class <code class="literal">org.apache.hadoop.fs.FileSystem</code> represents the
    client interface to a filesystem in Hadoop, and there are several concrete
    implementations. The main ones that ship with Hadoop are described in
    <a class="ulink" href="#calibre_link-170" title="Table&nbsp;3-1.&nbsp;Hadoop filesystems">Table&nbsp;3-1</a>.</p><div class="table"><a id="calibre_link-170" class="calibre"></a><div class="table-title">Table&nbsp;3-1.&nbsp;Hadoop filesystems</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="c3"><col class="c3"><col class="c4"><col class="newcol"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Filesystem</td><td class="calibre20">URI scheme</td><td class="calibre20">Java implementation
            (all under
            org.apache.hadoop)</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">Local</td><td class="calibre23"><code class="uri">file</code></td><td class="calibre23"><code class="uri">fs.LocalFileSystem</code></td><td class="calibre25">A filesystem for <a class="calibre" id="calibre_link-2346"></a>a locally connected disk with client-side checksums. Use <code class="uri">RawLocalFileSystem</code> for <a class="calibre" id="calibre_link-3137"></a>a local
            filesystem with no checksums. See <a class="ulink" href="#calibre_link-171" title="LocalFileSystem">LocalFileSystem</a>.</td></tr><tr class="calibre26"><td class="calibre23">HDFS</td><td class="calibre23"><code class="uri">hdfs</code></td><td class="calibre23"><code class="uri">hdfs.DistributedFileSystem</code></td><td class="calibre25">Hadoop’s <a class="calibre" id="calibre_link-1522"></a>distributed filesystem. HDFS is designed to work
            efficiently in conjunction with MapReduce.</td></tr><tr class="calibre19"><td class="calibre23">WebHDFS</td><td class="calibre23"><code class="uri">webhdfs</code></td><td class="calibre23"><code class="uri">hdfs.web.WebHdfsFileSystem</code></td><td class="calibre25">A filesystem <a class="calibre" id="calibre_link-3781"></a>providing authenticated read/write access to HDFS
            over HTTP. See <a class="ulink" href="#calibre_link-172" title="HTTP">HTTP</a>.</td></tr><tr class="calibre26"><td class="calibre23">Secure WebHDFS</td><td class="calibre23"><code class="uri">swebhdfs</code></td><td class="calibre23"><code class="uri">hdfs.web.SWebHdfsFileSystem</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-3565"></a>HTTPS version of WebHDFS.</td></tr><tr class="calibre19"><td class="calibre23">HAR</td><td class="calibre23"><code class="uri">har</code></td><td class="calibre23"><code class="uri">fs.HarFileSystem</code></td><td class="calibre25">A <a class="calibre" id="calibre_link-1885"></a>filesystem layered on another filesystem for
            a<a class="calibre" id="calibre_link-1884"></a>rchiving files. Hadoop Archives are used for packing
            lots of files in HDFS into a single archive file to reduce the
            namenode’s memory usage. Use the <code class="uri">hadoop archive</code>
            command to create <a class="calibre" id="calibre_link-1841"></a>HAR files.</td></tr><tr class="calibre26"><td class="calibre23">View</td><td class="calibre23"><code class="uri">viewfs</code></td><td class="calibre23"><code class="uri">viewfs.ViewFileSystem</code></td><td class="calibre25">A client-side <a class="calibre" id="calibre_link-3763"></a>mount table for other Hadoop filesystems. Commonly
            used to create mount points for federated namenodes (see <a class="ulink" href="#calibre_link-3" title="HDFS Federation">HDFS Federation</a>).</td></tr><tr class="calibre19"><td class="calibre23">FTP</td><td class="calibre23"><code class="uri">ftp</code></td><td class="calibre23"><code class="uri">fs.ftp.FTPFileSystem</code></td><td class="calibre25">A filesystem <a class="calibre" id="calibre_link-1760"></a>backed by an FTP server.</td></tr><tr class="calibre26"><td class="calibre23">S3</td><td class="calibre23"><code class="uri">s3a</code></td><td class="calibre23"><code class="uri">fs.s3a.S3AFileSystem</code></td><td class="calibre25">A filesystem <a class="calibre" id="calibre_link-3257"></a>backed by Amazon S3. Replaces the older
            <code class="uri">s3n</code> (S3 native) implementation.</td></tr><tr class="calibre19"><td class="calibre23">Azure</td><td class="calibre23"><code class="uri">wasb</code></td><td class="calibre23"><code class="uri">fs.azure.NativeAzureFileSystem</code></td><td class="calibre25">A filesystem <a class="calibre" id="calibre_link-2775"></a>backed by Microsoft Azure.</td></tr><tr class="calibre26"><td class="calibre27">Swift</td><td class="calibre27"><code class="uri">swift</code></td><td class="calibre27"><code class="uri">fs.swift.snative.SwiftNativeFileSystem</code></td><td class="calibre28">A filesystem <a class="calibre" id="calibre_link-3566"></a>backed by OpenStack Swift.</td></tr></tbody></table></div></div><p class="calibre2">Hadoop provides many interfaces to its filesystems, and it generally
    uses the URI scheme to pick the correct filesystem instance to communicate
    with. For example, the filesystem shell that we met in the previous
    section operates with all Hadoop filesystems. To list the files in the
    root directory of the local filesystem, type:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -ls file:///</code></strong></pre><p class="calibre2">Although it is possible (and sometimes very convenient) to run
    MapReduce programs that access any of these filesystems, when you are
    processing large volumes of data you should choose a distributed
    filesystem that has the data locality optimization, notably HDFS (see
    <a class="ulink" href="#calibre_link-173" title="Scaling Out">Scaling Out</a>).</p><div class="book" title="Interfaces"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4023">Interfaces</h3></div></div></div><p class="calibre2">Hadoop is <a class="calibre" id="calibre_link-2134"></a>written in Java, so most Hadoop filesystem interactions
      are mediated through the Java API. The filesystem shell, for example, is
      a Java application that uses <a class="calibre" id="calibre_link-1651"></a>the Java <code class="literal">FileSystem</code>
      class to provide filesystem operations. The other filesystem interfaces
      are discussed briefly in this section. These interfaces are most
      commonly used with HDFS, since the other filesystems in Hadoop typically
      have existing tools to access the underlying filesystem (FTP clients for
      FTP, S3 tools for S3, etc.), but many of them will work with any Hadoop
      filesystem.</p><div class="book" title="HTTP"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-172">HTTP</h4></div></div></div><p class="calibre2">By exposing its filesystem interface as a <a class="calibre" id="calibre_link-2059"></a>Java API, Hadoop makes it awkward for non-Java
        applications to access HDFS. The HTTP REST API exposed by the
        <a class="calibre" id="calibre_link-3780"></a>WebHDFS protocol makes it easier for other languages to
        interact with HDFS. Note that the HTTP interface is slower than the
        native Java client, so should be avoided for very large data transfers
        if possible.</p><p class="calibre2">There are two ways of accessing HDFS over HTTP: directly, where
        the HDFS daemons serve HTTP requests to clients; and via a proxy (or
        proxies), which accesses HDFS on the client’s behalf using the
        <a class="calibre" id="calibre_link-1524"></a>usual <code class="literal">DistributedFileSystem</code> API. The two ways are
        illustrated in <a class="ulink" href="#calibre_link-174" title="Figure&nbsp;3-1.&nbsp;Accessing HDFS over HTTP directly and via a bank of HDFS proxies">Figure&nbsp;3-1</a>. Both use the WebHDFS
        protocol.</p><div class="figure"><a id="calibre_link-174" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4024" class="calibre"></a><img alt="Accessing HDFS over HTTP directly and via a bank of HDFS proxies" src="images/000063.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;3-1.&nbsp;Accessing HDFS over HTTP directly and via a bank of HDFS
          proxies</div></div><p class="calibre2">In the first case, the embedded web servers in the namenode and
        datanodes act as WebHDFS endpoints. (WebHDFS is enabled by default,
        <a class="calibre" id="calibre_link-1485"></a>since <code class="literal">dfs.webhdfs.enabled</code> is set to <code class="literal">true</code>.) File metadata operations are handled
        by the namenode, while file read (and write) operations are sent first
        to the namenode, which sends an HTTP redirect to the client indicating
        the datanode to stream file data from (or to).</p><p class="calibre2">The second way of accessing HDFS over HTTP relies on one or more
        standalone proxy servers. (The proxies are stateless, so they can run
        behind a standard load balancer.) All traffic to the cluster passes
        through the proxy, so the client never accesses the namenode or
        datanode directly. This allows for stricter firewall and
        bandwidth-limiting policies to be put in place. It’s common to use a
        proxy for transfers between Hadoop clusters located in different data
        centers, or when accessing a Hadoop cluster running in the cloud from
        an external network.</p><p class="calibre2">The HttpFS proxy<a class="calibre" id="calibre_link-2061"></a> exposes the same HTTP (and HTTPS) interface as WebHDFS,
        so clients can access both using <code class="literal">webhdfs</code> (or
        <code class="literal">swebhdfs</code>) URIs. The HttpFS proxy is started independently
        of the namenode and datanode daemons, using the <em class="calibre10">httpfs.sh</em> script, and by default listens on
        a different port number (14000).</p></div><div class="book" title="C"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4025">C</h4></div></div></div><p class="calibre2">Hadoop provides a <a class="calibre" id="calibre_link-1043"></a><a class="calibre" id="calibre_link-2321"></a>C library called <em class="calibre10"><em class="calibre10">libhdfs</em></em> that mirrors the Java
        <code class="literal">FileSystem</code> interface (it was
        written as a C library for accessing HDFS, but despite its name it can
        be used to access any Hadoop filesystem). It works using the
        <em class="calibre10">Java Native Interface</em> (JNI) to call a
        <a class="calibre" id="calibre_link-2188"></a><a class="calibre" id="calibre_link-2208"></a>Java filesystem client. There is also a
        <em class="calibre10"><em class="calibre10">libwebhdfs</em></em>
        library that uses the WebHDFS interface described in the previous
        section.</p><p class="calibre2">The C API is very similar to the Java one, but it typically lags
        the Java one, so some newer features may not be supported. You can
        find the header file, <em class="calibre10">hdfs.h</em>, in
        the <em class="calibre10">include</em> directory of the
        Apache Hadoop binary tarball distribution.</p><p class="calibre2">The Apache Hadoop binary tarball comes with prebuilt <em class="calibre10">libhdfs</em> binaries for 64-bit Linux, but for
        other platforms you will need to build them yourself by following the
        <em class="calibre10">BUILDING.txt</em> instructions at the
        top level of the source tree.</p></div><div class="book" title="NFS"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4026">NFS</h4></div></div></div><p class="calibre2">It is possible to mount <a class="calibre" id="calibre_link-2789"></a>HDFS on a local client’s filesystem using Hadoop’s NFSv3
        gateway. You can then use Unix utilities (such as <code class="literal">ls</code> and <code class="literal">cat</code>) to interact with the filesystem, upload
        files, and in general use POSIX libraries to access the filesystem
        from any programming language. Appending to a file works, but random
        modifications of a file do not, since HDFS can only write to the end
        of a file.</p><p class="calibre2">Consult the Hadoop documentation for how to configure and run
        the NFS gateway and connect to it from a client.</p></div><div class="book" title="FUSE"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4027">FUSE</h4></div></div></div><p class="calibre2"><em class="calibre10">Filesystem in Userspace</em> (FUSE) allows
        <a class="calibre" id="calibre_link-1663"></a><a class="calibre" id="calibre_link-1783"></a>filesystems that are implemented in user space to be
        integrated as Unix filesystems. Hadoop’s Fuse-DFS contrib module
        allows HDFS (or any Hadoop filesystem) to be mounted as a standard
        local filesystem. Fuse-DFS is implemented in C using <em class="calibre10">libhdfs</em> as the interface to HDFS. At the
        time of writing, the Hadoop NFS gateway is the more robust solution to
        mounting HDFS, so should be preferred <a class="calibre" id="calibre_link-1677"></a><a class="calibre" id="calibre_link-2135"></a>over Fuse-DFS.</p></div></div></div><div class="book" title="The Java Interface"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4028">The Java Interface</h2></div></div></div><p class="calibre2">In this section, we <a class="calibre" id="calibre_link-1948"></a><a class="calibre" id="calibre_link-1680"></a>dig into the Hadoop <code class="literal">FileSystem</code> class: the <a class="calibre" id="calibre_link-1652"></a>API for interacting with one of Hadoop’s
    filesystems.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-175" id="calibre_link-202">30</a>]</sup> Although we focus mainly on the HDFS implementation,
    <code class="literal">DistributedFileSystem</code>, in <a class="calibre" id="calibre_link-1523"></a>general you should strive to write your code against the
    <code class="literal">FileSystem</code> abstract class, to retain
    portability across filesystems. This is very useful when testing your
    program, for example, because you can rapidly run tests using data stored
    on the local filesystem.</p><div class="book" title="Reading Data from a Hadoop URL"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4029">Reading Data from a Hadoop URL</h3></div></div></div><p class="calibre2">One of the <a class="calibre" id="calibre_link-2170"></a><a class="calibre" id="calibre_link-3162"></a>simplest ways to read a file from a Hadoop filesystem is
      by using a <code class="literal">java.net.URL</code> object
      <a class="calibre" id="calibre_link-3750"></a>to open a stream to read the data from. The general idiom
      is:</p><a id="calibre_link-4030" class="calibre"></a><pre class="screen1"><code class="n">InputStream</code> <code class="n">in</code> <code class="o">=</code> <code class="k">null</code><code class="o">;</code>
<code class="k">try</code> <code class="o">{</code>
  <code class="n">in</code> <code class="o">=</code> <code class="k">new</code> <code class="n">URL</code><code class="o">(</code><code class="sb">"hdfs://host/path"</code><code class="o">).</code><code class="na">openStream</code><code class="o">();</code>
  <code class="c2">// process in</code>
<code class="o">}</code> <code class="k">finally</code> <code class="o">{</code>
  <code class="n">IOUtils</code><code class="o">.</code><code class="na">closeStream</code><code class="o">(</code><code class="n">in</code><code class="o">);</code>
<code class="o">}</code></pre><p class="calibre2">There’s a little bit more work required to make Java recognize
      Hadoop’s <code class="literal">hdfs</code> URL scheme. This is
      achieved by calling the <code class="literal">setURLStreamHandlerFactory()</code> method on
      <code class="literal">URL</code> with an instance of <code class="literal">FsUrlStreamHandlerFactory</code>. This method can be
      called only once per JVM, so it is typically executed in a static block.
      This limitation means that if some other part of your program—perhaps a
      third-party component outside your control—sets a <code class="literal">URLStreamHandlerFactory</code>, you won’t be able to
      use this approach for reading data from Hadoop. The next section
      discusses an alternative.</p><p class="calibre2"><a class="ulink" href="#calibre_link-176" title="Example&nbsp;3-1.&nbsp;Displaying files from a Hadoop filesystem on standard output using a URLStreamHandler">Example&nbsp;3-1</a> shows a program for displaying files
      from Hadoop filesystems on standard output, like the Unix <code class="literal">cat</code> command.</p><div class="example"><a id="calibre_link-176" class="calibre"></a><div class="example-title">Example&nbsp;3-1.&nbsp;<span class="calibre">Displaying files from a Hadoop filesystem on standard
        output using a URL</span>StreamHandler</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">URLCat</code> <code class="o">{</code>

  <code class="k">static</code> <code class="o">{</code>
    <code class="n">URL</code><code class="o">.</code><code class="na">setURLStreamHandlerFactory</code><code class="o">(</code><code class="k">new</code> <code class="n">FsUrlStreamHandlerFactory</code><code class="o">());</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">InputStream</code> <code class="n">in</code> <code class="o">=</code> <code class="k">null</code><code class="o">;</code>
    <code class="k">try</code> <code class="o">{</code>
      <code class="n">in</code> <code class="o">=</code> <code class="k">new</code> <code class="n">URL</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]).</code><code class="na">openStream</code><code class="o">();</code>
      <code class="n">IOUtils</code><code class="o">.</code><code class="na">copyBytes</code><code class="o">(</code><code class="n">in</code><code class="o">,</code> <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">,</code> <code class="mi">4096</code><code class="o">,</code> <code class="k">false</code><code class="o">);</code>
    <code class="o">}</code> <code class="k">finally</code> <code class="o">{</code>
      <code class="n">IOUtils</code><code class="o">.</code><code class="na">closeStream</code><code class="o">(</code><code class="n">in</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">We make use of the handy <code class="literal">IOUtils</code> class that comes with Hadoop for
      closing the stream in the <code class="literal">finally</code>
      clause, and also for copying bytes between the input stream and the
      output stream (<code class="literal">System.out</code>, in this
      case). The last two arguments to the <code class="literal">copyBytes()</code> method are the buffer size used
      for copying and whether to close the streams when the copy is complete.
      We close the input stream ourselves, and <code class="literal">System.out</code> doesn’t need to be closed.</p><p class="calibre2">Here’s a sample run:<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-177" id="calibre_link-203">31</a>]</sup></p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">export HADOOP_CLASSPATH=hadoop-examples.jar</code></strong>
<code class="literal">% </code><strong class="userinput"><code class="calibre9">hadoop URLCat hdfs://localhost/user/tom/quangle.txt</code></strong>
On the top of the Crumpetty Tree
The Quangle Wangle sat,
But his face you could not see,
On account of his Beaver Hat.</pre></div><div class="book" title="Reading Data Using the FileSystem API"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4031">Reading Data Using the FileSystem API</h3></div></div></div><p class="calibre2">As the previous <a class="calibre" id="calibre_link-2171"></a><a class="calibre" id="calibre_link-1657"></a><a class="calibre" id="calibre_link-3159"></a>section explained, sometimes it is impossible to set a
      <code class="literal">URLStreamHandlerFactory</code> for your
      application. In this case, you will need to use the <code class="literal">FileSystem</code> API to open an input stream for a
      file.</p><p class="calibre2">A file in a Hadoop filesystem is represented by a <a class="calibre" id="calibre_link-2954"></a>Hadoop <code class="literal">Path</code> object (and
      not a <code class="literal">java.io.File</code> object, since its
      semantics are too closely tied to the local filesystem). You can think of a <code class="literal">Path</code> as a Hadoop filesystem URI, such as
      <code class="literal">hdfs://localhost/user/tom/quangle.txt</code>.</p><p class="calibre2"><code class="literal">FileSystem</code> is a general
      filesystem API, so the first step is to retrieve an instance for the
      filesystem we want to use—HDFS, in this case. There are several static
      factory methods for getting a <code class="literal">FileSystem</code> instance:</p><a id="calibre_link-4032" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">static</code> <code class="n">FileSystem</code> <code class="nf">get</code><code class="o">(</code><code class="n">Configuration</code> <code class="n">conf</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code>
<code class="k">public</code> <code class="k">static</code> <code class="n">FileSystem</code> <code class="nf">get</code><code class="o">(</code><code class="n">URI</code> <code class="n">uri</code><code class="o">,</code> <code class="n">Configuration</code> <code class="n">conf</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code>
<code class="k">public</code> <code class="k">static</code> <code class="n">FileSystem</code> <code class="nf">get</code><code class="o">(</code><code class="n">URI</code> <code class="n">uri</code><code class="o">,</code> <code class="n">Configuration</code> <code class="n">conf</code><code class="o">,</code> <code class="n">String</code> <code class="n">user</code><code class="o">)</code>
    <code class="k">throws</code> <code class="n">IOException</code></pre><p class="calibre2">A <code class="literal">Configuration</code> object
      <a class="calibre" id="calibre_link-1228"></a>encapsulates a client or server’s configuration, which is
      set using configuration files read from the classpath, such as <em class="calibre10">etc/hadoop/core-site.xml</em>. The first method
      returns the default filesystem (as specified in <em class="calibre10">core-site.xml</em>, or the default local
      filesystem if not specified there). The second uses the given <code class="literal">URI</code>’s scheme and authority to determine the
      filesystem to use, falling back to the default filesystem if no scheme
      is specified in the given <code class="literal">URI</code>. The
      third retrieves the filesystem as the given user, which is important in
      the context of security (see <a class="ulink" href="#calibre_link-169" title="Security">Security</a>).</p><p class="calibre2">In some cases, you may want to retrieve a local filesystem
      instance. For this, you can use the convenience method
      <code class="literal">getLocal()</code>:</p><a id="calibre_link-4033" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">static</code> <code class="n">LocalFileSystem</code> <code class="nf">getLocal</code><code class="o">(</code><code class="n">Configuration</code> <code class="n">conf</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code></pre><p class="calibre2">With a <code class="literal">FileSystem</code> instance in
      hand, we invoke an <code class="literal">open()</code> method to
      get the input stream for a file:</p><a id="calibre_link-4034" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="n">FSDataInputStream</code> <code class="nf">open</code><code class="o">(</code><code class="n">Path</code> <code class="n">f</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code>
<code class="k">public</code> <code class="k">abstract</code> <code class="n">FSDataInputStream</code> <code class="nf">open</code><code class="o">(</code><code class="n">Path</code> <code class="n">f</code><code class="o">,</code> <code class="kt">int</code> <code class="n">bufferSize</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code></pre><p class="calibre2">The first method uses a default buffer size of 4 KB.</p><p class="calibre2">Putting this together, we can rewrite <a class="ulink" href="#calibre_link-176" title="Example&nbsp;3-1.&nbsp;Displaying files from a Hadoop filesystem on standard output using a URLStreamHandler">Example&nbsp;3-1</a> as
      shown in <a class="ulink" href="#calibre_link-178" title="Example&nbsp;3-2.&nbsp;Displaying files from a Hadoop filesystem on standard output by using the FileSystem directly">Example&nbsp;3-2</a>.</p><div class="example"><a id="calibre_link-178" class="calibre"></a><div class="example-title">Example&nbsp;3-2.&nbsp;Displaying files from a Hadoop filesystem on standard output by
        using the FileSystem directly</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">FileSystemCat</code> <code class="o">{</code>

  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">String</code> <code class="n">uri</code> <code class="o">=</code> <code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">];</code>
    <code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Configuration</code><code class="o">();</code>
    <code class="n">FileSystem</code> <code class="n">fs</code> <code class="o">=</code> <code class="n">FileSystem</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="n">URI</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">uri</code><code class="o">),</code> <code class="n">conf</code><code class="o">);</code>
    <code class="n">InputStream</code> <code class="n">in</code> <code class="o">=</code> <code class="k">null</code><code class="o">;</code>
    <code class="k">try</code> <code class="o">{</code>
      <code class="n">in</code> <code class="o">=</code> <code class="n">fs</code><code class="o">.</code><code class="na">open</code><code class="o">(</code><code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">uri</code><code class="o">));</code>
      <code class="n">IOUtils</code><code class="o">.</code><code class="na">copyBytes</code><code class="o">(</code><code class="n">in</code><code class="o">,</code> <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">,</code> <code class="mi">4096</code><code class="o">,</code> <code class="k">false</code><code class="o">);</code>
    <code class="o">}</code> <code class="k">finally</code> <code class="o">{</code>
      <code class="n">IOUtils</code><code class="o">.</code><code class="na">closeStream</code><code class="o">(</code><code class="n">in</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The program runs as follows:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">hadoop FileSystemCat hdfs://localhost/user/tom/quangle.txt</code></strong>
On the top of the Crumpetty Tree
The Quangle Wangle sat,
But his face you could not see,
On account of his Beaver Hat.</pre><div class="book" title="FSDataInputStream"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4035">FSDataInputStream</h4></div></div></div><p class="calibre2">The <code class="literal">open()</code>
        method on <code class="literal">FileSystem</code> actually
        <a class="calibre" id="calibre_link-1749"></a>returns an <code class="literal">FSDataInputStream</code> rather than a standard
        <code class="literal">java.io</code> class. This class is a
        specialization of <code class="literal">java.io.DataInputStream</code> with support for
        random access, so you can read from any part of the stream:</p><a id="calibre_link-4036" class="calibre"></a><pre class="screen1"><code class="k">package</code> <code class="n">org</code><code class="o">.</code><code class="na">apache</code><code class="o">.</code><code class="na">hadoop</code><code class="o">.</code><code class="na">fs</code><code class="o">;</code>

<code class="k">public</code> <code class="k">class</code> <code class="nc">FSDataInputStream</code> <code class="k">extends</code> <code class="n">DataInputStream</code>
    <code class="k">implements</code> <code class="n">Seekable</code><code class="o">,</code> <code class="n">PositionedReadable</code> <code class="o">{</code>
  <code class="c2">// implementation elided</code>
<code class="o">}</code></pre><p class="calibre2">The <code class="literal">Seekable</code> interface
        <a class="calibre" id="calibre_link-3315"></a>permits seeking to a position in the file and provides a
        query method for the current offset from the start of the file
        (<code class="literal">getPos()</code>):</p><a id="calibre_link-4037" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">interface</code> <code class="nc">Seekable</code> <code class="o">{</code>
  <code class="kt">void</code> <code class="nf">seek</code><code class="o">(</code><code class="kt">long</code> <code class="n">pos</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">;</code>
  <code class="kt">long</code> <code class="nf">getPos</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">;</code>
<code class="o">}</code></pre><p class="calibre2">Calling <code class="literal">seek()</code> with a
        position that is greater than the length of the file will result in an
        <code class="literal">IOException</code>. Unlike the
        <code class="literal">skip()</code> method of <code class="literal">java.io.InputStream</code>, which <a class="calibre" id="calibre_link-2126"></a>positions the stream at a point later than the current
        position, <code class="literal">seek()</code> can move to an arbitrary,
        absolute position in the file.</p><p class="calibre2">A simple extension of <a class="ulink" href="#calibre_link-178" title="Example&nbsp;3-2.&nbsp;Displaying files from a Hadoop filesystem on standard output by using the FileSystem directly">Example&nbsp;3-2</a> is shown
        in <a class="ulink" href="#calibre_link-179" title="Example&nbsp;3-3.&nbsp;Displaying files from a Hadoop filesystem on standard output twice, by using seek()">Example&nbsp;3-3</a>, which writes a file to
        standard output twice: after writing it once, it seeks to the start of
        the file and streams through it once again.</p><div class="example"><a id="calibre_link-179" class="calibre"></a><div class="example-title">Example&nbsp;3-3.&nbsp;Displaying files from a Hadoop filesystem on standard output
          twice, by using seek()</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">FileSystemDoubleCat</code> <code class="o">{</code>

  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">String</code> <code class="n">uri</code> <code class="o">=</code> <code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">];</code>
    <code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Configuration</code><code class="o">();</code>
    <code class="n">FileSystem</code> <code class="n">fs</code> <code class="o">=</code> <code class="n">FileSystem</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="n">URI</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">uri</code><code class="o">),</code> <code class="n">conf</code><code class="o">);</code>
    <code class="n">FSDataInputStream</code> <code class="n">in</code> <code class="o">=</code> <code class="k">null</code><code class="o">;</code>
    <code class="k">try</code> <code class="o">{</code>
      <code class="n">in</code> <code class="o">=</code> <code class="n">fs</code><code class="o">.</code><code class="na">open</code><code class="o">(</code><code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">uri</code><code class="o">));</code>
      <code class="n">IOUtils</code><code class="o">.</code><code class="na">copyBytes</code><code class="o">(</code><code class="n">in</code><code class="o">,</code> <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">,</code> <code class="mi">4096</code><code class="o">,</code> <code class="k">false</code><code class="o">);</code>
      <code class="n">in</code><code class="o">.</code><code class="na">seek</code><code class="o">(</code><code class="mi">0</code><code class="o">);</code> <code class="c2">// go back to the start of the file</code>
      <code class="n">IOUtils</code><code class="o">.</code><code class="na">copyBytes</code><code class="o">(</code><code class="n">in</code><code class="o">,</code> <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">,</code> <code class="mi">4096</code><code class="o">,</code> <code class="k">false</code><code class="o">);</code>
    <code class="o">}</code> <code class="k">finally</code> <code class="o">{</code>
      <code class="n">IOUtils</code><code class="o">.</code><code class="na">closeStream</code><code class="o">(</code><code class="n">in</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">Here’s the result of running it on a small file:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">hadoop FileSystemDoubleCat hdfs://localhost/user/tom/quangle.txt</code></strong>
On the top of the Crumpetty Tree
The Quangle Wangle sat,
But his face you could not see,
On account of his Beaver Hat.
On the top of the Crumpetty Tree
The Quangle Wangle sat,
But his face you could not see,
On account of his Beaver Hat.</pre><p class="calibre2"><code class="literal">FSDataInputStream</code> also
        implements the <code class="literal">PositionedReadable</code>
        interface <a class="calibre" id="calibre_link-3057"></a>for reading parts of a file at a given offset:</p><a id="calibre_link-4038" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">interface</code> <code class="nc">PositionedReadable</code> <code class="o">{</code>

  <code class="k">public</code> <code class="kt">int</code> <code class="nf">read</code><code class="o">(</code><code class="kt">long</code> <code class="n">position</code><code class="o">,</code> <code class="kt">byte</code><code class="o">[]</code> <code class="n">buffer</code><code class="o">,</code> <code class="kt">int</code> <code class="n">offset</code><code class="o">,</code> <code class="kt">int</code> <code class="n">length</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">;</code>
  
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">readFully</code><code class="o">(</code><code class="kt">long</code> <code class="n">position</code><code class="o">,</code> <code class="kt">byte</code><code class="o">[]</code> <code class="n">buffer</code><code class="o">,</code> <code class="kt">int</code> <code class="n">offset</code><code class="o">,</code> <code class="kt">int</code> <code class="n">length</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">;</code>
  
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">readFully</code><code class="o">(</code><code class="kt">long</code> <code class="n">position</code><code class="o">,</code> <code class="kt">byte</code><code class="o">[]</code> <code class="n">buffer</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">;</code>
<code class="o">}</code></pre><p class="calibre2">The <code class="literal">read()</code> method reads up to
        <code class="literal">length</code> bytes from the given
        <code class="literal">position</code> in the file into the
        <code class="literal">buffer</code> at the given <code class="literal">offset</code> in the buffer. The return value is
        the number of bytes actually read; callers should check this value, as
        it may be less than <code class="literal">length</code>. The
        <code class="literal">readFully()</code> methods will read
        <code class="literal">length</code> bytes into the buffer (or
        <code class="literal">buffer.length</code> bytes for the version
        that just takes a byte array <code class="literal">buffer</code>), unless the end of the file is
        reached, in which case an <code class="literal">EOFException</code> is thrown.</p><p class="calibre2">All of these methods preserve the current offset in the file and
        are thread safe (although <code class="literal">FSDataInputStream</code> is not designed for
        concurrent access; therefore, it’s better to create multiple
        instances), so they provide a convenient way to access another part of
        the file—metadata, perhaps—while reading the main body of the
        file.</p><p class="calibre2">Finally, bear in mind that calling <code class="literal">seek()</code> is a relatively expensive operation
        and should be done sparingly. <br>You should structure your <u>application</u>
        <u style="
    text-decoration: underline 0.14em;
">access pattern</u>s<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;to rely on streaming data (by using MapReduce, for
        example) rather than performing a large number of <a class="calibre" id="calibre_link-3160"></a><a class="calibre" id="calibre_link-1658"></a><a class="calibre" id="calibre_link-2172"></a><a class="calibre" id="calibre_link-1750"></a>seeks.</p></div></div><div class="book" title="Writing Data"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-142">Writing Data</h3></div></div></div><p class="calibre2">The <code class="literal">FileSystem</code> class has
      <a class="calibre" id="calibre_link-1661"></a><a class="calibre" id="calibre_link-3813"></a><a class="calibre" id="calibre_link-2184"></a>a number of methods for creating a file. <br>The simplest is
      the method that <a class="calibre" id="calibre_link-2955"></a>takes a <code class="literal">Path</code> object for
      the file to be created and returns an output stream to write to:</p><a id="calibre_link-4039" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="n">FSDataOutputStream</code> <code class="nf">create</code><code class="o">(</code><code class="n">Path</code> <code class="n">f</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code></pre><p class="calibre2">There are overloaded versions of this method that allow you to
      specify whether to forcibly overwrite existing files, the replication
      factor of the file, the buffer size to use when writing the file, the
      block size for the file, and file permissions.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">The <code class="literal">create()</code> methods create
        any parent directories of the file to be written that don’t already
        exist. Though convenient, this behavior may be unexpected. If you want
        the write to fail when the parent directory doesn’t exist, you should
        check for the existence of the parent directory first by calling the
        <code class="literal">exists()</code> method. Alternatively, use
        <code class="literal">FileContext</code>, which allows you to control
        whether parent directories are created or not.</p></div><p class="calibre2">There’s also an overloaded method for passing a callback
      <a class="calibre" id="calibre_link-3065"></a>interface, <code class="literal">Progressable</code>, so your application can be
      notified of the progress of the data being written to the datanodes:</p><a id="calibre_link-4040" class="calibre"></a><pre class="screen1"><code class="k">package</code> <code class="n">org</code><code class="o">.</code><code class="na">apache</code><code class="o">.</code><code class="na">hadoop</code><code class="o">.</code><code class="na">util</code><code class="o">;</code>

<code class="k">public</code> <code class="k">interface</code> <code class="nc">Progressable</code> <code class="o">{</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">progress</code><code class="o">();</code>
<code class="o">}</code></pre><p class="calibre2">As an alternative to creating a new file, you can append to an
      existing file using the <code class="literal">append()</code> method (there
      are also some other overloaded versions):</p><a id="calibre_link-4041" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="n">FSDataOutputStream</code> <code class="nf">append</code><code class="o">(</code><code class="n">Path</code> <code class="n">f</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code></pre><p class="calibre2">The append operation allows a single writer to modify an already
      written file by opening it and writing data from the final offset in the
      file. With this API, applications that produce unbounded files, such as
      logfiles, can write to an existing file after having closed it. The
      append operation is optional and not implemented by all Hadoop
      filesystems. For example, HDFS supports append, but S3 filesystems
      don’t.</p><p class="calibre2"><a class="ulink" href="#calibre_link-180" title="Example&nbsp;3-4.&nbsp;Copying a local file to a Hadoop filesystem">Example&nbsp;3-4</a> shows how to copy a local
      file to a Hadoop filesystem. We illustrate progress by printing a period
      every time the <code class="literal">progress()</code> method is
      called by Hadoop, which is after each 64 KB packet of data is written to
      the datanode pipeline. (Note that this particular behavior is not
      specified by the API, so it is subject to change in later versions of
      Hadoop. The API merely allows you to infer that “something is
      happening.”)</p><div class="example"><a id="calibre_link-180" class="calibre"></a><div class="example-title">Example&nbsp;3-4.&nbsp;Copying a local file to a Hadoop filesystem</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">FileCopyWithProgress</code> <code class="o">{</code>
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">String</code> <code class="n">localSrc</code> <code class="o">=</code> <code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">];</code>
    <code class="n">String</code> <code class="n">dst</code> <code class="o">=</code> <code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">];</code>
    
    <code class="n">InputStream</code> <code class="n">in</code> <code class="o">=</code> <code class="k">new</code> <code class="n">BufferedInputStream</code><code class="o">(</code><code class="k">new</code> <code class="n">FileInputStream</code><code class="o">(</code><code class="n">localSrc</code><code class="o">));</code>
    
    <code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Configuration</code><code class="o">();</code>
    <code class="n">FileSystem</code> <code class="n">fs</code> <code class="o">=</code> <code class="n">FileSystem</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="n">URI</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">dst</code><code class="o">),</code> <code class="n">conf</code><code class="o">);</code>
    <code class="n">OutputStream</code> <code class="n">out</code> <code class="o">=</code> <code class="n">fs</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">dst</code><code class="o">),</code> <code class="k">new</code> <code class="n">Progressable</code><code class="o">()</code> <code class="o">{</code>
      <code class="k">public</code> <code class="kt">void</code> <code class="nf">progress</code><code class="o">()</code> <code class="o">{</code>
        <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">print</code><code class="o">(</code><code class="sb">"."</code><code class="o">);</code>
      <code class="o">}</code>
    <code class="o">});</code>
    
    <code class="n">IOUtils</code><code class="o">.</code><code class="na">copyBytes</code><code class="o">(</code><code class="n">in</code><code class="o">,</code> <code class="n">out</code><code class="o">,</code> <code class="mi">4096</code><code class="o">,</code> <code class="k">true</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">Typical usage:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">hadoop FileCopyWithProgress input/docs/1400-8.txt
hdfs://localhost/user/tom/1400-8.txt</code></strong>
.................</pre><p class="calibre2">Currently, none of the other Hadoop filesystems call
      <code class="literal">progress()</code> during writes. Progress is important
      in MapReduce applications, as you will see in later chapters.</p><div class="book" title="FSDataOutputStream"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4042">FSDataOutputStream</h4></div></div></div><p class="calibre2">The <code class="literal">create()</code> method
        <a class="calibre" id="calibre_link-1752"></a>on <code class="literal">FileSystem</code> returns
        an <code class="literal">FSDataOutputStream</code>, which, like
        <code class="literal">FSDataInputStream</code>, has a method for
        querying the current position in the file:</p><a id="calibre_link-4043" class="calibre"></a><pre class="screen1"><code class="k">package</code> <code class="n">org</code><code class="o">.</code><code class="na">apache</code><code class="o">.</code><code class="na">hadoop</code><code class="o">.</code><code class="na">fs</code><code class="o">;</code>

<code class="k">public</code> <code class="k">class</code> <code class="nc">FSDataOutputStream</code> <code class="k">extends</code> <code class="n">DataOutputStream</code> <code class="k">implements</code> <code class="n">Syncable</code> <code class="o">{</code>

  <code class="k">public</code> <code class="kt">long</code> <code class="nf">getPos</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="c2">// implementation elided</code>
  <code class="o">}</code>
  
  <code class="c2">// implementation elided</code>

<code class="o">}</code></pre><p class="calibre2">However, unlike <code class="literal">FSDataInputStream</code>, <code class="literal">FSDataOutputStream</code> does not permit seeking.
        This is because HDFS allows only sequential writes to an open file or
        appends to an already written file. In other words, there is no
        support for writing to anywhere other than the end of the file, so
        there is no value in being able to seek <a class="calibre" id="calibre_link-2185"></a><a class="calibre" id="calibre_link-3814"></a><a class="calibre" id="calibre_link-1662"></a>while writing.</p></div></div><div class="book" title="Directories"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4044">Directories</h3></div></div></div><p class="calibre2"><code class="literal">FileSystem</code> provides a
      <a class="calibre" id="calibre_link-1653"></a><a class="calibre" id="calibre_link-1500"></a><a class="calibre" id="calibre_link-2160"></a>method to create a directory:</p><a id="calibre_link-4045" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="kt">boolean</code> <code class="nf">mkdirs</code><code class="o">(</code><code class="n">Path</code> <code class="n">f</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code></pre><p class="calibre2">This method creates all of the necessary parent directories if
      they don’t already exist, just <a class="calibre" id="calibre_link-1621"></a>like the <code class="literal">java.io.File</code>’s
      <code class="literal">mkdirs()</code> method. It returns <code class="literal">true</code> if the directory (and all parent
      directories) was (were) successfully created.</p><p class="calibre2">Often, you don’t need to explicitly create a directory, because
      writing a file by calling <code class="literal">create()</code>
      will automatically create any parent directories.</p></div><div class="book" title="Querying the Filesystem"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4046">Querying the Filesystem</h3></div></div></div><div class="book" title="File metadata: FileStatus"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4047">File metadata: FileStatus</h4></div></div></div><p class="calibre2">An important feature of any <a class="calibre" id="calibre_link-3106"></a><a class="calibre" id="calibre_link-2683"></a><a class="calibre" id="calibre_link-1648"></a><a class="calibre" id="calibre_link-3108"></a><a class="calibre" id="calibre_link-2168"></a><a class="calibre" id="calibre_link-1655"></a><a class="calibre" id="calibre_link-1508"></a>filesystem is the ability to navigate its directory
        structure and retrieve information about the files and directories
        that it stores. The <code class="literal">FileStatus</code>
        class encapsulates filesystem metadata for files and directories,
        including file length, block size, replication, modification time,
        ownership, and permission information.</p><p class="calibre2">The method <code class="literal">getFileStatus()</code> on
        <code class="literal">FileSystem</code> provides a way of
        getting a <code class="literal">File</code><code class="literal">Status</code>
        object for a single file or directory. <a class="ulink" href="#calibre_link-181" title="Example&nbsp;3-5.&nbsp;Demonstrating file status information">Example&nbsp;3-5</a> shows an example of its use.</p><div class="example"><a id="calibre_link-181" class="calibre"></a><div class="example-title">Example&nbsp;3-5.&nbsp;Demonstrating file status information</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">ShowFileStatusTest</code> <code class="o">{</code>
  
  <code class="k">private</code> <code class="n">MiniDFSCluster</code> <code class="n">cluster</code><code class="o">;</code> <code class="c2">// use an in-process HDFS cluster for testing</code>
  <code class="k">private</code> <code class="n">FileSystem</code> <code class="n">fs</code><code class="o">;</code>

  <code class="nd">@Before</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">setUp</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Configuration</code><code class="o">();</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">System</code><code class="o">.</code><code class="na">getProperty</code><code class="o">(</code><code class="sb">"test.build.data"</code><code class="o">)</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">setProperty</code><code class="o">(</code><code class="sb">"test.build.data"</code><code class="o">,</code> <code class="sb">"/tmp"</code><code class="o">);</code>
    <code class="o">}</code>
    <code class="n">cluster</code> <code class="o">=</code> <code class="k">new</code> <code class="n">MiniDFSCluster</code><code class="o">.</code><code class="na">Builder</code><code class="o">(</code><code class="n">conf</code><code class="o">).</code><code class="na">build</code><code class="o">();</code>
    <code class="n">fs</code> <code class="o">=</code> <code class="n">cluster</code><code class="o">.</code><code class="na">getFileSystem</code><code class="o">();</code>
    <code class="n">OutputStream</code> <code class="n">out</code> <code class="o">=</code> <code class="n">fs</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="sb">"/dir/file"</code><code class="o">));</code>
    <code class="n">out</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="sb">"content"</code><code class="o">.</code><code class="na">getBytes</code><code class="o">(</code><code class="sb">"UTF-8"</code><code class="o">));</code>
    <code class="n">out</code><code class="o">.</code><code class="na">close</code><code class="o">();</code>
  <code class="o">}</code>
  
  <code class="nd">@After</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">tearDown</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">fs</code> <code class="o">!=</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code> <code class="n">fs</code><code class="o">.</code><code class="na">close</code><code class="o">();</code> <code class="o">}</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">cluster</code> <code class="o">!=</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code> <code class="n">cluster</code><code class="o">.</code><code class="na">shutdown</code><code class="o">();</code> <code class="o">}</code>
  <code class="o">}</code>
  
  <code class="nd">@Test</code><code class="o">(</code><code class="n">expected</code> <code class="o">=</code> <code class="n">FileNotFoundException</code><code class="o">.</code><code class="na">class</code><code class="o">)</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">throwsFileNotFoundForNonExistentFile</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="n">fs</code><code class="o">.</code><code class="na">getFileStatus</code><code class="o">(</code><code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="sb">"no-such-file"</code><code class="o">));</code>
  <code class="o">}</code>
  
  <code class="nd">@Test</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">fileStatusForFile</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="n">Path</code> <code class="n">file</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="sb">"/dir/file"</code><code class="o">);</code>
    <code class="n">FileStatus</code> <code class="n">stat</code> <code class="o">=</code> <code class="n">fs</code><code class="o">.</code><code class="na">getFileStatus</code><code class="o">(</code><code class="n">file</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">getPath</code><code class="o">().</code><code class="na">toUri</code><code class="o">().</code><code class="na">getPath</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"/dir/file"</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">isDirectory</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="k">false</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">getLen</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="mi">7L</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">getModificationTime</code><code class="o">(),</code>
        <code class="n">is</code><code class="o">(</code><code class="n">lessThanOrEqualTo</code><code class="o">(</code><code class="n">System</code><code class="o">.</code><code class="na">currentTimeMillis</code><code class="o">())));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">getReplication</code><code class="o">(),</code> <code class="n">is</code><code class="o">((</code><code class="kt">short</code><code class="o">)</code> <code class="mi">1</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">getBlockSize</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="mi">128</code> <code class="o">*</code> <code class="mi">1024</code> <code class="o">*</code> <code class="mi">1024L</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">getOwner</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="n">System</code><code class="o">.</code><code class="na">getProperty</code><code class="o">(</code><code class="sb">"user.name"</code><code class="o">)));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">getGroup</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"supergroup"</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">getPermission</code><code class="o">().</code><code class="na">toString</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"rw-r--r--"</code><code class="o">));</code>
  <code class="o">}</code>
  
  <code class="nd">@Test</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">fileStatusForDirectory</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="n">Path</code> <code class="n">dir</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="sb">"/dir"</code><code class="o">);</code>
    <code class="n">FileStatus</code> <code class="n">stat</code> <code class="o">=</code> <code class="n">fs</code><code class="o">.</code><code class="na">getFileStatus</code><code class="o">(</code><code class="n">dir</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">getPath</code><code class="o">().</code><code class="na">toUri</code><code class="o">().</code><code class="na">getPath</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"/dir"</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">isDirectory</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="k">true</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">getLen</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="mi">0L</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">getModificationTime</code><code class="o">(),</code>
        <code class="n">is</code><code class="o">(</code><code class="n">lessThanOrEqualTo</code><code class="o">(</code><code class="n">System</code><code class="o">.</code><code class="na">currentTimeMillis</code><code class="o">())));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">getReplication</code><code class="o">(),</code> <code class="n">is</code><code class="o">((</code><code class="kt">short</code><code class="o">)</code> <code class="mi">0</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">getBlockSize</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="mi">0L</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">getOwner</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="n">System</code><code class="o">.</code><code class="na">getProperty</code><code class="o">(</code><code class="sb">"user.name"</code><code class="o">)));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">getGroup</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"supergroup"</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">stat</code><code class="o">.</code><code class="na">getPermission</code><code class="o">().</code><code class="na">toString</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"rwxr-xr-x"</code><code class="o">));</code>
  <code class="o">}</code>
  
<code class="o">}</code></pre></div></div><p class="calibre2">If no file or directory exists, a <code class="literal">FileNotFoundException</code> is thrown. However, if
        you are interested only in the existence of a file or directory, the
        <code class="literal">exists()</code> method on <code class="literal">File</code><code class="literal">System</code> is more <a class="calibre" id="calibre_link-2684"></a><a class="calibre" id="calibre_link-3107"></a>convenient:</p><a id="calibre_link-4048" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="kt">boolean</code> <code class="nf">exists</code><code class="o">(</code><code class="n">Path</code> <code class="n">f</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code></pre></div><div class="book" title="Listing files"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4049">Listing files</h4></div></div></div><p class="calibre2">Finding information on a <a class="calibre" id="calibre_link-1630"></a><a class="calibre" id="calibre_link-2330"></a>single file or directory is useful, but you also often
        need to be able to list the contents of a directory. That’s what
        <code class="literal">FileSystem</code>’s <code class="literal">listStatus()</code> methods are for:</p><a id="calibre_link-4050" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="n">FileStatus</code><code class="o">[]</code> <code class="nf">listStatus</code><code class="o">(</code><code class="n">Path</code> <code class="n">f</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code>
<code class="k">public</code> <code class="n">FileStatus</code><code class="o">[]</code> <code class="nf">listStatus</code><code class="o">(</code><code class="n">Path</code> <code class="n">f</code><code class="o">,</code> <code class="n">PathFilter</code> <code class="n">filter</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code>
<code class="k">public</code> <code class="n">FileStatus</code><code class="o">[]</code> <code class="nf">listStatus</code><code class="o">(</code><code class="n">Path</code><code class="o">[]</code> <code class="n">files</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code>
<code class="k">public</code> <code class="n">FileStatus</code><code class="o">[]</code> <code class="nf">listStatus</code><code class="o">(</code><code class="n">Path</code><code class="o">[]</code> <code class="n">files</code><code class="o">,</code> <code class="n">PathFilter</code> <code class="n">filter</code><code class="o">)</code> 
    <code class="k">throws</code> <code class="n">IOException</code></pre><p class="calibre2">When the argument is a file, the simplest variant returns an
        array of <code class="literal">FileStatus</code> objects of
        length 1. When the argument is a directory, it returns zero or more
        <code class="literal">FileStatus</code> objects representing the
        files and directories contained in the directory.</p><p class="calibre2">Overloaded variants allow a <code class="literal">PathFilter</code> to be supplied <a class="calibre" id="calibre_link-2957"></a>to restrict the files and directories to match. You will
        see an example of this in the section <a class="ulink" href="#calibre_link-182" title="PathFilter">PathFilter</a>.
        Finally, if you specify an array of paths, the result is a shortcut
        for calling the equivalent single-path <code class="literal">listStatus()</code> method for each path in turn
        and accumulating the <code class="literal">File</code><code class="literal">Status</code>
        object arrays in a single array. This can be useful for
        building up lists of input files to process from distinct parts of the
        filesystem tree. <a class="ulink" href="#calibre_link-183" title="Example&nbsp;3-6.&nbsp;Showing the file statuses for a collection of paths in a Hadoop filesystem">Example&nbsp;3-6</a> is a simple
        demonstration of this idea. Note the use of <code class="literal">stat2Paths()</code> in Hadoop’s <code class="literal">FileUtil</code> for <a class="calibre" id="calibre_link-1690"></a>turning an array of <code class="literal">FileStatus</code> objects into an array of <code class="literal">Path</code> objects.</p><div class="example"><a id="calibre_link-183" class="calibre"></a><div class="example-title">Example&nbsp;3-6.&nbsp;Showing the file statuses for a collection of paths in a
          Hadoop filesystem</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">ListStatus</code> <code class="o">{</code>

  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">String</code> <code class="n">uri</code> <code class="o">=</code> <code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">];</code>
    <code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Configuration</code><code class="o">();</code>
    <code class="n">FileSystem</code> <code class="n">fs</code> <code class="o">=</code> <code class="n">FileSystem</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="n">URI</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">uri</code><code class="o">),</code> <code class="n">conf</code><code class="o">);</code>
    
    <code class="n">Path</code><code class="o">[]</code> <code class="n">paths</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">[</code><code class="n">args</code><code class="o">.</code><code class="na">length</code><code class="o">];</code>
    <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="n">paths</code><code class="o">.</code><code class="na">length</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>
      <code class="n">paths</code><code class="o">[</code><code class="n">i</code><code class="o">]</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="n">i</code><code class="o">]);</code>
    <code class="o">}</code>
    
    <code class="n">FileStatus</code><code class="o">[]</code> <code class="n">status</code> <code class="o">=</code> <code class="n">fs</code><code class="o">.</code><code class="na">listStatus</code><code class="o">(</code><code class="n">paths</code><code class="o">);</code>
    <code class="n">Path</code><code class="o">[]</code> <code class="n">listedPaths</code> <code class="o">=</code> <code class="n">FileUtil</code><code class="o">.</code><code class="na">stat2Paths</code><code class="o">(</code><code class="n">status</code><code class="o">);</code>
    <code class="k">for</code> <code class="o">(</code><code class="n">Path</code> <code class="n">p</code> <code class="o">:</code> <code class="n">listedPaths</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="n">p</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">We can use this program to find the union of directory listings
        for a collection of paths:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">hadoop ListStatus hdfs://localhost/ hdfs://localhost/user/tom</code></strong>
hdfs://localhost/user
hdfs://localhost/user/tom/books
hdfs://localhost/user/tom/quangle.txt</pre></div><div class="book" title="File patterns"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-578">File patterns</h4></div></div></div><p class="calibre2">It is a common requirement to process <a class="calibre" id="calibre_link-1625"></a>sets of files in a single operation. For example, a
        MapReduce job for log processing might analyze a month’s worth of
        files contained in a number of directories. Rather than having to
        enumerate each file and directory to specify the input, it is
        convenient to use wildcard characters to match multiple files with a
        single expression, an operation that is <a class="calibre" id="calibre_link-1800"></a>known as <em class="calibre10">globbing</em>. Hadoop
        provides two <code class="literal">FileSystem</code> methods for processing
        globs:</p><a id="calibre_link-4051" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="n">FileStatus</code><code class="o">[]</code> <code class="nf">globStatus</code><code class="o">(</code><code class="n">Path</code> <code class="n">pathPattern</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code>
<code class="k">public</code> <code class="n">FileStatus</code><code class="o">[]</code> <code class="nf">globStatus</code><code class="o">(</code><code class="n">Path</code> <code class="n">pathPattern</code><code class="o">,</code> <code class="n">PathFilter</code> <code class="n">filter</code><code class="o">)</code> 
    <code class="k">throws</code> <code class="n">IOException</code></pre><p class="calibre2">The <code class="literal">globStatus()</code> methods
        return an array of <code class="literal">FileStatus</code>
        objects whose paths match the supplied pattern, sorted by path. An
        optional <code class="literal">PathFilter</code> can be
        specified to restrict the matches further.</p><p class="calibre2">Hadoop supports the same set of glob characters as the Unix bash
        shell (see <a class="ulink" href="#calibre_link-184" title="Table&nbsp;3-2.&nbsp;Glob characters and their meanings">Table&nbsp;3-2</a>).</p><div class="table"><a id="calibre_link-184" class="calibre"></a><div class="table-title">Table&nbsp;3-2.&nbsp;Glob characters and their meanings</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Glob</td><td class="calibre20">Name</td><td class="calibre21">Matches</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">*</code></td><td class="calibre23"><em class="calibre10">asterisk</em></td><td class="calibre25">Matches zero or more characters</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">?</code></td><td class="calibre23"><em class="calibre10">question mark</em></td><td class="calibre25">Matches a single character</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">[ab]</code></td><td class="calibre23"><em class="calibre10">character class</em></td><td class="calibre25">Matches a single character in the set <code class="uri">{a, b}</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">[^ab]</code></td><td class="calibre23"><em class="calibre10"> negated
                character class </em></td><td class="calibre25">Matches a single character that is not in the set
                <code class="uri">{a, b}</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">[a-b]</code></td><td class="calibre23"><em class="calibre10">character range</em></td><td class="calibre25">Matches a single character in the (closed) range
                <code class="uri">[a, b]</code>, where <code class="uri">a</code> is lexicographically less than or
                equal to <code class="uri">b</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">[^a-b]</code></td><td class="calibre23"><em class="calibre10">negated character range</em></td><td class="calibre25">Matches a single character that is not in the (closed)
                range <code class="uri">[a, b]</code>, where
                <code class="uri">a</code> is lexicographically less
                than or equal to <code class="uri">b</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">{a,b}</code></td><td class="calibre23"><em class="calibre10">alternation</em></td><td class="calibre25">Matches either expression <code class="uri">a</code> or <code class="uri">b</code></td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">\c</code></td><td class="calibre27"><em class="calibre10">escaped character</em></td><td class="calibre28">Matches character <code class="uri">c</code>
                when it is a metacharacter</td></tr></tbody></table></div></div><p class="calibre2">Imagine that logfiles are stored in a directory structure
        organized hierarchically by date. So,
        logfiles for the last day of 2007 would go in a directory
        named <em class="calibre10">/2007/12/31</em>, for example.
        Suppose that the full file listing is:</p><pre class="screen1">/
├── 2007/
│&nbsp;&nbsp; └── 12/
│&nbsp;&nbsp;     ├── 30/
│&nbsp;&nbsp;     └── 31/
└── 2008/
    └── 01/
        ├── 01/
        └── 02/</pre><p class="calibre2">Here are some file globs and their <a class="calibre" id="calibre_link-1649"></a>expansions:</p><div class="informaltable"><table class="calibre31"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre11"><td class="calibre32">Glob</td><td class="calibre33">Expansion</td></tr></thead><tbody class="calibre22"><tr class="calibre11"><td class="calibre32"><code class="uri">/*</code></td><td class="calibre33"><em class="calibre10">/2007 </em> <em class="calibre10">/2008</em></td></tr><tr class="calibre13"><td class="calibre32"><code class="uri">/*/*</code></td><td class="calibre33"><em class="calibre10">/2007/12 </em>
                <em class="calibre10">/2008/01</em></td></tr><tr class="calibre11"><td class="calibre32"><code class="uri">/*/12/*</code></td><td class="calibre33"><em class="calibre10">/2007/12/30 </em>
                <em class="calibre10">/2007/12/31</em></td></tr><tr class="calibre13"><td class="calibre32"><code class="uri">/200?</code></td><td class="calibre33"><em class="calibre10">/2007</em> <em class="calibre10">/2008</em></td></tr><tr class="calibre11"><td class="calibre32"><code class="uri">/200[78]</code></td><td class="calibre33"><em class="calibre10">/2007</em> <em class="calibre10">/2008</em></td></tr><tr class="calibre13"><td class="calibre32"><code class="uri">/200[7-8]</code></td><td class="calibre33"><em class="calibre10">/2007</em> <em class="calibre10">/2008</em></td></tr><tr class="calibre11"><td class="calibre32"><code class="uri">/200[^01234569]</code></td><td class="calibre33"><em class="calibre10">/2007</em> <em class="calibre10">/2008</em></td></tr><tr class="calibre13"><td class="calibre32"><code class="uri">/*/*/{31,01}</code></td><td class="calibre33"><em class="calibre10">/2007/12/31</em>
                <em class="calibre10">/2008/01/01</em></td></tr><tr class="calibre11"><td class="calibre32"><code class="uri">/*/*/3{0,1}</code></td><td class="calibre33"><em class="calibre10">/2007/12/30</em>
                <em class="calibre10">/2007/12/31</em></td></tr><tr class="calibre13"><td class="calibre34"><code class="uri">/*/{12/31,01/01}</code></td><td class="calibre35"><em class="calibre10">/2007/12/31</em>
                <em class="calibre10"> /2008/01/01</em></td></tr></tbody></table></div></div><div class="book" title="PathFilter"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-182">PathFilter</h4></div></div></div><p class="calibre2">Glob patterns are not always powerful enough to describe a set
        of files you want to access. For example, it is not generally possible
        to exclude a particular file using a glob pattern. The <code class="literal">listStatus()</code> and <code class="literal">globStatus()</code> methods of <code class="literal">FileSystem</code> take an optional <code class="literal">PathFilter</code>, which allows programmatic
        control over matching:</p><a id="calibre_link-4052" class="calibre"></a><pre class="screen1"><code class="k">package</code> <code class="n">org</code><code class="o">.</code><code class="na">apache</code><code class="o">.</code><code class="na">hadoop</code><code class="o">.</code><code class="na">fs</code><code class="o">;</code>

<code class="k">public</code> <code class="k">interface</code> <code class="nc">PathFilter</code> <code class="o">{</code>
  <code class="kt">boolean</code> <code class="nf">accept</code><code class="o">(</code><code class="n">Path</code> <code class="n">path</code><code class="o">);</code>
<code class="o">}</code></pre><p class="calibre2"><code class="literal">PathFilter</code> is the equivalent
        of <code class="literal">java.io.FileFilter</code> for <code class="literal">Path</code> objects <a class="calibre" id="calibre_link-1637"></a>rather than <code class="literal">File</code>
        objects.</p><p class="calibre2"><a class="ulink" href="#calibre_link-185" title="Example&nbsp;3-7.&nbsp;A PathFilter for excluding paths that match a regular expression">Example&nbsp;3-7</a> shows a <code class="literal">PathFilter</code> for excluding paths that match a
        regular expression.</p><div class="example"><a id="calibre_link-185" class="calibre"></a><div class="example-title">Example&nbsp;3-7.&nbsp;A PathFilter for excluding paths that match a regular
          expression</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">RegexExcludePathFilter</code> <code class="k">implements</code> <code class="n">PathFilter</code> <code class="o">{</code>
  
  <code class="k">private</code> <code class="k">final</code> <code class="n">String</code> <code class="n">regex</code><code class="o">;</code>

  <code class="k">public</code> <code class="nf">RegexExcludePathFilter</code><code class="o">(</code><code class="n">String</code> <code class="n">regex</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">this</code><code class="o">.</code><code class="na">regex</code> <code class="o">=</code> <code class="n">regex</code><code class="o">;</code>
  <code class="o">}</code>

  <code class="k">public</code> <code class="kt">boolean</code> <code class="nf">accept</code><code class="o">(</code><code class="n">Path</code> <code class="n">path</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">return</code> <code class="o">!</code><code class="n">path</code><code class="o">.</code><code class="na">toString</code><code class="o">().</code><code class="na">matches</code><code class="o">(</code><code class="n">regex</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The filter passes only those files that
        <span class="calibre"><em class="calibre10">don’t</em></span> match the regular expression. After the
        glob picks out an initial set of files to include, the filter is used
        to refine the results. For example:</p><a id="calibre_link-4053" class="calibre"></a><pre class="screen1"><code class="n">fs</code><code class="o">.</code><code class="na">globStatus</code><code class="o">(</code><code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="sb">"/2007/*/*"</code><code class="o">),</code> <code class="k">new</code> <code class="n">RegexExcludeFilter</code><code class="o">(</code><code class="sb">"^.*/2007/12/31$"</code><code class="o">))</code></pre><p class="calibre2">will expand to <em class="calibre10">/2007/12/30</em>.</p><p class="calibre2">Filters can act only on a file’s name, as represented by a
        <code class="literal">Path</code>. They can’t use a file’s
        properties, such as creation time, as their basis. Nevertheless, they
        can perform matching that neither glob patterns nor regular
        expressions can achieve. For example, if you store files in a
        directory structure that is laid out by date (like in the previous
        section), you can write a <code class="literal">PathFilter</code> to pick out files that fall in a
        given date <a class="calibre" id="calibre_link-2169"></a><a class="calibre" id="calibre_link-1656"></a><a class="calibre" id="calibre_link-1509"></a><a class="calibre" id="calibre_link-3109"></a><a class="calibre" id="calibre_link-2958"></a>range.</p></div></div><div class="book" title="Deleting Data"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4054">Deleting Data</h3></div></div></div><p class="calibre2">Use <a class="calibre" id="calibre_link-1654"></a><a class="calibre" id="calibre_link-2161"></a>the <code class="literal">delete()</code> method on
      <code class="literal">FileSystem</code> to permanently remove
      files or directories:</p><a id="calibre_link-4055" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="kt">boolean</code> <code class="nf">delete</code><code class="o">(</code><code class="n">Path</code> <code class="n">f</code><code class="o">,</code> <code class="kt">boolean</code> <code class="n">recursive</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code></pre><p class="calibre2">If <code class="literal">f</code> is a file or an empty
      directory, the value of <code class="literal">recursive</code> is
      ignored. A nonempty directory is deleted, along with its contents, only
      if <code class="literal">recursive</code> is <code class="literal">true</code> (otherwise, <a class="calibre" id="calibre_link-1949"></a><a class="calibre" id="calibre_link-1681"></a>an <code class="literal">IOException</code> is
      thrown).</p></div></div><div class="book" title="Data Flow"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4056">Data Flow</h2></div></div></div><div class="book" title="Anatomy of a File Read"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4057">Anatomy of a File Read</h3></div></div></div><p class="calibre2">To get an idea of how data <a class="calibre" id="calibre_link-1954"></a><a class="calibre" id="calibre_link-3163"></a>flows between the client interacting with HDFS, the
      namenode, and the datanodes, consider <a class="ulink" href="#calibre_link-186" title="Figure&nbsp;3-2.&nbsp;A client reading data from HDFS">Figure&nbsp;3-2</a>, which shows the main sequence of
      events when reading a file.</p><div class="book"><div class="figure"><a id="calibre_link-186" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4058" class="calibre"></a><img alt="A client reading data from HDFS" src="images/000071.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;3-2.&nbsp;A client reading data from HDFS</div></div></div><p class="calibre2">The client opens the file it wishes to read by calling <code class="literal">open()</code> on the <code class="literal">FileSystem</code> object, <a class="calibre" id="calibre_link-1659"></a><a class="calibre" id="calibre_link-3161"></a>which for HDFS is an <a class="calibre" id="calibre_link-1525"></a>instance of <code class="literal">DistributedFileSystem</code> (step 1 in <a class="ulink" href="#calibre_link-186" title="Figure&nbsp;3-2.&nbsp;A client reading data from HDFS">Figure&nbsp;3-2</a>). <code class="literal">DistributedFileSystem</code> calls the namenode,
      using remote procedure calls (RPCs), to determine the locations of the
      first few blocks in the file (step 2). For each block, the namenode
      returns the addresses of the datanodes that have a copy of that block.
      Furthermore, the datanodes are
      sorted according to their proximity to the client (according to the
      topology of the cluster’s network; see <a class="ulink" href="#calibre_link-26" title="Network Topology and Hadoop">Network Topology and Hadoop</a>). If the client is itself a datanode
      (in the case of a MapReduce task, for instance), the client will read
      from the local datanode if that datanode hosts a copy of the block (see
      also <a class="ulink" href="#calibre_link-187" title="Figure&nbsp;2-2.&nbsp;Data-local (a), rack-local (b), and off-rack (c) map tasks">Figure&nbsp;2-2</a> and <a class="ulink" href="#calibre_link-188" title="Short-circuit local reads">Short-circuit local reads</a>).</p><p class="calibre2">The <code class="literal">DistributedFileSystem</code>
      returns an <code class="literal">FSDataInputStream</code> (an
      input <a class="calibre" id="calibre_link-1751"></a>stream that supports file seeks) to the client for it to
      read data from. <code class="literal">FSDataInputStream</code> in
      turn wraps a <code class="literal">DFSInputStream</code>, which
      manages the <a class="calibre" id="calibre_link-1385"></a>datanode and namenode I/O.</p><p class="calibre2">The client then calls <code class="literal">read()</code> on
      the stream (step 3). <code class="literal">DFSInputStream</code>,
      which has stored the datanode addresses for the first few blocks in the
      file, then connects to the first (closest) datanode for the first block
      in the file. Data is streamed from the datanode back to the client,
      which calls <code class="literal">read()</code> repeatedly on the
      stream (step 4). When the end of the block is <a class="calibre" id="calibre_link-1489"></a>reached, <code class="literal">DFSInputStream</code>
      will close the connection to the datanode, then find the best datanode
      for the next block (step 5). This happens transparently to the client,
      which from its point of view is just reading a continuous stream.</p><p class="calibre2">Blocks are read in order, with the <code class="literal">DFSInputStream</code> opening new connections to
      datanodes as the client reads
      through the stream. It will also call the namenode to retrieve the
      datanode locations for the next batch of blocks as needed. When the
      client has finished reading, it calls <code class="literal">close()</code>
      on the <code class="literal">FSDataInputStream</code> (step
      6).</p><p class="calibre2">During reading, if the <code class="literal">DFSInputStream</code> encounters an error while
      communicating with a datanode, it will try the next closest one for that
      block. It will also remember datanodes that have failed so that it
      doesn’t needlessly retry them for later blocks. The <code class="literal">DFSInputStream</code> also verifies checksums for the
      data transferred to it from the datanode. If a corrupted block is found,
      the <code class="literal">DFSInputStream</code> attempts to read a
      replica of the block from another datanode; it also reports the
      corrupted block to the <a class="calibre" id="calibre_link-2752"></a>namenode.</p><p class="calibre2">One important aspect of this design is that the client contacts
      datanodes directly to retrieve data and is guided by the namenode to the
      best datanode for each block. This design allows HDFS to scale to a
      large number of concurrent clients because the data traffic is spread
      across all the datanodes in the cluster. Meanwhile, the namenode merely
      has to service block location requests (which it stores in memory,
      making them very efficient) and does not, for example, serve data, which
      would quickly become a bottleneck as the number of clients <a class="calibre" id="calibre_link-1955"></a><a class="calibre" id="calibre_link-3164"></a>grew.</p><div class="sidebar"><a id="calibre_link-26" class="calibre"></a><div class="sidebar-title">Network Topology and Hadoop</div><p class="calibre2">What does it mean for two <a class="calibre" id="calibre_link-2785"></a><a class="calibre" id="calibre_link-978"></a>nodes in a local network to be “close” to each other? In
        the context of high-volume data processing, the limiting factor is the
        rate at which we can transfer data between nodes—bandwidth is a scarce
        commodity. The idea is to use the bandwidth between two nodes as a
        measure of distance.</p><p class="calibre2">Rather than measuring bandwidth between nodes, which can be
        difficult to do in practice (it requires a quiet cluster, and the
        number of pairs of nodes in a cluster grows as the square of the
        number of nodes), Hadoop takes a simple approach in which the network
        is represented as a tree and the distance between two nodes is the sum
        of their distances to their closest common ancestor. Levels in the
        tree are not predefined, but it is common to have levels that
        correspond to the data center, the rack, and the node that a process
        is running on. The idea is that the bandwidth available for each of
        the following scenarios becomes progressively less:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">Processes on the same node</p></li><li class="listitem"><p class="calibre2">Different nodes on the same rack</p></li><li class="listitem"><p class="calibre2">Nodes on different racks in the same data center</p></li><li class="listitem"><p class="calibre2">Nodes in different data centers<sup class="calibre5">[<a class="firstname" type="noteref" href="#calibre_link-189" id="calibre_link-204">32</a>]</sup></p></li></ul></div><p class="calibre2">For example, imagine a node <span class="calibre"><em class="calibre10">n1</em></span> on rack
        <span class="calibre"><em class="calibre10">r1</em></span> in data center <span class="calibre"><em class="calibre10">d1</em></span>. This
        can be represented as <span class="calibre"><em class="calibre10">/d1/r1/n1</em></span>. Using this
        notation, here are the distances for the four scenarios:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2"><span class="calibre"><em class="calibre10">distance(/d1/r1/n1, /d1/r1/n1)</em></span> = 0
            (processes on the same node)</p></li><li class="listitem"><p class="calibre2"><span class="calibre"><em class="calibre10">distance(/d1/r1/n1, /d1/r1/n2)</em></span> = 2
            (different nodes on the same rack)</p></li><li class="listitem"><p class="calibre2"><span class="calibre"><em class="calibre10">distance(/d1/r1/n1, /d1/r2/n3)</em></span> = 4
            (nodes on different racks in the same data center)</p></li><li class="listitem"><p class="calibre2"><span class="calibre"><em class="calibre10">distance(/d1/r1/n1, /d2/r3/n4)</em></span> = 6
            (nodes in different data centers)</p></li></ul></div><p class="calibre2">This is illustrated schematically in <a class="ulink" href="#calibre_link-190" title="Figure&nbsp;3-3.&nbsp;Network distance in Hadoop">Figure&nbsp;3-3</a>. (Mathematically inclined readers will
        notice that this is an example of a distance metric.)</p><div class="book"><div class="figure1"><a id="calibre_link-190" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4059" class="calibre"></a><img alt="Network distance in Hadoop" src="images/000080.png" class="calibre29"></div></div><div class="figure-title1">Figure&nbsp;3-3.&nbsp;Network distance in Hadoop</div></div></div><p class="calibre2">Finally, it is important to realize that Hadoop cannot magically
        discover your network topology for you; it needs some help (we’ll
        cover how to configure topology in <a class="ulink" href="#calibre_link-191" title="Network Topology">Network Topology</a>). By default, though, it
        assumes that the network is flat—a single-level hierarchy—or in other
        words, that all nodes are on a single rack in a single data center.
        For small clusters, this may actually be the case, and no further
        configuration is required.</p></div></div><div class="book" title="Anatomy of a File Write"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4060">Anatomy of a File Write</h3></div></div></div><p class="calibre2">Next we’ll look <a class="calibre" id="calibre_link-1965"></a><a class="calibre" id="calibre_link-3815"></a>at how files are written to HDFS. Although quite detailed,
      it is instructive to understand the data flow because it clarifies
      HDFS’s coherency model.</p><p class="calibre2">We’re going to consider the case of creating a new file, writing
      data to it, then closing the file. This is illustrated in <a class="ulink" href="#calibre_link-192" title="Figure&nbsp;3-4.&nbsp;A client writing data to HDFS">Figure&nbsp;3-4</a>.</p><div class="book"><div class="figure"><a id="calibre_link-192" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4061" class="calibre"></a><img alt="A client writing data to HDFS" src="images/000086.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;3-4.&nbsp;A client writing data to HDFS</div></div></div><p class="calibre2">The client creates the <a class="calibre" id="calibre_link-1526"></a>file by calling <code class="literal">create()</code> on <code class="literal">DistributedFileSystem</code> (step 1 in <a class="ulink" href="#calibre_link-192" title="Figure&nbsp;3-4.&nbsp;A client writing data to HDFS">Figure&nbsp;3-4</a>). <code class="literal">DistributedFileSystem</code> makes an RPC call to the
      namenode to create a new file in the filesystem’s namespace, with no
      blocks associated with it (step 2). The namenode performs various checks
      to make sure the file doesn’t already exist and that the client has the
      right permissions to create the file. If these checks pass, the namenode
      makes a record of the new file; otherwise, file creation fails and the
      client is thrown an <code class="literal">IOException</code>. The
      <code class="literal">DistributedFileSystem</code> returns an
      <code class="literal">FSDataOutputStream</code> for <a class="calibre" id="calibre_link-1753"></a>the client to start writing data to. Just as in the read
      case, <code class="literal">FSDataOutputStream</code> wraps a
      <code class="literal">DFSOutputStream</code>, which handles
      communication with the datanodes and namenode.</p><p class="calibre2">As the client writes data <a class="calibre" id="calibre_link-1490"></a>(step 3), the <code class="literal">DFSOutputStream</code> splits it into packets, which
      it writes to an internal queue called <a class="calibre" id="calibre_link-1343"></a>the <em class="calibre10">data queue</em>. The data queue is
      consumed by <a class="calibre" id="calibre_link-1394"></a>the <code class="literal">DataStreamer</code>, which
      is responsible for asking the <a class="calibre" id="calibre_link-2749"></a><a class="calibre" id="calibre_link-1382"></a>namenode to allocate new blocks by picking a list of
      suitable datanodes to store the replicas. The list of datanodes forms a
      pipeline, and here we’ll assume the replication level is three, so there
      are three nodes in the pipeline. The <code class="literal">DataStreamer</code> streams the packets to the first
      datanode in the pipeline, which stores each packet and forwards it to
      the second datanode in the pipeline. Similarly, the second datanode
      stores the packet and forwards it to the third (and last) datanode in
      the pipeline (step 4).</p><p class="calibre2">The <code class="literal">DFSOutputStream</code> also
      maintains an internal queue of packets that are waiting to be
      acknowledged by datanodes, called the <em class="calibre10">ack queue</em>.
      A packet is removed from the ack queue only when it has been
      acknowledged by all the datanodes in the pipeline (step 5).</p><p class="calibre2">If any datanode fails while data is being written to it, then the
      following actions are taken, which are transparent to the client writing
      the data. First, the pipeline is
      closed, and any packets in the ack queue are added to the front of the
      data queue so that datanodes that
      are downstream from the failed node will not miss any packets. The
      current block on the good datanodes is given a new identity, which
      is communicated to the namenode, so that the partial block on the failed
      datanode will be deleted if the failed datanode recovers later on. The
      failed datanode is removed from the pipeline, and a new pipeline is
      constructed from the two good datanodes. The remainder of the block’s data is written
      to the good datanodes in the pipeline. The namenode notices that the
      block is under-replicated, and it arranges for a further replica to be
      created on another node. Subsequent blocks are then treated as
      normal.</p><p class="calibre2">It’s possible, but unlikely, for multiple datanodes to fail while
      a block is being written. As long as <code class="literal">dfs.namenode.replication.min</code> replicas (which
      defaults to 1) are written, the write will succeed, and the block will
      be asynchronously replicated across the cluster until its target
      replication factor is reached (<code class="literal">dfs.replication</code>, which defaults to 3).</p><p class="calibre2">When the client has finished writing data, it calls <code class="literal">close()</code> on the stream (step 6). This action
      flushes all the remaining packets to the datanode pipeline and waits for
      acknowledgments before contacting the namenode to signal that the file
      is complete (step 7). The namenode already knows which blocks the file
      is made up of (because <code class="literal">DataStreamer</code>
      asks for block allocations), so it only has to wait for blocks to be
      minimally replicated before returning <a class="calibre" id="calibre_link-1966"></a><a class="calibre" id="calibre_link-3816"></a><a class="calibre" id="calibre_link-1491"></a><a class="calibre" id="calibre_link-1395"></a>successfully.</p><div class="sidebar"><a id="calibre_link-10" class="calibre"></a><div class="sidebar-title">Replica Placement</div><p class="calibre2">How does the namenode choose which <a class="calibre" id="calibre_link-2760"></a><a class="calibre" id="calibre_link-1390"></a>datanodes to store replicas on? There’s a trade-off
        between reliability and write bandwidth and read bandwidth here. For
        example, placing all replicas on a single node incurs the lowest write
        bandwidth penalty (since the replication pipeline runs on a single
        node), but this offers no real redundancy (if the node fails, the data
        for that block is lost). Also, the read bandwidth is high for off-rack
        reads. At the other extreme, placing replicas in different data
        centers may maximize redundancy, but at the cost of bandwidth. Even in
        the same data center (which is what all Hadoop clusters to date have
        run in), there are a variety of possible placement strategies.</p><p class="calibre2">Hadoop’s default strategy is to place the first replica on the
        same node as the client (for clients running outside the cluster, a
        node is chosen at random, although the system tries not to pick nodes
        that are too full or too busy). The second replica is placed on a
        different rack from the first (<em class="calibre10">off-rack</em>),
        chosen at random. The third replica is placed on the same rack as the
        second, but on a different node chosen at random. Further replicas are
        placed on random nodes in the cluster, although the system tries to
        avoid placing too many replicas on the same rack.</p><p class="calibre2">Once the replica locations have been chosen, a pipeline is
        built, <a class="calibre" id="calibre_link-2786"></a>taking network topology into account. For a replication
        factor of 3, the pipeline might look like <a class="ulink" href="#calibre_link-193" title="Figure&nbsp;3-5.&nbsp;A typical replica pipeline">Figure&nbsp;3-5</a>.</p><div class="figure1"><a id="calibre_link-193" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4062" class="calibre"></a><img alt="A typical replica pipeline" src="images/000038.png" class="calibre29"></div></div><div class="figure-title1">Figure&nbsp;3-5.&nbsp;A typical replica pipeline</div></div><p class="calibre2">Overall, this strategy gives a good balance among reliability
        (blocks are stored on two racks), write bandwidth (writes only have to
        traverse a single network switch), read performance (there’s a choice
        of two racks to read from), and block distribution across the cluster
        (clients only write a single block on the local rack).</p></div></div><div class="book" title="Coherency Model"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-269">Coherency Model</h3></div></div></div><p class="calibre2">A coherency model for a <a class="calibre" id="calibre_link-1934"></a><a class="calibre" id="calibre_link-1170"></a><a class="calibre" id="calibre_link-1673"></a>filesystem describes the data visibility of reads and
      writes for a file. HDFS trades off some POSIX requirements for
      performance, so some operations may behave differently than you expect
      them to.</p><p class="calibre2">After creating a file, it is visible in the filesystem namespace,
      as expected:</p><a id="calibre_link-4063" class="calibre"></a><pre class="screen1">    <code class="n">Path</code> <code class="n">p</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="sb">"p"</code><code class="o">);</code>
    <code class="n">fs</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">p</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">fs</code><code class="o">.</code><code class="na">exists</code><code class="o">(</code><code class="n">p</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="k">true</code><code class="o">));</code></pre><p class="calibre2">However, any content written to the file is not guaranteed to be
      visible, even if the stream is flushed. So, the file appears to have a
      length of zero:</p><a id="calibre_link-4064" class="calibre"></a><pre class="screen1">    <code class="n">Path</code> <code class="n">p</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="sb">"p"</code><code class="o">);</code>
    <code class="n">OutputStream</code> <code class="n">out</code> <code class="o">=</code> <code class="n">fs</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">p</code><code class="o">);</code>
    <code class="n">out</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="sb">"content"</code><code class="o">.</code><code class="na">getBytes</code><code class="o">(</code><code class="sb">"UTF-8"</code><code class="o">));</code>
    <span class="calibre24"><strong class="calibre9"><code class="n1">out</code><code class="o1">.</code><code class="na1">flush</code><code class="o1">();</code></strong></span>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">fs</code><code class="o">.</code><code class="na">getFileStatus</code><code class="o">(</code><code class="n">p</code><code class="o">).</code><code class="na">getLen</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="mi">0L</code><code class="o">));</code></pre><p class="calibre2">Once more than a block’s worth of data has been written, the first
      block will be visible to new readers. This is true of subsequent blocks,
      too: it is always the current block being written that is not visible to
      other readers.</p><p class="calibre2">HDFS provides a way to force all buffers to be flushed to the
      datanodes via the <code class="literal">hflush()</code> method
      <a class="calibre" id="calibre_link-1754"></a>on <code class="literal">FSDataOutputStream</code>.
      After a successful return from <code class="literal">hflush()</code>, HDFS guarantees that the data
      written up to that point in the file has reached all the datanodes in
      the write pipeline and is visible to all new readers:</p><a id="calibre_link-4065" class="calibre"></a><pre class="screen1">    <code class="n">Path</code> <code class="n">p</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="sb">"p"</code><code class="o">);</code>
    <code class="n">FSDataOutputStream</code> <code class="n">out</code> <code class="o">=</code> <code class="n">fs</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">p</code><code class="o">);</code>
    <code class="n">out</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="sb">"content"</code><code class="o">.</code><code class="na">getBytes</code><code class="o">(</code><code class="sb">"UTF-8"</code><code class="o">));</code>
    <span class="calibre24"><strong class="calibre9"><code class="n1">out</code><code class="o1">.</code><code class="na1">hflush</code><code class="o1">();</code></strong></span>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">fs</code><code class="o">.</code><code class="na">getFileStatus</code><code class="o">(</code><code class="n">p</code><code class="o">).</code><code class="na">getLen</code><code class="o">(),</code> <code class="n">is</code><code class="o">(((</code><code class="kt">long</code><code class="o">)</code> <code class="sb">"content"</code><code class="o">.</code><code class="na">length</code><code class="o">())));</code></pre><p class="calibre2">Note that <code class="literal">hflush()</code> does not guarantee
      that the datanodes have written the data to disk, only that it’s in the
      datanodes’ memory (so in the event of a data center power outage, for
      example, data could be lost). For this stronger guarantee, use <code class="literal">hsync()</code> instead.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-194" id="calibre_link-205">33</a>]</sup></p><p class="calibre2">The behavior of <code class="literal">hsync()</code> is
      similar to that of the <code class="literal">fsync()</code> system
      call in POSIX that commits buffered data for a file descriptor. For
      example, using the standard Java API to write a local file, we are
      guaranteed to see the content after flushing the stream and
      synchronizing:</p><a id="calibre_link-4066" class="calibre"></a><pre class="screen1">    <code class="n">FileOutputStream</code> <code class="n">out</code> <code class="o">=</code> <code class="k">new</code> <code class="n">FileOutputStream</code><code class="o">(</code><code class="n">localFile</code><code class="o">);</code>
    <code class="n">out</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="sb">"content"</code><code class="o">.</code><code class="na">getBytes</code><code class="o">(</code><code class="sb">"UTF-8"</code><code class="o">));</code>
    <code class="n">out</code><code class="o">.</code><code class="na">flush</code><code class="o">();</code> <code class="c2">// flush to operating system</code>
    <code class="n">out</code><code class="o">.</code><code class="na">getFD</code><code class="o">().</code><code class="na">sync</code><code class="o">();</code> <code class="c2">// sync to disk</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">localFile</code><code class="o">.</code><code class="na">length</code><code class="o">(),</code> <code class="n">is</code><code class="o">(((</code><code class="kt">long</code><code class="o">)</code> <code class="sb">"content"</code><code class="o">.</code><code class="na">length</code><code class="o">())));</code></pre><p class="calibre2">Closing a file in HDFS performs an implicit
      <code class="literal">hflush()</code>, too:</p><a id="calibre_link-4067" class="calibre"></a><pre class="screen1">    <code class="n">Path</code> <code class="n">p</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="sb">"p"</code><code class="o">);</code>
    <code class="n">OutputStream</code> <code class="n">out</code> <code class="o">=</code> <code class="n">fs</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">p</code><code class="o">);</code>
    <code class="n">out</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="sb">"content"</code><code class="o">.</code><code class="na">getBytes</code><code class="o">(</code><code class="sb">"UTF-8"</code><code class="o">));</code>
    <span class="calibre24"><strong class="calibre9"><code class="n1">out</code><code class="o1">.</code><code class="na1">close</code><code class="o1">();</code></strong></span>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">fs</code><code class="o">.</code><code class="na">getFileStatus</code><code class="o">(</code><code class="n">p</code><code class="o">).</code><code class="na">getLen</code><code class="o">(),</code> <code class="n">is</code><code class="o">(((</code><code class="kt">long</code><code class="o">)</code> <code class="sb">"content"</code><code class="o">.</code><code class="na">length</code><code class="o">())));</code></pre><div class="book" title="Consequences for application design"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4068">Consequences for application design</h4></div></div></div><p class="calibre2">This coherency model has implications for the way you design
        applications. With no calls to <code class="literal">hflush()</code> or
        <code class="literal">hsync()</code>, you should be prepared to lose up to
        a block of data in the event of client or system failure. For many
        applications, this is unacceptable, so you should call <code class="literal">hflush()</code> at suitable points, such as after
        writing a certain number of records or number of bytes. Though the
        <code class="literal">hflush()</code> operation is designed to
        not unduly tax HDFS, it does have some overhead (and
        <code class="literal">hsync()</code> has more), so there is a trade-off
        between data robustness and throughput. What constitutes an acceptable
        trade-off is application dependent, and suitable values can be
        selected after measuring your application’s performance with
        <a class="calibre" id="calibre_link-1171"></a><a class="calibre" id="calibre_link-1674"></a><a class="calibre" id="calibre_link-1935"></a>different <code class="literal">hflush()</code> (or
        <code class="literal">hsync()</code>) frequencies.</p></div></div></div><div class="book" title="Parallel Copying with distcp"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-228">Parallel Copying with distcp</h2></div></div></div><p class="calibre2">The HDFS <u style="
    text-decoration: underline 0.14em;
">access pattern</u>s that <a class="calibre" id="calibre_link-1950"></a><a class="calibre" id="calibre_link-1512"></a><a class="calibre" id="calibre_link-2903"></a><a class="calibre" id="calibre_link-1686"></a>we have seen so far focus on single-threaded access. <br>It’s
    possible to act on a collection of files—by specifying file globs, for
    example—but for efficient parallel processing of these files, you would
    have to write a program yourself. Hadoop comes with a useful program
    called <em class="calibre10"><em class="calibre10">distcp</em></em>
    for copying data to and from Hadoop filesystems in parallel.</p><p class="calibre2">One use for <em class="calibre10">distcp</em> is as an
    efficient replacement <a class="calibre" id="calibre_link-1842"></a>for <code class="literal">hadoop fs -cp</code>. For
    example, you can copy one file to another with:<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-195" id="calibre_link-206">34</a>]</sup></p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop distcp file1 file2</code></strong></pre><p class="calibre2">You can also copy directories:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop distcp dir1 dir2</code></strong></pre><p class="calibre2">If <em class="calibre10">dir2</em> does not exist, it will
    be created, and the contents of the <em class="calibre10">dir1</em> directory will be copied there. You can
    specify multiple source paths, and all will be copied to the
    destination.</p><p class="calibre2">If <em class="calibre10">dir2</em> already exists, then
    <em class="calibre10">dir1</em> will be copied under it,
    creating the directory structure <em class="calibre10">dir2/dir1</em>. If this isn’t what you want, you can
    supply the <code class="literal">-overwrite</code> option to keep
    the same directory structure and force files to be overwritten. You can
    also update only the files that have changed using the <code class="literal">-update</code> option. This is best shown with an
    example. If we changed a file in the <em class="calibre10">dir1</em> subtree, we could synchronize the change
    with <em class="calibre10">dir2</em> by running:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop distcp -update dir1 dir2</code></strong></pre><div class="note" title="Tip"><h3 class="title3">Tip</h3><p class="calibre2">If you are unsure of the effect of a <em class="calibre10">distcp</em> operation, it is a good idea to try it
      out on a small test directory tree first.</p></div><p class="calibre2"><em class="calibre10">distcp</em> is implemented as a
    MapReduce job where the work of copying is done by the maps that run in
    parallel across the cluster. There are no reducers. Each file is copied by
    a single map, and <em class="calibre10">distcp</em> tries to give
    each map approximately the same amount of data by bucketing files into
    roughly equal allocations. By default, up to 20 maps are used, but this
    can be changed by specifying the <code class="literal">-m</code>
    argument to <em class="calibre10">distcp</em>.</p><p class="calibre2">A very common use case for <em class="calibre10">distcp</em>
    is for transferring data between two HDFS clusters. For example, the
    following creates a backup of the first cluster’s <em class="calibre10">/foo</em> directory on the second:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop distcp -update -delete -p hdfs://namenode1/foo hdfs://namenode2/foo</code></strong></pre><p class="calibre2">The <code class="literal">-delete</code> flag causes <em class="calibre10">distcp</em> to delete any files or directories from
    the destination that are not present in the source, and <code class="literal">-p</code> means that file status attributes like
    permissions, block size, and replication are preserved. You can run
    <em class="calibre10">distcp</em> with no arguments to see precise
    usage instructions.</p><p class="calibre2">If the two clusters are running incompatible versions of HDFS, then
    you can use the <code class="literal">webhdfs</code> protocol to <em class="calibre10">distcp</em> between them:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop distcp webhdfs://namenode1:50070/foo webhdfs://namenode2:50070/foo</code></strong></pre><p class="calibre2">Another variant is to use an <a class="calibre" id="calibre_link-2062"></a>HttpFs proxy as the <em class="calibre10">distcp</em> source or destination (again using the
    <code class="literal">webhdfs</code> protocol), which has the advantage of being able to set
    firewall and bandwidth controls (see <a class="ulink" href="#calibre_link-172" title="HTTP">HTTP</a>).</p><div class="book" title="Keeping an HDFS Cluster Balanced"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4069">Keeping an HDFS Cluster Balanced</h3></div></div></div><p class="calibre2">When copying <a class="calibre" id="calibre_link-1112"></a><a class="calibre" id="calibre_link-1931"></a>data into HDFS, it’s important to consider cluster
      balance. HDFS works best when the file blocks are evenly spread across
      the cluster, so you want to ensure that <em class="calibre10">distcp</em> doesn’t disrupt this. For example, if
      you specified <code class="literal">-m 1</code>, a single map
      would do the copy, which—apart from being slow and not using the cluster
      resources efficiently—would mean that the first replica of each block
      would reside on the node running the map (until the disk filled up). The
      second and third replicas would be spread across the cluster, but this
      one node would be unbalanced. By having more maps than nodes in the
      cluster, this problem is avoided. For this reason, it’s best to start by
      running <em class="calibre10">distcp</em> with the default of 20
      maps per node.</p><p class="calibre2">However, it’s not always possible to prevent a cluster from
      becoming unbalanced. Perhaps you want to limit the number of maps so
      that some of the nodes can be used by other jobs. In this case, you can
      use <a class="calibre" id="calibre_link-974"></a>the <em class="calibre10">balancer</em> tool
      (see <a class="ulink" href="#calibre_link-17" title="Balancer">Balancer</a>) to subsequently even out the block
      distribution across the <a class="calibre" id="calibre_link-1513"></a><a class="calibre" id="calibre_link-1687"></a><a class="calibre" id="calibre_link-1951"></a><a class="calibre" id="calibre_link-2904"></a>cluster.</p></div></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" id="calibre_link-162"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-196">25</a>] </sup>The architecture of HDFS is described in Robert Chansler et
        al.’s, <a class="ulink" href="http://www.aosabook.org/en/hdfs.html" target="_top">“The Hadoop
        Distributed File System,”</a> which appeared in <span class="calibre"><em class="calibre10">The
        Architecture of Open Source Applications: Elegance, Evolution, and a
        Few Fearless Hacks</em></span> by Amy Brown and Greg Wilson
        (eds.).</p></div><div class="footnote" id="calibre_link-163"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-197">26</a>] </sup>See Konstantin V. Shvachko and Arun C. Murthy, <a class="ulink" href="http://bit.ly/scaling_hadoop" target="_top">“Scaling
              Hadoop to 4000 nodes at Yahoo!”</a>, September 30,
              2008.</p></div><div class="footnote" type="footnote" id="calibre_link-164"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-198">27</a>] </sup>See <a class="ulink" href="#calibre_link-1" title="Chapter&nbsp;10.&nbsp;Setting Up a Hadoop Cluster">Chapter&nbsp;10</a> for a typical machine
              specification.</p></div><div class="footnote" id="calibre_link-165"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-199">28</a>] </sup>For an exposition of the scalability limits of HDFS, see
              Konstantin V. Shvachko, <a class="ulink" href="http://bit.ly/limits_to_growth" target="_top">“HDFS
              Scalability: The Limits to Growth”</a>, April 2010.</p></div><div class="footnote" type="footnote" id="calibre_link-168"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-200">29</a>] </sup>In Hadoop 1, the name for this property was <code class="literal">fs.default.name</code>. Hadoop 2 introduced many
        new property names, and deprecated the old ones (see <a class="ulink" href="#calibre_link-201" title="Which Properties Can I Set?">Which Properties Can I Set?</a>). This book uses the new property
        names.</p></div><div class="footnote" type="footnote" id="calibre_link-175"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-202">30</a>] </sup>In Hadoop 2 and later, there is a new filesystem interface
        <a class="calibre" id="calibre_link-1636"></a>called <code class="literal">FileContext</code>
        with better handling of multiple filesystems (so a single <code class="literal">FileContext</code> can resolve multiple filesystem
        schemes, for example) and a cleaner, more consistent interface.
        <code class="literal">FileSystem</code> is still more widely
        used, however.</p></div><div class="footnote" type="footnote" id="calibre_link-177"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-203">31</a>] </sup>The text is from <em class="calibre10">The Quangle Wangle’s
          Hat</em> by Edward Lear.</p></div><div class="footnote" type="footnote" id="calibre_link-189"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-204">32</a>] </sup>At the time of this writing, Hadoop is not suited for
                running across data centers.</p></div><div class="footnote" type="footnote" id="calibre_link-194"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-205">33</a>] </sup>In Hadoop 1.x, <code class="literal">hflush()</code> was called
          <code class="literal">sync()</code>, and <code class="literal">hsync()</code> did not exist.</p></div><div class="footnote" type="footnote" id="calibre_link-195"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-206">34</a>] </sup>Even for a single file copy, the <em class="calibre10">distcp</em> variant is preferred for large files
        since <code class="literal">hadoop fs -cp</code> copies the file
        via the client running the command.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-318"><section type="chapter" id="calibre_link-4070" title="Chapter&nbsp;4.&nbsp;YARN"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;4.&nbsp;YARN</h2></div></div></div><p class="calibre2">Apache YARN (Yet Another Resource Negotiator) is <span style="
    text-shadow: 0 0 1em cyan;
">Hadoop’s <a class="calibre" id="calibre_link-3825"></a><u style="
    text-decoration: underline dashed;
">cluster</u> <u style="">resource management system</u></span>. <br>YARN was introduced in
  Hadoop 2 to improve the MapReduce implementation, <br>but it is general enough
  to support other distributed computing paradigms as well.</p><p class="calibre2">YARN provides APIs for requesting and working with cluster resources,
  but these APIs are not typically used directly by user code. <br>Instead, users
  write to higher-level APIs provided by distributed computing frameworks,
<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;which themselves are built on YARN and <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;hide the resource management details
  from the user. <br>The situation is illustrated in <a class="ulink" href="#calibre_link-319" title="Figure&nbsp;4-1.&nbsp;YARN applications">Figure&nbsp;4-1</a>, <br>which shows some distributed computing frameworks
  (MapReduce, Spark, and so on) <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; running as <em class="calibre10">YARN
  applications</em> on the cluster compute layer (YARN) and <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;the cluster
  storage layer (HDFS and HBase).</p><div class="figure"><a id="calibre_link-319" class="calibre"></a><div class="book"><div class="book"><img alt="YARN applications" src="images/000013.png" class="calibre29" style="
    width: 35em;
"></div></div><div class="figure-title">Figure&nbsp;4-1.&nbsp;YARN applications</div></div><p class="calibre2">There is also a layer of applications that build on the frameworks
  shown in <a class="ulink" href="#calibre_link-319" title="Figure&nbsp;4-1.&nbsp;YARN applications">Figure&nbsp;4-1</a>. <br>Pig, Hive, and Crunch are all
  examples of processing frameworks that run on MapReduce, Spark, or Tez (or
  on all three), and <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;don’t interact with YARN directly.</p><p class="calibre2">This chapter walks through the features in YARN and <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;provides a basis
  for understanding later chapters in <a class="ulink" href="#calibre_link-88" title="Part&nbsp;IV.&nbsp;Related Projects">Part&nbsp;IV</a> that
  cover Hadoop’s distributed processing frameworks.</p><div class="book" title="Anatomy of a YARN Application Run"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4071">Anatomy of a YARN Application Run</h2></div></div></div><p class="calibre2">YARN provides its <a class="calibre" id="calibre_link-3827"></a>core services via two types of long-running daemon: <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; a
    <em class="calibre10">resource manager</em> (one per cluster) to <a class="calibre" id="calibre_link-3218"></a>manage the use of resources across the cluster, and<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<em class="calibre10">node managers</em> running <a class="calibre" id="calibre_link-2793"></a>on all the nodes in the cluster to launch and<a class="calibre" id="calibre_link-1248"></a> monitor <em class="calibre10">containers</em>. <br>A container
    executes an application-specific process with a constrained set of
    resources (memory, CPU, and so on). <br>Depending on how YARN is configured
    (see <a class="ulink" href="#calibre_link-320" title="YARN">YARN</a>), a container may be a Unix process or a
    Linux cgroup. <br><a class="ulink" href="#calibre_link-321" title="Figure&nbsp;4-2.&nbsp;How YARN runs an application">Figure&nbsp;4-2</a> illustrates how YARN runs
    an application.</p><div class="figure"><a id="calibre_link-321" class="calibre"></a><div class="book"><div class="book"><img alt="How YARN runs an application" src="images/000079.png" class="calibre29" style="
    width: 35em;
"></div></div><div class="figure-title">Figure&nbsp;4-2.&nbsp;How YARN runs an application</div></div><p class="calibre2">To run an application on YARN, a client contacts the resource
    manager and asks it to run <a class="calibre" id="calibre_link-905"></a>an <em class="calibre10">application master</em> process (step 1
    in <a class="ulink" href="#calibre_link-321" title="Figure&nbsp;4-2.&nbsp;How YARN runs an application">Figure&nbsp;4-2</a>). <br>The resource manager then finds a
    node manager that can launch the application master in a container (steps
    2a and 2b).<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-322" id="calibre_link-339">35</a>]</sup> <br>Precisely what the application master does once it is
    running depends on the application. <br>It could simply run a computation in
    the container it is running in and return the result to the client. <br>Or it
    could request more containers from the resource managers (step 3), and use
    them to run a distributed computation (steps 4a and 4b). <br>The latter is
    what the MapReduce YARN application does, which we’ll look at in more
    detail in <a class="ulink" href="#calibre_link-52" title="Anatomy of a MapReduce Job Run">Anatomy of a MapReduce Job Run</a>.</p><p class="calibre2">Notice from <a class="ulink" href="#calibre_link-321" title="Figure&nbsp;4-2.&nbsp;How YARN runs an application">Figure&nbsp;4-2</a> that YARN itself does
    not provide any way for the parts of the application (client, master,
    process) to communicate with one another. <br>Most nontrivial YARN
    applications use some form of remote communication (such as Hadoop’s RPC
    layer) to pass status updates and results back to the client, but these
    are specific to the application.</p><div class="book" title="Resource Requests"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-495">Resource Requests</h3></div></div></div><p class="calibre2">YARN has a flexible <a class="calibre" id="calibre_link-3236"></a>model for making resource requests. <br>A request for a set of
      containers can express the amount of computer resources required for
      each container (memory and CPU), as well as locality constraints for the
      containers in that request.</p><p class="calibre2">Locality is critical in <a class="calibre" id="calibre_link-2348"></a>ensuring that distributed data processing algorithms use
      the cluster bandwidth efficiently,<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-323" id="calibre_link-340">36</a>]</sup> so YARN allows an application to specify locality
      constraints for the containers it is requesting. <br>Locality constraints
      can be used to request a container on a specific node or rack, or
      anywhere on the cluster (off-rack).</p><p class="calibre2">Sometimes the locality constraint cannot be met, in which case
      either no allocation is made or, optionally, the constraint can be
      loosened. <br>For example, if a specific node was requested but it is not
      possible to start a container on it (because other containers are
      running on it), then YARN will try to start a container on a node in the
      same rack, or, if that’s not possible, on any node in the cluster.</p><p class="calibre2">In the common case of launching a container to process an HDFS
      block (to run a map task in MapReduce, say), the application will
      request a container on one of the nodes hosting the block’s three
      replicas, or on a node in one of the racks hosting the replicas, or,
      failing that, on any node in the cluster.</p><p class="calibre2">A YARN application can make resource requests at any time while it
      is running. For example, an application can make all of its requests up
      front, or it can take a more dynamic approach whereby it requests more
      resources dynamically to meet the changing needs of the
      application.</p><p class="calibre2">Spark takes the first approach, starting <a class="calibre" id="calibre_link-3461"></a>a fixed number of executors on the cluster (see <a class="ulink" href="#calibre_link-324" title="Spark on YARN">Spark on YARN</a>). MapReduce, on <a class="calibre" id="calibre_link-2496"></a>the other hand, has two phases: the map task containers
      are requested up front, but the reduce task containers are not started
      until later. Also, if any tasks fail, additional containers will be
      requested so the failed tasks can be rerun.</p></div><div class="book" title="Application Lifespan"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4072">Application Lifespan</h3></div></div></div><p class="calibre2">The lifespan <a class="calibre" id="calibre_link-3829"></a>of a YARN application can vary dramatically: from a
      short-lived application of a few seconds to a long-running application
      that runs for days or even months. Rather than look at how long the
      application runs for, it’s useful to categorize applications in terms of
      how they map to the jobs that users run. The simplest case is one
      application per user job, which is the approach that MapReduce
      takes.</p><p class="calibre2">The second model is to run one application per workflow or user
      session of (possibly unrelated) jobs. This approach can be more
      efficient than the first, since containers can be reused between jobs,
      and there is also the potential to cache intermediate data between jobs.
      Spark is an example that uses this model.</p><p class="calibre2">The third model is a long-running application that is shared by
      different users. Such an application often acts in some kind of
      coordination role. For example, <a class="ulink" href="http://slider.incubator.apache.org/" target="_top">Apache Slider</a> has a
      long-running <a class="calibre" id="calibre_link-897"></a>application master for launching other applications on the
      cluster. This approach is also used by Impala (see <a class="ulink" href="#calibre_link-83" title="SQL-on-Hadoop Alternatives">SQL-on-Hadoop Alternatives</a>) to provide a proxy application
      that the<a class="calibre" id="calibre_link-2078"></a> Impala daemons communicate with to request cluster
      resources. The “always on” application master means that users have very
      low-latency responses to their queries since the overhead of starting a
      new application master is avoided.<sup class="calibre6">[<a class="firstname" href="#calibre_link-325" id="calibre_link-341">37</a>]</sup></p></div><div class="book" title="Building YARN Applications"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4073">Building YARN Applications</h3></div></div></div><p class="calibre2">Writing a YARN <a class="calibre" id="calibre_link-3831"></a>application from scratch is fairly involved, but in many
      cases is not necessary, as it is often possible to use an existing
      application that fits the bill. For example, if you are interested in
      running a directed acyclic graph (DAG) of jobs, then Spark or Tez is
      appropriate; or for stream processing, Spark, Samza, or Storm works.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-326" id="calibre_link-342">38</a>]</sup></p><p class="calibre2">There are a couple of projects that simplify the process of
      building a YARN application. Apache Slider, mentioned earlier, makes it
      possible to run existing distributed applications on YARN. Users can run
      their own instances of an application (such as HBase) on a cluster,
      independently of other users, which means that different users can run
      different versions of the same application. Slider provides controls to
      change the number of nodes an application is running on, and to suspend
      then resume a running application.</p><p class="calibre2"><a class="ulink" href="http://twill.incubator.apache.org/" target="_top">Apache
      Twill</a> is similar to <a class="calibre" id="calibre_link-902"></a>Slider, but in addition provides a simple programming
      model for developing distributed applications on YARN. Twill allows you
      to define cluster processes as an extension of a <a class="calibre" id="calibre_link-3255"></a>Java <code class="literal">Runnable</code>, then runs them in
      YARN containers on the cluster. Twill also provides support for, among
      other things, real-time logging (log events from runnables are streamed
      back to the client) and command messages (sent from the client to
      runnables).</p><p class="calibre2">In cases where none of these options are sufficient—such as an
      application that has complex scheduling requirements—then the
      <em class="calibre10">distributed shell</em> application <a class="calibre" id="calibre_link-3836"></a>that is a part of the YARN project itself serves as an
      example of how to write a YARN application. It demonstrates how to use
      YARN’s client APIs to handle communication between the client or
      application master and the <a class="calibre" id="calibre_link-3828"></a>YARN daemons.</p></div></div><div class="book" title="YARN Compared to MapReduce 1"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4074">YARN Compared to MapReduce 1</h2></div></div></div><p class="calibre2">The distributed <a class="calibre" id="calibre_link-2519"></a><a class="calibre" id="calibre_link-3838"></a>implementation of MapReduce in the original version of
    Hadoop (version 1 and earlier) is sometimes referred to as “MapReduce 1”
    to distinguish it from MapReduce 2, the implementation that uses YARN (in
    Hadoop 2 and later).</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">It’s important to realize that the old and new MapReduce APIs are
      not the same thing as the MapReduce 1 and MapReduce 2 implementations.
      The APIs are user-facing client-side features and determine how you
      write MapReduce programs (see <a class="ulink" href="#calibre_link-249" title="Appendix&nbsp;D.&nbsp;The Old and New Java MapReduce APIs">Appendix&nbsp;D</a>), whereas the
      implementations are just different ways of running MapReduce programs.
      All four combinations are supported: both the old and new MapReduce APIs
      run on both MapReduce 1 and 2.</p></div><p class="calibre2">In MapReduce 1, there are two types of daemon that control the job
    execution <a class="calibre" id="calibre_link-2271"></a><a class="calibre" id="calibre_link-3644"></a>process: a <em class="calibre10">jobtracker</em> and one or more
    <em class="calibre10">tasktrackers</em>. The jobtracker coordinates all the
    jobs run on the system by scheduling tasks to run on tasktrackers.
    Tasktrackers run tasks and send progress reports to the jobtracker, which keeps a
    record of the overall progress of each job. If a task fails, the
    jobtracker can reschedule it on a different tasktracker.</p><p class="calibre2">In MapReduce 1, the jobtracker takes care of both job scheduling
    (matching tasks with tasktrackers) and task progress monitoring (keeping
    track of tasks, restarting failed or slow tasks, and doing task
    bookkeeping, such as maintaining counter totals). <br>By contrast, in YARN
    these responsibilities are handled by separate entities: the resource
    manager and an application master (one for each MapReduce job).
    The jobtracker is also responsible for storing job history for completed
    jobs, although it is possible to run a job history server as a separate
    daemon to take the load off the jobtracker. <br>In YARN, the equivalent role
    is the timeline server, which stores application history.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-327" id="calibre_link-343">39</a>]</sup></p><p class="calibre2">The YARN equivalent of a tasktracker is a <a class="calibre" id="calibre_link-2807"></a>node manager. The mapping is summarized in <a class="ulink" href="#calibre_link-328" title="Table&nbsp;4-1.&nbsp;A comparison of MapReduce 1 and YARN components">Table&nbsp;4-1</a>.</p><div class="table"><a id="calibre_link-328" class="calibre"></a><div class="table-title">Table&nbsp;4-1.&nbsp;A comparison of MapReduce 1 and YARN components</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre36"><col class="c4"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">MapReduce 1</td><td class="calibre21">YARN</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">Jobtracker</td><td class="calibre25">Resource <a class="calibre" id="calibre_link-3229"></a><a class="calibre" id="calibre_link-910"></a><a class="calibre" id="calibre_link-3692"></a><a class="calibre" id="calibre_link-1249"></a>manager, application master, timeline server</td></tr><tr class="calibre26"><td class="calibre23">Tasktracker</td><td class="calibre25">Node manager</td></tr><tr class="calibre19"><td class="calibre27">Slot</td><td class="calibre28">Container</td></tr></tbody></table></div></div><p class="calibre2">YARN was designed to address many of the limitations in MapReduce 1.
<br>&nbsp;The benefits to using YARN include the following:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Scalability</span></dt><dd class="calibre8"><p class="calibre2">YARN can run on larger clusters than MapReduce 1. MapReduce 1
          hits scalability bottlenecks in the region of 4,000 nodes and 40,000
          tasks,<sup class="calibre6">[<a class="firstname" href="#calibre_link-329" id="calibre_link-345">40</a>]</sup> stemming from the fact that the jobtracker has to
          manage both jobs <span class="calibre">and</span> tasks. YARN
          overcomes these limitations by virtue of its split resource
          manager/application master architecture: it is designed to scale up
          to 10,000 nodes and 100,000 tasks.</p><p class="calibre2">In contrast to the jobtracker, each instance of an
          application—here, a MapReduce job—has a dedicated application
          master, which runs for the duration of the application. This model
          is actually closer to the original Google MapReduce paper, which
          describes how a master process is started to coordinate map and
          reduce tasks running on a set of workers.</p></dd><dt class="calibre7"><span class="term">Availability</span></dt><dd class="calibre8"><p class="calibre2">High availability (HA) is <a class="calibre" id="calibre_link-1995"></a>usually achieved by replicating the state needed for
          another daemon to take over the work needed to provide the service,
          in the event of the service daemon failing. However, the large
          amount of rapidly changing complex state in the jobtracker’s memory
          (each task status is updated every few seconds, for example) makes
          it very difficult to retrofit HA into the jobtracker service.</p><p class="calibre2">With the jobtracker’s responsibilities split between the
          resource manager and application master in YARN, making the service
          highly available became a divide-and-conquer problem: provide HA for
          the resource manager, then for YARN applications (on a
          per-application basis). And indeed, Hadoop 2 supports HA both for
          the resource manager and for the application master
          for MapReduce jobs. Failure recovery in YARN is
          discussed in more detail in <a class="ulink" href="#calibre_link-330" title="Failures">Failures</a>.</p></dd><dt class="calibre7"><span class="term">Utilization</span></dt><dd class="calibre8"><p class="calibre2">In MapReduce 1, each tasktracker is configured with a static
          allocation of fixed-size “slots,” which are divided into map slots
          and reduce slots at configuration time. A map slot can only be used
          to run a map task, and a reduce slot can only be used for a reduce
          task.</p><p class="calibre2">In YARN, a node manager manages a pool of resources, rather
          than a fixed number of designated slots. MapReduce running on YARN
          will not hit the situation where a reduce task has to wait because
          only map slots are available on the cluster, which can happen in
          MapReduce 1. If the resources to run the task are available, then
          the application will be eligible for them.</p><p class="calibre2">Furthermore, resources in YARN are fine grained, so an
          application can make a request for what it needs, rather than for an
          indivisible slot, which may be too big (which is wasteful of
          resources) or too small (which may cause a failure) for the
          particular task.</p></dd><dt class="calibre7"><span class="term">Multitenancy</span></dt><dd class="calibre8"><p class="calibre2">In some ways, the biggest benefit of YARN is that it opens up
          Hadoop to other types of distributed application beyond MapReduce.
          MapReduce is just one YARN application among many.</p><p class="calibre2">It is even possible for users to run different versions of
          MapReduce on the same YARN cluster, which makes the process of
          upgrading MapReduce more manageable. (Note, however, that some parts
          of MapReduce, such as the job history server and the shuffle
          handler, as well as YARN itself, still need to be upgraded across
          the cluster.)</p></dd></dl></div><p class="calibre2">Since Hadoop 2 is widely used and is the latest stable version, in
    the rest of this book the term “MapReduce” refers to MapReduce 2 unless
    otherwise stated. <a class="ulink" href="#calibre_link-331" title="Chapter&nbsp;7.&nbsp;How MapReduce Works">Chapter&nbsp;7</a> looks in detail at how
    MapReduce running on YARN <a class="calibre" id="calibre_link-3839"></a><a class="calibre" id="calibre_link-2520"></a>works.</p></div><div class="book" title="Scheduling in YARN"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-49">Scheduling in YARN</h2></div></div></div><p class="calibre2">In an ideal world, the requests <a class="calibre" id="calibre_link-3272"></a><a class="calibre" id="calibre_link-3841"></a>that a YARN application makes would be granted immediately.
    In the real world, however, resources are limited, and on a busy cluster,
    an application will often need to wait to have some of its requests
    fulfilled. It is the job of the YARN scheduler to allocate resources to
    applications according to some defined policy. Scheduling in general is a
    difficult problem and there is no one “best” policy, which is why YARN
    provides a choice of schedulers and configurable policies. We look at
    these next.</p><p class="calibre2"></p><div class="book" title="Scheduler Options"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4075">Scheduler Options</h3></div></div></div><p class="calibre2">Three schedulers are available in YARN: the FIFO, Capacity, and
      Fair Schedulers. The FIFO Scheduler places <a class="calibre" id="calibre_link-3279"></a><a class="calibre" id="calibre_link-1619"></a>applications in a queue and runs them in the order of
      submission (first in, first out). Requests for the first application in
      the queue are allocated first; once its requests have been satisfied,
      the next application in the queue is served, and so on.</p><p class="calibre2">The FIFO Scheduler has the merit of being simple to understand and
      not needing any configuration, but it’s not suitable for shared
      clusters. Large applications will use all the resources in a cluster, so
      each application has to wait its turn. On a shared cluster it is better
      to use the Capacity Scheduler or the Fair Scheduler. Both of these allow
      long-running jobs to complete in a timely manner, while still allowing
      users who are running concurrent smaller ad hoc queries to get results
      back in a reasonable time.</p><p class="calibre2">The difference between schedulers is illustrated in <a class="ulink" href="#calibre_link-332" title="Figure&nbsp;4-3.&nbsp;Cluster utilization over time when running a large job and a small job under the FIFO Scheduler (i), Capacity Scheduler (ii), and Fair Scheduler (iii)">Figure&nbsp;4-3</a>, which shows that under the FIFO Scheduler
      (i) the small job is blocked until the large job completes.</p><p class="calibre2">With the Capacity Scheduler (ii in <a class="ulink" href="#calibre_link-332" title="Figure&nbsp;4-3.&nbsp;Cluster utilization over time when running a large job and a small job under the FIFO Scheduler (i), Capacity Scheduler (ii), and Fair Scheduler (iii)">Figure&nbsp;4-3</a>), a separate dedicated queue allows the
      small job to start as soon as it is submitted, although this is at the
      cost of overall cluster utilization since the queue capacity is reserved
      for jobs in that queue. This means that the large job finishes later
      than when using the FIFO Scheduler.</p><p class="calibre2">With the Fair Scheduler (iii in <a class="ulink" href="#calibre_link-332" title="Figure&nbsp;4-3.&nbsp;Cluster utilization over time when running a large job and a small job under the FIFO Scheduler (i), Capacity Scheduler (ii), and Fair Scheduler (iii)">Figure&nbsp;4-3</a>), there is no need to reserve a set amount
      of capacity, since it will dynamically balance resources between all
      running jobs. Just after the first (large) job starts, it is the only
      job running, so it gets all the resources in the cluster. When the
      second (small) job starts, it is allocated half of the cluster resources
      so that each job is using its fair share of resources.</p><p class="calibre2">Note that there is a lag between the time the second job starts
      and when it receives its fair share, since it has to wait for resources
      to free up as containers used by the first job complete. After the small
      job completes and no longer requires resources, the large job goes back
      to using the full cluster capacity again. The overall effect is both
      high cluster utilization and timely small job completion.</p><p class="calibre2"><a class="ulink" href="#calibre_link-332" title="Figure&nbsp;4-3.&nbsp;Cluster utilization over time when running a large job and a small job under the FIFO Scheduler (i), Capacity Scheduler (ii), and Fair Scheduler (iii)">Figure&nbsp;4-3</a> contrasts the basic operation
      of the three schedulers. In the next two sections, we examine some of
      the more advanced configuration options for the Capacity and Fair
      <a class="calibre" id="calibre_link-1620"></a>Schedulers.</p><div class="figure"><a id="calibre_link-332" class="calibre"></a><div class="book"><div class="book"><img alt="Cluster utilization over time when running a large job and a small job under the FIFO Scheduler (i), Capacity Scheduler (ii), and Fair Scheduler (iii)" src="images/000081.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;4-3.&nbsp;Cluster utilization over time when running a large job and a
          small job under the FIFO Scheduler (i), Capacity Scheduler (ii), and
          Fair Scheduler (iii)</div></div></div><div class="book" title="Capacity Scheduler Configuration"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4076">Capacity Scheduler Configuration</h3></div></div></div><p class="calibre2">The Capacity Scheduler <a class="calibre" id="calibre_link-3273"></a><a class="calibre" id="calibre_link-1046"></a><a class="calibre" id="calibre_link-3119"></a>allows sharing of a Hadoop cluster along organizational
      lines, whereby each organization is allocated a certain capacity of the
      overall cluster. Each organization is set up with a dedicated queue that
      is configured to use a given fraction of the cluster capacity. Queues
      may be further divided in hierarchical fashion, allowing each
      organization to share its cluster allowance between different groups of
      users within the organization. Within a queue, applications are
      scheduled using FIFO scheduling.</p><p class="calibre2">As we saw in <a class="ulink" href="#calibre_link-332" title="Figure&nbsp;4-3.&nbsp;Cluster utilization over time when running a large job and a small job under the FIFO Scheduler (i), Capacity Scheduler (ii), and Fair Scheduler (iii)">Figure&nbsp;4-3</a>, a single job does
      not use more resources than its queue’s capacity. However, if there is
      more than one job in the queue and there are idle resources available,
      then the Capacity Scheduler may allocate the spare resources to jobs in
      the queue, even if that causes the queue’s capacity to be
      exceeded.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-333" id="calibre_link-346">41</a>]</sup> This behavior is known <a class="calibre" id="calibre_link-3118"></a>as <em class="calibre10">queue elasticity</em>.</p><p class="calibre2">In normal operation, the Capacity Scheduler does not preempt
      containers by forcibly killing them,<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-334" id="calibre_link-347">42</a>]</sup> so if a queue is under capacity due to lack of demand, and
      then demand increases, the queue will only return to capacity as
      resources are released from other queues as containers complete. It is
      possible to mitigate this by configuring queues with a maximum capacity
      so that they don’t eat into other queues’ capacities too much. This is
      at the cost of queue elasticity, of course, so a reasonable trade-off
      should be found by trial and error.</p><p class="calibre2">Imagine a queue hierarchy that looks like this:</p><pre class="screen1">root
├── prod
└── dev
    ├── eng
    └── science</pre><p class="calibre2">The listing in <a class="ulink" href="#calibre_link-335" title="Example&nbsp;4-1.&nbsp;A basic configuration file for the Capacity Scheduler">Example&nbsp;4-1</a> shows a
      sample Capacity Scheduler configuration file, called <em class="calibre10">capacity-scheduler.xml</em>, for this hierarchy.
      It defines two queues under the <code class="literal">root</code>
      queue, <code class="literal">prod</code> and <code class="literal">dev</code>, which have 40% and 60% of the capacity,
      respectively. Notice that a particular queue is configured by setting
      configuration properties of the form <code class="literal">yarn.scheduler.capacity.<em class="replaceable"><code class="replaceable">&lt;queue-path&gt;</code></em>.<em class="replaceable"><code class="replaceable">&lt;sub-property&gt;</code></em></code>,
      where <code class="literal"><em class="replaceable"><code class="replaceable">&lt;queue-path&gt;</code></em></code>
      is the hierarchical (dotted) path of the queue, such as <code class="literal">root.prod</code>.</p><div class="example"><a id="calibre_link-335" class="calibre"></a><div class="example-title">Example&nbsp;4-1.&nbsp;A basic configuration file for the Capacity Scheduler</div><div class="book"><pre class="screen"><code class="cp">&lt;?xml version="1.0"?&gt;</code>
<code class="nt">&lt;configuration&gt;</code>
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>yarn.scheduler.capacity.root.queues<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>prod,dev<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>yarn.scheduler.capacity.root.dev.queues<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>eng,science<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>yarn.scheduler.capacity.root.prod.capacity<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>40<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>yarn.scheduler.capacity.root.dev.capacity<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>60<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>yarn.scheduler.capacity.root.dev.maximum-capacity<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>75<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>yarn.scheduler.capacity.root.dev.eng.capacity<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>50<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>yarn.scheduler.capacity.root.dev.science.capacity<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>50<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
<code class="nt">&lt;/configuration&gt;</code></pre></div></div><p class="calibre2">As you can see, the <code class="literal">dev</code> queue
      is further divided into <code class="literal">eng</code> and
      <code class="literal">science</code> queues of equal capacity. So
      that the <code class="literal">dev</code> queue does not use up
      all the cluster resources when the <code class="literal">prod</code> queue is idle, it has its maximum
      capacity set to 75%. In other words, the <code class="literal">prod</code> queue always has 25% of the cluster
      available for immediate use. Since no maximum capacities have been set
      for other queues, it’s possible for jobs in the <code class="literal">eng</code> or <code class="literal">science</code> queues to use all of the <code class="literal">dev</code> queue’s capacity (up to 75% of the
      cluster), or indeed for the <code class="literal">prod</code>
      queue to use the entire cluster.</p><p class="calibre2">Beyond configuring queue hierarchies and capacities, there are
      settings to control the maximum number of resources a single user or
      application can be allocated, how many applications can be running at
      any one time, and ACLs on queues. See the <a class="ulink" href="http://bit.ly/capacity_scheduler" target="_top">reference
      page</a> for details.</p><div class="book" title="Queue placement"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4077">Queue placement</h4></div></div></div><p class="calibre2">The way that you specify which queue an application is placed in
        is specific to the application. For example, in MapReduce, you set the
        property <code class="literal">mapreduce.job.queuename</code> to
        the <a class="calibre" id="calibre_link-2559"></a>name of the queue you want to use. If the queue does not
        exist, then you’ll get an error at submission time. If no queue is
        specified, applications will be placed in a queue <a class="calibre" id="calibre_link-3274"></a><a class="calibre" id="calibre_link-1047"></a><a class="calibre" id="calibre_link-3120"></a>called <code class="literal">default</code>.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">For the Capacity Scheduler, the queue name should be the last
          part of the hierarchical name since the full hierarchical name is
          not recognized. So, for the preceding example configuration,
          <code class="literal">prod</code> and <code class="literal">eng</code> are OK, but <code class="literal">root.dev.eng</code> and <code class="literal">dev.eng</code> do not work.</p></div></div></div><div class="book" title="Fair Scheduler Configuration"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4078">Fair Scheduler Configuration</h3></div></div></div><p class="calibre2">The Fair Scheduler <a class="calibre" id="calibre_link-3277"></a><a class="calibre" id="calibre_link-1611"></a><a class="calibre" id="calibre_link-3121"></a>attempts to allocate resources so that all running
      applications get the same share of resources. <a class="ulink" href="#calibre_link-332" title="Figure&nbsp;4-3.&nbsp;Cluster utilization over time when running a large job and a small job under the FIFO Scheduler (i), Capacity Scheduler (ii), and Fair Scheduler (iii)">Figure&nbsp;4-3</a> showed how fair sharing works for
      applications in the same queue; however, fair sharing actually works
      <span class="calibre"><em class="calibre10">between</em></span> queues, too, as we’ll see next.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">The terms <span class="calibre">queue</span> and <span class="calibre">pool</span> are used interchangeably in the context
        of the Fair Scheduler.</p></div><p class="calibre2">To understand how resources are shared between queues, imagine two
      users <span class="calibre">A</span> and <span class="calibre">B</span>, each with their own queue (<a class="ulink" href="#calibre_link-336" title="Figure&nbsp;4-4.&nbsp;Fair sharing between user queues">Figure&nbsp;4-4</a>). <span class="calibre"><em class="calibre10">A</em></span> starts a job,
      and it is allocated all the resources available since there is no demand
      from <span class="calibre">B</span>. Then <span class="calibre">B</span> starts a job while <span class="calibre">A</span>’s job is still running, and after a while
      each job is using half of the resources, in the way we saw earlier. Now
      if <span class="calibre">B</span> starts a second job while the
      other jobs are still running, it will share its resources with <span class="calibre">B</span>’s other job, so each of <span class="calibre">B</span>’s jobs will have one-fourth of the resources,
      while <span class="calibre">A</span>’s will continue to have half.
      The result is that resources are shared fairly between users.</p><div class="figure"><a id="calibre_link-336" class="calibre"></a><div class="book"><div class="book"><img alt="Fair sharing between user queues" src="images/000034.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;4-4.&nbsp;Fair sharing between user queues</div></div><div class="book" title="Enabling the Fair Scheduler"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4079">Enabling the Fair Scheduler</h4></div></div></div><p class="calibre2">The scheduler in use is determined by the setting <a class="calibre" id="calibre_link-3896"></a>of <span class="calibre"><code class="literal">yarn.resourcemanager.scheduler</code></span><code class="literal">.class</code>. The Capacity Scheduler is used by
        default (although the Fair Scheduler is the default in some Hadoop
        distributions, such as CDH), but this can be changed by setting
        <span class="calibre"><code class="literal">yarn.resourcemanager</code></span><code class="literal">.scheduler.class</code> in <em class="calibre10">yarn-site.xml</em> to the fully qualified
        classname of the scheduler,
        <code class="literal">org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</code>.</p></div><div class="book" title="Queue configuration"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4080">Queue configuration</h4></div></div></div><p class="calibre2">The Fair Scheduler is configured using an allocation file named
        <em class="calibre10">fair-scheduler.xml</em> that is loaded
        from the classpath. (The name can be changed by setting the <a class="calibre" id="calibre_link-3899"></a>property <code class="literal">yarn.scheduler.fair.allocation.file</code>.) In the
        absence of an allocation file, the Fair Scheduler operates as
        described earlier: each application is placed in a queue named after
        the user and queues are created dynamically when users submit their
        first applications.</p><p class="calibre2">Per-queue configuration is specified in the allocation file.
        This allows configuration of hierarchical queues like those supported
        by the Capacity Scheduler. For example, we can define <code class="literal">prod</code> and <code class="literal">dev</code> queues like we did for the Capacity
        Scheduler using the allocation file in <a class="ulink" href="#calibre_link-337" title="Example&nbsp;4-2.&nbsp;An allocation file for the Fair Scheduler">Example&nbsp;4-2</a>.</p><div class="example"><a id="calibre_link-337" class="calibre"></a><div class="example-title">Example&nbsp;4-2.&nbsp;An allocation file for the Fair Scheduler</div><div class="book"><pre class="screen"><code class="cp">&lt;?xml version="1.0"?&gt;</code>
<code class="nt">&lt;allocations&gt;</code>
  <code class="nt">&lt;defaultQueueSchedulingPolicy&gt;</code>fair<code class="nt">&lt;/defaultQueueSchedulingPolicy&gt;</code>

  <code class="nt">&lt;queue</code> <code class="na">name=</code><code class="sb">"prod"</code><code class="nt">&gt;</code>
    <code class="nt">&lt;weight&gt;</code>40<code class="nt">&lt;/weight&gt;</code>
    <code class="nt">&lt;schedulingPolicy&gt;</code>fifo<code class="nt">&lt;/schedulingPolicy&gt;</code>
  <code class="nt">&lt;/queue&gt;</code>

  <code class="nt">&lt;queue</code> <code class="na">name=</code><code class="sb">"dev"</code><code class="nt">&gt;</code>
    <code class="nt">&lt;weight&gt;</code>60<code class="nt">&lt;/weight&gt;</code>
    <code class="nt">&lt;queue</code> <code class="na">name=</code><code class="sb">"eng"</code> <code class="nt">/&gt;</code>
    <code class="nt">&lt;queue</code> <code class="na">name=</code><code class="sb">"science"</code> <code class="nt">/&gt;</code>
  <code class="nt">&lt;/queue&gt;</code>

  <code class="nt">&lt;queuePlacementPolicy&gt;</code>
    <code class="nt">&lt;rule</code> <code class="na">name=</code><code class="sb">"specified"</code> <code class="na">create=</code><code class="sb">"false"</code> <code class="nt">/&gt;</code>
    <code class="nt">&lt;rule</code> <code class="na">name=</code><code class="sb">"primaryGroup"</code> <code class="na">create=</code><code class="sb">"false"</code> <code class="nt">/&gt;</code>
    <code class="nt">&lt;rule</code> <code class="na">name=</code><code class="sb">"default"</code> <code class="na">queue=</code><code class="sb">"dev.eng"</code> <code class="nt">/&gt;</code>
  <code class="nt">&lt;/queuePlacementPolicy&gt;</code>
<code class="nt">&lt;/allocations&gt;</code></pre></div></div><p class="calibre2">The queue hierarchy is defined using nested <code class="literal">queue</code> elements. All queues are children of
        the <code class="literal">root</code> queue, even if not
        actually nested in a <code class="literal">root</code> <code class="literal">queue</code> element. Here we subdivide the
        <code class="literal">dev</code> queue into a queue called
        <code class="literal">eng</code> and another called <code class="literal">science</code>.</p><p class="calibre2">Queues can have weights, which are used in the fair share
        calculation. In this example, the cluster allocation is considered
        fair when it is divided into a 40:60 proportion between <code class="literal">prod</code> and <code class="literal">dev</code>. The <code class="literal">eng</code> and <code class="literal">science</code> queues do not have weights
        specified, so they are divided evenly. Weights are not quite the same
        as percentages, even though the example uses numbers that add up to
        100 for the sake of simplicity. We could have specified weights of 2
        and 3 for the <code class="literal">prod</code> and <code class="literal">dev</code> queues to achieve the same queue
        weighting.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">When setting weights, remember to consider the default queue
          and dynamically created queues (such as queues named after users).
          These are not specified in the allocation file, but still have
          weight 1.</p></div><p class="calibre2">Queues can have different scheduling policies. The default
        policy for queues can be set in the top-level <code class="literal">defaultQueueSchedulingPolicy</code> element; if it
        is omitted, fair scheduling is used. Despite its name, the Fair
        Scheduler also supports a FIFO (<code class="literal">fifo</code>) policy on queues, as well as Dominant
        Resource Fairness (<code class="literal">drf</code>), described
        later in the chapter.</p><p class="calibre2">The policy for a particular queue can be overridden using the
        <code class="literal">schedulingPolicy</code> element for that
        queue. In this case, the <code class="literal">prod</code> queue
        uses FIFO scheduling since we want each production job to run serially
        and complete in the shortest possible amount of time. Note that fair
        sharing is still used to divide resources between the <code class="literal">prod</code> and <code class="literal">dev</code> queues, as well as between (and within)
        the <code class="literal">eng</code> and <code class="literal">science</code> queues.</p><p class="calibre2">Although not shown in this allocation file, queues can be
        configured with minimum and maximum resources, and a maximum number of
        running applications. (See the <a class="ulink" href="http://bit.ly/fair_scheduler" target="_top">reference
        page</a> for details.) The minimum resources setting is not a hard
        limit, but rather is used by the scheduler to prioritize resource
        allocations. If two queues are below their fair share, then the one
        that is furthest below its minimum is allocated resources first. The
        minimum resource setting is also used for preemption, discussed
        momentarily.</p></div><div class="book" title="Queue placement"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4081">Queue placement</h4></div></div></div><p class="calibre2">The Fair Scheduler uses a rules-based system to determine which
        queue an application is placed in. In <a class="ulink" href="#calibre_link-337" title="Example&nbsp;4-2.&nbsp;An allocation file for the Fair Scheduler">Example&nbsp;4-2</a>, the <code class="literal">queuePlacementPolicy</code> element contains a list
        of rules, each of which is tried in turn until a match occurs. The
        first rule, <code class="literal">specified</code>, places an
        application in the queue it specified; if none is specified, or if the
        specified queue doesn’t exist, then the rule doesn’t match and the
        next rule is tried. The <code class="literal">primaryGroup</code> rule tries to place an
        application in a queue with the name of the user’s primary Unix group;
        if there is no such queue, rather than creating it, the next rule is
        tried. The <code class="literal">default</code> rule is a
        catch-all and always places the application in the <code class="literal">dev.eng</code> queue.</p><p class="calibre2">The <code class="literal">queuePlacementPolicy</code> can
        be omitted entirely, in which case the default behavior is as if it
        had been specified with the following:</p><pre class="screen1"><code class="nt">&lt;queuePlacementPolicy&gt;</code>
  <code class="nt">&lt;rule</code> <code class="na">name=</code><code class="sb">"specified"</code> <code class="nt">/&gt;</code>
  <code class="nt">&lt;rule</code> <code class="na">name=</code><code class="sb">"user"</code> <code class="nt">/&gt;</code>
<code class="nt">&lt;/queuePlacementPolicy&gt;</code></pre><p class="calibre2">In other words, unless the queue is explicitly specified, the
        user’s name is used for the queue, creating it if necessary.</p><p class="calibre2">Another simple queue placement policy is one where all
        applications are placed in the same (default) queue. This allows
        resources to be shared fairly between applications, rather than users.
        The definition is equivalent to this:</p><pre class="screen1"><code class="nt">&lt;queuePlacementPolicy&gt;</code>
  <code class="nt">&lt;rule</code> <code class="na">name=</code><code class="sb">"default"</code> <code class="nt">/&gt;</code>
<code class="nt">&lt;/queuePlacementPolicy&gt;</code></pre><p class="calibre2">It’s also possible to set this policy without using an
        allocation file, by <a class="calibre" id="calibre_link-3904"></a>setting <code class="literal">yarn.scheduler.fair.user-as-default-queue</code> to
        <code class="literal">false</code> so that applications will be
        placed in the default queue rather than a per-user queue. In
        <a class="calibre" id="calibre_link-3900"></a>addition, <code class="literal">yarn.scheduler.fair.allow-undeclared-pools</code>
        should be set to <code class="literal">false</code> so that
        users can’t create queues on the fly.</p></div><div class="book" title="Preemption"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4082">Preemption</h4></div></div></div><p class="calibre2">When a job is submitted to an empty <a class="calibre" id="calibre_link-3058"></a>queue on a busy cluster, the job cannot start until
        resources free up from jobs that are already running on the cluster.
        To make the time taken for a job to start more predictable, the Fair
        Scheduler supports <em class="calibre10">preemption</em>.</p><p class="calibre2">Preemption allows the scheduler to kill containers for queues
        that are running with more than their fair share of resources so that
        the resources can be allocated to a queue that is under its fair
        share. Note that preemption reduces overall cluster efficiency, since
        the terminated containers need to be reexecuted.</p><p class="calibre2">Preemption is enabled globally by <a class="calibre" id="calibre_link-3903"></a>setting <code class="literal">yarn.scheduler.fair.preemption</code> to <code class="literal">true</code>. There are two relevant preemption
        timeout settings: one for minimum share and one for fair share, both
        specified in seconds. By default, the timeouts are not set, so you
        need to set at least one to allow containers to be preempted.</p><p class="calibre2">If a queue waits for as long as its <em class="calibre10">minimum share
        preemption timeout</em> without receiving its minimum
        guaranteed share, then the scheduler may preempt other containers. The
        default timeout is set for all queues via the <code class="literal">defaultMinSharePreemptionTimeout</code> top-level
        element in the allocation file, and on a per-queue basis by setting
        the <code class="literal">minSharePreemptionTimeout</code>
        element for a queue.</p><p class="calibre2">Likewise, if a queue remains below <span class="calibre">half</span> of its fair share for as long as the
        <em class="calibre10">fair share preemption timeout</em>, then the
        scheduler may preempt other containers. The default timeout is set for
        all queues via the <code class="literal">defaultFairSharePreemptionTimeout</code> top-level
        element in the allocation file, and on a per-queue basis by setting
        <code class="literal">fairSharePreemptionTimeout</code> on a
        queue. The threshold may also be changed from its default of 0.5 by
        <a class="calibre" id="calibre_link-3278"></a><a class="calibre" id="calibre_link-1612"></a><a class="calibre" id="calibre_link-3122"></a>setting <code class="literal">defaultFairSharePreemptionThreshold</code> and
        <code class="literal">fairSharePreemptionThreshold</code>
        (per-queue).</p></div></div><div class="book" title="Delay Scheduling"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4083">Delay Scheduling</h3></div></div></div><p class="calibre2">All the YARN schedulers <a class="calibre" id="calibre_link-3275"></a><a class="calibre" id="calibre_link-1421"></a>try to honor locality requests. On a busy cluster, if an
      application requests a particular node, there is a good chance that
      other containers are running on it at the time of the request. The
      obvious course of action is to immediately loosen the locality
      requirement and allocate a container on the same rack. However, it has
      been observed in practice that waiting a short time (no more than a few
      seconds) can dramatically increase the chances of being allocated a
      container on the requested node, and therefore increase the efficiency
      of the cluster. This feature is called <em class="calibre10">delay
      scheduling</em>, and it is supported by both the Capacity
      Scheduler and the Fair Scheduler.</p><p class="calibre2">Every node manager in a <a class="calibre" id="calibre_link-2800"></a><a class="calibre" id="calibre_link-3226"></a>YARN cluster periodically sends a heartbeat request to the
      resource manager—by default, one per second. Heartbeats carry
      information about the node manager’s running containers and the
      resources available for new containers, so each heartbeat is a potential
      <em class="calibre10">scheduling opportunity</em> for an application to run
      a container.</p><p class="calibre2">When using delay scheduling, the scheduler doesn’t simply use the
      first scheduling opportunity it receives, but waits for up to a given
      maximum number of scheduling opportunities to occur before loosening the
      locality constraint and taking the next scheduling opportunity.</p><p class="calibre2">For the Capacity Scheduler, delay scheduling is configured by
      <a class="calibre" id="calibre_link-3898"></a>setting <code class="literal">yarn.scheduler.capacity.node-locality-delay</code> to
      a positive integer representing the number of scheduling opportunities
      that it is prepared to miss before loosening the node constraint to
      match any node in the same rack.</p><p class="calibre2">The Fair Scheduler also uses the number of scheduling
      opportunities to determine the delay, although it is expressed as a
      proportion of the cluster size. For example, <a class="calibre" id="calibre_link-3901"></a>setting <code class="literal">yarn.scheduler.fair.locality.threshold.node</code> to
      0.5 means that the scheduler should wait until half of the nodes in the
      cluster have presented scheduling opportunities before accepting another
      node in the same rack. There is a corresponding property, <code class="literal">yarn.scheduler.fair.locality.threshold.rack</code>,
      for <a class="calibre" id="calibre_link-3902"></a>setting the threshold before another rack is accepted
      instead of the one requested.</p></div><div class="book" title="Dominant Resource Fairness"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4084">Dominant Resource Fairness</h3></div></div></div><p class="calibre2">When there is only a <a class="calibre" id="calibre_link-3276"></a><a class="calibre" id="calibre_link-1539"></a><a class="calibre" id="calibre_link-1543"></a>single resource type being scheduled, such as memory, then
      the concept of capacity or fairness is easy to determine. If two users
      are running applications, you can measure the amount of memory that each
      is using to compare the two applications. However, when there are
      multiple resource types in play, things get more complicated. If one
      user’s application requires lots of CPU but little memory and the
      other’s requires little CPU and lots of memory, how are these two
      applications compared?</p><p class="calibre2">The way that the schedulers in YARN address this problem is to
      look at each user’s dominant resource and use it as a measure of the
      cluster usage. This approach is called <em class="calibre10">Dominant Resource
      Fairness</em>, or DRF for short.<sup class="calibre6">[<a class="firstname" href="#calibre_link-338" id="calibre_link-348">43</a>]</sup> The idea is best illustrated with a simple example.</p><p class="calibre2">Imagine a cluster with a total of 100 CPUs and 10 TB of memory.
      Application <span class="calibre">A</span> requests containers of
      (2 CPUs, 300 GB), and application <span class="calibre">B</span>
      requests containers of (6 CPUs, 100 GB). <span class="calibre">A</span>’s request is (2%, 3%) of the cluster, so
      memory is dominant since its proportion (3%) is larger than CPU’s (2%).
      <span class="calibre">B</span>’s request is (6%, 1%), so CPU is
      dominant. Since <span class="calibre">B</span>’s container
      requests are twice as big in the dominant resource (6% versus 3%), it
      will be allocated half as many containers under fair sharing.</p><p class="calibre2">By default DRF is not used, so during resource calculations, only
      memory is considered and CPU is ignored. The Capacity Scheduler can be
      configured to use DRF by setting <span class="calibre"><code class="literal">yarn.scheduler.capacity.resource-calculator</code> to
      <span class="calibre"><span class="calibre"><code class="literal">org.apache.hadoop</code></span></span><span class="calibre"><span class="calibre"><code class="literal">.yarn</code></span></span></span><span class="calibre"><span class="calibre"><code class="literal">.util</code></span></span><span class="calibre"><code class="literal">.resource.DominantResourceCalculator</code>
      in <em class="calibre10">capacity-scheduler.xml</em>.</span></p><p class="calibre2">For the Fair Scheduler, DRF can be enabled by setting the
      top-level element <code class="literal">defaultQueueSchedulingPolicy</code> in the allocation
      <a class="calibre" id="calibre_link-3842"></a>file to <code class="literal">drf</code>.</p></div></div><div class="book" title="Further Reading"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4085">Further Reading</h2></div></div></div><p class="calibre2">This chapter has given a <a class="calibre" id="calibre_link-3826"></a>short overview of YARN. For more detail, see <span class="calibre"><a class="ulink" href="http://yarn-book.com/" target="_top">Apache Hadoop
    YARN</a></span> by Arun C. Murthy et al. (Addison-Wesley,
    2014).</p></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" type="footnote" id="calibre_link-322"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-339">35</a>] </sup>It’s also possible for the client to start the application
        master, possibly outside the cluster, or in the same JVM as the
        client. This is called <a class="calibre" id="calibre_link-3745"></a><a class="calibre" id="calibre_link-917"></a>an <em class="calibre10">unmanaged application
        master</em>.</p></div><div class="footnote" type="footnote" id="calibre_link-323"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-340">36</a>] </sup>For more on this topic see <a class="ulink" href="#calibre_link-173" title="Scaling Out">Scaling Out</a> and
          <a class="ulink" href="#calibre_link-26" title="Network Topology and Hadoop">Network Topology and Hadoop</a>.</p></div><div class="footnote" id="calibre_link-325"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-341">37</a>] </sup>The low-latency application master code lives in <a class="calibre" id="calibre_link-2331"></a>the <a class="ulink" href="http://cloudera.github.io/llama/" target="_top">Llama project</a>.</p></div><div class="footnote" type="footnote" id="calibre_link-326"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-342">38</a>] </sup>All of these projects are <a class="calibre" id="calibre_link-899"></a>Apache Software Foundation projects.</p></div><div class="footnote" type="footnote" id="calibre_link-327"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-343">39</a>] </sup>As of Hadoop 2.5.1, the YARN timeline server does not yet store
        MapReduce job history, so a MapReduce job history server daemon is
        still needed (see <a class="ulink" href="#calibre_link-344" title="Cluster Setup and Installation">Cluster Setup and Installation</a>).</p></div><div class="footnote" id="calibre_link-329"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-345">40</a>] </sup>Arun C. Murthy, <a class="ulink" href="http://bit.ly/next_gen_mapreduce" target="_top">“The
              Next Generation of Apache Hadoop MapReduce,”</a> February
              14, 2011.</p></div><div class="footnote" type="footnote" id="calibre_link-333"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-346">41</a>] </sup>If the property <code class="literal">yarn.scheduler.capacity.<em class="replaceable"><code class="replaceable">&lt;queue-path&gt;</code></em>.user-limit-factor</code>
          is set to a value larger than 1 (the default), then a single job is
          allowed to use more than its queue’s capacity.</p></div><div class="footnote" type="footnote" id="calibre_link-334"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-347">42</a>] </sup>However, the Capacity Scheduler can perform work-preserving
          preemption, where the resource manager asks applications to return
          containers to balance capacity.</p></div><div class="footnote" id="calibre_link-338"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-348">43</a>] </sup>DRF was introduced in Ghodsi et al.’s <a class="ulink" href="http://bit.ly/fair_allocation" target="_top">“Dominant
          Resource Fairness: Fair Allocation of Multiple Resource
          Types,”</a> March 2011.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-226"><section type="chapter" id="calibre_link-4086" title="Chapter&nbsp;5.&nbsp;Hadoop I/O"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;5.&nbsp;Hadoop I/O</h2></div></div></div><p class="calibre2">Hadoop comes with a set of primitives for data I/O. Some of these are
  techniques that are more general than Hadoop, such as data integrity and
  compression, but deserve special consideration when dealing with
  multiterabyte datasets. Others are Hadoop tools or APIs that form the
  building blocks for developing distributed systems, such as serialization
  frameworks and on-disk data structures.</p><div class="book" title="Data Integrity"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-166">Data Integrity</h2></div></div></div><p class="calibre2">Users of Hadoop <a class="calibre" id="calibre_link-2067"></a><a class="calibre" id="calibre_link-1336"></a>rightly expect that no data will be lost or corrupted during
    storage or processing. However, because every I/O operation on the disk or
    network carries with it a small chance of introducing errors into the data
    that it is reading or writing, when the volumes of data flowing through
    the system are as large as the ones Hadoop is capable of handling, the
    chance of data corruption occurring is high.</p><p class="calibre2">The usual way of detecting corrupted data is by computing <a class="calibre" id="calibre_link-1097"></a>a <em class="calibre10">checksum</em> for the data when it first
    enters the system, and again whenever it is transmitted across a channel
    that is unreliable and hence capable of corrupting the data. The data is
    deemed to be corrupt if the newly generated checksum doesn’t exactly match
    the original. This technique doesn’t offer any way to fix the data—it is
    merely error detection. (And this is a reason for not using low-end
    hardware; in particular, be sure to use ECC memory.) Note that it is
    possible that it’s the checksum that is corrupt, not the data, but this is
    very unlikely, because the checksum is much smaller than the data.</p><p class="calibre2">A commonly used error-detecting code is <a class="calibre" id="calibre_link-1279"></a><a class="calibre" id="calibre_link-1319"></a>CRC-32 (32-bit cyclic redundancy check), which computes a
    32-bit integer checksum for input of any size. CRC-32 is used for
    checksumming <a class="calibre" id="calibre_link-1095"></a>in Hadoop’s <code class="literal">ChecksumFileSystem</code>, while HDFS uses a more
    efficient variant called CRC-32C.</p><div class="book" title="Data Integrity in HDFS"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4087">Data Integrity in HDFS</h3></div></div></div><p class="calibre2">HDFS transparently <a class="calibre" id="calibre_link-1338"></a>checksums all data written to it and by default verifies
      checksums when reading data. A separate checksum is created for
      <a class="calibre" id="calibre_link-1452"></a>every <code class="literal">dfs.bytes-per-checksum</code> bytes of data. The
      default is 512 bytes, and because a CRC-32C checksum is 4 bytes long,
      the storage overhead is less than 1%.</p><p class="calibre2">Datanodes are responsible for verifying the data they receive
      before storing the data and its checksum. This applies to data that they
      receive from clients and from other datanodes during replication. A client
      writing data sends it to a pipeline of datanodes (as explained in <a class="ulink" href="#calibre_link-161" title="Chapter&nbsp;3.&nbsp;The Hadoop Distributed Filesystem">Chapter&nbsp;3</a>), and the last datanode in the pipeline verifies
      the checksum. If the datanode detects an error, the client receives a
      subclass of <code class="literal">IOException</code>, which it
      should handle in an application-specific manner (for example, by
      retrying the operation).</p><p class="calibre2">When clients read data from <a class="calibre" id="calibre_link-1381"></a>datanodes, they verify checksums as well, comparing them
      with the ones stored at the datanodes. Each datanode keeps a persistent
      log of checksum verifications, so it knows the last time each of its
      blocks was verified. When a client successfully verifies a block, it
      tells the datanode, which updates its log. Keeping
      statistics such as these is valuable in detecting bad disks.</p><p class="calibre2">In addition to block verification on client reads, <a class="calibre" id="calibre_link-1367"></a>each datanode runs a <code class="literal">DataBlockScanner</code> in a background thread that
      periodically verifies all the blocks stored on the datanode. This is to
      guard against corruption due to “bit rot” in the physical storage media.
      See <a class="ulink" href="#calibre_link-227" title="Datanode block scanner">Datanode block scanner</a> for details on how to access the
      scanner reports.</p><p class="calibre2">Because HDFS stores replicas of blocks, it can “heal” corrupted
      blocks by copying one of the good replicas to produce a new, uncorrupt
      replica. The way this works is that if a client detects an error when
      reading a block, it reports the bad block and the datanode it was trying
      to read from to the namenode before throwing a <code class="literal">ChecksumException</code>. The <a class="calibre" id="calibre_link-2748"></a>namenode marks the block replica as corrupt so it doesn’t
      direct any more clients to it or try to copy this replica to another
      datanode. It then schedules a copy of the block to be replicated on
      another datanode, so its replication factor is back at the expected
      level. Once this has happened, the corrupt replica is deleted.</p><p class="calibre2">It is possible to disable verification of checksums by passing
      <code class="literal">false</code> to the <code class="literal">setVerifyChecksum()</code> method on <code class="literal">FileSystem</code> before <a class="calibre" id="calibre_link-1660"></a>using the <code class="literal">open()</code> method
      to read a file. The same effect is possible from the shell by using the
      <code class="literal">-ignoreCrc</code> option with the <code class="literal">-get</code> or the equivalent <code class="literal">-copyToLocal</code> command. This feature is useful
      if you have a corrupt file that you want to inspect so you can decide
      what to do with it. For example, you might want to see whether it can be
      salvaged before you delete it.</p><p class="calibre2">You can find a file’s checksum <a class="calibre" id="calibre_link-1843"></a><a class="calibre" id="calibre_link-1629"></a>with <code class="literal">hadoop fs
      -checksum</code>. This is useful to check whether two files in HDFS
      have the same contents—something <a class="calibre" id="calibre_link-1514"></a>that <em class="calibre10">distcp</em> does, for
      example (see <a class="ulink" href="#calibre_link-228" title="Parallel Copying with distcp">Parallel Copying with distcp</a>).</p></div><div class="book" title="LocalFileSystem"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-171">LocalFileSystem</h3></div></div></div><p class="calibre2">The Hadoop <code class="literal">LocalFileSystem</code>
      performs<a class="calibre" id="calibre_link-1339"></a><a class="calibre" id="calibre_link-2347"></a> client-side checksumming. This means that when you write
      a file called <em class="calibre10">filename</em>, the
      filesystem client transparently creates a hidden file, <em class="calibre10">.filename.crc</em>, in the same directory
      containing the checksums for each chunk of the file. The chunk size is
      controlled by the <code class="literal">file.bytes-per-checksum</code> property, <a class="calibre" id="calibre_link-1635"></a>which defaults to 512 bytes. The chunk size is stored as
      metadata in the <em class="calibre10">.crc</em> file, so the
      file can be read back correctly even if the setting for the chunk size
      has changed. Checksums are verified when the file is read, and if an
      error is detected, <code class="literal">LocalFileSystem</code> throws a <code class="literal">ChecksumException</code>.</p><p class="calibre2">Checksums are fairly cheap to compute (in Java, they are
      implemented in native code), typically adding a few percent overhead to
      the time to read or write a file. For
      most applications, this is an acceptable price to pay for data
      integrity. It is, however, possible to disable checksums, which is
      typically done when the underlying filesystem supports checksums
      natively. This is accomplished by <a class="calibre" id="calibre_link-3138"></a>using <code class="literal">RawLocalFileSystem</code> in place of <code class="literal">LocalFile</code><code class="literal">System</code>. To do this globally in an
      application, it suffices to remap the <a class="calibre" id="calibre_link-1742"></a>implementation for
      <code class="literal">file</code> URIs by setting the property <code class="literal">fs.file.impl</code> to the value <code class="literal">org.</code><code class="literal">apache.</code><code class="literal">hadoop.fs.RawLocalFileSystem</code>.
      Alternatively, you can directly create a <code class="literal">RawLocal</code><code class="literal">FileSystem</code> instance, which may be
      useful if you want to disable checksum verification for only some reads,
      for example:</p><a id="calibre_link-4088" class="calibre"></a><pre class="screen1"><code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="o">...</code> 
<code class="n">FileSystem</code> <code class="n">fs</code> <code class="o">=</code> <code class="k">new</code> <code class="n">RawLocalFileSystem</code><code class="o">();</code>
<code class="n">fs</code><code class="o">.</code><code class="na">initialize</code><code class="o">(</code><code class="k">null</code><code class="o">,</code> <code class="n">conf</code><code class="o">);</code></pre></div><div class="book" title="ChecksumFileSystem"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4089">ChecksumFileSystem</h3></div></div></div><p class="calibre2"><code class="literal">LocalFileSystem</code> uses <code class="literal">ChecksumFileSystem</code> to do its work, <a class="calibre" id="calibre_link-1096"></a><a class="calibre" id="calibre_link-1337"></a>and this class makes it easy to add checksumming to other
      (nonchecksummed) filesystems, as <code class="literal">ChecksumFile</code><code class="literal">System</code>
      is just a wrapper around <code class="literal">FileSystem</code>.
      The general idiom is as follows:</p><a id="calibre_link-4090" class="calibre"></a><pre class="screen1"><code class="n">FileSystem</code> <code class="n">rawFs</code> <code class="o">=</code> <code class="o">...</code> 
<code class="n">FileSystem</code> <code class="n">checksummedFs</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ChecksumFileSystem</code><code class="o">(</code><code class="n">rawFs</code><code class="o">);</code></pre><p class="calibre2">The underlying filesystem is called the <em class="calibre10">raw</em>
      filesystem, and may be retrieved using the <code class="literal">getRawFileSystem()</code> method on <code class="literal">ChecksumFileSystem</code>. <code class="literal">ChecksumFileSystem</code> has a few more useful
      methods for working with checksums, such as <code class="literal">getChecksumFile()</code> for getting the path of a
      checksum file for any file. Check the documentation for the
      others.</p><p class="calibre2">If an error is detected by <code class="literal">ChecksumFileSystem</code> when reading a file, it
      will call its <code class="literal">reportChecksumFailure()</code> method. The
      default implementation does nothing, but <code class="literal">LocalFileSystem</code> moves the offending file and
      its checksum to a side directory on the same device called <em class="calibre10">bad_files</em>. Administrators should periodically
      check for these bad files and take <a class="calibre" id="calibre_link-2068"></a><a class="calibre" id="calibre_link-1098"></a>action on them.</p></div></div><div class="book" title="Compression"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-506">Compression</h2></div></div></div><p class="calibre2">File compression <a class="calibre" id="calibre_link-2065"></a><a class="calibre" id="calibre_link-1205"></a><a class="calibre" id="calibre_link-1622"></a>brings two major benefits: it reduces the space needed to
    store files, and it speeds up data transfer across the network or to or
    from disk. When dealing with large volumes of data, both of these savings
    can be significant, so it pays to carefully consider how to use
    compression in Hadoop.</p><p class="calibre2">There are many different compression formats, tools, and algorithms,
    each with different characteristics. <a class="ulink" href="#calibre_link-229" title="Table&nbsp;5-1.&nbsp;A summary of compression formats">Table&nbsp;5-1</a>
    lists some of the <a class="calibre" id="calibre_link-1209"></a>more common ones that can be used with Hadoop.</p><div class="table"><a id="calibre_link-229" class="calibre"></a><div class="table-title">Table&nbsp;5-1.&nbsp;A summary of compression formats</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Compression format</td><td class="calibre20">Tool</td><td class="calibre20">Algorithm</td><td class="calibre20">Filename extension</td><td class="calibre21">Splittable?</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">DEFLATE<sup class="calibre5">[<a class="firstname" href="#calibre_link-230" id="calibre_link-232">a</a>]</sup></td><td class="calibre23">N/A</td><td class="calibre23">DEFLATE</td><td class="calibre23"><em class="calibre10">.deflate</em></td><td class="calibre25">No</td></tr><tr class="calibre26"><td class="calibre23">gzip</td><td class="calibre23"><em class="calibre10">gzip</em></td><td class="calibre23">DEFLATE</td><td class="calibre23"><em class="calibre10">.gz</em></td><td class="calibre25">No</td></tr><tr class="calibre19"><td class="calibre23">bzip2</td><td class="calibre23"><em class="calibre10">bzip2</em></td><td class="calibre23">bzip2</td><td class="calibre23"><em class="calibre10">.bz2</em></td><td class="calibre25">Yes</td></tr><tr class="calibre26"><td class="calibre23">LZO</td><td class="calibre23"><em class="calibre10">lzop</em></td><td class="calibre23">LZO</td><td class="calibre23"><em class="calibre10">.lzo</em></td><td class="calibre25">No<sup class="calibre5">[<a class="firstname" href="#calibre_link-231" id="calibre_link-233">b</a>]</sup></td></tr><tr class="calibre19"><td class="calibre23">LZ4</td><td class="calibre23">N/A</td><td class="calibre23">LZ4</td><td class="calibre23"><em class="calibre10">.lz4</em></td><td class="calibre25">No</td></tr><tr class="calibre26"><td class="calibre27">Snappy</td><td class="calibre27">N/A</td><td class="calibre27">Snappy</td><td class="calibre27"><em class="calibre10">.snappy</em></td><td class="calibre28">No</td></tr></tbody><tbody class="calibre22"><tr class="calibre19"><td colspan="5" class="calibre28"><div class="footnote1" id="calibre_link-230"><p class="calibre2"><sup class="calibre37">[<a class="firstname" href="#calibre_link-232">a</a>] </sup>DEFLATE is a compression algorithm whose standard
                implementation is zlib. There is no commonly available
                command-line tool for producing files in DEFLATE format, as
                gzip is normally used. (Note that the gzip file format is
                DEFLATE with extra headers and a footer.) The <em class="calibre10">.deflate</em> filename extension is a
                Hadoop convention.</p></div><div class="footnote1" id="calibre_link-231"><p class="calibre2"><sup class="calibre37">[<a class="firstname" href="#calibre_link-233">b</a>] </sup>However, LZO files are splittable if they have been
                indexed in a preprocessing step. See <a class="ulink" href="#calibre_link-234" title="Compression and Input Splits">Compression and Input Splits</a>.</p></div></td></tr></tbody></table></div></div><p class="calibre2">All compression <a class="calibre" id="calibre_link-1416"></a><a class="calibre" id="calibre_link-1830"></a><a class="calibre" id="calibre_link-1039"></a><a class="calibre" id="calibre_link-2375"></a><a class="calibre" id="calibre_link-2371"></a><a class="calibre" id="calibre_link-3418"></a>algorithms exhibit a space/time trade-off: faster
    compression and decompression speeds usually come at the expense of
    smaller space savings. The tools listed in <a class="ulink" href="#calibre_link-229" title="Table&nbsp;5-1.&nbsp;A summary of compression formats">Table&nbsp;5-1</a> typically give some control over this
    trade-off at compression time by offering nine different options: <code class="literal">–1</code> means optimize for speed, and <code class="literal">-9</code> means optimize for space. For example, the
    following command creates a compressed file <em class="calibre10">file.gz</em> using the fastest compression
    method:</p><pre class="screen1">% <strong class="userinput"><code class="calibre9">gzip -1 file</code></strong></pre><p class="calibre2">The different tools have very different compression characteristics.
    gzip is a general-purpose compressor and sits in the middle of the
    space/time trade-off. bzip2 compresses more effectively than gzip, but is
    slower. bzip2’s decompression speed is faster than its compression speed,
    but it is still slower than the other formats. LZO, LZ4, and Snappy, on
    the other hand, all optimize for speed and are around an order of
    magnitude faster than gzip, but compress less effectively. Snappy and LZ4
    are also significantly faster than LZO for decompression.<sup class="calibre6">[<a class="firstname" href="#calibre_link-235" id="calibre_link-277">44</a>]</sup></p><p class="calibre2">The “Splittable” column in <a class="ulink" href="#calibre_link-229" title="Table&nbsp;5-1.&nbsp;A summary of compression formats">Table&nbsp;5-1</a>
    indicates whether the compression format supports splitting (that is,
    whether you can seek to any point in the stream and start reading from
    some point further on). Splittable compression formats are especially
    suitable for MapReduce; see <a class="ulink" href="#calibre_link-234" title="Compression and Input Splits">Compression and Input Splits</a>
    for further discussion.</p><div class="book" title="Codecs"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-242">Codecs</h3></div></div></div><p class="calibre2">A <em class="calibre10">codec</em> is the <a class="calibre" id="calibre_link-1206"></a><a class="calibre" id="calibre_link-1157"></a>implementation of a compression-decompression algorithm.
      In Hadoop, a codec is represented by an implementation of <a class="calibre" id="calibre_link-1217"></a>the <code class="literal">CompressionCodec</code>
      interface. So, for example, <code class="literal">GzipCodec</code>
      encapsulates the compression and decompression algorithm for gzip. <a class="ulink" href="#calibre_link-236" title="Table&nbsp;5-2.&nbsp;Hadoop compression codecs">Table&nbsp;5-2</a> lists the codecs <a class="calibre" id="calibre_link-1161"></a>that are available for Hadoop.</p><div class="table"><a id="calibre_link-236" class="calibre"></a><div class="table-title">Table&nbsp;5-2.&nbsp;Hadoop compression codecs</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Compression format</td><td class="calibre21">Hadoop CompressionCodec</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">DEFLATE</td><td class="calibre25"><code class="uri">org.apache.hadoop.io.compress.DefaultCodec</code></td></tr><tr class="calibre26"><td class="calibre23">gzip</td><td class="calibre25"><code class="uri">org.apache.hadoop.io.compress.GzipCodec</code></td></tr><tr class="calibre19"><td class="calibre23">bzip2</td><td class="calibre25"><code class="uri">org.apache.hadoop.io.compress.BZip2Codec</code></td></tr><tr class="calibre26"><td class="calibre23">LZO</td><td class="calibre25"><code class="uri">com.hadoop.compression.lzo.LzopCodec</code></td></tr><tr class="calibre19"><td class="calibre23">LZ4</td><td class="calibre25"><code class="uri">org.apache.hadoop.io.compress.Lz4Codec</code></td></tr><tr class="calibre26"><td class="calibre27">Snappy</td><td class="calibre28"><code class="uri">org.apache.hadoop.io.compress.SnappyCodec</code></td></tr></tbody></table></div></div><p class="calibre2">The LZO libraries are GPL licensed and <a class="calibre" id="calibre_link-1411"></a><a class="calibre" id="calibre_link-1833"></a><a class="calibre" id="calibre_link-1042"></a><a class="calibre" id="calibre_link-2374"></a><a class="calibre" id="calibre_link-3421"></a>may not be included in Apache distributions, so for this
      reason the Hadoop codecs must be downloaded separately from <a class="ulink" href="http://code.google.com/p/hadoop-gpl-compression/" target="_top">Google</a>
      (or <a class="ulink" href="http://github.com/kevinweil/hadoop-lzo" target="_top">GitHub</a>,
      which includes bug fixes and more tools). <a class="calibre" id="calibre_link-2380"></a>The <code class="literal">LzopCodec</code>, which is
      compatible <a class="calibre" id="calibre_link-2378"></a><a class="calibre" id="calibre_link-2379"></a>with the <span class="calibre"><em class="calibre10">lzop</em></span> tool, is essentially
      the LZO format with extra headers, and is the one you normally want.
      There is also an <code class="literal">LzoCodec</code> for the
      pure LZO format, which uses the <em class="calibre10">.lzo_deflate</em> filename extension (by analogy
      with DEFLATE, which is gzip without <a class="calibre" id="calibre_link-1040"></a><a class="calibre" id="calibre_link-1417"></a><a class="calibre" id="calibre_link-1831"></a><a class="calibre" id="calibre_link-2376"></a><a class="calibre" id="calibre_link-2372"></a><a class="calibre" id="calibre_link-3419"></a>the headers).</p><div class="book" title="Compressing and decompressing streams with CompressionCodec"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4091">Compressing and decompressing streams with
        CompressionCodec</h4></div></div></div><p class="calibre2"><code class="literal">CompressionCodec</code> has two
        <a class="calibre" id="calibre_link-1218"></a><a class="calibre" id="calibre_link-1219"></a><a class="calibre" id="calibre_link-1158"></a><a class="calibre" id="calibre_link-1159"></a>methods that allow you to easily compress or decompress
        data. To compress data being written to an output stream, use the
        <code class="literal">createOutputStream(OutputStream
        out)</code> method to create a <code class="literal">CompressionOutputStream</code> to which <a class="calibre" id="calibre_link-1224"></a>you write your uncompressed data to have it written in
          compressed form to the underlying stream. Conversely, to decompress
        data being read from an input stream, call <code class="literal">createInputStream(InputStream in)</code> to
        obtain a <code class="literal">CompressionInputStream</code>,
        which <a class="calibre" id="calibre_link-1223"></a>allows you to read uncompressed data from the underlying
        stream.</p><p class="calibre2"><code class="literal">CompressionOutputStream</code> and <code class="literal">CompressionInputStream</code> are similar to
        <code class="literal">java.util.</code><span class="calibre"><code class="literal">zip.DeflaterOutputStream</code> and <code class="literal">java.util.zip.DeflaterInputStream</code></span>,
        except <a class="calibre" id="calibre_link-1420"></a><a class="calibre" id="calibre_link-1419"></a>that both of the former provide the ability to reset
        their underlying compressor or decompressor. This is important for
        applications that compress sections of the data stream as separate
        blocks, such <a class="calibre" id="calibre_link-3327"></a>as in a <code class="literal">SequenceFile</code>,
        described in <a class="ulink" href="#calibre_link-141" title="SequenceFile">SequenceFile</a>.</p><p class="calibre2"><a class="ulink" href="#calibre_link-237" title="Example&nbsp;5-1.&nbsp;A program to compress data read from standard input and write it to standard output">Example&nbsp;5-1</a> illustrates how to use the
        API to compress data read from standard input and write it to standard
        output.</p><div class="example"><a id="calibre_link-237" class="calibre"></a><div class="example-title">Example&nbsp;5-1.&nbsp;A program to compress data read from standard input and write
          it to standard output</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">StreamCompressor</code> <code class="o">{</code>

  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">String</code> <code class="n">codecClassname</code> <code class="o">=</code> <code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">];</code>
    <code class="n">Class</code><code class="o">&lt;?&gt;</code> <code class="n">codecClass</code> <code class="o">=</code> <code class="n">Class</code><code class="o">.</code><code class="na">forName</code><code class="o">(</code><code class="n">codecClassname</code><code class="o">);</code>
    <code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Configuration</code><code class="o">();</code>
    <code class="n">CompressionCodec</code> <code class="n">codec</code> <code class="o">=</code> <code class="o">(</code><code class="n">CompressionCodec</code><code class="o">)</code>
      <code class="n">ReflectionUtils</code><code class="o">.</code><code class="na">newInstance</code><code class="o">(</code><code class="n">codecClass</code><code class="o">,</code> <code class="n">conf</code><code class="o">);</code>
    
    <code class="n">CompressionOutputStream</code> <code class="n">out</code> <code class="o">=</code> <code class="n">codec</code><code class="o">.</code><code class="na">createOutputStream</code><code class="o">(</code><code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">);</code>
    <code class="n">IOUtils</code><code class="o">.</code><code class="na">copyBytes</code><code class="o">(</code><code class="n">System</code><code class="o">.</code><code class="na">in</code><code class="o">,</code> <code class="n">out</code><code class="o">,</code> <code class="mi">4096</code><code class="o">,</code> <code class="k">false</code><code class="o">);</code>
    <code class="n">out</code><code class="o">.</code><code class="na">finish</code><code class="o">();</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The application expects the fully qualified name of the <code class="literal">CompressionCodec</code> implementation as the first
        command-line argument. We <a class="calibre" id="calibre_link-3204"></a>use <code class="literal">ReflectionUtils</code>
        to construct a new instance of the codec, then obtain a compression
        wrapper around <code class="literal">System.out</code>. Then we
        call the utility method <code class="literal">copyBytes()</code>
        on <code class="literal">IOUtils</code> to <a class="calibre" id="calibre_link-2150"></a>copy the input to the output, which is compressed by the
        <code class="literal">CompressionOutputStream</code>. Finally,
        we call <code class="literal">finish()</code> on <code class="literal">CompressionOutputStream</code>, which tells
        the compressor to finish writing to the compressed stream, but doesn’t
        close the stream. We can try it out with the following command line,
        which compresses the string “Text” using the <code class="literal">StreamCompressor</code> program with the <code class="literal">GzipCodec</code>, then decompresses it from
        standard input using <em class="calibre10">gunzip</em>:</p><pre class="screen1">% <strong class="userinput"><code class="calibre9">echo "Text" | hadoop StreamCompressor org.apache.hadoop.io.compress.GzipCodec \</code></strong>
<strong class="userinput"><code class="calibre9">  | gunzip -</code></strong>
Text</pre></div><div class="book" title="Inferring CompressionCodecs using CompressionCodecFactory"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-243">Inferring CompressionCodecs using
        CompressionCodecFactory</h4></div></div></div><p class="calibre2">If you are reading <a class="calibre" id="calibre_link-1220"></a><a class="calibre" id="calibre_link-1160"></a>a compressed file, normally you can infer which codec to
        use by looking at its filename extension. A file ending in <em class="calibre10">.gz</em> can be read with <code class="literal">GzipCodec</code>, and so on. The extensions for
        each compression format are listed in <a class="ulink" href="#calibre_link-229" title="Table&nbsp;5-1.&nbsp;A summary of compression formats">Table&nbsp;5-1</a>.</p><p class="calibre2"><code class="literal">CompressionCodecFactory</code>
        provides <a class="calibre" id="calibre_link-1222"></a>a way of mapping a filename extension to a <code class="literal">CompressionCodec</code> using
        its <code class="literal">getCodec()</code> method, which takes
        a <code class="literal">Path</code> object for the file in
        question. <a class="ulink" href="#calibre_link-238" title="Example&nbsp;5-2.&nbsp;A program to decompress a compressed file using a codec inferred from the file’s extension">Example&nbsp;5-2</a> shows an application
        that uses this feature to decompress files.</p><div class="example"><a id="calibre_link-238" class="calibre"></a><div class="example-title">Example&nbsp;5-2.&nbsp;A program to decompress a compressed file using a codec
          inferred from the file’s extension</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">FileDecompressor</code> <code class="o">{</code>

  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">String</code> <code class="n">uri</code> <code class="o">=</code> <code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">];</code>
    <code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Configuration</code><code class="o">();</code>
    <code class="n">FileSystem</code> <code class="n">fs</code> <code class="o">=</code> <code class="n">FileSystem</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="n">URI</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">uri</code><code class="o">),</code> <code class="n">conf</code><code class="o">);</code>
    
    <code class="n">Path</code> <code class="n">inputPath</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">uri</code><code class="o">);</code>
    <code class="n">CompressionCodecFactory</code> <code class="n">factory</code> <code class="o">=</code> <code class="k">new</code> <code class="n">CompressionCodecFactory</code><code class="o">(</code><code class="n">conf</code><code class="o">);</code>
    <code class="n">CompressionCodec</code> <code class="n">codec</code> <code class="o">=</code> <code class="n">factory</code><code class="o">.</code><code class="na">getCodec</code><code class="o">(</code><code class="n">inputPath</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">codec</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="sb">"No codec found for "</code> <code class="o">+</code> <code class="n">uri</code><code class="o">);</code>
      <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="mi">1</code><code class="o">);</code>
    <code class="o">}</code>

    <code class="n">String</code> <code class="n">outputUri</code> <code class="o">=</code>
        <code class="n">CompressionCodecFactory</code><code class="o">.</code><code class="na">removeSuffix</code><code class="o">(</code><code class="n">uri</code><code class="o">,</code> <code class="n">codec</code><code class="o">.</code><code class="na">getDefaultExtension</code><code class="o">());</code>

    <code class="n">InputStream</code> <code class="n">in</code> <code class="o">=</code> <code class="k">null</code><code class="o">;</code>
    <code class="n">OutputStream</code> <code class="n">out</code> <code class="o">=</code> <code class="k">null</code><code class="o">;</code>
    <code class="k">try</code> <code class="o">{</code>
      <code class="n">in</code> <code class="o">=</code> <code class="n">codec</code><code class="o">.</code><code class="na">createInputStream</code><code class="o">(</code><code class="n">fs</code><code class="o">.</code><code class="na">open</code><code class="o">(</code><code class="n">inputPath</code><code class="o">));</code>
      <code class="n">out</code> <code class="o">=</code> <code class="n">fs</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">outputUri</code><code class="o">));</code>
      <code class="n">IOUtils</code><code class="o">.</code><code class="na">copyBytes</code><code class="o">(</code><code class="n">in</code><code class="o">,</code> <code class="n">out</code><code class="o">,</code> <code class="n">conf</code><code class="o">);</code>
    <code class="o">}</code> <code class="k">finally</code> <code class="o">{</code>
      <code class="n">IOUtils</code><code class="o">.</code><code class="na">closeStream</code><code class="o">(</code><code class="n">in</code><code class="o">);</code>
      <code class="n">IOUtils</code><code class="o">.</code><code class="na">closeStream</code><code class="o">(</code><code class="n">out</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">Once the codec has been found, it is used to strip off the file
        suffix to form the output filename (via the <code class="literal">removeSuffix()</code> static method of <code class="literal">CompressionCodecFactory</code>). In this way, a
        file named <em class="calibre10">file.gz</em> is
        decompressed to <em class="calibre10">file</em> by invoking
        the program as follows:</p><pre class="screen1">% <strong class="userinput"><code class="calibre9">hadoop FileDecompressor file.gz</code></strong></pre><p class="calibre2"><code class="literal">CompressionCodecFactory</code> loads
        all the codecs in <a class="ulink" href="#calibre_link-236" title="Table&nbsp;5-2.&nbsp;Hadoop compression codecs">Table&nbsp;5-2</a>, except LZO, as
        well as any listed in the <code class="literal">io.compression.codecs</code> configuration property
        (<a class="ulink" href="#calibre_link-239" title="Table&nbsp;5-3.&nbsp;Compression codec properties">Table&nbsp;5-3</a>). By default, the
        property is empty; you would need to alter it only if you have a
        custom codec that you wish to register (such as the externally hosted
        LZO codecs). Each codec knows its default filename extension, thus
        permitting <code class="literal">CompressionCodecFactory</code>
        to search through the registered codecs to find a match for the given
        extension (if any).</p><div class="table"><a id="calibre_link-239" class="calibre"></a><div class="table-title">Table&nbsp;5-3.&nbsp;Compression codec properties</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Type</td><td class="calibre20">Default value</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre27"><code class="uri">io.compression.codecs</code></td><td class="calibre27">Comma-separated
                <code class="uri">Class</code> names</td><td class="calibre27">&nbsp;</td><td class="calibre28">A list of <a class="calibre" id="calibre_link-2146"></a><a class="calibre" id="calibre_link-1221"></a>additional <code class="uri">CompressionCodec</code> classes
                for compression/decompression</td></tr></tbody></table></div></div></div><div class="book" title="Native libraries"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4092">Native libraries</h4></div></div></div><p class="calibre2">For performance, it is <a class="calibre" id="calibre_link-1162"></a>preferable to use a native library for compression and
        decompression. For example, in
        one test, using the native gzip libraries reduced decompression times
        by up to 50% and compression times by around 10% (compared to the
        built-in Java implementation). <a class="ulink" href="#calibre_link-240" title="Table&nbsp;5-4.&nbsp;Compression library implementations">Table&nbsp;5-4</a> shows the availability of Java
        and native implementations for
        each compression format. All formats have native implementations, but
        not all have a Java implementation (LZO, for example).</p><div class="table"><a id="calibre_link-240" class="calibre"></a><div class="table-title">Table&nbsp;5-4.&nbsp;Compression library implementations</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Compression format</td><td class="calibre20">Java implementation?</td><td class="calibre21">Native implementation?</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">DEFLATE</td><td class="calibre23">Yes</td><td class="calibre25">Yes</td></tr><tr class="calibre26"><td class="calibre23">gzip</td><td class="calibre23">Yes</td><td class="calibre25">Yes</td></tr><tr class="calibre19"><td class="calibre23">bzip2</td><td class="calibre23">Yes</td><td class="calibre25">Yes</td></tr><tr class="calibre26"><td class="calibre23">LZO</td><td class="calibre23">No</td><td class="calibre25">Yes</td></tr><tr class="calibre19"><td class="calibre23">LZ4</td><td class="calibre23">No</td><td class="calibre25">Yes</td></tr><tr class="calibre26"><td class="calibre27">Snappy</td><td class="calibre27">No</td><td class="calibre28">Yes</td></tr></tbody></table></div></div><p class="calibre2">The Apache Hadoop <a class="calibre" id="calibre_link-1418"></a><a class="calibre" id="calibre_link-1832"></a><a class="calibre" id="calibre_link-1041"></a><a class="calibre" id="calibre_link-2377"></a><a class="calibre" id="calibre_link-2373"></a><a class="calibre" id="calibre_link-3420"></a>binary tarball comes with prebuilt native compression
        binaries for 64-bit Linux, called <em class="calibre10">libhadoop.so</em>. For other platforms, you will
        need to compile the libraries yourself, following the <em class="calibre10">BUILDING.txt</em> instructions at the top level
        of the source tree.</p><p class="calibre2">The native libraries are picked up using the Java system
        <a class="calibre" id="calibre_link-2193"></a>property <code class="literal">java.library.path</code>. The <em class="calibre10">hadoop</em> script in the <em class="calibre10">etc/hadoop</em> directory sets this property for
        you, but if you don’t use this script, you will need to set the
        property in your application.</p><p class="calibre2">By default, Hadoop looks for native libraries for the platform
        it is running on, and loads them automatically if they are found. This
        means you don’t have to change any configuration settings to use the
        native libraries. In some circumstances, however, you may wish to
        disable use of native libraries, such as when you are debugging a
        compression-related problem. You can do this by setting the <a class="calibre" id="calibre_link-2148"></a>property <code class="literal">io.native.lib.available</code> to <code class="literal">false</code>, which ensures that the built-in Java
        equivalents will be used (if they are available).</p><div class="book" title="CodecPool"><div class="titlepage1"><div class="book"><div class="book"><h5 class="title8" id="calibre_link-4093">CodecPool</h5></div></div></div><p class="calibre2">If you are using a native <a class="calibre" id="calibre_link-1156"></a>library and you are doing a lot of compression or
          decompression in your application, consider using <code class="literal">CodecPool</code>, which allows you to reuse
          compressors and decompressors, thereby amortizing the cost of
          creating these objects.</p><p class="calibre2">The code in <a class="ulink" href="#calibre_link-241" title="Example&nbsp;5-3.&nbsp;A program to compress data read from standard input and write it to standard output using a pooled compressor">Example&nbsp;5-3</a> shows
          the API, although in this program, which creates only a single
          <code class="literal">Compressor</code>, there is really no
          need to use a pool.</p><div class="example"><a id="calibre_link-241" class="calibre"></a><div class="example-title">Example&nbsp;5-3.&nbsp;A program to compress data read from standard input and
            write it to standard output using a pooled compressor</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">PooledStreamCompressor</code> <code class="o">{</code>

  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">String</code> <code class="n">codecClassname</code> <code class="o">=</code> <code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">];</code>
    <code class="n">Class</code><code class="o">&lt;?&gt;</code> <code class="n">codecClass</code> <code class="o">=</code> <code class="n">Class</code><code class="o">.</code><code class="na">forName</code><code class="o">(</code><code class="n">codecClassname</code><code class="o">);</code>
    <code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Configuration</code><code class="o">();</code>
    <code class="n">CompressionCodec</code> <code class="n">codec</code> <code class="o">=</code> <code class="o">(</code><code class="n">CompressionCodec</code><code class="o">)</code>
        <code class="n">ReflectionUtils</code><code class="o">.</code><code class="na">newInstance</code><code class="o">(</code><code class="n">codecClass</code><code class="o">,</code> <code class="n">conf</code><code class="o">);</code>
    <span class="calibre24"><strong class="calibre9"><code class="n1">Compressor</code> <code class="n1">compressor</code> <code class="o1">=</code> <code class="kc">null</code><code class="o1">;</code>
    <code class="kc">try</code> <code class="o1">{</code>
      <code class="n1">compressor</code> <code class="o1">=</code> <code class="n1">CodecPool</code><code class="o1">.</code><code class="na1">getCompressor</code><code class="o1">(</code><code class="n1">codec</code><code class="o1">);</code></strong></span>
      <code class="n">CompressionOutputStream</code> <code class="n">out</code> <code class="o">=</code>
          <code class="n">codec</code><code class="o">.</code><code class="na">createOutputStream</code><code class="o">(</code><code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">,</code> <span class="calibre24"><strong class="calibre9"><code class="n1">compressor</code></strong></span><code class="o">);</code>
      <code class="n">IOUtils</code><code class="o">.</code><code class="na">copyBytes</code><code class="o">(</code><code class="n">System</code><code class="o">.</code><code class="na">in</code><code class="o">,</code> <code class="n">out</code><code class="o">,</code> <code class="mi">4096</code><code class="o">,</code> <code class="k">false</code><code class="o">);</code>
      <code class="n">out</code><code class="o">.</code><code class="na">finish</code><code class="o">();</code>
    <span class="calibre24"><strong class="calibre9"><code class="o1">}</code> <code class="kc">finally</code> <code class="o1">{</code>
      <code class="n1">CodecPool</code><code class="o1">.</code><code class="na1">returnCompressor</code><code class="o1">(</code><code class="n1">compressor</code><code class="o1">);</code>
    <code class="o1">}</code></strong></span>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">We retrieve a <code class="literal">Compressor</code>
          instance from the pool for a given <code class="literal">CompressionCodec</code>, which we use in the
          codec’s overloaded <code class="literal">createOutputStream()</code>
          method. By using a <code class="literal">finally</code> block,
          we ensure that the compressor is returned to the pool even if there
          is an <code class="literal">IOException</code> while copying the bytes
          between the <a class="calibre" id="calibre_link-1207"></a><a class="calibre" id="calibre_link-1163"></a>streams.</p></div></div></div><div class="book" title="Compression and Input Splits"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-234">Compression and Input Splits</h3></div></div></div><p class="calibre2">When <a class="calibre" id="calibre_link-1208"></a><a class="calibre" id="calibre_link-2115"></a><a class="calibre" id="calibre_link-2104"></a>considering how to compress data that will be processed by
      MapReduce, it is important to understand whether the compression format
      supports splitting. Consider an uncompressed file stored in HDFS whose
      size is 1 GB. With an HDFS block size of 128 MB, the file will be stored
      as eight blocks, and a MapReduce job using this file as input will
      create eight input splits, each processed independently as input to a
      separate map task.</p><p class="calibre2">Imagine now that the file is a gzip-compressed file whose
      compressed size is 1 GB. As before, HDFS will store the file as eight
      blocks. However, creating a split for each block won’t work, because it
      is impossible to start reading at an arbitrary point in the gzip stream
      and therefore impossible for a map task to read its split independently
      of the others. The gzip format uses DEFLATE to store the compressed
      data, and DEFLATE stores data as a series of compressed blocks. The
      problem is that the start of each block is not distinguished in any way
      that would allow a reader positioned at an arbitrary point in the stream
      to advance to the beginning of the next block, thereby synchronizing
      itself with the stream. For this reason, gzip does not support
      splitting.</p><p class="calibre2">In this case, <a class="calibre" id="calibre_link-1211"></a><a class="calibre" id="calibre_link-2448"></a>MapReduce will do the right thing and not try to split the
      gzipped file, since it knows that the input is gzip-compressed (by
      looking at the filename extension) and that gzip does not support
      splitting. This will work, but at the expense of locality: a single map
      will process the eight HDFS blocks, most of which will not be local to
      the map. Also, with fewer maps, the job is less granular and so may take
      longer to run.</p><p class="calibre2">If the file in our hypothetical example were an LZO file, we would
      have the same problem because the underlying compression format does not
      provide a way for a reader to synchronize itself with the stream.
      However, it is possible to preprocess LZO files using an indexer tool
      that comes with the Hadoop LZO libraries, which you can obtain from the
      Google and GitHub sites listed in <a class="ulink" href="#calibre_link-242" title="Codecs">Codecs</a>. The tool
      builds an index of split points, effectively making them splittable when
      the appropriate MapReduce input format is used.</p><p class="calibre2">A bzip2 file, on the other hand, does provide a synchronization
      marker between blocks (a 48-bit approximation of pi), so it does support
      splitting. (<a class="ulink" href="#calibre_link-229" title="Table&nbsp;5-1.&nbsp;A summary of compression formats">Table&nbsp;5-1</a> lists whether each
      compression format supports splitting.)</p><div class="sidebar"><a id="calibre_link-4094" class="calibre"></a><div class="sidebar-title">Which Compression Format Should I Use?</div><p class="calibre2">Hadoop applications <a class="calibre" id="calibre_link-1214"></a>process large datasets, so you should strive to take
        advantage of compression. Which compression format you use depends on
        such considerations as file size, format, and the tools you are using
        for processing. Here are some suggestions, arranged roughly in order
        of most to least effective:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">Use a container file format such as sequence files (see the section), Avro
            datafiles (see the section), ORCFiles (see the section), or Parquet files (see the section),
            all of which support both compression and splitting. A fast
            compressor such as LZO, LZ4, or Snappy is generally a good
            choice.</p></li><li class="listitem"><p class="calibre2">Use a compression format that supports splitting, such as
            bzip2 (although bzip2 is fairly slow), or one that can be indexed
            to support splitting, such as LZO.</p></li><li class="listitem"><p class="calibre2">Split the file into chunks in the application, and compress
            each chunk separately using any supported compression format (it
            doesn’t matter whether it is splittable). In this case, you should
            choose the chunk size so that the compressed chunks are
            approximately the size of an HDFS block.</p></li><li class="listitem"><p class="calibre2">Store the files uncompressed.</p></li></ul></div><p class="calibre2">For large files, you should <span class="calibre">not</span> use a compression format that does not
        support splitting on the whole file, because you lose locality and
        make MapReduce applications very inefficient.</p></div></div><div class="book" title="Using Compression in MapReduce"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-599">Using Compression in MapReduce</h3></div></div></div><p class="calibre2">As described in <a class="ulink" href="#calibre_link-243" title="Inferring CompressionCodecs using CompressionCodecFactory">Inferring CompressionCodecs using
        CompressionCodecFactory</a>, if
      your input files are compressed, they will be decompressed automatically as they are
      read by MapReduce, using the filename extension to determine which codec
      to use.</p><p class="calibre2">In order to compress the output of a MapReduce job, in the job
      configuration, set the <code class="literal">mapreduce.output.fileoutputformat.compress</code>
      property <a class="calibre" id="calibre_link-2596"></a>to <code class="literal">true</code> and set the
      <code class="literal">mapreduce.output.fileoutputformat.compress.codec</code>
      property <a class="calibre" id="calibre_link-2598"></a>to the classname of the compression codec you want to use.
      Alternatively, you can use the static convenience methods on <code class="literal">FileOutputFormat</code> to <a class="calibre" id="calibre_link-1644"></a>set these properties, as shown in <a class="ulink" href="#calibre_link-244" title="Example&nbsp;5-4.&nbsp;Application to run the maximum temperature job producing compressed output">Example&nbsp;5-4</a>.</p><div class="example"><a id="calibre_link-244" class="calibre"></a><div class="example-title">Example&nbsp;5-4.&nbsp;Application to run the maximum temperature job producing
        compressed output</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">MaxTemperatureWithCompression</code> <code class="o">{</code>

  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="na">length</code> <code class="o">!=</code> <code class="mi">2</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="sb">"Usage: MaxTemperatureWithCompression &lt;input path&gt; "</code> <code class="o">+</code>
          <code class="sb">"&lt;output path&gt;"</code><code class="o">);</code>
      <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(-</code><code class="mi">1</code><code class="o">);</code>
    <code class="o">}</code>

    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Job</code><code class="o">();</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setJarByClass</code><code class="o">(</code><code class="n">MaxTemperature</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="n">FileInputFormat</code><code class="o">.</code><code class="na">addInputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]));</code>
    <code class="n">FileOutputFormat</code><code class="o">.</code><code class="na">setOutputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">]));</code>
    
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputKeyClass</code><code class="o">(</code><code class="n">Text</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputValueClass</code><code class="o">(</code><code class="n">IntWritable</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    
    <span class="calibre24"><strong class="calibre9"><code class="n1">FileOutputFormat</code><code class="o1">.</code><code class="na1">setCompressOutput</code><code class="o1">(</code><code class="n1">job</code><code class="o1">,</code> <code class="kc">true</code><code class="o1">);</code>
    <code class="n1">FileOutputFormat</code><code class="o1">.</code><code class="na1">setOutputCompressorClass</code><code class="o1">(</code><code class="n1">job</code><code class="o1">,</code> <code class="n1">GzipCodec</code><code class="o1">.</code><code class="na1">class</code><code class="o1">);</code></strong></span>
    
    <code class="n">job</code><code class="o">.</code><code class="na">setMapperClass</code><code class="o">(</code><code class="n">MaxTemperatureMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setCombinerClass</code><code class="o">(</code><code class="n">MaxTemperatureReducer</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setReducerClass</code><code class="o">(</code><code class="n">MaxTemperatureReducer</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">We run the program over compressed input (which doesn’t have to
      use the same compression format as the output, although it does in this
      example) as follows:</p><pre class="screen1">% <strong class="userinput"><code class="calibre9">hadoop MaxTemperatureWithCompression input/ncdc/sample.txt.gz output</code></strong></pre><p class="calibre2">Each part of the final output is compressed; in this case, there
      is a single part:</p><pre class="screen1">% <strong class="userinput"><code class="calibre9">gunzip -c output/part-r-00000.gz</code></strong>
1949    111
1950    22</pre><p class="calibre2">If you are emitting sequence files for your output, you can set
      the <code class="literal">mapreduce.output.fileoutputformat.compress.type</code>
      property <a class="calibre" id="calibre_link-2599"></a>to control the type of compression to use. The default is
      <code class="literal">RECORD</code>, which compresses individual
      records. Changing this to <code class="literal">BLOCK</code>,
      which compresses groups of
      records, is recommended because it compresses better (see <a class="ulink" href="#calibre_link-245" title="The SequenceFile format">The SequenceFile format</a>).</p><p class="calibre2">There is also a static convenience <a class="calibre" id="calibre_link-3345"></a>method on <code class="literal">SequenceFileOutputFormat</code> called <code class="literal">setOutputCompressionType()</code> to set
      this property.</p><p class="calibre2">The configuration properties to set compression for MapReduce job
      outputs are summarized in <a class="ulink" href="#calibre_link-246" title="Table&nbsp;5-5.&nbsp;MapReduce compression properties">Table&nbsp;5-5</a>. If your MapReduce driver
      uses the <code class="literal">Tool</code> interface (described in
      <a class="ulink" href="#calibre_link-247" title="GenericOptionsParser, Tool, and ToolRunner">GenericOptionsParser, Tool, and ToolRunner</a>), you can pass
      any of these properties to the program on the command line, which may be
      more convenient than modifying your program to hardcode the compression
      properties.</p><div class="table"><a id="calibre_link-246" class="calibre"></a><div class="table-title">Table&nbsp;5-5.&nbsp;MapReduce compression properties</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre38"><col class="calibre39"><col class="calibre38"><col class="calibre36"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Type</td><td class="calibre20">Default
              value</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.output.fileoutputformat.compress</code></td><td class="calibre23"><code class="uri">boolean</code></td><td class="calibre23"><code class="uri">false</code></td><td class="calibre25">Whether to compress outputs</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.output.fileoutputformat.compress.codec</code></td><td class="calibre23"><code class="uri">Class</code> name</td><td class="calibre23"><code class="uri">org.apache.hadoop.io.compress.DefaultCodec</code></td><td class="calibre25">The compression codec to use for outputs</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">mapreduce.output.fileoutputformat.compress.type</code></td><td class="calibre27"><code class="uri">String</code></td><td class="calibre27"><code class="uri">RECORD</code></td><td class="calibre28">The type of compression to use for sequence file outputs:
              <code class="uri">NONE</code>, <code class="uri">RECORD</code>, or
              <code class="uri">BLOCK</code></td></tr></tbody></table></div></div><div class="book" title="Compressing map output"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-698">Compressing map output</h4></div></div></div><p class="calibre2">Even if your MapReduce application reads and writes uncompressed
        data, it may benefit from compressing the intermediate output of the
        <a class="calibre" id="calibre_link-2388"></a>map phase. The map output is written to disk and
        transferred across the network to the reducer nodes, so by using a
        fast compressor such as LZO, LZ4, or Snappy, you can get performance
        gains simply because the volume of data to transfer is reduced. The
        configuration properties to enable compression for map outputs and to
        set the compression format are shown in <a class="ulink" href="#calibre_link-248" title="Table&nbsp;5-6.&nbsp;Map output compression properties">Table&nbsp;5-6</a>.</p><div class="table"><a id="calibre_link-248" class="calibre"></a><div class="table-title">Table&nbsp;5-6.&nbsp;Map output compression properties</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre40"><col class="calibre39"><col class="calibre41"><col class="calibre42"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Type</td><td class="calibre20">Default value</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.map.output.compress</code></td><td class="calibre23"><code class="uri">boolean</code></td><td class="calibre23"><code class="uri">false</code></td><td class="calibre25">Whether to compress map outputs</td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">mapreduce.map.output.compress.codec</code></td><td class="calibre27"><code class="uri">Class</code></td><td class="calibre27"><code class="uri">org.apache.hadoop.io.compress.DefaultCodec</code></td><td class="calibre28">The compression codec to use for map outputs</td></tr></tbody></table></div></div><p class="calibre2">Here are the lines to <a class="calibre" id="calibre_link-2584"></a><a class="calibre" id="calibre_link-2587"></a>add to enable gzip map output compression in your job
        (using the new API):</p><pre class="screen1">    <code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Configuration</code><code class="o">();</code>
    <code class="n">conf</code><code class="o">.</code><code class="na">setBoolean</code><code class="o">(</code><code class="n">Job</code><code class="o">.</code><code class="na">MAP_OUTPUT_COMPRESS</code><code class="o">,</code> <code class="k">true</code><code class="o">);</code>
    <code class="n">conf</code><code class="o">.</code><code class="na">setClass</code><code class="o">(</code><code class="n">Job</code><code class="o">.</code><code class="na">MAP_OUTPUT_COMPRESS_CODEC</code><code class="o">,</code> <code class="n">GzipCodec</code><code class="o">.</code><code class="na">class</code><code class="o">,</code>
        <code class="n">CompressionCodec</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Job</code><code class="o">(</code><code class="n">conf</code><code class="o">);</code></pre><p class="calibre2">In the old API (see <a class="ulink" href="#calibre_link-249" title="Appendix&nbsp;D.&nbsp;The Old and New Java MapReduce APIs">Appendix&nbsp;D</a>), there are
        convenience <a class="calibre" id="calibre_link-2227"></a>methods on the <code class="literal">JobConf</code> object for doing the same <a class="calibre" id="calibre_link-2066"></a><a class="calibre" id="calibre_link-1623"></a><a class="calibre" id="calibre_link-1212"></a><a class="calibre" id="calibre_link-2449"></a>thing:</p><a id="calibre_link-4095" class="calibre"></a><pre class="screen1">    <code class="n">conf</code><code class="o">.</code><code class="na">setCompressMapOutput</code><code class="o">(</code><code class="k">true</code><code class="o">);</code>
    <code class="n">conf</code><code class="o">.</code><code class="na">setMapOutputCompressorClass</code><code class="o">(</code><code class="n">GzipCodec</code><code class="o">.</code><code class="na">class</code><code class="o">);</code></pre></div></div></div><div class="book" title="Serialization"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-733">Serialization</h2></div></div></div><p class="calibre2"><em class="calibre10">Serialization</em> is the <a class="calibre" id="calibre_link-2071"></a><a class="calibre" id="calibre_link-3353"></a>process of turning structured objects into a byte stream for
    transmission over a network or for writing to persistent storage.
    <em class="calibre10">Deserialization</em> is <a class="calibre" id="calibre_link-1436"></a>the reverse process of
    turning a byte stream back into a series of structured objects.</p><p class="calibre2">Serialization is used in two quite distinct areas of distributed
    data processing: for interprocess
    communication and for persistent storage.</p><p class="calibre2">In Hadoop, interprocess communication between nodes in the system is
    implemented <a class="calibre" id="calibre_link-3212"></a><a class="calibre" id="calibre_link-3250"></a>using <em class="calibre10">remote procedure calls</em> (RPCs).
    The RPC protocol uses serialization to render the message into a binary
    stream to be sent to the remote node, which then deserializes the binary
    stream into the original message. In general, it is desirable that an RPC
    serialization format is:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Compact</span></dt><dd class="calibre8"><p class="calibre2">A compact format makes the best use of network bandwidth,
          which is the most scarce resource in a data center.</p></dd><dt class="calibre7"><span class="term">Fast</span></dt><dd class="calibre8"><p class="calibre2">Interprocess communication forms the backbone for a
          distributed system, so it is essential that there is as little
          performance overhead as possible for the serialization and
          deserialization process.</p></dd><dt class="calibre7"><span class="term">Extensible</span></dt><dd class="calibre8"><p class="calibre2">Protocols change over time to meet new requirements, so it
          should be straightforward to
          evolve the protocol in a controlled manner for clients and servers.
          For example, it should be possible to add a new argument to a method
          call and have the new servers accept messages in the old format
          (without the new argument) from old clients.</p></dd><dt class="calibre7"><span class="term">Interoperable</span></dt><dd class="calibre8"><p class="calibre2">For some systems, it is desirable to be able to support
          clients that are written in different languages to the server, so
          the format needs to be designed to make this possible.</p></dd></dl></div><p class="calibre2">On the face of it, the data format chosen for persistent storage
    would have different requirements from a serialization framework. After
    all, the lifespan of an RPC is less than a second, whereas persistent data
    may be read years after it was written. But it turns out, the four
    desirable properties of an RPC’s serialization format are also crucial for
    a persistent storage format. We want the storage format to be compact (to
    make efficient use of storage space), fast (so the overhead in reading or
    writing terabytes of data is minimal), extensible (so we can transparently
    read data written in an older format), and interoperable (so we can read
    or write persistent data using different languages).</p><p class="calibre2">Hadoop uses its own serialization format, Writables, <a class="calibre" id="calibre_link-3796"></a><a class="calibre" id="calibre_link-3369"></a>which is certainly compact and fast, but not so easy to
    extend or use from languages other than Java. Because Writables are
    central to Hadoop (most MapReduce programs use them for their key and
    value types), we look at them in some depth in the next three sections,
    before looking at some of the other serialization frameworks supported in
    Hadoop. Avro (a serialization system that was designed to overcome some of
    the limitations of Writables) is <a class="calibre" id="calibre_link-3354"></a>covered in <a class="ulink" href="#calibre_link-133" title="Chapter&nbsp;12.&nbsp;Avro">Chapter&nbsp;12</a>.</p><div class="book" title="The Writable Interface"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4096">The Writable Interface</h3></div></div></div><p class="calibre2">The <code class="literal">Writable</code> interface defines two methods—one
      for writing its state to <a class="calibre" id="calibre_link-1392"></a>a <code class="literal">DataOutput</code> binary
      stream and one for reading its state <a class="calibre" id="calibre_link-1373"></a>from a <code class="literal">DataInput</code> binary
      stream:</p><a id="calibre_link-4097" class="calibre"></a><pre class="screen1"><code class="k">package</code> <code class="n">org</code><code class="o">.</code><code class="na">apache</code><code class="o">.</code><code class="na">hadoop</code><code class="o">.</code><code class="na">io</code><code class="o">;</code>
    
<code class="k">import</code> <code class="nn">java.io.DataOutput</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">java.io.DataInput</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">java.io.IOException</code><code class="o">;</code>

<code class="k">public</code> <code class="k">interface</code> <code class="nc">Writable</code> <code class="o">{</code>
  <code class="kt">void</code> <code class="nf">write</code><code class="o">(</code><code class="n">DataOutput</code> <code class="n">out</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">;</code>
  <code class="kt">void</code> <code class="nf">readFields</code><code class="o">(</code><code class="n">DataInput</code> <code class="n">in</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">;</code>
<code class="o">}</code></pre><p class="calibre2">Let’s look at a particular <code class="literal">Writable</code> to see what we can do with it.
      <a class="calibre" id="calibre_link-2138"></a>We will use <code class="literal">IntWritable</code>, a wrapper for a Java
      <code class="literal">int</code>. We can create one and set its
      value using the <code class="literal">set()</code> method:</p><a id="calibre_link-4098" class="calibre"></a><pre class="screen1">    <code class="n">IntWritable</code> <code class="n">writable</code> <code class="o">=</code> <code class="k">new</code> <code class="n">IntWritable</code><code class="o">();</code>
    <code class="n">writable</code><code class="o">.</code><code class="na">set</code><code class="o">(</code><code class="mi">163</code><code class="o">);</code></pre><p class="calibre2">Equivalently, we can use the constructor that takes the integer
      value:</p><a id="calibre_link-4099" class="calibre"></a><pre class="screen1">    <code class="n">IntWritable</code> <code class="n">writable</code> <code class="o">=</code> <code class="k">new</code> <code class="n">IntWritable</code><code class="o">(</code><code class="mi">163</code><code class="o">);</code></pre><p class="calibre2">To examine the serialized form of the <code class="literal">IntWritable</code>, we write a small helper method
      <a class="calibre" id="calibre_link-1033"></a>that wraps a <code class="literal">java.io.ByteArrayOutputStream</code> in a <code class="literal">java.io.DataOutputStream</code> (an <a class="calibre" id="calibre_link-1393"></a>implementation of <code class="literal">java.io.DataOutput</code>) to capture the bytes in
      the serialized stream:</p><a id="calibre_link-4100" class="calibre"></a><pre class="screen1">  <code class="k">public</code> <code class="k">static</code> <code class="kt">byte</code><code class="o">[]</code> <code class="nf">serialize</code><code class="o">(</code><code class="n">Writable</code> <code class="n">writable</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="n">ByteArrayOutputStream</code> <code class="n">out</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ByteArrayOutputStream</code><code class="o">();</code>
    <code class="n">DataOutputStream</code> <code class="n">dataOut</code> <code class="o">=</code> <code class="k">new</code> <code class="n">DataOutputStream</code><code class="o">(</code><code class="n">out</code><code class="o">);</code>
    <code class="n">writable</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">dataOut</code><code class="o">);</code>
    <code class="n">dataOut</code><code class="o">.</code><code class="na">close</code><code class="o">();</code>
    <code class="k">return</code> <code class="n">out</code><code class="o">.</code><code class="na">toByteArray</code><code class="o">();</code>
  <code class="o">}</code></pre><p class="calibre2">An integer is written using four bytes (as we see using JUnit 4
      assertions):</p><a id="calibre_link-4101" class="calibre"></a><pre class="screen1">    <code class="kt">byte</code><code class="o">[]</code> <code class="n">bytes</code> <code class="o">=</code> <code class="n">serialize</code><code class="o">(</code><code class="n">writable</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">bytes</code><code class="o">.</code><code class="na">length</code><code class="o">,</code> <code class="n">is</code><code class="o">(</code><code class="mi">4</code><code class="o">));</code></pre><p class="calibre2">The bytes are written in big-endian order (so the most significant
      byte is written to the stream first, which is dictated by the <code class="literal">java.io.DataOutput</code> interface), and we
      <a class="calibre" id="calibre_link-3560"></a>can see their hexadecimal representation by using a method
      on Hadoop’s <code class="literal">StringUtils</code>:</p><a id="calibre_link-4102" class="calibre"></a><pre class="screen1">    <code class="n">assertThat</code><code class="o">(</code><code class="n">StringUtils</code><code class="o">.</code><code class="na">byteToHexString</code><code class="o">(</code><code class="n">bytes</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"000000a3"</code><code class="o">));</code></pre><p class="calibre2">Let’s try deserialization. Again, we create a helper method to
      read a <code class="literal">Writable</code> object from a byte
      array:</p><a id="calibre_link-4103" class="calibre"></a><pre class="screen1">  <code class="k">public</code> <code class="k">static</code> <code class="kt">byte</code><code class="o">[]</code> <code class="nf">deserialize</code><code class="o">(</code><code class="n">Writable</code> <code class="n">writable</code><code class="o">,</code> <code class="kt">byte</code><code class="o">[]</code> <code class="n">bytes</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="n">ByteArrayInputStream</code> <code class="n">in</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ByteArrayInputStream</code><code class="o">(</code><code class="n">bytes</code><code class="o">);</code>
    <code class="n">DataInputStream</code> <code class="n">dataIn</code> <code class="o">=</code> <code class="k">new</code> <code class="n">DataInputStream</code><code class="o">(</code><code class="n">in</code><code class="o">);</code>
    <code class="n">writable</code><code class="o">.</code><code class="na">readFields</code><code class="o">(</code><code class="n">dataIn</code><code class="o">);</code>
    <code class="n">dataIn</code><code class="o">.</code><code class="na">close</code><code class="o">();</code>
    <code class="k">return</code> <code class="n">bytes</code><code class="o">;</code>
  <code class="o">}</code></pre><p class="calibre2">We construct a new, value-less <code class="literal">IntWritable</code>, and then call <code class="literal">deserialize()</code> to read from the output data
      that we just wrote. Then we check that its value, retrieved using the
      <code class="literal">get()</code> method, is the original value,
      163:</p><a id="calibre_link-4104" class="calibre"></a><pre class="screen1">    <code class="n">IntWritable</code> <code class="n">newWritable</code> <code class="o">=</code> <code class="k">new</code> <code class="n">IntWritable</code><code class="o">();</code>
    <code class="n">deserialize</code><code class="o">(</code><code class="n">newWritable</code><code class="o">,</code> <code class="n">bytes</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">newWritable</code><code class="o">.</code><code class="na">get</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="mi">163</code><code class="o">));</code></pre><div class="book" title="WritableComparable and comparators"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-258">WritableComparable and comparators</h4></div></div></div><p class="calibre2"><code class="literal">IntWritable</code> implements
        <a class="calibre" id="calibre_link-3803"></a>the <code class="literal">WritableComparable</code> interface, which
        <a class="calibre" id="calibre_link-1201"></a>is just a subinterface of the <code class="literal">Writable</code> and <code class="literal">java.lang.Comparable</code> interfaces:</p><a id="calibre_link-4105" class="calibre"></a><pre class="screen1"><code class="k">package</code> <code class="n">org</code><code class="o">.</code><code class="na">apache</code><code class="o">.</code><code class="na">hadoop</code><code class="o">.</code><code class="na">io</code><code class="o">;</code>
     
<code class="k">public</code> <code class="k">interface</code> <code class="nc">WritableComparable</code><code class="o">&lt;</code><code class="n">T</code><code class="o">&gt;</code> <code class="k">extends</code> <code class="n">Writable</code><code class="o">,</code> <code class="n">Comparable</code><code class="o">&lt;</code><code class="n">T</code><code class="o">&gt;</code> <code class="o">{</code>
<code class="o">}</code></pre><p class="calibre2">Comparison of types is crucial for MapReduce, where there is a
        sorting phase during which keys <a class="calibre" id="calibre_link-1202"></a>are compared with one another. One optimization that
        Hadoop provides <a class="calibre" id="calibre_link-3134"></a>is the <code class="literal">RawComparator</code>
        extension of Java’s <code class="literal">Comparator</code>:</p><a id="calibre_link-4106" class="calibre"></a><pre class="screen1"><code class="k">package</code> <code class="n">org</code><code class="o">.</code><code class="na">apache</code><code class="o">.</code><code class="na">hadoop</code><code class="o">.</code><code class="na">io</code><code class="o">;</code>
     
<code class="k">import</code> <code class="nn">java.util.Comparator</code><code class="o">;</code>

<code class="k">public</code> <code class="k">interface</code> <code class="nc">RawComparator</code><code class="o">&lt;</code><code class="n">T</code><code class="o">&gt;</code> <code class="k">extends</code> <code class="n">Comparator</code><code class="o">&lt;</code><code class="n">T</code><code class="o">&gt;</code> <code class="o">{</code>
     
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">compare</code><code class="o">(</code><code class="kt">byte</code><code class="o">[]</code> <code class="n">b1</code><code class="o">,</code> <code class="kt">int</code> <code class="n">s1</code><code class="o">,</code> <code class="kt">int</code> <code class="n">l1</code><code class="o">,</code> <code class="kt">byte</code><code class="o">[]</code> <code class="n">b2</code><code class="o">,</code> <code class="kt">int</code> <code class="n">s2</code><code class="o">,</code> <code class="kt">int</code> <code class="n">l2</code><code class="o">);</code>
     
<code class="o">}</code></pre><p class="calibre2">This interface permits implementors to compare records read from
        a stream without deserializing them into objects, thereby avoiding any
        overhead of object creation. For example, the comparator for <code class="literal">IntWritable</code>s implements the raw <code class="literal">compare()</code> method by reading an integer from
        each of the byte arrays <code class="literal">b1</code> and
        <code class="literal">b2</code> and comparing them directly from
        the given start positions (<code class="literal">s1</code> and
        <code class="literal">s2</code>) and lengths (<code class="literal">l1</code> and <code class="literal">l2</code>).</p><p class="calibre2"><code class="literal">WritableComparator</code> is a
        <a class="calibre" id="calibre_link-3805"></a>general-purpose implementation of <code class="literal">RawComparator</code> for <code class="literal">WritableComparable</code> classes. It provides
        two main functions. First, it provides a default implementation of the
        raw <code class="literal">compare()</code> method that
        deserializes the objects to be compared from the stream and invokes
        the object <code class="literal">compare()</code> method.
        Second, it acts as a factory for <code class="literal">RawComparator</code> instances (that <code class="literal">Writable</code> implementations have registered).
        For example, to obtain a comparator for <code class="literal">IntWritable</code>, we just use:</p><a id="calibre_link-4107" class="calibre"></a><pre class="screen1">    <code class="n">RawComparator</code><code class="o">&lt;</code><code class="n">IntWritable</code><code class="o">&gt;</code> <code class="n">comparator</code> <code class="o">=</code>
        <code class="n">WritableComparator</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="n">IntWritable</code><code class="o">.</code><code class="na">class</code><code class="o">);</code></pre><p class="calibre2">The comparator can be used to compare two <code class="literal">IntWritable</code> objects:</p><a id="calibre_link-4108" class="calibre"></a><pre class="screen1">    <code class="n">IntWritable</code> <code class="n">w1</code> <code class="o">=</code> <code class="k">new</code> <code class="n">IntWritable</code><code class="o">(</code><code class="mi">163</code><code class="o">);</code>
    <code class="n">IntWritable</code> <code class="n">w2</code> <code class="o">=</code> <code class="k">new</code> <code class="n">IntWritable</code><code class="o">(</code><code class="mi">67</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">comparator</code><code class="o">.</code><code class="na">compare</code><code class="o">(</code><code class="n">w1</code><code class="o">,</code> <code class="n">w2</code><code class="o">),</code> <code class="n">greaterThan</code><code class="o">(</code><code class="mi">0</code><code class="o">));</code></pre><p class="calibre2">or their serialized <a class="calibre" id="calibre_link-3797"></a><a class="calibre" id="calibre_link-3370"></a>representations:</p><a id="calibre_link-4109" class="calibre"></a><pre class="screen1">    <code class="kt">byte</code><code class="o">[]</code> <code class="n">b1</code> <code class="o">=</code> <code class="n">serialize</code><code class="o">(</code><code class="n">w1</code><code class="o">);</code>
    <code class="kt">byte</code><code class="o">[]</code> <code class="n">b2</code> <code class="o">=</code> <code class="n">serialize</code><code class="o">(</code><code class="n">w2</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">comparator</code><code class="o">.</code><code class="na">compare</code><code class="o">(</code><code class="n">b1</code><code class="o">,</code> <code class="mi">0</code><code class="o">,</code> <code class="n">b1</code><code class="o">.</code><code class="na">length</code><code class="o">,</code> <code class="n">b2</code><code class="o">,</code> <code class="mi">0</code><code class="o">,</code> <code class="n">b2</code><code class="o">.</code><code class="na">length</code><code class="o">),</code>
        <code class="n">greaterThan</code><code class="o">(</code><code class="mi">0</code><code class="o">));</code></pre></div></div><div class="book" title="Writable Classes"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4110">Writable Classes</h3></div></div></div><p class="calibre2">Hadoop comes with a <a class="calibre" id="calibre_link-3367"></a><a class="calibre" id="calibre_link-3798"></a>large selection of <code class="literal">Writable</code> classes, which are available in
      <a class="calibre" id="calibre_link-2864"></a>the <code class="literal">org.apache.hadoop.io</code> package. They form the
      class hierarchy shown in <a class="ulink" href="#calibre_link-250" title="Figure&nbsp;5-1.&nbsp;Writable class hierarchy">Figure&nbsp;5-1</a>.</p><div class="book" title="Writable wrappers for Java primitives"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4111">Writable wrappers for Java primitives</h4></div></div></div><p class="calibre2">There are <code class="literal">Writable</code> wrappers
        for all the Java <a class="calibre" id="calibre_link-2182"></a>primitive types (see <a class="ulink" href="#calibre_link-251" title="Table&nbsp;5-7.&nbsp;Writable wrapper classes for Java primitives">Table&nbsp;5-7</a>) except <code class="literal">char</code> (which can be stored in an <code class="literal">IntWritable</code>). All have a <code class="literal">get()</code> and <code class="literal">set()</code> method for retrieving and storing the
        wrapped value.</p><div class="table"><a id="calibre_link-251" class="calibre"></a><div class="table-title">Table&nbsp;5-7.&nbsp;Writable wrapper classes for Java primitives</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Java primitive</td><td class="calibre20">Writable implementation</td><td class="calibre21">Serialized size (bytes)</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">boolean</code></td><td class="calibre23"><code class="uri">BooleanWritable</code></td><td class="calibre25">1</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">byte</code></td><td class="calibre23"><code class="uri">ByteWritable</code></td><td class="calibre25">1</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">short</code></td><td class="calibre23"><code class="uri">ShortWritable</code></td><td class="calibre25">2</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">int</code></td><td class="calibre23"><code class="uri">IntWritable</code></td><td class="calibre25">4</td></tr><tr class="calibre19"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">VIntWritable</code></td><td class="calibre25">1–5</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">float</code></td><td class="calibre23"><code class="uri">FloatWritable</code></td><td class="calibre25">4</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">long</code></td><td class="calibre23"><code class="uri">LongWritable</code></td><td class="calibre25">8</td></tr><tr class="calibre26"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">VLongWritable</code></td><td class="calibre25">1–9</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">double</code></td><td class="calibre27"><code class="uri">DoubleWritable</code></td><td class="calibre28">8</td></tr></tbody></table></div></div><p class="calibre2">When it comes to <a class="calibre" id="calibre_link-1021"></a><a class="calibre" id="calibre_link-1038"></a><a class="calibre" id="calibre_link-3390"></a><a class="calibre" id="calibre_link-2139"></a><a class="calibre" id="calibre_link-3765"></a><a class="calibre" id="calibre_link-1700"></a><a class="calibre" id="calibre_link-2367"></a><a class="calibre" id="calibre_link-3768"></a><a class="calibre" id="calibre_link-1541"></a>encoding integers, there is a choice between the
        fixed-length formats (<code class="literal">IntWritable</code>
        and <code class="literal">LongWritable</code>) and the
        variable-length formats (<code class="literal">VIntWritable</code> and <code class="literal">VLongWritable</code>). The variable-length formats
        use only a single byte to encode the value if it is small enough
        (between –112 and 127, inclusive); otherwise, they use the first byte
        to indicate whether the value is positive or negative, and how many
        bytes follow. For example, 163 requires two bytes:</p><a id="calibre_link-4112" class="calibre"></a><pre class="screen1">    <code class="kt">byte</code><code class="o">[]</code> <code class="n">data</code> <code class="o">=</code> <code class="n">serialize</code><code class="o">(</code><code class="k">new</code> <code class="n">VIntWritable</code><code class="o">(</code><code class="mi">163</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">StringUtils</code><code class="o">.</code><code class="na">byteToHexString</code><code class="o">(</code><code class="n">data</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"8fa3"</code><code class="o">));</code></pre><div class="book"><div class="figure"><a id="calibre_link-250" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4113" class="calibre"></a><img alt="Writable class hierarchy" src="images/000043.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;5-1.&nbsp;Writable class hierarchy</div></div></div><p class="calibre2">How do you choose between a fixed-length and a variable-length
        encoding? Fixed-length encodings are good when the distribution of
        values is fairly uniform across the whole value space, such as when
        using a (well-designed) hash function. Most numeric variables tend to
        have nonuniform distributions, though, and on average, the
        variable-length encoding will save space. Another advantage of
        variable-length encodings is that you can switch from <code class="literal">VIntWritable</code> to <code class="literal">VLongWritable</code>, because their encodings are
        actually the same. So, by choosing a variable-length representation,
        you have room to grow without committing to an 8-byte <code class="literal">long</code> representation from the beginning.</p></div><div class="book" title="Text"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4114">Text</h4></div></div></div><p class="calibre2"><code class="literal">Text</code> is a Writable for UTF-8
        <a class="calibre" id="calibre_link-3663"></a><a class="calibre" id="calibre_link-3556"></a>sequences. It can be thought of as the Writable
        equivalent of <code class="literal">java.lang.String</code>.</p><p class="calibre2">The <code class="literal">Text</code> class uses an
        <code class="literal">int</code> (with a variable-length
        encoding) to store the number of bytes in the string encoding, so the
        maximum value is 2 GB. Furthermore, <code class="literal">Text</code> uses standard UTF-8, which makes it
        potentially easier to interoperate with other tools that understand
        UTF-8.</p><div class="book" title="Indexing"><div class="titlepage1"><div class="book"><div class="book"><h5 class="title8" id="calibre_link-4115">Indexing</h5></div></div></div><p class="calibre2">Because of its emphasis on using standard UTF-8, there are
          some differences between <code class="literal">Text</code> and
          the Java <code class="literal">String</code> class. Indexing
          for the <code class="literal">Text</code> class is in terms of
          position in the encoded byte sequence, not the Unicode character in
          the string or the Java <code class="literal">char</code> code
          unit (as it is for <code class="literal">String</code>). For
          ASCII strings, these three concepts of index position coincide. Here
          is an example to demonstrate the use of the <code class="literal">charAt()</code> method:</p><a id="calibre_link-4116" class="calibre"></a><pre class="screen1">    <code class="n">Text</code> <code class="n">t</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="sb">"hadoop"</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">t</code><code class="o">.</code><code class="na">getLength</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="mi">6</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">t</code><code class="o">.</code><code class="na">getBytes</code><code class="o">().</code><code class="na">length</code><code class="o">,</code> <code class="n">is</code><code class="o">(</code><code class="mi">6</code><code class="o">));</code>
    
    <code class="n">assertThat</code><code class="o">(</code><code class="n">t</code><code class="o">.</code><code class="na">charAt</code><code class="o">(</code><code class="mi">2</code><code class="o">),</code> <code class="n">is</code><code class="o">((</code><code class="kt">int</code><code class="o">)</code> <code class="sb">'d'</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="sb">"Out of bounds"</code><code class="o">,</code> <code class="n">t</code><code class="o">.</code><code class="na">charAt</code><code class="o">(</code><code class="mi">100</code><code class="o">),</code> <code class="n">is</code><code class="o">(-</code><code class="mi">1</code><code class="o">));</code></pre><p class="calibre2">Notice that <code class="literal">charAt()</code>
          returns an <code class="literal">int</code> representing a
          Unicode code point, unlike the <code class="literal">String</code> variant that returns a <code class="literal">char</code>. <code class="literal">Text</code> also has a <code class="literal">find()</code> method, which is analogous to
          <code class="literal">String</code>’s <code class="literal">indexOf()</code>:</p><a id="calibre_link-4117" class="calibre"></a><pre class="screen1">    <code class="n">Text</code> <code class="n">t</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="sb">"hadoop"</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="sb">"Find a substring"</code><code class="o">,</code> <code class="n">t</code><code class="o">.</code><code class="na">find</code><code class="o">(</code><code class="sb">"do"</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">2</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="sb">"Finds first 'o'"</code><code class="o">,</code> <code class="n">t</code><code class="o">.</code><code class="na">find</code><code class="o">(</code><code class="sb">"o"</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">3</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="sb">"Finds 'o' from position 4 or later"</code><code class="o">,</code> <code class="n">t</code><code class="o">.</code><code class="na">find</code><code class="o">(</code><code class="sb">"o"</code><code class="o">,</code> <code class="mi">4</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">4</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="sb">"No match"</code><code class="o">,</code> <code class="n">t</code><code class="o">.</code><code class="na">find</code><code class="o">(</code><code class="sb">"pig"</code><code class="o">),</code> <code class="n">is</code><code class="o">(-</code><code class="mi">1</code><code class="o">));</code></pre></div><div class="book" title="Unicode"><div class="titlepage1"><div class="book"><div class="book"><h5 class="title8" id="calibre_link-4118">Unicode</h5></div></div></div><p class="calibre2">When we start using characters that are encoded with more than
          a single byte, the differences between <code class="literal">Text</code> and <code class="literal">String</code> become clear. Consider the
          <a class="calibre" id="calibre_link-3737"></a>Unicode characters shown in <a class="ulink" href="#calibre_link-252" title="Table&nbsp;5-8.&nbsp;Unicode characters">Table&nbsp;5-8</a>.<sup class="calibre6">[<a class="firstname" href="#calibre_link-253" id="calibre_link-278">45</a>]</sup></p><div class="table"><a id="calibre_link-252" class="calibre"></a><div class="table-title">Table&nbsp;5-8.&nbsp;Unicode characters</div><div class="book"><table class="calibre43"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><span class="calibre24"><strong class="calibre24">Unicode code
                  point</strong></span></td><td class="calibre23">U+0041</td><td class="calibre23">U+00DF</td><td class="calibre23">U+6771</td><td class="calibre25">U+10400</td></tr><tr class="calibre26"><td class="calibre23"><span class="calibre24"><strong class="calibre24">Name</strong></span></td><td class="calibre23">LATIN CAPITAL LETTER A</td><td class="calibre23">LATIN SMALL LETTER SHARP S</td><td class="calibre23">N/A (a unified Han ideograph)</td><td class="calibre25">DESERET CAPITAL LETTER LONG I</td></tr><tr class="calibre19"><td class="calibre23"><span class="calibre24"><strong class="calibre24">UTF-8 code
                  units</strong></span></td><td class="calibre23"><code class="uri">41</code></td><td class="calibre23"><code class="uri">c3</code> <code class="uri">9f</code></td><td class="calibre23"><code class="uri">e6</code> <code class="uri">9d</code> <code class="uri">b1</code></td><td class="calibre25"><code class="uri">f0</code> <code class="uri">90</code> <code class="uri">90</code> <code class="uri">80</code></td></tr><tr class="calibre26"><td class="calibre27"><span class="calibre24"><strong class="calibre24">Java representation</strong></span></td><td class="calibre27"><code class="uri">\u0041</code></td><td class="calibre27"><code class="uri">\u00DF</code></td><td class="calibre27"><code class="uri">\u6771</code></td><td class="calibre28"><code class="uri">\uD801\uDC00</code></td></tr></tbody></table></div></div><p class="calibre2">All but the last character in the table, U+10400, can be
          expressed using a single Java <code class="literal">char</code>. U+10400 is a supplementary character
          and is represented by two Java <code class="literal">char</code>s, known as a <span class="calibre"><em class="calibre10">surrogate
          pair</em></span>. The tests in <a class="ulink" href="#calibre_link-254" title="Example&nbsp;5-5.&nbsp;Tests showing the differences between the String and Text classes">Example&nbsp;5-5</a> show the differences between
          <code class="literal">String</code> and <code class="literal">Text</code> when processing a string of the four
          characters from <a class="ulink" href="#calibre_link-252" title="Table&nbsp;5-8.&nbsp;Unicode characters">Table&nbsp;5-8</a>.</p><div class="example"><a id="calibre_link-254" class="calibre"></a><div class="example-title">Example&nbsp;5-5.&nbsp;Tests showing the differences between the String and Text
            classes</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">StringTextComparisonTest</code> <code class="o">{</code>

  <code class="nd">@Test</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">string</code><code class="o">()</code> <code class="k">throws</code> <code class="n">UnsupportedEncodingException</code> <code class="o">{</code>
    
    <code class="n">String</code> <code class="n">s</code> <code class="o">=</code> <code class="sb">"\u0041\u00DF\u6771\uD801\uDC00"</code><code class="o">;</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">s</code><code class="o">.</code><code class="na">length</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="mi">5</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">s</code><code class="o">.</code><code class="na">getBytes</code><code class="o">(</code><code class="sb">"UTF-8"</code><code class="o">).</code><code class="na">length</code><code class="o">,</code> <code class="n">is</code><code class="o">(</code><code class="mi">10</code><code class="o">));</code>
    
    <code class="n">assertThat</code><code class="o">(</code><code class="n">s</code><code class="o">.</code><code class="na">indexOf</code><code class="o">(</code><code class="sb">"\u0041"</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">0</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">s</code><code class="o">.</code><code class="na">indexOf</code><code class="o">(</code><code class="sb">"\u00DF"</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">1</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">s</code><code class="o">.</code><code class="na">indexOf</code><code class="o">(</code><code class="sb">"\u6771"</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">2</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">s</code><code class="o">.</code><code class="na">indexOf</code><code class="o">(</code><code class="sb">"\uD801\uDC00"</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">3</code><code class="o">));</code>
    
    <code class="n">assertThat</code><code class="o">(</code><code class="n">s</code><code class="o">.</code><code class="na">charAt</code><code class="o">(</code><code class="mi">0</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="sb">'\u0041'</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">s</code><code class="o">.</code><code class="na">charAt</code><code class="o">(</code><code class="mi">1</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="sb">'\u00DF'</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">s</code><code class="o">.</code><code class="na">charAt</code><code class="o">(</code><code class="mi">2</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="sb">'\u6771'</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">s</code><code class="o">.</code><code class="na">charAt</code><code class="o">(</code><code class="mi">3</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="sb">'\uD801'</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">s</code><code class="o">.</code><code class="na">charAt</code><code class="o">(</code><code class="mi">4</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="sb">'\uDC00'</code><code class="o">));</code>
    
    <code class="n">assertThat</code><code class="o">(</code><code class="n">s</code><code class="o">.</code><code class="na">codePointAt</code><code class="o">(</code><code class="mi">0</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">0x0041</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">s</code><code class="o">.</code><code class="na">codePointAt</code><code class="o">(</code><code class="mi">1</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">0x00DF</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">s</code><code class="o">.</code><code class="na">codePointAt</code><code class="o">(</code><code class="mi">2</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">0x6771</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">s</code><code class="o">.</code><code class="na">codePointAt</code><code class="o">(</code><code class="mi">3</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">0x10400</code><code class="o">));</code>
  <code class="o">}</code>
  
  <code class="nd">@Test</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">text</code><code class="o">()</code> <code class="o">{</code>
    
    <code class="n">Text</code> <code class="n">t</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="sb">"\u0041\u00DF\u6771\uD801\uDC00"</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">t</code><code class="o">.</code><code class="na">getLength</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="mi">10</code><code class="o">));</code>
    
    <code class="n">assertThat</code><code class="o">(</code><code class="n">t</code><code class="o">.</code><code class="na">find</code><code class="o">(</code><code class="sb">"\u0041"</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">0</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">t</code><code class="o">.</code><code class="na">find</code><code class="o">(</code><code class="sb">"\u00DF"</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">1</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">t</code><code class="o">.</code><code class="na">find</code><code class="o">(</code><code class="sb">"\u6771"</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">3</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">t</code><code class="o">.</code><code class="na">find</code><code class="o">(</code><code class="sb">"\uD801\uDC00"</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">6</code><code class="o">));</code>

    <code class="n">assertThat</code><code class="o">(</code><code class="n">t</code><code class="o">.</code><code class="na">charAt</code><code class="o">(</code><code class="mi">0</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">0x0041</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">t</code><code class="o">.</code><code class="na">charAt</code><code class="o">(</code><code class="mi">1</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">0x00DF</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">t</code><code class="o">.</code><code class="na">charAt</code><code class="o">(</code><code class="mi">3</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">0x6771</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">t</code><code class="o">.</code><code class="na">charAt</code><code class="o">(</code><code class="mi">6</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">0x10400</code><code class="o">));</code>
  <code class="o">}</code>  
<code class="o">}</code></pre></div></div><p class="calibre2">The test confirms that the length of a <code class="literal">String</code> is the number of <code class="literal">char</code> code units it contains (five, made up
          of one from each of the first three characters in the string and a
          surrogate pair from the last), whereas the length of a <code class="literal">Text</code> object is the number of bytes in its
          UTF-8 encoding (10 = 1+2+3+4). Similarly, the <code class="literal">indexOf()</code> method in <code class="literal">String</code> returns an index in <code class="literal">char</code> code units, and <code class="literal">find()</code> for <code class="literal">Text</code> returns a byte offset.</p><p class="calibre2">The <code class="literal">charAt()</code> method in
          <code class="literal">String</code> returns the <code class="literal">char</code> code unit for the given index, which
          in the case of a surrogate pair will not represent a whole Unicode
          character. The <code class="literal">codePointAt()</code>
          method, indexed by <code class="literal">char</code> code
          unit, is needed to retrieve a single Unicode character represented
          as an <code class="literal">int</code>. In fact, the <code class="literal">charAt()</code> method in <code class="literal">Text</code> is more like the <code class="literal">codePointAt()</code> method than its namesake in
          <code class="literal">String</code>. The only difference is
          that it is indexed by byte <a class="calibre" id="calibre_link-3738"></a>offset.</p></div><div class="book" title="Iteration"><div class="titlepage1"><div class="book"><div class="book"><h5 class="title8" id="calibre_link-4119">Iteration</h5></div></div></div><p class="calibre2">Iterating over the Unicode characters in <code class="literal">Text</code> is complicated by the use of byte
          offsets for indexing, since you can’t just increment the index. The
          idiom for iteration is a little obscure (see <a class="ulink" href="#calibre_link-255" title="Example&nbsp;5-6.&nbsp;Iterating over the characters in a Text object">Example&nbsp;5-6</a>): turn the <code class="literal">Text</code> object into a <code class="literal">java.nio.ByteBuffer</code>, then repeatedly call
          the <code class="literal">bytesToCodePoint()</code> static
          method on <code class="literal">Text</code> with the buffer.
          This method extracts the next code point as an <code class="literal">int</code> and updates the position in the
          buffer. The end of the string is detected when <code class="literal">bytesToCodePoint()</code> returns –1.</p><div class="example"><a id="calibre_link-255" class="calibre"></a><div class="example-title">Example&nbsp;5-6.&nbsp;Iterating over the characters in a Text object</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">TextIterator</code> <code class="o">{</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="o">{</code>    
    <code class="n">Text</code> <code class="n">t</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="sb">"\u0041\u00DF\u6771\uD801\uDC00"</code><code class="o">);</code>
    
    <code class="n">ByteBuffer</code> <code class="n">buf</code> <code class="o">=</code> <code class="n">ByteBuffer</code><code class="o">.</code><code class="na">wrap</code><code class="o">(</code><code class="n">t</code><code class="o">.</code><code class="na">getBytes</code><code class="o">(),</code> <code class="mi">0</code><code class="o">,</code> <code class="n">t</code><code class="o">.</code><code class="na">getLength</code><code class="o">());</code>
    <code class="kt">int</code> <code class="n">cp</code><code class="o">;</code>
    <code class="k">while</code> <code class="o">(</code><code class="n">buf</code><code class="o">.</code><code class="na">hasRemaining</code><code class="o">()</code> <code class="o">&amp;&amp;</code> <code class="o">(</code><code class="n">cp</code> <code class="o">=</code> <code class="n">Text</code><code class="o">.</code><code class="na">bytesToCodePoint</code><code class="o">(</code><code class="n">buf</code><code class="o">))</code> <code class="o">!=</code> <code class="o">-</code><code class="mi">1</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="n">Integer</code><code class="o">.</code><code class="na">toHexString</code><code class="o">(</code><code class="n">cp</code><code class="o">));</code>
    <code class="o">}</code>
  <code class="o">}</code>  
<code class="o">}</code></pre></div></div><p class="calibre2">Running the program prints the code points for the four
          characters in the string:</p><pre class="screen1">% <strong class="userinput"><code class="calibre9">hadoop TextIterator</code></strong>
41
df
6771
10400</pre></div><div class="book" title="Mutability"><div class="titlepage1"><div class="book"><div class="book"><h5 class="title8" id="calibre_link-4120">Mutability</h5></div></div></div><p class="calibre2">Another difference from <code class="literal">String</code> is that <code class="literal">Text</code> is mutable (like all <code class="literal">Writable</code>
          implementations in Hadoop, except <code class="literal">NullWritable</code>, which is a <a class="calibre" id="calibre_link-2810"></a>singleton). You can reuse a <code class="literal">Text</code> instance by calling one of the
          <code class="literal">set()</code> methods on it. For
          example:</p><a id="calibre_link-4121" class="calibre"></a><pre class="screen1">    <code class="n">Text</code> <code class="n">t</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="sb">"hadoop"</code><code class="o">);</code>
    <code class="n">t</code><code class="o">.</code><code class="na">set</code><code class="o">(</code><code class="sb">"pig"</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">t</code><code class="o">.</code><code class="na">getLength</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="mi">3</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">t</code><code class="o">.</code><code class="na">getBytes</code><code class="o">().</code><code class="na">length</code><code class="o">,</code> <code class="n">is</code><code class="o">(</code><code class="mi">3</code><code class="o">));</code></pre><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">In some situations, the byte array returned by the <code class="literal">getBytes()</code> method may be longer than the
            length returned by <code class="literal">getLength()</code>:</p><pre class="programlisting">    <code class="n">Text</code> <code class="n">t</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="sb">"hadoop"</code><code class="o">);</code>
    <code class="n">t</code><code class="o">.</code><code class="na">set</code><code class="o">(</code><span class="calibre24"><strong class="calibre9"><code class="kc">new</code> <code class="nf1">Text</code><code class="o1">(</code><code class="s">"pig"</code><code class="o1">)</code></strong></span><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">t</code><code class="o">.</code><code class="na">getLength</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="mi">3</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="sb">"Byte length not shortened"</code><code class="o">,</code> <code class="n">t</code><code class="o">.</code><code class="na">getBytes</code><code class="o">().</code><code class="na">length</code><code class="o">,</code>
        <span class="calibre24"><strong class="calibre9"><code class="n1">is</code><code class="o1">(</code><code class="mi1">6</code><code class="o1">)</code></strong></span><code class="o">);</code></pre><p class="calibre2">This shows why it is imperative that you always call
            <code class="literal">getLength()</code> when calling
            <code class="literal">getBytes()</code>, so you know how
            much of the byte array is valid data.</p></div></div><div class="book" title="Resorting to String"><div class="titlepage1"><div class="book"><div class="book"><h5 class="title8" id="calibre_link-4122">Resorting to String</h5></div></div></div><p class="calibre2"><code class="literal">Text</code> doesn’t have as rich
          an API for manipulating strings as <code class="literal">java.lang.String</code>, so in many cases, you
          need to convert the <code class="literal">Text</code> object
          to a <code class="literal">String</code>. This is done in the
          usual way, <a class="calibre" id="calibre_link-3664"></a><a class="calibre" id="calibre_link-3557"></a>using the <code class="literal">toString()</code> method:</p><a id="calibre_link-4123" class="calibre"></a><pre class="screen1">    <code class="n">assertThat</code><code class="o">(</code><code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="sb">"hadoop"</code><code class="o">).</code><code class="na">toString</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"hadoop"</code><code class="o">));</code></pre></div></div><div class="book" title="BytesWritable"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4124">BytesWritable</h4></div></div></div><p class="calibre2"><code class="literal">BytesWritable</code> is a <a class="calibre" id="calibre_link-1034"></a>wrapper for an array of binary data. Its serialized
        format is a 4-byte integer field that specifies the number of bytes to
        follow, followed by the bytes themselves. For example, the byte array
        of length 2 with values 3 and 5 is serialized as a 4-byte integer
        (<code class="literal">00000002</code>) followed by the two
        bytes from the array (<code class="literal">03</code> and
        <code class="literal">05</code>):</p><a id="calibre_link-4125" class="calibre"></a><pre class="screen1">    <code class="n">BytesWritable</code> <code class="n">b</code> <code class="o">=</code> <code class="k">new</code> <code class="n">BytesWritable</code><code class="o">(</code><code class="k">new</code> <code class="kt">byte</code><code class="o">[]</code> <code class="o">{</code> <code class="mi">3</code><code class="o">,</code> <code class="mi">5</code> <code class="o">});</code>
    <code class="kt">byte</code><code class="o">[]</code> <code class="n">bytes</code> <code class="o">=</code> <code class="n">serialize</code><code class="o">(</code><code class="n">b</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">StringUtils</code><code class="o">.</code><code class="na">byteToHexString</code><code class="o">(</code><code class="n">bytes</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"000000020305"</code><code class="o">));</code></pre><p class="calibre2"><code class="literal">BytesWritable</code> is mutable, and
        its value may be changed by calling its <code class="literal">set()</code> method. As with <code class="literal">Text</code>, the size of the byte array returned
        from the <code class="literal">getBytes()</code> method for
        <code class="literal">BytesWritable</code>—the capacity—may not
        reflect the actual size of the data stored in the <code class="literal">BytesWritable</code>. You can determine the size of
        the <code class="literal">BytesWritable</code> by calling
        <code class="literal">getLength()</code>. To demonstrate:</p><a id="calibre_link-4126" class="calibre"></a><pre class="screen1">    <code class="n">b</code><code class="o">.</code><code class="na">setCapacity</code><code class="o">(</code><code class="mi">11</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">b</code><code class="o">.</code><code class="na">getLength</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="mi">2</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">b</code><code class="o">.</code><code class="na">getBytes</code><code class="o">().</code><code class="na">length</code><code class="o">,</code> <code class="n">is</code><code class="o">(</code><code class="mi">11</code><code class="o">));</code></pre></div><div class="book" title="NullWritable"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4127">NullWritable</h4></div></div></div><p class="calibre2"><code class="literal">NullWritable</code> is a <a class="calibre" id="calibre_link-2811"></a>special type of Writable, as it has a zero-length
        serialization. No bytes are written to or read from the stream. It is
        used as a placeholder; for example, in MapReduce, a key or a value can
        be declared as a <code class="literal">NullWritable</code> when
        you don’t need to use that position, effectively storing a constant
        empty value. <code class="literal">NullWritable</code> can also
        be useful as a key in a <code class="literal">SequenceFile</code> <a class="calibre" id="calibre_link-3333"></a>when you want to store a list of values, as opposed to
        key-value pairs. It is an immutable singleton, and the instance can be
        retrieved by calling <code class="literal">NullWritable.get()</code>.</p></div><div class="book" title="ObjectWritable and GenericWritable"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4128">ObjectWritable and GenericWritable</h4></div></div></div><p class="calibre2"><code class="literal">ObjectWritable</code> is a
        general-purpose <a class="calibre" id="calibre_link-2826"></a><a class="calibre" id="calibre_link-1792"></a>wrapper for the following: Java primitives, <code class="literal">String</code>, <code class="literal">enum</code>, <code class="literal">Writable</code>, <code class="literal">null</code>, or arrays of any of these types. It is
        used in Hadoop RPC to marshal and unmarshal method arguments and
        return types.</p><p class="calibre2"><code class="literal">ObjectWritable</code> is useful when
        a field can be of more than one type. For example, if the values in
        <a class="calibre" id="calibre_link-3334"></a>a <code class="literal">SequenceFile</code> have
        multiple types, you can declare the value type as an <code class="literal">ObjectWritable</code> and wrap each type in an
        <code class="literal">ObjectWritable</code>. Being a
        general-purpose mechanism, it wastes a fair amount of space because it
        writes the classname of the wrapped type every time it is serialized.
        In cases where the number of types is small and known ahead of time,
        this can be improved by having a static array of types and using the
        index into the array as the serialized reference to the type. This is
        the approach that <code class="literal">GenericWritable</code>
        takes, and you have to subclass it to specify which types to
        support.</p></div><div class="book" title="Writable collections"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4129">Writable collections</h4></div></div></div><p class="calibre2">The <code class="literal">org.apache.hadoop.io</code>
        package includes six <code class="literal">Writable</code> collection types:
        <code class="literal">ArrayWritable</code>, <code class="literal">ArrayPrimitiveWritable</code>, <code class="literal">TwoDArrayWritable</code>, <code class="literal">MapWritable</code>, <code class="literal">SortedMap</code><code class="literal">Writable</code>, and <code class="literal">EnumSetWritable</code>.</p><p class="calibre2"><code class="literal">ArrayWritable</code> and <code class="literal">TwoDArrayWritable</code> are <code class="literal">Writable</code> implementations for arrays and
        two-dimensional arrays (array of arrays) of <code class="literal">Writable</code> instances. <a class="calibre" id="calibre_link-920"></a>All the elements of an <code class="literal">ArrayWritable</code> or a <code class="literal">TwoD</code><code class="literal">ArrayWritable</code> must <a class="calibre" id="calibre_link-3722"></a>be instances of the same class, which is specified at
        construction as follows:</p><a id="calibre_link-4130" class="calibre"></a><pre class="screen1">    <code class="n">ArrayWritable</code> <code class="n">writable</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ArrayWritable</code><code class="o">(</code><code class="n">Text</code><code class="o">.</code><code class="na">class</code><code class="o">);</code></pre><p class="calibre2">In contexts where the <code class="literal">Writable</code> is defined by type, such as in
        <code class="literal">SequenceFile</code> keys or values or as
        input to MapReduce in general, you need to subclass <code class="literal">ArrayWritable</code> (or <code class="literal">TwoDArrayWritable</code>, as appropriate) to set
        the type statically. For example:</p><a id="calibre_link-4131" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">class</code> <code class="nc">TextArrayWritable</code> <code class="k">extends</code> <code class="n">ArrayWritable</code> <code class="o">{</code>
  <code class="k">public</code> <code class="nf">TextArrayWritable</code><code class="o">()</code> <code class="o">{</code>
    <code class="k">super</code><code class="o">(</code><code class="n">Text</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre><p class="calibre2"><code class="literal">ArrayWritable</code> and <code class="literal">TwoDArrayWritable</code> both have <code class="literal">get()</code> and <code class="literal">set()</code> methods, as well as a <code class="literal">toArray()</code> method, which creates a shallow
        copy of the array (or 2D array).</p><p class="calibre2"><code class="literal">ArrayPrimitiveWritable</code> is a
        <a class="calibre" id="calibre_link-919"></a>wrapper for arrays of Java primitives. The component
        type is detected when you call <code class="literal">set()</code>, so there is no need to subclass to
        set the type.</p><p class="calibre2"><code class="literal">MapWritable</code> is an <a class="calibre" id="calibre_link-2643"></a>implementation of <code class="literal">java.util.Map&lt;Writable, Writable&gt;</code>, and
        <code class="literal">SortedMapWritable</code> is an <a class="calibre" id="calibre_link-3425"></a>implementation of <code class="literal">java.util.SortedMap&lt;WritableComparable,
        Writable&gt;</code>. The type of each key and value field is a part
        of the serialization format for that field. The type is stored as a
        single byte that acts as an index into an array of types. The array is
        populated with the standard types in the <code class="literal">org.apache.hadoop.io</code> package, but custom
        <code class="literal">Writable</code> types are accommodated,
        too, by writing a header that encodes the type array for nonstandard
        types. As they are implemented, <code class="literal">MapWritable</code> and <code class="literal">SortedMapWritable</code> use positive
        <code class="literal">byte</code> values for custom types, so a
        maximum of 127 distinct nonstandard <code class="literal">Writable</code> classes can be used in any
        particular <code class="literal">MapWritable</code> or <code class="literal">SortedMapWritable</code> instance. Here’s a
        demonstration of using a <code class="literal">MapWritable</code> with different types for keys
        and values:</p><pre class="screen1">    <code class="n">MapWritable</code> <code class="n">src</code> <code class="o">=</code> <code class="k">new</code> <code class="n">MapWritable</code><code class="o">();</code>
    <code class="n">src</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="k">new</code> <code class="n">IntWritable</code><code class="o">(</code><code class="mi">1</code><code class="o">),</code> <code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="sb">"cat"</code><code class="o">));</code>
    <code class="n">src</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="k">new</code> <code class="n">VIntWritable</code><code class="o">(</code><code class="mi">2</code><code class="o">),</code> <code class="k">new</code> <code class="n">LongWritable</code><code class="o">(</code><code class="mi">163</code><code class="o">));</code>
    
    <code class="n">MapWritable</code> <code class="n">dest</code> <code class="o">=</code> <code class="k">new</code> <code class="n">MapWritable</code><code class="o">();</code>
    <code class="n">WritableUtils</code><code class="o">.</code><code class="na">cloneInto</code><code class="o">(</code><code class="n">dest</code><code class="o">,</code> <code class="n">src</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">((</code><code class="n">Text</code><code class="o">)</code> <code class="n">dest</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="k">new</code> <code class="n">IntWritable</code><code class="o">(</code><code class="mi">1</code><code class="o">)),</code> <code class="n">is</code><code class="o">(</code><code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="sb">"cat"</code><code class="o">)));</code>
    <code class="n">assertThat</code><code class="o">((</code><code class="n">LongWritable</code><code class="o">)</code> <code class="n">dest</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="k">new</code> <code class="n">VIntWritable</code><code class="o">(</code><code class="mi">2</code><code class="o">)),</code>
        <code class="n">is</code><code class="o">(</code><code class="k">new</code> <code class="n">LongWritable</code><code class="o">(</code><code class="mi">163</code><code class="o">)));</code></pre><p class="calibre2">Conspicuous by their absence are <code class="literal">Writable</code> collection implementations for sets
        and lists. A general set can be emulated by using a <code class="literal">MapWritable</code> (or a <code class="literal">SortedMapWritable</code> for a sorted set) with
        <code class="literal">NullWritable</code> values. There is also
        <code class="literal">EnumSetWritable</code> for <a class="calibre" id="calibre_link-1569"></a>sets of enum types. For lists of a single type of
        <code class="literal">Writable</code>, <code class="literal">ArrayWritable</code> is adequate, but to store
        different types of <code class="literal">Writable</code> in a
        single list, you <a class="calibre" id="calibre_link-1793"></a>can use <code class="literal">GenericWritable</code> to wrap the elements in
        an <code class="literal">ArrayWritable</code>. Alternatively,
        you could write a general <code class="literal">ListWritable</code> using the ideas <a class="calibre" id="calibre_link-3368"></a><a class="calibre" id="calibre_link-3799"></a><a class="calibre" id="calibre_link-2183"></a>from <code class="literal">MapWritable</code>.</p></div></div><div class="book" title="Implementing a Custom Writable"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-724">Implementing a Custom Writable</h3></div></div></div><p class="calibre2">Hadoop comes with a <a class="calibre" id="calibre_link-3801"></a><a class="calibre" id="calibre_link-3360"></a>useful set of <code class="literal">Writable</code>
      implementations that serve most purposes; however, on occasion, you may
      need to write your own custom implementation. With a custom <code class="literal">Writable</code>, you have full control over the
      binary representation and the sort order. Because <code class="literal">Writable</code>s are at the heart of the MapReduce
      data path, tuning the binary representation can have a significant
      effect on performance. The stock <code class="literal">Writable</code> implementations that come with Hadoop are
      well tuned, but for more elaborate structures, it is often better to
      create a new <code class="literal">Writable</code> type rather
      than composing the stock types.</p><div class="note" title="Tip"><h3 class="title3">Tip</h3><p class="calibre2">If you are considering writing a custom <code class="literal">Writable</code>, it may be worth trying another
        serialization framework, like Avro, that allows you to define custom
        types declaratively. See <a class="ulink" href="#calibre_link-256" title="Serialization Frameworks">Serialization Frameworks</a>
        and <a class="ulink" href="#calibre_link-133" title="Chapter&nbsp;12.&nbsp;Avro">Chapter&nbsp;12</a>.</p></div><p class="calibre2">To demonstrate how to create a custom <code class="literal">Writable</code>, we shall write an implementation
      that represents a pair of strings, called <code class="literal">TextPair</code>. The basic implementation is
      <a class="calibre" id="calibre_link-3665"></a>shown in <a class="ulink" href="#calibre_link-257" title="Example&nbsp;5-7.&nbsp;A Writable implementation that stores a pair of Text objects">Example&nbsp;5-7</a>.</p><div class="example"><a id="calibre_link-257" class="calibre"></a><div class="example-title">Example&nbsp;5-7.&nbsp;A Writable implementation that stores a pair of Text
        objects</div><div class="book"><pre class="screen"><code class="k">import</code> <code class="nn">java.io.*</code><code class="o">;</code>

<code class="k">import</code> <code class="nn">org.apache.hadoop.io.*</code><code class="o">;</code>

<code class="k">public</code> <code class="k">class</code> <code class="nc">TextPair</code> <code class="k">implements</code> <code class="n">WritableComparable</code><code class="o">&lt;</code><code class="n">TextPair</code><code class="o">&gt;</code> <code class="o">{</code>

  <code class="k">private</code> <code class="n">Text</code> <code class="n">first</code><code class="o">;</code>
  <code class="k">private</code> <code class="n">Text</code> <code class="n">second</code><code class="o">;</code>
  
  <code class="k">public</code> <code class="nf">TextPair</code><code class="o">()</code> <code class="o">{</code>
    <code class="n">set</code><code class="o">(</code><code class="k">new</code> <code class="n">Text</code><code class="o">(),</code> <code class="k">new</code> <code class="n">Text</code><code class="o">());</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="nf">TextPair</code><code class="o">(</code><code class="n">String</code> <code class="n">first</code><code class="o">,</code> <code class="n">String</code> <code class="n">second</code><code class="o">)</code> <code class="o">{</code>
    <code class="n">set</code><code class="o">(</code><code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="n">first</code><code class="o">),</code> <code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="n">second</code><code class="o">));</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="nf">TextPair</code><code class="o">(</code><code class="n">Text</code> <code class="n">first</code><code class="o">,</code> <code class="n">Text</code> <code class="n">second</code><code class="o">)</code> <code class="o">{</code>
    <code class="n">set</code><code class="o">(</code><code class="n">first</code><code class="o">,</code> <code class="n">second</code><code class="o">);</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">set</code><code class="o">(</code><code class="n">Text</code> <code class="n">first</code><code class="o">,</code> <code class="n">Text</code> <code class="n">second</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">this</code><code class="o">.</code><code class="na">first</code> <code class="o">=</code> <code class="n">first</code><code class="o">;</code>
    <code class="k">this</code><code class="o">.</code><code class="na">second</code> <code class="o">=</code> <code class="n">second</code><code class="o">;</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="n">Text</code> <code class="nf">getFirst</code><code class="o">()</code> <code class="o">{</code>
    <code class="k">return</code> <code class="n">first</code><code class="o">;</code>
  <code class="o">}</code>

  <code class="k">public</code> <code class="n">Text</code> <code class="nf">getSecond</code><code class="o">()</code> <code class="o">{</code>
    <code class="k">return</code> <code class="n">second</code><code class="o">;</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">write</code><code class="o">(</code><code class="n">DataOutput</code> <code class="n">out</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="n">first</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">out</code><code class="o">);</code>
    <code class="n">second</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">out</code><code class="o">);</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">readFields</code><code class="o">(</code><code class="n">DataInput</code> <code class="n">in</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="n">first</code><code class="o">.</code><code class="na">readFields</code><code class="o">(</code><code class="n">in</code><code class="o">);</code>
    <code class="n">second</code><code class="o">.</code><code class="na">readFields</code><code class="o">(</code><code class="n">in</code><code class="o">);</code>
  <code class="o">}</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">hashCode</code><code class="o">()</code> <code class="o">{</code>
    <code class="k">return</code> <code class="n">first</code><code class="o">.</code><code class="na">hashCode</code><code class="o">()</code> <code class="o">*</code> <code class="mi">163</code> <code class="o">+</code> <code class="n">second</code><code class="o">.</code><code class="na">hashCode</code><code class="o">();</code>
  <code class="o">}</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">boolean</code> <code class="nf">equals</code><code class="o">(</code><code class="n">Object</code> <code class="n">o</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">o</code> <code class="k">instanceof</code> <code class="n">TextPair</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">TextPair</code> <code class="n">tp</code> <code class="o">=</code> <code class="o">(</code><code class="n">TextPair</code><code class="o">)</code> <code class="n">o</code><code class="o">;</code>
      <code class="k">return</code> <code class="n">first</code><code class="o">.</code><code class="na">equals</code><code class="o">(</code><code class="n">tp</code><code class="o">.</code><code class="na">first</code><code class="o">)</code> <code class="o">&amp;&amp;</code> <code class="n">second</code><code class="o">.</code><code class="na">equals</code><code class="o">(</code><code class="n">tp</code><code class="o">.</code><code class="na">second</code><code class="o">);</code>
    <code class="o">}</code>
    <code class="k">return</code> <code class="k">false</code><code class="o">;</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="n">String</code> <code class="nf">toString</code><code class="o">()</code> <code class="o">{</code>
    <code class="k">return</code> <code class="n">first</code> <code class="o">+</code> <code class="sb">"\t"</code> <code class="o">+</code> <code class="n">second</code><code class="o">;</code>
  <code class="o">}</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">compareTo</code><code class="o">(</code><code class="n">TextPair</code> <code class="n">tp</code><code class="o">)</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">cmp</code> <code class="o">=</code> <code class="n">first</code><code class="o">.</code><code class="na">compareTo</code><code class="o">(</code><code class="n">tp</code><code class="o">.</code><code class="na">first</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">cmp</code> <code class="o">!=</code> <code class="mi">0</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">return</code> <code class="n">cmp</code><code class="o">;</code>
    <code class="o">}</code>
    <code class="k">return</code> <code class="n">second</code><code class="o">.</code><code class="na">compareTo</code><code class="o">(</code><code class="n">tp</code><code class="o">.</code><code class="na">second</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The first part of the implementation is straightforward: there are
      two <code class="literal">Text</code> instance variables, <code class="literal">first</code> and <code class="literal">second</code>, and associated constructors, getters,
      and setters. All <code class="literal">Writable</code> implementations must have a
      default constructor so that the MapReduce framework can instantiate
      them, then populate their fields by calling
      <code class="literal">readFields()</code>. <code class="literal">Writable</code>
      instances are mutable and often reused, so you should take care to avoid
      allocating objects in the <code class="literal">write()</code> or
      <code class="literal">readFields()</code> methods.</p><p class="calibre2"><code class="literal">TextPair</code>’s
      <code class="literal">write()</code> method serializes each <code class="literal">Text</code> object in turn to the output stream by
      delegating to the <code class="literal">Text</code> objects
      themselves. Similarly, <code class="literal">readFields()</code>
      deserializes the <a class="calibre" id="calibre_link-1440"></a>bytes from the input stream by delegating to each <code class="literal">Text</code> object. The <code class="literal">DataOutput</code> and <code class="literal">DataInput</code> interfaces have a
      rich set of methods for serializing and deserializing Java primitives,
      so, in general, you have complete control over the wire format of your
      <code class="literal">Writable</code> object.</p><p class="calibre2">Just as you would for any value object you write in Java, you
      should override the <code class="literal">hashCode()</code>,
      <code class="literal">equals()</code>, and
      <code class="literal">toString()</code> methods <a class="calibre" id="calibre_link-2823"></a>from <code class="literal">java.lang.Object</code>.
      The <code class="literal">hashCode()</code> method is used by <a class="calibre" id="calibre_link-1886"></a>the <code class="literal">HashPartitioner</code>
      (the default partitioner in MapReduce) to choose a reduce partition, so
      you should make sure that you write a good hash function that mixes well
      to ensure reduce partitions are of a similar size.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">If you plan to use your custom <code class="literal">Writable</code> with <code class="literal">TextOutputFormat</code>, you <a class="calibre" id="calibre_link-3680"></a>must implement its <code class="literal">toString()</code>
        method. <code class="literal">TextOutputFormat</code> calls
        <code class="literal">toString()</code> on keys and values for their
        output representation. For <code class="literal">TextPair</code>, we write the underlying <code class="literal">Text</code> objects as strings separated by a tab
        character.</p></div><p class="calibre2"><code class="literal">TextPair</code> is an implementation
      of <code class="literal">WritableComparable</code>, so it provides
      an implementation of the <code class="literal">compareTo()</code> method
      that imposes the ordering you would expect: it sorts by the first string
      followed by the second. Notice that, apart from the number of
      <code class="literal">Text</code> objects it can store, <code class="literal">TextPair</code> differs from <code class="literal">TextArrayWritable</code> (which we discussed in the
      previous section), since <code class="literal">TextArrayWritable</code> is only a <code class="literal">Writable</code>, not a <code class="literal">WritableComparable</code>.</p><div class="book" title="Implementing a RawComparator for speed"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-147">Implementing a RawComparator for speed</h4></div></div></div><p class="calibre2">The code for <code class="literal">TextPair</code> in
        <a class="ulink" href="#calibre_link-257" title="Example&nbsp;5-7.&nbsp;A Writable implementation that stores a pair of Text objects">Example&nbsp;5-7</a> will <a class="calibre" id="calibre_link-3135"></a>work as it stands; however, there is a further
        optimization we can make. As explained in <a class="ulink" href="#calibre_link-258" title="WritableComparable and comparators">WritableComparable and comparators</a>, when <code class="literal">TextPair</code> is being used as a key in
        MapReduce, it will have to be deserialized into an object for the
        <code class="literal">compareTo()</code> method to be invoked. What if it
        were possible to compare two <code class="literal">TextPair</code> objects just by looking at their
        serialized representations?</p><p class="calibre2">It turns out that we can do this because <code class="literal">TextPair</code> is the concatenation of two
        <code class="literal">Text</code> objects, and the binary
        representation of a <code class="literal">Text</code> object is
        a variable-length integer containing the number of bytes in the UTF-8
        representation of the string, followed by the UTF-8 bytes themselves.
        The trick is to read the initial length so we know how long the first
        <code class="literal">Text</code> object’s byte representation
        is; then we can delegate to <code class="literal">Text</code>’s
        <code class="literal">RawComparator</code> and invoke it with
        the appropriate offsets for the first or second string. <a class="ulink" href="#calibre_link-259" title="Example&nbsp;5-8.&nbsp;A RawComparator for comparing TextPair byte representations">Example&nbsp;5-8</a> gives the details (note that this code
        is nested in the <code class="literal">TextPair</code>
        class).</p><div class="example"><a id="calibre_link-259" class="calibre"></a><div class="example-title">Example&nbsp;5-8.&nbsp;A RawComparator for comparing TextPair byte
          representations</div><div class="book"><pre class="screen">  <code class="k">public</code> <code class="k">static</code> <code class="k">class</code> <code class="nc">Comparator</code> <code class="k">extends</code> <code class="n">WritableComparator</code> <code class="o">{</code>
    
    <code class="k">private</code> <code class="k">static</code> <code class="k">final</code> <code class="n">Text</code><code class="o">.</code><code class="na">Comparator</code> <code class="n">TEXT_COMPARATOR</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Text</code><code class="o">.</code><code class="na">Comparator</code><code class="o">();</code>
    
    <code class="k">public</code> <code class="nf">Comparator</code><code class="o">()</code> <code class="o">{</code>
      <code class="k">super</code><code class="o">(</code><code class="n">TextPair</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="o">}</code>

    <code class="nd">@Override</code>
    <code class="k">public</code> <code class="kt">int</code> <code class="nf">compare</code><code class="o">(</code><code class="kt">byte</code><code class="o">[]</code> <code class="n">b1</code><code class="o">,</code> <code class="kt">int</code> <code class="n">s1</code><code class="o">,</code> <code class="kt">int</code> <code class="n">l1</code><code class="o">,</code>
                       <code class="kt">byte</code><code class="o">[]</code> <code class="n">b2</code><code class="o">,</code> <code class="kt">int</code> <code class="n">s2</code><code class="o">,</code> <code class="kt">int</code> <code class="n">l2</code><code class="o">)</code> <code class="o">{</code>
      
      <code class="k">try</code> <code class="o">{</code>
        <code class="kt">int</code> <code class="n">firstL1</code> <code class="o">=</code> <code class="n">WritableUtils</code><code class="o">.</code><code class="na">decodeVIntSize</code><code class="o">(</code><code class="n">b1</code><code class="o">[</code><code class="n">s1</code><code class="o">])</code> <code class="o">+</code> <code class="n">readVInt</code><code class="o">(</code><code class="n">b1</code><code class="o">,</code> <code class="n">s1</code><code class="o">);</code>
        <code class="kt">int</code> <code class="n">firstL2</code> <code class="o">=</code> <code class="n">WritableUtils</code><code class="o">.</code><code class="na">decodeVIntSize</code><code class="o">(</code><code class="n">b2</code><code class="o">[</code><code class="n">s2</code><code class="o">])</code> <code class="o">+</code> <code class="n">readVInt</code><code class="o">(</code><code class="n">b2</code><code class="o">,</code> <code class="n">s2</code><code class="o">);</code>
        <code class="kt">int</code> <code class="n">cmp</code> <code class="o">=</code> <code class="n">TEXT_COMPARATOR</code><code class="o">.</code><code class="na">compare</code><code class="o">(</code><code class="n">b1</code><code class="o">,</code> <code class="n">s1</code><code class="o">,</code> <code class="n">firstL1</code><code class="o">,</code> <code class="n">b2</code><code class="o">,</code> <code class="n">s2</code><code class="o">,</code> <code class="n">firstL2</code><code class="o">);</code>
        <code class="k">if</code> <code class="o">(</code><code class="n">cmp</code> <code class="o">!=</code> <code class="mi">0</code><code class="o">)</code> <code class="o">{</code>
          <code class="k">return</code> <code class="n">cmp</code><code class="o">;</code>
        <code class="o">}</code>
        <code class="k">return</code> <code class="n">TEXT_COMPARATOR</code><code class="o">.</code><code class="na">compare</code><code class="o">(</code><code class="n">b1</code><code class="o">,</code> <code class="n">s1</code> <code class="o">+</code> <code class="n">firstL1</code><code class="o">,</code> <code class="n">l1</code> <code class="o">-</code> <code class="n">firstL1</code><code class="o">,</code>
                                       <code class="n">b2</code><code class="o">,</code> <code class="n">s2</code> <code class="o">+</code> <code class="n">firstL2</code><code class="o">,</code> <code class="n">l2</code> <code class="o">-</code> <code class="n">firstL2</code><code class="o">);</code>
      <code class="o">}</code> <code class="k">catch</code> <code class="o">(</code><code class="n">IOException</code> <code class="n">e</code><code class="o">)</code> <code class="o">{</code>
        <code class="k">throw</code> <code class="k">new</code> <code class="nf">IllegalArgumentException</code><code class="o">(</code><code class="n">e</code><code class="o">);</code>
      <code class="o">}</code>
    <code class="o">}</code>
  <code class="o">}</code>

  <code class="k">static</code> <code class="o">{</code>
    <code class="n">WritableComparator</code><code class="o">.</code><code class="na">define</code><code class="o">(</code><code class="n">TextPair</code><code class="o">.</code><code class="na">class</code><code class="o">,</code> <code class="k">new</code> <code class="n">Comparator</code><code class="o">());</code>
  <code class="o">}</code></pre></div></div><p class="calibre2">We actually subclass <code class="literal">WritableComparator</code> rather than implementing
        <code class="literal">RawComparator</code> directly, since it
        provides some convenience methods and default implementations. The
        subtle part of this code is calculating <code class="literal">firstL1</code> and <code class="literal">firstL2</code>, the lengths of the first <code class="literal">Text</code> field in each byte stream. Each is made
        up of the length of the variable-length integer (returned by
        <code class="literal">decodeVIntSize()</code> on <code class="literal">WritableUtils</code>) and the value it is encoding
        (returned by <code class="literal">readVInt()</code>).</p><p class="calibre2">The static block registers the raw comparator so that whenever
        MapReduce sees the <code class="literal">TextPair</code> class,
        it knows to use the raw comparator as its default <a class="calibre" id="calibre_link-3666"></a>comparator.</p></div><div class="book" title="Custom comparators"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4132">Custom comparators</h4></div></div></div><p class="calibre2">As you can see with <code class="literal">TextPair</code>,
        writing raw comparators takes some care because you have to deal with
        details at the byte level. It is worth looking at some of the
        implementations of <code class="literal">Writable</code> in the
        <code class="literal">org.apache.hadoop.io</code> package for
        further ideas if you need to write your own. The utility methods
        <a class="calibre" id="calibre_link-3807"></a>on <code class="literal">WritableUtils</code> are
        very handy, too.</p><p class="calibre2">Custom comparators should also be written to be <code class="literal">RawComparator</code>s, if possible. These are
        comparators that implement a different sort order from the natural
        sort order defined by the default comparator. <a class="ulink" href="#calibre_link-260" title="Example&nbsp;5-9.&nbsp;A custom RawComparator for comparing the first field of TextPair byte representations">Example&nbsp;5-9</a> shows a comparator for <code class="literal">TextPair</code>, called <code class="literal">FirstComparator</code>, that considers only the
        first string of the pair. Note that we override the
        <code class="literal">compare()</code> method that takes objects so both
        <code class="literal">compare()</code> methods have the same semantics.</p><p class="calibre2">We will make use of this comparator in <a class="ulink" href="#calibre_link-261" title="Chapter&nbsp;9.&nbsp;MapReduce Features">Chapter&nbsp;9</a>, when we look at joins and secondary
        sorting in <a class="calibre" id="calibre_link-3361"></a><a class="calibre" id="calibre_link-3802"></a>MapReduce (see <a class="ulink" href="#calibre_link-262" title="Joins">Joins</a>).</p><div class="example"><a id="calibre_link-260" class="calibre"></a><div class="example-title">Example&nbsp;5-9.&nbsp;A custom RawComparator for comparing the first field of
          TextPair byte representations</div><div class="book"><pre class="screen">  <code class="k">public</code> <code class="k">static</code> <code class="k">class</code> <code class="nc">FirstComparator</code> <code class="k">extends</code> <code class="n">WritableComparator</code> <code class="o">{</code>
    
    <code class="k">private</code> <code class="k">static</code> <code class="k">final</code> <code class="n">Text</code><code class="o">.</code><code class="na">Comparator</code> <code class="n">TEXT_COMPARATOR</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Text</code><code class="o">.</code><code class="na">Comparator</code><code class="o">();</code>
    
    <code class="k">public</code> <code class="nf">FirstComparator</code><code class="o">()</code> <code class="o">{</code>
      <code class="k">super</code><code class="o">(</code><code class="n">TextPair</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="o">}</code>

    <code class="nd">@Override</code>
    <code class="k">public</code> <code class="kt">int</code> <code class="nf">compare</code><code class="o">(</code><code class="kt">byte</code><code class="o">[]</code> <code class="n">b1</code><code class="o">,</code> <code class="kt">int</code> <code class="n">s1</code><code class="o">,</code> <code class="kt">int</code> <code class="n">l1</code><code class="o">,</code>
                       <code class="kt">byte</code><code class="o">[]</code> <code class="n">b2</code><code class="o">,</code> <code class="kt">int</code> <code class="n">s2</code><code class="o">,</code> <code class="kt">int</code> <code class="n">l2</code><code class="o">)</code> <code class="o">{</code>
      
      <code class="k">try</code> <code class="o">{</code>
        <code class="kt">int</code> <code class="n">firstL1</code> <code class="o">=</code> <code class="n">WritableUtils</code><code class="o">.</code><code class="na">decodeVIntSize</code><code class="o">(</code><code class="n">b1</code><code class="o">[</code><code class="n">s1</code><code class="o">])</code> <code class="o">+</code> <code class="n">readVInt</code><code class="o">(</code><code class="n">b1</code><code class="o">,</code> <code class="n">s1</code><code class="o">);</code>
        <code class="kt">int</code> <code class="n">firstL2</code> <code class="o">=</code> <code class="n">WritableUtils</code><code class="o">.</code><code class="na">decodeVIntSize</code><code class="o">(</code><code class="n">b2</code><code class="o">[</code><code class="n">s2</code><code class="o">])</code> <code class="o">+</code> <code class="n">readVInt</code><code class="o">(</code><code class="n">b2</code><code class="o">,</code> <code class="n">s2</code><code class="o">);</code>
        <code class="k">return</code> <code class="n">TEXT_COMPARATOR</code><code class="o">.</code><code class="na">compare</code><code class="o">(</code><code class="n">b1</code><code class="o">,</code> <code class="n">s1</code><code class="o">,</code> <code class="n">firstL1</code><code class="o">,</code> <code class="n">b2</code><code class="o">,</code> <code class="n">s2</code><code class="o">,</code> <code class="n">firstL2</code><code class="o">);</code>
      <code class="o">}</code> <code class="k">catch</code> <code class="o">(</code><code class="n">IOException</code> <code class="n">e</code><code class="o">)</code> <code class="o">{</code>
        <code class="k">throw</code> <code class="k">new</code> <code class="nf">IllegalArgumentException</code><code class="o">(</code><code class="n">e</code><code class="o">);</code>
      <code class="o">}</code>
    <code class="o">}</code>
    
    <code class="nd">@Override</code>
    <code class="k">public</code> <code class="kt">int</code> <code class="nf">compare</code><code class="o">(</code><code class="n">WritableComparable</code> <code class="n">a</code><code class="o">,</code> <code class="n">WritableComparable</code> <code class="n">b</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">a</code> <code class="k">instanceof</code> <code class="n">TextPair</code> <code class="o">&amp;&amp;</code> <code class="n">b</code> <code class="k">instanceof</code> <code class="n">TextPair</code><code class="o">)</code> <code class="o">{</code>
        <code class="k">return</code> <code class="o">((</code><code class="n">TextPair</code><code class="o">)</code> <code class="n">a</code><code class="o">).</code><code class="na">first</code><code class="o">.</code><code class="na">compareTo</code><code class="o">(((</code><code class="n">TextPair</code><code class="o">)</code> <code class="n">b</code><code class="o">).</code><code class="na">first</code><code class="o">);</code>
      <code class="o">}</code>
      <code class="k">return</code> <code class="k">super</code><code class="o">.</code><code class="na">compare</code><code class="o">(</code><code class="n">a</code><code class="o">,</code> <code class="n">b</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code></pre></div></div></div></div><div class="book" title="Serialization Frameworks"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-256">Serialization Frameworks</h3></div></div></div><p class="calibre2">Although most MapReduce <a class="calibre" id="calibre_link-3362"></a>programs use <code class="literal">Writable</code>
      key and value types, this isn’t mandated by the MapReduce API. In fact,
      any type can be used; the only requirement is a mechanism that
      translates to and from a binary representation of each type.</p><p class="calibre2">To support this, Hadoop has an API for pluggable serialization
      frameworks. A serialization framework is represented by an
      implementation <a class="calibre" id="calibre_link-3371"></a>of <code class="literal">Serialization</code> (in
      <a class="calibre" id="calibre_link-2865"></a>the <code class="literal">org.apache.hadoop.io.serializer</code> package).
      <code class="literal">WritableSerialization</code>, for <a class="calibre" id="calibre_link-3806"></a>example, is the implementation of <code class="literal">Serialization</code> for <code class="literal">Writable</code> types.</p><p class="calibre2">A <code class="literal">Serialization</code> defines a
      mapping from types <a class="calibre" id="calibre_link-3372"></a>to <code class="literal">Serializer</code> instances
      (for turning an object into a byte stream) and <code class="literal">Deserializer</code> instances <a class="calibre" id="calibre_link-1441"></a>(for turning a byte stream into an object).</p><p class="calibre2">Set the <code class="literal">io.serializations</code>
      property <a class="calibre" id="calibre_link-2149"></a>to a comma-separated list of classnames in order to
      register <code class="literal">Serialization</code>
      implementations. Its default value includes <code class="literal">org.apache.hadoop.io.serializer.WritableSerialization</code>
      and the Avro Specific and Reflect serializations (see <a class="ulink" href="#calibre_link-263" title="Avro Data Types and Schemas">Avro Data Types and Schemas</a>), which means that only <code class="literal">Writable</code> or Avro objects
      can be serialized or deserialized out of the box.</p><p class="calibre2">Hadoop includes a class <a class="calibre" id="calibre_link-2198"></a>called <code class="literal">JavaSerialization</code> that uses <a class="calibre" id="calibre_link-2189"></a>Java Object Serialization. Although it makes it convenient
      to be able to use standard Java types such as <code class="literal">Integer</code> or <code class="literal">String</code> in MapReduce programs, Java Object
      Serialization is not as efficient as Writables, so it’s not worth making
      this trade-off (see the following sidebar).</p><div class="sidebar"><a id="calibre_link-4133" class="calibre"></a><div class="sidebar-title">Why Not Use Java Object Serialization?</div><p class="calibre2">Java comes with its own serialization mechanism, called Java
        Object Serialization (often referred to simply as “Java
        Serialization”), that is tightly integrated with the language, so it’s
        natural to ask why this wasn’t used in Hadoop. Here’s what Doug
        Cutting said in response to that question:</p><div class="blockquote"><blockquote class="blockquote"><p class="calibre4">Why didn’t I use Serialization when we first started Hadoop?
          Because it looked big and hairy and I thought we needed something
          lean and mean, where we had precise control over exactly how objects
          are written and read, since that is central to Hadoop. With
          Serialization you can get some control, but you have to fight for
          it.</p><p class="calibre4">The logic for not using RMI [Remote Method Invocation] was
          similar. Effective, high-performance inter-process communications
          are critical to Hadoop. I felt like we’d need to precisely control
          how things like connections, timeouts and buffers are handled, and
          RMI gives you little control over those.</p></blockquote></div><p class="calibre2">The problem is that Java Serialization doesn’t meet the criteria
        for a serialization format listed earlier: compact, fast, extensible,
        and interoperable.</p></div><div class="book" title="Serialization IDL"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4134">Serialization IDL</h4></div></div></div><p class="calibre2">There are a <a class="calibre" id="calibre_link-3359"></a>number of other serialization frameworks that approach
        the problem in a different way: rather than defining types through
        code, you define them in a language-neutral, declarative fashion,
        using an <em class="calibre10">interface description language</em> (IDL).
        The<a class="calibre" id="calibre_link-2133"></a><a class="calibre" id="calibre_link-2073"></a> system can then generate types for different languages,
        which is good for interoperability. They also typically define
        versioning schemes that make type evolution straightforward.</p><p class="calibre2"><a class="ulink" href="http://thrift.apache.org/" target="_top">Apache Thrift </a>
        and <a class="ulink" href="http://code.google.com/p/protobuf/" target="_top">Google Protocol
        Buffers</a> <a class="calibre" id="calibre_link-901"></a><a class="calibre" id="calibre_link-1802"></a>are both popular serialization frameworks, and both are
        commonly used as a format for persistent binary data. There is limited
        support for these as MapReduce formats;<sup class="calibre6">[<a class="firstname" href="#calibre_link-264" id="calibre_link-279">46</a>]</sup> however, they are used internally in parts of Hadoop for
        RPC and data exchange.</p><p class="calibre2">Avro is an IDL-based serialization <a class="calibre" id="calibre_link-933"></a><a class="calibre" id="calibre_link-4135"></a>framework designed to work well with large-scale data
        processing in Hadoop. It is covered <a class="calibre" id="calibre_link-2072"></a><a class="calibre" id="calibre_link-3363"></a>in <a class="ulink" href="#calibre_link-133" title="Chapter&nbsp;12.&nbsp;Avro">Chapter&nbsp;12</a>.</p></div></div></div><div class="book" title="File-Based Data Structures"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4136">File-Based Data Structures</h2></div></div></div><p class="calibre2">For some applications, you need a <a class="calibre" id="calibre_link-2069"></a><a class="calibre" id="calibre_link-1627"></a>specialized data structure to hold your data. For doing
    MapReduce-based processing, putting each blob of binary data into its own
    file doesn’t scale, so Hadoop developed a number of higher-level
    containers for these situations.</p><div class="book" title="SequenceFile"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-141">SequenceFile</h3></div></div></div><p class="calibre2">Imagine a logfile <a class="calibre" id="calibre_link-2357"></a>where each <a class="calibre" id="calibre_link-1363"></a><a class="calibre" id="calibre_link-3326"></a>log record is a new line of text. If you want to log
      binary types, plain text isn’t a suitable format. Hadoop’s <code class="literal">SequenceFile</code> class fits the bill in this situation, providing a persistent
      data structure for binary key-value pairs. To use it as a logfile
      format, you would choose a key, such as timestamp represented by a
      <code class="literal">LongWritable</code>,
      and the value would be a <code class="literal">Writable</code>
      that represents the quantity being logged.</p><p class="calibre2"><code class="literal">SequenceFile</code>s also work well as
      containers for smaller files. HDFS and MapReduce are optimized for large files, so packing
      files into a <code class="literal">SequenceFile</code> makes
      storing and processing the smaller
      files more efficient (<a class="ulink" href="#calibre_link-265" title="Processing a whole file as a record">Processing a whole file as a record</a> contains a
      program to pack files into a <code class="literal">SequenceFile</code>).<sup class="calibre6">[<a class="firstname" href="#calibre_link-266" id="calibre_link-280">47</a>]</sup></p><div class="book" title="Writing a SequenceFile"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4137">Writing a SequenceFile</h4></div></div></div><p class="calibre2">To <a class="calibre" id="calibre_link-3339"></a><a class="calibre" id="calibre_link-3819"></a>create a <code class="literal">SequenceFile</code>, use one of its <code class="literal">createWriter()</code> static methods, which return
        a <code class="literal">SequenceFile.Writer</code> instance.
        There are several overloaded versions, but they all require you to specify a stream to write
        to (either an <code class="literal">FSDataOutputStream</code> or
        a <code class="literal">File</code><code class="literal">System</code> and
        <code class="literal">Path</code> pairing), a <code class="literal">Configuration</code> object, and the key and value
        types. Optional arguments include the compression type and codec, a
        <code class="literal">Progressable</code> callback to be
        informed of write progress, and a <code class="literal">Metadata</code> instance to be stored in the
        <code class="literal">SequenceFile</code> header.</p><p class="calibre2">The keys and values stored in a <code class="literal">SequenceFile</code> do not necessarily need to be
        <code class="literal">Writable</code>s. Any types that can be
        serialized and deserialized by a <code class="literal">Serialization</code> may be used.</p><p class="calibre2">Once you have a <code class="literal">SequenceFile.Writer</code>, you then write
        key-value pairs using the <code class="literal">append()</code> method. When
        <a class="calibre" id="calibre_link-1100"></a>you’ve finished, you call the <code class="literal">close()</code> method (<code class="literal">SequenceFile.Writer</code> implements <code class="literal">java.io.Closeable</code>).</p><p class="calibre2"><a class="ulink" href="#calibre_link-267" title="Example&nbsp;5-10.&nbsp;Writing a SequenceFile">Example&nbsp;5-10</a> shows a short program
        to write some key-value pairs to a <code class="literal">SequenceFile</code> using the API just
        described.</p><div class="example"><a id="calibre_link-267" class="calibre"></a><div class="example-title">Example&nbsp;5-10.&nbsp;Writing a SequenceFile</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">SequenceFileWriteDemo</code> <code class="o">{</code>
  
  <code class="k">private</code> <code class="k">static</code> <code class="k">final</code> <code class="n">String</code><code class="o">[]</code> <code class="n">DATA</code> <code class="o">=</code> <code class="o">{</code>
    <code class="sb">"One, two, buckle my shoe"</code><code class="o">,</code>
    <code class="sb">"Three, four, shut the door"</code><code class="o">,</code>
    <code class="sb">"Five, six, pick up sticks"</code><code class="o">,</code>
    <code class="sb">"Seven, eight, lay them straight"</code><code class="o">,</code>
    <code class="sb">"Nine, ten, a big fat hen"</code>
  <code class="o">};</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="n">String</code> <code class="n">uri</code> <code class="o">=</code> <code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">];</code>
    <code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Configuration</code><code class="o">();</code>
    <code class="n">FileSystem</code> <code class="n">fs</code> <code class="o">=</code> <code class="n">FileSystem</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="n">URI</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">uri</code><code class="o">),</code> <code class="n">conf</code><code class="o">);</code>
    <code class="n">Path</code> <code class="n">path</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">uri</code><code class="o">);</code>

    <code class="n">IntWritable</code> <code class="n">key</code> <code class="o">=</code> <code class="k">new</code> <code class="n">IntWritable</code><code class="o">();</code>
    <code class="n">Text</code> <code class="n">value</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Text</code><code class="o">();</code>
    <code class="n">SequenceFile</code><code class="o">.</code><code class="na">Writer</code> <code class="n">writer</code> <code class="o">=</code> <code class="k">null</code><code class="o">;</code>
    <code class="k">try</code> <code class="o">{</code>
      <code class="n">writer</code> <code class="o">=</code> <code class="n">SequenceFile</code><code class="o">.</code><code class="na">createWriter</code><code class="o">(</code><code class="n">fs</code><code class="o">,</code> <code class="n">conf</code><code class="o">,</code> <code class="n">path</code><code class="o">,</code>
          <code class="n">key</code><code class="o">.</code><code class="na">getClass</code><code class="o">(),</code> <code class="n">value</code><code class="o">.</code><code class="na">getClass</code><code class="o">());</code>
      
      <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="mi">100</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>
        <code class="n">key</code><code class="o">.</code><code class="na">set</code><code class="o">(</code><code class="mi">100</code> <code class="o">-</code> <code class="n">i</code><code class="o">);</code>
        <code class="n">value</code><code class="o">.</code><code class="na">set</code><code class="o">(</code><code class="n">DATA</code><code class="o">[</code><code class="n">i</code> <code class="o">%</code> <code class="n">DATA</code><code class="o">.</code><code class="na">length</code><code class="o">]);</code>
        <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"[%s]\t%s\t%s\n"</code><code class="o">,</code> <code class="n">writer</code><code class="o">.</code><code class="na">getLength</code><code class="o">(),</code> <code class="n">key</code><code class="o">,</code> <code class="n">value</code><code class="o">);</code>
        <code class="n">writer</code><code class="o">.</code><code class="na">append</code><code class="o">(</code><code class="n">key</code><code class="o">,</code> <code class="n">value</code><code class="o">);</code>
      <code class="o">}</code>
    <code class="o">}</code> <code class="k">finally</code> <code class="o">{</code>
      <code class="n">IOUtils</code><code class="o">.</code><code class="na">closeStream</code><code class="o">(</code><code class="n">writer</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The keys in the sequence file are integers counting down from
        100 to 1, represented as <code class="literal">IntWritable</code> objects. The values are <code class="literal">Text</code> objects. Before each record is appended
        to the <code class="literal">SequenceFile.Writer</code>, we call
        the <code class="literal">getLength()</code> method to discover
        the current position in the file. (We will use this information about
        record boundaries in the next section, when we read the file
        nonsequentially.) We write the position out to the console, along with
        the key and value pairs. The result of running it is shown <a class="calibre" id="calibre_link-3820"></a><a class="calibre" id="calibre_link-3340"></a>here:</p><pre class="screen1">% <strong class="userinput"><code class="calibre9">hadoop SequenceFileWriteDemo numbers.seq</code></strong>
[128]   100     One, two, buckle my shoe
[173]   99      Three, four, shut the door
[220]   98      Five, six, pick up sticks
[264]   97      Seven, eight, lay them straight
[314]   96      Nine, ten, a big fat hen
[359]   95      One, two, buckle my shoe
[404]   94      Three, four, shut the door
[451]   93      Five, six, pick up sticks
[495]   92      Seven, eight, lay them straight
[545]   91      Nine, ten, a big fat hen
...
[1976]  60      One, two, buckle my shoe
[2021]  59      Three, four, shut the door
[2088]  58      Five, six, pick up sticks
[2132]  57      Seven, eight, lay them straight
[2182]  56      Nine, ten, a big fat hen
...
[4557]  5       One, two, buckle my shoe
[4602]  4       Three, four, shut the door
[4649]  3       Five, six, pick up sticks
[4693]  2       Seven, eight, lay them straight
[4743]  1       Nine, ten, a big fat hen</pre></div><div class="book" title="Reading a SequenceFile"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-583">Reading a SequenceFile</h4></div></div></div><p class="calibre2">Reading sequence files <a class="calibre" id="calibre_link-3335"></a><a class="calibre" id="calibre_link-3167"></a>from beginning to end is a matter of creating an
        instance of <code class="literal">SequenceFile.Reader</code> and
        iterating over records by repeatedly invoking one of the <code class="literal">next()</code> methods. Which one you use depends on
        the serialization framework you are using. If you are using <code class="literal">Writable</code> types, you can use the <code class="literal">next()</code> method that takes a key and a value
        argument and reads the next key and value in the stream into these
        variables:</p><a id="calibre_link-4138" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="kt">boolean</code> <code class="nf">next</code><code class="o">(</code><code class="n">Writable</code> <code class="n">key</code><code class="o">,</code> <code class="n">Writable</code> <code class="n">val</code><code class="o">)</code></pre><p class="calibre2">The return value is <code class="literal">true</code> if a
        key-value pair was read and <code class="literal">false</code>
        if the end of the file has been reached.</p><p class="calibre2">For other, non-Writable serialization frameworks (such as Apache
        Thrift), you should use these two methods:</p><a id="calibre_link-4139" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="n">Object</code> <code class="nf">next</code><code class="o">(</code><code class="n">Object</code> <code class="n">key</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code>
<code class="k">public</code> <code class="n">Object</code> <code class="nf">getCurrentValue</code><code class="o">(</code><code class="n">Object</code> <code class="n">val</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code></pre><p class="calibre2">In this case, you need to make sure that the serialization you
        want to use has been set in the <code class="literal">io.serializations</code> property; see <a class="ulink" href="#calibre_link-256" title="Serialization Frameworks">Serialization Frameworks</a>.</p><p class="calibre2">If the <code class="literal">next()</code> method returns
        a non-<code class="literal">null</code> object, a key-value pair
        was read from the stream, and the value can be retrieved using the
        <code class="literal">getCurrentValue()</code> method.
        Otherwise, if <code class="literal">next()</code> returns
        <code class="literal">null</code>, the end of the file has been
        reached.</p><p class="calibre2">The program in <a class="ulink" href="#calibre_link-268" title="Example&nbsp;5-11.&nbsp;Reading a SequenceFile">Example&nbsp;5-11</a>
        demonstrates how to read a sequence file that has <code class="literal">Writable</code> keys and
        values. Note how the types are discovered from the <code class="literal">SequenceFile.Reader</code> via calls to <code class="literal">getKeyClass()</code> and <code class="literal">getValueClass()</code>, and then <code class="literal">ReflectionUtils</code> is used <a class="calibre" id="calibre_link-3205"></a>to create an instance for the key and an instance for
        the value. This technique allows the program to be used with any
        sequence file that has <code class="literal">Writable</code>
        keys and values.</p><div class="example"><a id="calibre_link-268" class="calibre"></a><div class="example-title">Example&nbsp;5-11.&nbsp;Reading a SequenceFile</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">SequenceFileReadDemo</code> <code class="o">{</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="n">String</code> <code class="n">uri</code> <code class="o">=</code> <code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">];</code>
    <code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Configuration</code><code class="o">();</code>
    <code class="n">FileSystem</code> <code class="n">fs</code> <code class="o">=</code> <code class="n">FileSystem</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="n">URI</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">uri</code><code class="o">),</code> <code class="n">conf</code><code class="o">);</code>
    <code class="n">Path</code> <code class="n">path</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">uri</code><code class="o">);</code>

    <code class="n">SequenceFile</code><code class="o">.</code><code class="na">Reader</code> <code class="n">reader</code> <code class="o">=</code> <code class="k">null</code><code class="o">;</code>
    <code class="k">try</code> <code class="o">{</code>
      <code class="n">reader</code> <code class="o">=</code> <code class="k">new</code> <code class="n">SequenceFile</code><code class="o">.</code><code class="na">Reader</code><code class="o">(</code><code class="n">fs</code><code class="o">,</code> <code class="n">path</code><code class="o">,</code> <code class="n">conf</code><code class="o">);</code>
      <code class="n">Writable</code> <code class="n">key</code> <code class="o">=</code> <code class="o">(</code><code class="n">Writable</code><code class="o">)</code>
          <code class="n">ReflectionUtils</code><code class="o">.</code><code class="na">newInstance</code><code class="o">(</code><code class="n">reader</code><code class="o">.</code><code class="na">getKeyClass</code><code class="o">(),</code> <code class="n">conf</code><code class="o">);</code>
      <code class="n">Writable</code> <code class="n">value</code> <code class="o">=</code> <code class="o">(</code><code class="n">Writable</code><code class="o">)</code>
          <code class="n">ReflectionUtils</code><code class="o">.</code><code class="na">newInstance</code><code class="o">(</code><code class="n">reader</code><code class="o">.</code><code class="na">getValueClass</code><code class="o">(),</code> <code class="n">conf</code><code class="o">);</code>
      <code class="kt">long</code> <code class="n">position</code> <code class="o">=</code> <code class="n">reader</code><code class="o">.</code><code class="na">getPosition</code><code class="o">();</code>
      <code class="k">while</code> <code class="o">(</code><code class="n">reader</code><code class="o">.</code><code class="na">next</code><code class="o">(</code><code class="n">key</code><code class="o">,</code> <code class="n">value</code><code class="o">))</code> <code class="o">{</code>
        <code class="n">String</code> <code class="n">syncSeen</code> <code class="o">=</code> <code class="n">reader</code><code class="o">.</code><code class="na">syncSeen</code><code class="o">()</code> <code class="o">?</code> <code class="sb">"*"</code> <code class="o">:</code> <code class="sb">""</code><code class="o">;</code>
        <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"[%s%s]\t%s\t%s\n"</code><code class="o">,</code> <code class="n">position</code><code class="o">,</code> <code class="n">syncSeen</code><code class="o">,</code> <code class="n">key</code><code class="o">,</code> <code class="n">value</code><code class="o">);</code>
        <code class="n">position</code> <code class="o">=</code> <code class="n">reader</code><code class="o">.</code><code class="na">getPosition</code><code class="o">();</code> <code class="c2">// beginning of next record</code>
      <code class="o">}</code>
    <code class="o">}</code> <code class="k">finally</code> <code class="o">{</code>
      <code class="n">IOUtils</code><code class="o">.</code><code class="na">closeStream</code><code class="o">(</code><code class="n">reader</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">Another feature of the program is that it displays the positions
        of the <em class="calibre10">sync points</em> in the sequence file. A sync
        point is a point in the stream that can be used to resynchronize with
        a record boundary if the reader is “lost”—for example, after seeking
        to an arbitrary position in the stream. Sync points are recorded by
        <code class="literal">SequenceFile.Writer</code>, which inserts
        a special entry to mark the sync point every few records as a sequence
        file is being written. Such entries are small enough to incur only a
        modest storage overhead—less than 1%. Sync points always align with
        record boundaries.</p><p class="calibre2">Running the program in <a class="ulink" href="#calibre_link-268" title="Example&nbsp;5-11.&nbsp;Reading a SequenceFile">Example&nbsp;5-11</a>
        shows the sync points in the sequence file as asterisks. The first one
        occurs at position 2021 (the second one occurs at position 4075, but
        is not shown in the output):</p><pre class="screen1">% <strong class="userinput"><code class="calibre9">hadoop SequenceFileReadDemo numbers.seq</code></strong>
[128]   100     One, two, buckle my shoe
[173]   99      Three, four, shut the door
[220]   98      Five, six, pick up sticks
[264]   97      Seven, eight, lay them straight
[314]   96      Nine, ten, a big fat hen
[359]   95      One, two, buckle my shoe
[404]   94      Three, four, shut the door
[451]   93      Five, six, pick up sticks
[495]   92      Seven, eight, lay them straight
[545]   91      Nine, ten, a big fat hen
[590]   90      One, two, buckle my shoe
...
[1976]  60      One, two, buckle my shoe
[2021*] 59      Three, four, shut the door
[2088]  58      Five, six, pick up sticks
[2132]  57      Seven, eight, lay them straight
[2182]  56      Nine, ten, a big fat hen
...
[4557]  5       One, two, buckle my shoe
[4602]  4       Three, four, shut the door
[4649]  3       Five, six, pick up sticks
[4693]  2       Seven, eight, lay them straight
[4743]  1       Nine, ten, a big fat hen</pre><p class="calibre2">There are two ways to seek to a given position in a sequence
        file. The first is the <code class="literal">seek()</code>
        method, which positions the reader at the given point in the file. For
        example, seeking to a record boundary works as expected:</p><a id="calibre_link-4140" class="calibre"></a><pre class="screen1">    <code class="n">reader</code><code class="o">.</code><code class="na">seek</code><code class="o">(</code><code class="mi">359</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">reader</code><code class="o">.</code><code class="na">next</code><code class="o">(</code><code class="n">key</code><code class="o">,</code> <code class="n">value</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="k">true</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(((</code><code class="n">IntWritable</code><code class="o">)</code> <code class="n">key</code><code class="o">).</code><code class="na">get</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="mi">95</code><code class="o">));</code></pre><p class="calibre2">But if the position in the file is not at a record boundary, the
        reader fails when the <code class="literal">next()</code> method
        is called:</p><a id="calibre_link-4141" class="calibre"></a><pre class="screen1">    <code class="n">reader</code><code class="o">.</code><code class="na">seek</code><code class="o">(</code><code class="mi">360</code><code class="o">);</code>
    <code class="n">reader</code><code class="o">.</code><code class="na">next</code><code class="o">(</code><code class="n">key</code><code class="o">,</code> <code class="n">value</code><code class="o">);</code> <code class="c2">// fails with IOException</code></pre><p class="calibre2">The second way to find a record boundary makes use of sync
        points. The <code class="literal">sync(long position)</code>
        method on <code class="literal">SequenceFile.Reader</code>
        positions the reader at the next sync point after <code class="literal">position</code>. (If there are no sync points in
        the file after this position, then the reader will be positioned at
        the end of the file.) Thus, we can call <code class="literal">sync()</code> with any position in the stream—not
        necessarily a record boundary—and the reader will reestablish itself
        at the next sync point so reading can continue:</p><a id="calibre_link-4142" class="calibre"></a><pre class="screen1">    <code class="n">reader</code><code class="o">.</code><code class="na">sync</code><code class="o">(</code><code class="mi">360</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">reader</code><code class="o">.</code><code class="na">getPosition</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="mi">2021L</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">reader</code><code class="o">.</code><code class="na">next</code><code class="o">(</code><code class="n">key</code><code class="o">,</code> <code class="n">value</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="k">true</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(((</code><code class="n">IntWritable</code><code class="o">)</code> <code class="n">key</code><code class="o">).</code><code class="na">get</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="mi">59</code><code class="o">));</code></pre><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2"><code class="literal">SequenceFile.Writer</code> has a
          method called <code class="literal">sync()</code> for
          inserting a sync point at the current position in the stream. This
          is not to be confused with the <code class="literal">hsync()</code> method defined by the <code class="literal">Syncable</code> interface for synchronizing
          buffers to the underlying
          device (see <a class="ulink" href="#calibre_link-269" title="Coherency Model">Coherency Model</a>).</p></div><p class="calibre2">Sync points come into their own when using sequence files as
        input to MapReduce, since they permit the files to be split and
        different portions to be processed independently by separate map
        <a class="calibre" id="calibre_link-3336"></a><a class="calibre" id="calibre_link-3168"></a>tasks (see <a class="ulink" href="#calibre_link-270" title="SequenceFileInputFormat">SequenceFileInputFormat</a>).</p></div><div class="book" title="Displaying a SequenceFile with the command-line interface"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4143">Displaying a SequenceFile with the command-line
        interface</h4></div></div></div><p class="calibre2">The <code class="literal">hadoop fs</code> command
        <a class="calibre" id="calibre_link-1188"></a><a class="calibre" id="calibre_link-3329"></a>has a <code class="literal">-text</code> option to
        display sequence files in textual form. It looks at a file’s magic
        number so that it can attempt to detect the type of the file and
        appropriately convert it to text. It can recognize gzipped files,
        sequence files, and Avro datafiles; otherwise, it assumes the input is
        plain text.</p><p class="calibre2">For sequence files, this command is really useful only if the
        keys and values have meaningful string representations (as defined by
        the <code class="literal">toString()</code> method). Also, if
        you have your own key or value classes, you will need to make sure
        they are on Hadoop’s classpath.</p><p class="calibre2">Running it on the sequence file we created in the previous
        section gives the following output:</p><pre class="screen1">% <strong class="userinput"><code class="calibre9">hadoop fs -text numbers.seq | head</code></strong>
100     One, two, buckle my shoe
99      Three, four, shut the door
98      Five, six, pick up sticks
97      Seven, eight, lay them straight
96      Nine, ten, a big fat hen
95      One, two, buckle my shoe
94      Three, four, shut the door
93      Five, six, pick up sticks
92      Seven, eight, lay them straight
91      Nine, ten, a big fat hen</pre></div><div class="book" title="Sorting and merging SequenceFiles"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-743">Sorting and merging SequenceFiles</h4></div></div></div><p class="calibre2">The most powerful <a class="calibre" id="calibre_link-3337"></a>way of sorting (and merging) one or more sequence files
        is to use MapReduce. MapReduce is inherently parallel and will let you
        specify the number of reducers to use, which determines the number of
        output partitions. For example, by specifying one reducer, you get a
        single output file. We can use the sort example that comes with Hadoop
        by specifying that the input and output are sequence files and by
        setting the key and value types:</p><pre class="screen1">% <strong class="userinput"><code class="calibre9">hadoop jar \</code></strong>
<strong class="userinput"><code class="calibre9">  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \</code></strong>
<strong class="userinput"><code class="calibre9">  sort -r 1 \</code></strong>
<strong class="userinput"><code class="calibre9">  -inFormat org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat \</code></strong>
<strong class="userinput"><code class="calibre9">  -outFormat org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat \</code></strong>
<strong class="userinput"><code class="calibre9">  -outKey org.apache.hadoop.io.IntWritable \</code></strong>
<strong class="userinput"><code class="calibre9">  -outValue org.apache.hadoop.io.Text \</code></strong>
<strong class="userinput"><code class="calibre9">  numbers.seq sorted</code></strong>
% <strong class="userinput"><code class="calibre9">hadoop fs -text sorted/part-r-00000 | head</code></strong>
1       Nine, ten, a big fat hen
2       Seven, eight, lay them straight
3       Five, six, pick up sticks
4       Three, four, shut the door
5       One, two, buckle my shoe
6       Nine, ten, a big fat hen
7       Seven, eight, lay them straight
8       Five, six, pick up sticks
9       Three, four, shut the door
10      One, two, buckle my shoe</pre><p class="calibre2">Sorting is covered in more detail in <a class="ulink" href="#calibre_link-271" title="Sorting">Sorting</a>.</p><p class="calibre2">An alternative to using MapReduce for sort/merge is the <code class="literal">SequenceFile.Sorter</code> class, which has a
        number of <code class="literal">sort()</code> and <code class="literal">merge()</code> methods. These functions predate
        MapReduce and are lower-level functions than MapReduce (for example,
        to get parallelism, you need to partition your data manually), so in
        general MapReduce is the preferred approach to sort and merge sequence
        files.</p></div><div class="book" title="The SequenceFile format"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-245">The SequenceFile format</h4></div></div></div><p class="calibre2">A sequence file consists of a <a class="calibre" id="calibre_link-3331"></a>header followed by one or more records (see <a class="ulink" href="#calibre_link-272" title="Figure&nbsp;5-2.&nbsp;The internal structure of a sequence file with no compression and with record compression">Figure&nbsp;5-2</a>). The first three bytes of a sequence file
        are the bytes <code class="literal">SEQ</code>, which act as a
        magic number; these are followed by a single byte representing the
        version number. The header contains other fields, including the names
        of the key and value classes, compression details, user-defined metadata, and the sync
        marker.<sup class="calibre6">[<a class="firstname" href="#calibre_link-273" id="calibre_link-281">48</a>]</sup> Recall that the sync marker is used to allow a reader to
        synchronize to a record boundary from any position in the file. Each
        file has a randomly generated sync marker, whose value is stored in
        the header. Sync markers appear between records in the sequence file.
        They are designed to incur less than a 1% storage overhead, so they
        don’t necessarily appear between every pair of records (such is the
        case for short records).</p><div class="figure"><a id="calibre_link-272" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4144" class="calibre"></a><img alt="The internal structure of a sequence file with no compression and with record compression" src="images/000051.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;5-2.&nbsp;The internal structure of a sequence file with no compression
          and with record compression</div></div><p class="calibre2">The internal format of the records depends on whether
        compression is enabled, and if it is, whether it is record compression
        or block compression.</p><p class="calibre2">If no compression is enabled (the default), each record is made
        up of the record length (in bytes), the key length, the key, and then
        the value. The length fields are written as 4-byte integers adhering
        to the contract of the <code class="literal">writeInt()</code> method of
        <code class="literal">java.io.DataOutput</code>. Keys and values
        are serialized using the <code class="literal">Serialization</code> defined for the class being
        written to the sequence file.</p><p class="calibre2">The format for record compression is <a class="calibre" id="calibre_link-1215"></a><a class="calibre" id="calibre_link-1624"></a>almost identical to that for no compression, except the
        value bytes are compressed using the codec defined in the header. Note
        that keys are not compressed.</p><p class="calibre2">Block compression (<a class="ulink" href="#calibre_link-274" title="Figure&nbsp;5-3.&nbsp;The internal structure of a sequence file with block compression">Figure&nbsp;5-3</a>)
        compresses multiple records at once; it is therefore more compact than
        and should generally be preferred over record compression because it
        has the opportunity to take advantage of similarities between records.
        Records are added to a block until it reaches a minimum size in bytes,
        defined by the <code class="literal">io.seqfile.compress.blocksize</code> property;
        the default is one million bytes. A sync marker is written before the
        start of every block. The format of a block is a field indicating the
        number of records in the block, followed by four compressed fields:
        the key lengths, the keys, the value lengths, and the <a class="calibre" id="calibre_link-1364"></a><a class="calibre" id="calibre_link-3332"></a><a class="calibre" id="calibre_link-2358"></a>values.</p><div class="figure"><a id="calibre_link-274" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4145" class="calibre"></a><img alt="The internal structure of a sequence file with block compression" src="images/000060.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;5-3.&nbsp;The internal structure of a sequence file with block
          compression</div></div></div></div><div class="book" title="MapFile"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-735">MapFile</h3></div></div></div><p class="calibre2">A <code class="literal">MapFile</code> is a <a class="calibre" id="calibre_link-1360"></a><a class="calibre" id="calibre_link-2409"></a>sorted <code class="literal">SequenceFile</code>
      with an index to permit lookups by key. The index is itself a <code class="literal">SequenceFile</code> that contains a fraction of the
      keys in the map (every 128th key, by default). The idea is that the
      index can be loaded into memory to provide fast lookups from the main
      data file, which is another <code class="literal">SequenceFile</code> containing all the map entries in
      sorted key order.</p><p class="calibre2"><code class="literal">MapFile</code> offers a very similar
      interface to <code class="literal">SequenceFile</code> for reading
      and writing—the main thing to be aware of is that when writing using
      <code class="literal">MapFile.Writer</code>,
      map entries must be added in order, otherwise an <code class="literal">IOException</code> will be thrown.</p><div class="book" title="MapFile variants"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4146">MapFile variants</h4></div></div></div><p class="calibre2">Hadoop comes with a few variants on the general key-value
        <code class="literal">MapFile</code> interface:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2"><code class="literal">SetFile</code> is a <a class="calibre" id="calibre_link-3382"></a>specialization of <code class="literal">MapFile</code> for storing a set of <code class="literal">Writable</code> keys. The keys must be added in
            sorted order.</p></li><li class="listitem"><p class="calibre2"><code class="literal">ArrayFile</code> is a <code class="literal">MapFile</code> where<a class="calibre" id="calibre_link-918"></a> the key is an integer representing the index of the
            element in the array and the value is a <code class="literal">Writable</code> value.</p></li><li class="listitem"><p class="calibre2"><code class="literal">BloomMapFile</code> is a
            <code class="literal">MapFile</code> that offers <a class="calibre" id="calibre_link-1019"></a>a fast version of the <code class="literal">get()</code> method, especially for sparsely
            populated files. The implementation uses a dynamic Bloom filter
            for testing whether a given key is in the map. The test is very
            fast because it is in-memory, and it has a nonzero probability of
            false positives. Only if the test passes (the key is present) is
            the regular <code class="literal">get()</code> method called.</p></li></ul></div></div></div><div class="book" title="Other File Formats and Column-Oriented Formats"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-369">Other File Formats and Column-Oriented Formats</h3></div></div></div><p class="calibre2">While sequence <a class="calibre" id="calibre_link-1358"></a><a class="calibre" id="calibre_link-1349"></a><a class="calibre" id="calibre_link-1173"></a>files and map files are the oldest binary file formats in
      Hadoop, they are not the only ones, and in fact there are better
      alternatives that should be considered for new projects.</p><p class="calibre2">Avro datafiles (covered in <a class="ulink" href="#calibre_link-275" title="Avro Datafiles">Avro Datafiles</a>) are
      like <a class="calibre" id="calibre_link-940"></a>sequence files in that they are designed for large-scale
      data processing—they are compact and splittable—but they are portable
      across different programming languages. Objects stored in Avro datafiles
      are described by a schema, rather than in the Java code of the
      implementation of a <code class="literal">Writable</code> object (as is the case
      for sequence files), making them very Java-centric. Avro datafiles are
      widely supported across components in the Hadoop ecosystem, so they are
      a good default choice for a binary format.</p><p class="calibre2">Sequence files, map files, and Avro datafiles are all row-oriented
      file formats, which means that the values for each row are stored
      contiguously in the file. In a column-oriented format, the rows in a
      file (or, equivalently, a table in Hive) are broken up into row splits,
      then each split is stored in column-oriented fashion: the values for
      each row in the first column are stored first, followed by the values
      for each row in the second column, and so on. This is shown
      diagrammatically in <a class="ulink" href="#calibre_link-209" title="Figure&nbsp;5-4.&nbsp;Row-oriented versus column-oriented storage">Figure&nbsp;5-4</a>.</p><p class="calibre2">A column-oriented layout permits columns that are not accessed in
      a query to be skipped. Consider a query of the table in <a class="ulink" href="#calibre_link-209" title="Figure&nbsp;5-4.&nbsp;Row-oriented versus column-oriented storage">Figure&nbsp;5-4</a> that processes only column 2. With
      row-oriented storage, like a sequence file, the whole row (stored in a
      sequence file record) is loaded into memory, even though only the second
      column is actually read. Lazy <a class="calibre" id="calibre_link-1439"></a>deserialization saves some processing cycles by
      deserializing only the column fields that are accessed, but it can’t
      avoid the cost of reading each row’s bytes from disk.</p><p class="calibre2">With column-oriented storage, only the column 2 parts of the file
      (highlighted in the figure) need to be read into memory. In general,
      column-oriented formats work well when queries access only a small
      number of columns in the table. Conversely, row-oriented formats are
      appropriate when a large number of columns of a single row are needed
      for processing at the same time.</p><div class="book"><div class="figure"><a id="calibre_link-209" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4147" class="calibre"></a><img alt="Row-oriented versus column-oriented storage" src="images/000068.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;5-4.&nbsp;Row-oriented versus column-oriented storage</div></div></div><p class="calibre2">Column-oriented formats need more memory for reading and writing,
      since they have to buffer a row split in memory, rather than just a
      single row. Also, it’s not usually possible to control when writes occur
      (via flush or sync operations), so column-oriented formats are not
      suited to streaming writes, as the current file cannot be recovered if
      the writer process fails. On the other hand, row-oriented formats like
      sequence files and Avro datafiles can be read up to the last sync point
      after a writer failure. It is for this reason that Flume (see <a class="ulink" href="#calibre_link-276" title="Chapter&nbsp;14.&nbsp;Flume">Chapter&nbsp;14</a>) uses row-oriented formats.</p><p class="calibre2">The first column-oriented file format in Hadoop was <a class="calibre" id="calibre_link-2001"></a>Hive’s <em class="calibre10">RCFile</em>, short for
      <em class="calibre10">Record Columnar File</em>. It has since been
      superseded by Hive’s <em class="calibre10">ORCFile</em>
      (<em class="calibre10">Optimized Record Columnar File</em>), and
      <em class="calibre10">Parquet</em> (covered in <a class="ulink" href="#calibre_link-208" title="Chapter&nbsp;13.&nbsp;Parquet">Chapter&nbsp;13</a>). Parquet <a class="calibre" id="calibre_link-2908"></a>is a general-purpose column-oriented file format based on
      Google’s Dremel, and has wide support across Hadoop components. Avro
      also has a column-oriented format <a class="calibre" id="calibre_link-1628"></a><a class="calibre" id="calibre_link-2070"></a><a class="calibre" id="calibre_link-1359"></a><a class="calibre" id="calibre_link-1174"></a><a class="calibre" id="calibre_link-1350"></a>called <em class="calibre10">Trevni</em>.</p></div></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" id="calibre_link-235"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-277">44</a>] </sup>For a comprehensive set of compression benchmarks, <a class="ulink" href="https://github.com/ning/jvm-compressor-benchmark" target="_top"><span class="calibre"><em class="calibre10">jvm-compressor-benchmark</em></span></a>
        is a good reference for JVM-compatible libraries (including some
        native libraries).</p></div><div class="footnote" id="calibre_link-253"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-278">45</a>] </sup>This example is based on one from Norbert Lindenberg and
              Masayoshi Okutsu’s <a class="ulink" href="http://bit.ly/java_supp_characters" target="_top">“Supplementary
              Characters in the Java Platform,”</a> May 2004.</p></div><div class="footnote" id="calibre_link-264"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-279">46</a>] </sup>Twitter’s <a class="ulink" href="http://github.com/kevinweil/elephant-bird" target="_top">Elephant Bird
            project</a> includes <a class="calibre" id="calibre_link-1564"></a>tools for working with Thrift and Protocol Buffers
            in Hadoop.</p></div><div class="footnote" id="calibre_link-266"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-280">47</a>] </sup>In a similar vein, the blog post <a class="ulink" href="http://stuartsierra.com/2008/04/24/a-million-little-files" target="_top">“A
          Million Little Files”</a> by Stuart Sierra <a class="calibre" id="calibre_link-3407"></a>includes code for converting a tar file into
          <a class="calibre" id="calibre_link-3328"></a>a <code class="literal">SequenceFile</code>.</p></div><div class="footnote" id="calibre_link-273"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-281">48</a>] </sup>Full details of the format of these fields may be found in
            <code class="literal">SequenceFile</code>’s <a class="ulink" href="http://bit.ly/sequence_file_docs" target="_top">documentation</a>
            and source code.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-409"><div class="book" type="part" id="calibre_link-4148" title="Part&nbsp;II.&nbsp;MapReduce"><div class="book"><div class="book"><div class="book"><h1 class="title6">Part&nbsp;II.&nbsp;MapReduce</h1></div></div></div></div></div>

<div class="calibre1" id="calibre_link-79"><section type="chapter" id="calibre_link-4149" title="Chapter&nbsp;6.&nbsp;Developing a MapReduce Application"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;6.&nbsp;Developing a MapReduce Application</h2></div></div></div><p class="calibre2">In <a class="ulink" href="#calibre_link-462" title="Chapter&nbsp;2.&nbsp;MapReduce">Chapter&nbsp;2</a>, we introduced the MapReduce
  model. In this chapter, we look at the practical aspects of developing a
  MapReduce application in Hadoop.</p><p class="calibre2">Writing a program in <a class="calibre" id="calibre_link-2459"></a>MapReduce follows a certain pattern. You start by writing your
  map and reduce functions, ideally with unit tests to make sure they do what
  you expect. Then you write a driver program to run a job, which can run from
  your IDE using a small subset of the data to check that it is working. If it
  fails, you can use your IDE’s debugger to find the source of the problem.
  With this information, you can expand your unit tests to cover this case and
  improve your mapper or reducer as appropriate to handle such input
  correctly.</p><p class="calibre2">When the program runs as expected against the small dataset, you are
  ready to unleash it on a cluster. Running against the full dataset is likely
  to expose some more issues, which you can fix as before, by expanding your
  tests and altering your mapper or reducer to handle the new cases. Debugging
  failing programs in the cluster is a challenge, so we’ll look at some common
  techniques to make it easier.</p><p class="calibre2">After the program is working, you may wish to do some tuning, first by
  running through some standard checks for making MapReduce programs faster
  and then by doing task profiling. Profiling distributed programs is not
  easy, but Hadoop has hooks to aid in the process.</p><p class="calibre2">Before we start writing a MapReduce program, however, we need to set
  up and configure the development environment. And to do that, we need to
  learn a bit about how Hadoop does configuration.</p><div class="book" title="The Configuration API"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-41">The Configuration API</h2></div></div></div><p class="calibre2">Components in <a class="calibre" id="calibre_link-2460"></a><a class="calibre" id="calibre_link-1229"></a>Hadoop are configured using Hadoop’s own configuration API.
    An instance of the <code class="literal">Configuration</code> class
    (found in <a class="calibre" id="calibre_link-2859"></a>the <code class="literal">org.apache.hadoop.conf</code> package) represents a
    collection of configuration <em class="calibre10">properties</em> and their
    values. Each property is named by a <code class="literal">String</code>, and the type of a value may be one of
    several, including Java primitives such as <code class="literal">boolean</code>, <code class="literal">int</code>,
    <code class="literal">long</code>, and <code class="literal">float</code>; other useful types such as <code class="literal">String</code>, <code class="literal">Class</code>, and <code class="literal">java.io.File</code>; and collections of <code class="literal">String</code>s.</p><p class="calibre2"><code class="literal">Configuration</code>s read their
    properties from <em class="calibre10">resources</em>—XML files with a simple
    structure for defining name-value pairs. See <a class="ulink" href="#calibre_link-669" title="Example&nbsp;6-1.&nbsp;A simple configuration file, configuration-1.xml">Example&nbsp;6-1</a>.</p><div class="example"><a id="calibre_link-669" class="calibre"></a><div class="example-title">Example&nbsp;6-1.&nbsp;A simple configuration file, configuration-1.xml</div><div class="book"><pre class="screen"><code class="cp">&lt;?xml version="1.0"?&gt;</code>
<code class="nt">&lt;configuration&gt;</code>
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>color<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>yellow<code class="nt">&lt;/value&gt;</code>
    <code class="nt">&lt;description&gt;</code>Color<code class="nt">&lt;/description&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
  
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>size<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>10<code class="nt">&lt;/value&gt;</code>
    <code class="nt">&lt;description&gt;</code>Size<code class="nt">&lt;/description&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
  
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>weight<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>heavy<code class="nt">&lt;/value&gt;</code>
    <code class="nt">&lt;final&gt;</code>true<code class="nt">&lt;/final&gt;</code>
    <code class="nt">&lt;description&gt;</code>Weight<code class="nt">&lt;/description&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
  
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>size-weight<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>${size},${weight}<code class="nt">&lt;/value&gt;</code>
    <code class="nt">&lt;description&gt;</code>Size and weight<code class="nt">&lt;/description&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
<code class="nt">&lt;/configuration&gt;</code></pre></div></div><p class="calibre2">Assuming this <code class="literal">Configuration</code> is in a file called
    <em class="calibre10">configuration-1.xml</em>, we can access
    its properties using a piece of code like this:</p><a id="calibre_link-4150" class="calibre"></a><pre class="screen1">    <code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Configuration</code><code class="o">();</code>
    <code class="n">conf</code><code class="o">.</code><code class="na">addResource</code><code class="o">(</code><code class="sb">"configuration-1.xml"</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">conf</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"color"</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"yellow"</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">conf</code><code class="o">.</code><code class="na">getInt</code><code class="o">(</code><code class="sb">"size"</code><code class="o">,</code> <code class="mi">0</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">10</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">conf</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"breadth"</code><code class="o">,</code> <code class="sb">"wide"</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"wide"</code><code class="o">));</code></pre><p class="calibre2">There are a couple of things to note: type information is not stored
    in the XML file; instead, properties can be interpreted as a given type
    when they are read. Also, the <code class="literal">get()</code>
    methods allow you to specify a default value, which is used if the
    property is not defined in the XML file, as in the <a class="calibre" id="calibre_link-1230"></a>case of <code class="literal">breadth</code>
    here.</p><div class="book" title="Combining Resources"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4151">Combining Resources</h3></div></div></div><p class="calibre2">Things get interesting <a class="calibre" id="calibre_link-1232"></a>when more than one resource is used to define a
      <code class="literal">Configuration</code>. This is used in Hadoop to separate out
      the default properties for the system, defined internally in a file
      called <em class="calibre10">core-default.xml</em>, from the
      site-specific overrides in <em class="calibre10">core-site.xml</em>. The file in <a class="ulink" href="#calibre_link-670" title="Example&nbsp;6-2.&nbsp;A second configuration file, configuration-2.xml">Example&nbsp;6-2</a> defines the <code class="literal">size</code> and <code class="literal">weight</code> properties.</p><div class="example"><a id="calibre_link-670" class="calibre"></a><div class="example-title">Example&nbsp;6-2.&nbsp;A second configuration file, configuration-2.xml</div><div class="book"><pre class="screen"><code class="cp">&lt;?xml version="1.0"?&gt;</code>
<code class="nt">&lt;configuration&gt;</code>
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>size<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>12<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
  
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>weight<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>light<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
<code class="nt">&lt;/configuration&gt;</code></pre></div></div><p class="calibre2">Resources are added to a <code class="literal">Configuration</code> in order:</p><a id="calibre_link-4152" class="calibre"></a><pre class="screen1">    <code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Configuration</code><code class="o">();</code>
    <code class="n">conf</code><code class="o">.</code><code class="na">addResource</code><code class="o">(</code><code class="sb">"configuration-1.xml"</code><code class="o">);</code>
    <code class="n">conf</code><code class="o">.</code><code class="na">addResource</code><code class="o">(</code><code class="sb">"configuration-2.xml"</code><code class="o">);</code></pre><p class="calibre2">Properties defined in resources that are added later override the
      earlier definitions. So the <code class="literal">size</code>
      property takes its value from the second configuration file, <em class="calibre10">configuration-2.xml</em>:</p><a id="calibre_link-4153" class="calibre"></a><pre class="screen1">    <code class="n">assertThat</code><code class="o">(</code><code class="n">conf</code><code class="o">.</code><code class="na">getInt</code><code class="o">(</code><code class="sb">"size"</code><code class="o">,</code> <code class="mi">0</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="mi">12</code><code class="o">));</code></pre><p class="calibre2">However, properties that are marked as <code class="literal">final</code> cannot be overridden in later
      definitions. The <code class="literal">weight</code> property is
      <code class="literal">final</code> in the first configuration
      file, so the attempt to override it in the second fails, and it takes
      the value from the first:</p><a id="calibre_link-4154" class="calibre"></a><pre class="screen1">    <code class="n">assertThat</code><code class="o">(</code><code class="n">conf</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"weight"</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"heavy"</code><code class="o">));</code></pre><p class="calibre2">Attempting to override <code class="literal">final</code> properties usually
      indicates a configuration error, so this results in a warning message
      being logged to aid diagnosis. Administrators mark properties as
      <code class="literal">final</code> in the daemon’s site files that they don’t want
      users to change in their client-side configuration files or job
      submission parameters.</p></div><div class="book" title="Variable Expansion"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-674">Variable Expansion</h3></div></div></div><p class="calibre2">Configuration properties can be defined in <a class="calibre" id="calibre_link-1234"></a>terms of other properties, or system properties. For
      example, the property <code class="literal">size-weight</code> in
      the first configuration file is defined as <code class="literal">${size},${weight}</code>, and these properties are
      expanded using the values found in the configuration:</p><a id="calibre_link-4155" class="calibre"></a><pre class="screen1">    <code class="n">assertThat</code><code class="o">(</code><code class="n">conf</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"size-weight"</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"12,heavy"</code><code class="o">));</code></pre><p class="calibre2">System properties take priority over properties defined in
      resource files:</p><a id="calibre_link-4156" class="calibre"></a><pre class="screen1">    <code class="n">System</code><code class="o">.</code><code class="na">setProperty</code><code class="o">(</code><code class="sb">"size"</code><code class="o">,</code> <code class="sb">"14"</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">conf</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"size-weight"</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"14,heavy"</code><code class="o">));</code></pre><p class="calibre2">This feature is useful for overriding properties on the command
      line by using <code class="literal">-D<em class="replaceable"><code class="replaceable">property</code></em>=<em class="replaceable"><code class="replaceable">value</code></em></code> JVM arguments.</p><p class="calibre2">Note that although configuration properties can be defined in
      terms of system properties, unless system properties are redefined using
      configuration properties, they are <span class="calibre">not</span> accessible through the configuration API.
      <a class="calibre" id="calibre_link-2461"></a>Hence:</p><a id="calibre_link-4157" class="calibre"></a><pre class="screen1">    <code class="n">System</code><code class="o">.</code><code class="na">setProperty</code><code class="o">(</code><code class="sb">"length"</code><code class="o">,</code> <code class="sb">"2"</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">conf</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"length"</code><code class="o">),</code> <code class="n">is</code><code class="o">((</code><code class="n">String</code><code class="o">)</code> <code class="k">null</code><code class="o">));</code></pre></div></div><div class="book" title="Setting Up the Development Environment"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-675">Setting Up the Development Environment</h2></div></div></div><p class="calibre2">The first step is to <a class="calibre" id="calibre_link-2466"></a><a class="calibre" id="calibre_link-1446"></a>create a project so you can build MapReduce programs and run
    them in local (standalone) mode from the command line or within your IDE.
    The <a class="calibre" id="calibre_link-2654"></a>Maven Project Object Model (POM) in <a class="ulink" href="#calibre_link-671" title="Example&nbsp;6-3.&nbsp;A Maven POM for building and testing a MapReduce application">Example&nbsp;6-3</a> shows the dependencies needed for building and
    testing MapReduce programs.</p><div class="example"><a id="calibre_link-671" class="calibre"></a><div class="example-title">Example&nbsp;6-3.&nbsp;A Maven POM for building and testing a MapReduce
      application</div><div class="book"><pre class="screen"><code class="nt">&lt;project&gt;</code>
  <code class="nt">&lt;modelVersion&gt;</code>4.0.0<code class="nt">&lt;/modelVersion&gt;</code>
  <code class="nt">&lt;groupId&gt;</code>com.hadoopbook<code class="nt">&lt;/groupId&gt;</code>
  <code class="nt">&lt;artifactId&gt;</code>hadoop-book-mr-dev<code class="nt">&lt;/artifactId&gt;</code>
  <code class="nt">&lt;version&gt;</code>4.0<code class="nt">&lt;/version&gt;</code>
  <code class="nt">&lt;properties&gt;</code>
    <code class="nt">&lt;project.build.sourceEncoding&gt;</code>UTF-8<code class="nt">&lt;/project.build.sourceEncoding&gt;</code>
    <code class="nt">&lt;hadoop.version&gt;</code>2.5.1<code class="nt">&lt;/hadoop.version&gt;</code>
  <code class="nt">&lt;/properties&gt;</code>
  <code class="nt">&lt;dependencies&gt;</code>
    <code class="c1">&lt;!-- Hadoop main client artifact --&gt;</code>
    <code class="nt">&lt;dependency&gt;</code>
      <code class="nt">&lt;groupId&gt;</code>org.apache.hadoop<code class="nt">&lt;/groupId&gt;</code>
      <code class="nt">&lt;artifactId&gt;</code>hadoop-client<code class="nt">&lt;/artifactId&gt;</code>
      <code class="nt">&lt;version&gt;</code>${hadoop.version}<code class="nt">&lt;/version&gt;</code>
    <code class="nt">&lt;/dependency&gt;</code>
    <code class="c1">&lt;!-- Unit test artifacts --&gt;</code>
    <code class="nt">&lt;dependency&gt;</code>
      <code class="nt">&lt;groupId&gt;</code>junit<code class="nt">&lt;/groupId&gt;</code>
      <code class="nt">&lt;artifactId&gt;</code>junit<code class="nt">&lt;/artifactId&gt;</code>
      <code class="nt">&lt;version&gt;</code>4.11<code class="nt">&lt;/version&gt;</code>
      <code class="nt">&lt;scope&gt;</code>test<code class="nt">&lt;/scope&gt;</code>
    <code class="nt">&lt;/dependency&gt;</code>
    <code class="nt">&lt;dependency&gt;</code>
      <code class="nt">&lt;groupId&gt;</code>org.apache.mrunit<code class="nt">&lt;/groupId&gt;</code>
      <code class="nt">&lt;artifactId&gt;</code>mrunit<code class="nt">&lt;/artifactId&gt;</code>
      <code class="nt">&lt;version&gt;</code>1.1.0<code class="nt">&lt;/version&gt;</code>
      <code class="nt">&lt;classifier&gt;</code>hadoop2<code class="nt">&lt;/classifier&gt;</code>
      <code class="nt">&lt;scope&gt;</code>test<code class="nt">&lt;/scope&gt;</code>
    <code class="nt">&lt;/dependency&gt;</code>
    <code class="c1">&lt;!-- Hadoop test artifact for running mini clusters --&gt;</code>
    <code class="nt">&lt;dependency&gt;</code>
      <code class="nt">&lt;groupId&gt;</code>org.apache.hadoop<code class="nt">&lt;/groupId&gt;</code>
      <code class="nt">&lt;artifactId&gt;</code>hadoop-minicluster<code class="nt">&lt;/artifactId&gt;</code>
      <code class="nt">&lt;version&gt;</code>${hadoop.version}<code class="nt">&lt;/version&gt;</code>
      <code class="nt">&lt;scope&gt;</code>test<code class="nt">&lt;/scope&gt;</code>
    <code class="nt">&lt;/dependency&gt;</code>
  <code class="nt">&lt;/dependencies&gt;</code>
  <code class="nt">&lt;build&gt;</code>
    <code class="nt">&lt;finalName&gt;</code>hadoop-examples<code class="nt">&lt;/finalName&gt;</code>
    <code class="nt">&lt;plugins&gt;</code>
      <code class="nt">&lt;plugin&gt;</code>
        <code class="nt">&lt;groupId&gt;</code>org.apache.maven.plugins<code class="nt">&lt;/groupId&gt;</code>
        <code class="nt">&lt;artifactId&gt;</code>maven-compiler-plugin<code class="nt">&lt;/artifactId&gt;</code>
        <code class="nt">&lt;version&gt;</code>3.1<code class="nt">&lt;/version&gt;</code>
        <code class="nt">&lt;configuration&gt;</code>
          <code class="nt">&lt;source&gt;</code>1.6<code class="nt">&lt;/source&gt;</code>
          <code class="nt">&lt;target&gt;</code>1.6<code class="nt">&lt;/target&gt;</code>
        <code class="nt">&lt;/configuration&gt;</code>
      <code class="nt">&lt;/plugin&gt;</code>
      <code class="nt">&lt;plugin&gt;</code>
        <code class="nt">&lt;groupId&gt;</code>org.apache.maven.plugins<code class="nt">&lt;/groupId&gt;</code>
        <code class="nt">&lt;artifactId&gt;</code>maven-jar-plugin<code class="nt">&lt;/artifactId&gt;</code>
        <code class="nt">&lt;version&gt;</code>2.5<code class="nt">&lt;/version&gt;</code>
        <code class="nt">&lt;configuration&gt;</code>
          <code class="nt">&lt;outputDirectory&gt;</code>${basedir}<code class="nt">&lt;/outputDirectory&gt;</code>
        <code class="nt">&lt;/configuration&gt;</code>
      <code class="nt">&lt;/plugin&gt;</code>
    <code class="nt">&lt;/plugins&gt;</code>
  <code class="nt">&lt;/build&gt;</code>
<code class="nt">&lt;/project&gt;</code></pre></div></div><p class="calibre2">The dependencies section is the interesting part of the POM. (It is
    straightforward to use another build tool, such as Gradle or Ant
    <a class="calibre" id="calibre_link-886"></a><a class="calibre" id="calibre_link-1804"></a>with Ivy, as long as you use the same set of dependencies
    defined here.) For building MapReduce jobs, you only need to have the
    <code class="literal">hadoop-client</code> dependency, which
    contains all the Hadoop client-side classes needed to interact with HDFS
    and MapReduce. For running unit tests, we use <code class="literal">junit</code>, and <a class="calibre" id="calibre_link-2292"></a>for writing MapReduce tests, we <a class="calibre" id="calibre_link-3741"></a><a class="calibre" id="calibre_link-2709"></a>use <code class="literal">mrunit</code>. The <code class="literal">hadoop-minicluster</code> library contains the “mini-”
    clusters that are useful for testing with Hadoop clusters running in a
    single JVM.</p><p class="calibre2">Many IDEs can read Maven POMs directly, so you can just point them
    at the directory containing the <em class="calibre10">pom.xml</em> file and start writing code.
    Alternatively, you can use Maven to generate configuration files for your
    IDE. For example, the following creates Eclipse configuration files so you
    can import the project into<a class="calibre" id="calibre_link-1447"></a><a class="calibre" id="calibre_link-2655"></a> Eclipse:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">mvn eclipse:eclipse -DdownloadSources=true -DdownloadJavadocs=true</code></strong></pre><div class="book" title="Managing Configuration"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4158">Managing Configuration</h3></div></div></div><p class="calibre2">When developing <a class="calibre" id="calibre_link-1442"></a>Hadoop applications, it is common to switch between
      running the application locally and running it on a cluster. In fact,
      you may have several clusters you work with, or you may have a local
      “pseudodistributed” cluster that you like to test on (a
      pseudodistributed cluster is one whose daemons all run on the local
      machine; setting up this mode is covered in <a class="ulink" href="#calibre_link-28" title="Appendix&nbsp;A.&nbsp;Installing Apache Hadoop">Appendix&nbsp;A</a>).</p><p class="calibre2">One way to accommodate these variations is to have Hadoop
      configuration files containing the connection settings for each cluster
      you run against and specify which one you are using when you run Hadoop
      applications or tools. As a matter of best practice, it’s recommended to
      keep these files outside Hadoop’s installation directory tree, as this
      makes it easy to switch between Hadoop versions without duplicating or
      losing settings.</p><p class="calibre2">For the purposes of this book, we assume the existence of a
      directory called <em class="calibre10">conf</em> that contains
      three configuration files: <em class="calibre10">hadoop-local.xml</em>, <em class="calibre10">hadoop-localhost.xml</em>, and <em class="calibre10">hadoop-cluster.xml</em>
      (these are available in the example code for this book). Note that there
      is nothing special about the names of these files; they are just
      convenient ways to package up some configuration settings. (Compare this
      to <a class="ulink" href="#calibre_link-461" title="Table&nbsp;A-1.&nbsp;Key configuration properties for different modes">Table&nbsp;A-1</a> in <a class="ulink" href="#calibre_link-28" title="Appendix&nbsp;A.&nbsp;Installing Apache Hadoop">Appendix&nbsp;A</a>,
      which sets out the equivalent server-side configurations.)</p><p class="calibre2">The <em class="calibre10">hadoop-local.xml</em> file
      contains the default Hadoop configuration for the default filesystem and
      the local (in-JVM) framework for running MapReduce jobs:</p><pre class="screen1"><code class="cp">&lt;?xml version="1.0"?&gt;</code>
<code class="nt">&lt;configuration&gt;</code>

  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>fs.defaultFS<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>file:///<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
  
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>mapreduce.framework.name<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>local<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
  
<code class="nt">&lt;/configuration&gt;</code></pre><p class="calibre2">The settings in <em class="calibre10">hadoop-localhost.xml</em> point to a namenode and
      a YARN resource manager both running on localhost:</p><pre class="screen1"><code class="cp">&lt;?xml version="1.0"?&gt;</code>
<code class="nt">&lt;configuration&gt;</code>

  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>fs.defaultFS<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>hdfs://localhost/<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>

  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>mapreduce.framework.name<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>yarn<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>

  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>yarn.resourcemanager.address<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>localhost:8032<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
  
<code class="nt">&lt;/configuration&gt;</code></pre><p class="calibre2">Finally, <em class="calibre10">hadoop-cluster.xml</em>
      contains details of the cluster’s namenode and YARN resource manager
      addresses (in practice, you would name the file after the name of the
      cluster, rather than “cluster” as we have here):</p><pre class="screen1"><code class="cp">&lt;?xml version="1.0"?&gt;</code>
<code class="nt">&lt;configuration&gt;</code>

  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>fs.defaultFS<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>hdfs://namenode/<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>

  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>mapreduce.framework.name<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>yarn<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>

  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>yarn.resourcemanager.address<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>resourcemanager:8032<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>

<code class="nt">&lt;/configuration&gt;</code></pre><p class="calibre2">You can add other configuration properties to these files as
      needed.</p><div class="sidebar"><a id="calibre_link-4159" class="calibre"></a><div class="sidebar-title">Setting User Identity</div><p class="calibre2">The user identity that <a class="calibre" id="calibre_link-3752"></a>Hadoop uses for permissions in HDFS is determined by
        <a class="calibre" id="calibre_link-3785"></a>running the <code class="literal">whoami</code>
        command on the client system. Similarly, the group names are derived
        from the output of running <code class="literal">groups</code>.</p><p class="calibre2">If, however, your Hadoop user identity is different from the
        name of your user account on your client machine, you can explicitly
        set your Hadoop username by <a class="calibre" id="calibre_link-1882"></a>setting the <code class="literal">HADOOP_USER_NAME</code> environment variable. You
        can also override user group mappings by means of <a class="calibre" id="calibre_link-1864"></a>the <code class="literal">hadoop.user.group.static.mapping.overrides</code>
          configuration property. For example, <code class="literal">dr.who=;preston=directors,inventors</code> means
        that the <code class="literal">dr.who</code> user is in no
        groups, but <code class="literal">preston</code> is in the
        <code class="literal">directors</code> and <code class="literal">inventors</code> groups.</p><p class="calibre2">You can set the user identity that the Hadoop web interfaces run
        as by setting the <code class="literal">hadoop.http.staticuser.user</code> property.
        <a class="calibre" id="calibre_link-1859"></a>By default, it is <code class="literal">dr.who</code>, which is not a superuser, so system
        files are not accessible through the web interface.</p><p class="calibre2">Notice that, by default, there is no authentication with this
        system. See <a class="ulink" href="#calibre_link-169" title="Security">Security</a> for how to use Kerberos
        authentication with Hadoop.</p></div><p class="calibre2">With this setup, it is easy to use any configuration with the
      <code class="literal">-conf</code> command-line switch. For
      example, the following command shows a directory listing on the HDFS
      server running in pseudodistributed mode on localhost:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -conf conf/hadoop-localhost.xml -ls .</code></strong>
Found 2 items
drwxr-xr-x   - tom supergroup          0 2014-09-08 10:19 input
drwxr-xr-x   - tom supergroup          0 2014-09-08 10:19 output</pre><p class="calibre2">If you omit the <code class="literal">-conf</code> option,
      you pick up the Hadoop configuration in the <em class="calibre10">etc/hadoop</em> subdirectory under <code class="literal">$HADOOP_HOME</code>. Or, if <code class="literal">HADOOP_CONF_DIR</code> is set, Hadoop configuration
      files will be read from that location.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Here’s an alternative way of managing configuration settings.
        Copy the <em class="calibre10">etc/hadoop</em> directory
        from your Hadoop installation to another location, place the <em class="calibre10">*-site.xml</em> configuration files there (with
        appropriate settings), and <a class="calibre" id="calibre_link-1867"></a>set the <code class="literal">HADOOP_CONF_DIR</code> environment variable to the
        alternative location. The main advantage of this approach is that you
        don’t need to specify <code class="literal">-conf</code> for
        every command. It also allows you to isolate changes to files other
        than the Hadoop XML configuration files (e.g., <em class="calibre10">log4j.properties</em>) since the <code class="literal">HADOOP_CONF_DIR</code> directory has a copy of all
        the configuration files (see <a class="ulink" href="#calibre_link-30" title="Hadoop Configuration">Hadoop Configuration</a>).</p></div><p class="calibre2">Tools that come with Hadoop support the <code class="literal">-conf</code> option, but it’s straightforward to make
      your programs (such as programs that run MapReduce jobs) support it,
      too, <a class="calibre" id="calibre_link-1443"></a>using the <code class="literal">Tool</code>
      interface.</p></div><div class="book" title="GenericOptionsParser, Tool, and ToolRunner"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-247">GenericOptionsParser, Tool, and ToolRunner</h3></div></div></div><p class="calibre2">Hadoop comes <a class="calibre" id="calibre_link-1444"></a><a class="calibre" id="calibre_link-1192"></a>with a few helper classes for making it easier to run jobs
      from the command <a class="calibre" id="calibre_link-1787"></a><a class="calibre" id="calibre_link-3701"></a><a class="calibre" id="calibre_link-3703"></a>line. <code class="literal">GenericOptionsParser</code> is a class that
      interprets common Hadoop command-line options and sets them on a
      <code class="literal">Configuration</code> object for your
      application to use as desired. You don’t usually use <code class="literal">GenericOptionsParser</code> directly, as it’s more
      convenient to implement the
      <code class="literal">Tool</code> interface and run your
      application with the <code class="literal">ToolRunner</code>, which uses <code class="literal">GenericOptionsParser</code> internally:</p><a id="calibre_link-4160" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">interface</code> <code class="nc">Tool</code> <code class="k">extends</code> <code class="n">Configurable</code> <code class="o">{</code>
  <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code> <code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code><code class="o">;</code>
<code class="o">}</code></pre><p class="calibre2"><a class="ulink" href="#calibre_link-672" title="Example&nbsp;6-4.&nbsp;An example Tool implementation for printing the properties in a Configuration">Example&nbsp;6-4</a> shows a very simple
      <a class="calibre" id="calibre_link-3060"></a><a class="calibre" id="calibre_link-3069"></a><a class="calibre" id="calibre_link-2824"></a>implementation of <code class="literal">Tool</code>
      that prints the keys and values of all the properties in <a class="calibre" id="calibre_link-1231"></a>the <code class="literal">Tool</code>’s <code class="literal">Configuration</code> object.</p><div class="example"><a id="calibre_link-672" class="calibre"></a><div class="example-title">Example&nbsp;6-4.&nbsp;An example Tool implementation for printing the properties in a
          Configuration</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">ConfigurationPrinter</code> <code class="k">extends</code> <code class="n">Configured</code> <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>
  
  <code class="k">static</code> <code class="o">{</code>
    <code class="n">Configuration</code><code class="o">.</code><code class="na">addDefaultResource</code><code class="o">(</code><code class="sb">"hdfs-default.xml"</code><code class="o">);</code>
    <code class="n">Configuration</code><code class="o">.</code><code class="na">addDefaultResource</code><code class="o">(</code><code class="sb">"hdfs-site.xml"</code><code class="o">);</code>
    <code class="n">Configuration</code><code class="o">.</code><code class="na">addDefaultResource</code><code class="o">(</code><code class="sb">"yarn-default.xml"</code><code class="o">);</code>
    <code class="n">Configuration</code><code class="o">.</code><code class="na">addDefaultResource</code><code class="o">(</code><code class="sb">"yarn-site.xml"</code><code class="o">);</code>
    <code class="n">Configuration</code><code class="o">.</code><code class="na">addDefaultResource</code><code class="o">(</code><code class="sb">"mapred-default.xml"</code><code class="o">);</code>
    <code class="n">Configuration</code><code class="o">.</code><code class="na">addDefaultResource</code><code class="o">(</code><code class="sb">"mapred-site.xml"</code><code class="o">);</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="n">getConf</code><code class="o">();</code>
    <code class="k">for</code> <code class="o">(</code><code class="n">Entry</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;</code> <code class="nd">entry:</code> <code class="n">conf</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"%s=%s\n"</code><code class="o">,</code> <code class="n">entry</code><code class="o">.</code><code class="na">getKey</code><code class="o">(),</code> <code class="n">entry</code><code class="o">.</code><code class="na">getValue</code><code class="o">());</code>
    <code class="o">}</code>
    <code class="k">return</code> <code class="mi">0</code><code class="o">;</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="k">new</code> <code class="n">ConfigurationPrinter</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">We make <code class="literal">ConfigurationPrinter</code> a
      <a class="calibre" id="calibre_link-1236"></a>subclass of <code class="literal">Configured</code>,
      which is an implementation of the <code class="literal">Configurable</code> interface. All implementations of
      <code class="literal">Tool</code> need to implement <code class="literal">Configurable</code> (since
      <code class="literal">Tool</code> extends it), and subclassing
      <code class="literal">Configured</code> is often the easiest way
      to achieve this. The <code class="literal">run()</code> method obtains
      <a class="calibre" id="calibre_link-1227"></a>the <code class="literal">Configuration</code> using
      <code class="literal">Configurable</code>’s
      <code class="literal">getConf()</code> method and then iterates over it,
      printing each property to standard output.</p><p class="calibre2">The static block makes sure that the HDFS, YARN, and MapReduce
      configurations are picked up, in addition to the core ones (which
      <code class="literal">Configuration</code> knows about
      already).</p><p class="calibre2"><code class="literal">ConfigurationPrinter</code>’s
      <code class="literal">main()</code> method does not invoke its own
      <code class="literal">run()</code> method directly. Instead, we call
      <code class="literal">ToolRunner</code>’s static
      <code class="literal">run()</code> method, which takes care of creating a
      <code class="literal">Configuration</code>
      object for the <code class="literal">Tool</code> before calling
      its <code class="literal">run()</code> method. <code class="literal">ToolRunner</code> also uses a <code class="literal">GenericOptionsParser</code> to pick up any standard
      options specified on the command line and to set them on the <code class="literal">Configuration</code> instance. We can see the effect
      of picking up the properties specified in <em class="calibre10">conf/hadoop-localhost.xml</em> by running the
      following commands:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">mvn compile</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">export HADOOP_CLASSPATH=target/classes/</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop ConfigurationPrinter -conf conf/hadoop-localhost.xml \</code></strong>
<strong class="userinput"><code class="calibre9">  | grep yarn.resourcemanager.address=</code></strong>
yarn.resourcemanager.address=localhost:8032</pre><div class="sidebar"><a id="calibre_link-201" class="calibre"></a><div class="sidebar-title">Which Properties Can I Set?</div><p class="calibre2"><code class="literal">ConfigurationPrinter</code> is a
        useful tool for discovering what a property is set to in your
        environment. For a running daemon, like the namenode, you can see its
        configuration by viewing the <span class="calibre"><em class="calibre10">/conf</em></span> page on its
        web server. (See <a class="ulink" href="#calibre_link-48" title="Table&nbsp;10-6.&nbsp;HTTP server properties">Table&nbsp;10-6</a> to find port
        numbers.)</p><p class="calibre2">You can also see the default settings for all the public
        properties in Hadoop by looking in the <em class="calibre10">share/doc</em> directory of your Hadoop
        installation for files called <em class="calibre10">core-</em><em class="calibre10">default.xml</em>, <em class="calibre10">hdfs-default.xml</em>, <em class="calibre10">yarn-default.xml</em>, and <em class="calibre10">mapred-default.xml</em>. Each property has a
        description that explains what it is for and what values it can be set
        to.</p><p class="calibre2">The default settings files’ documentation can be found online at
        pages linked from <a class="ulink" href="http://hadoop.apache.org/docs/current/" target="_top">http://hadoop.apache.org/docs/current/</a> (look for the
        “Configuration” heading in the navigation). You can find the defaults
        for a particular Hadoop release by replacing
        <span class="calibre"><em class="calibre10">current</em></span> in the preceding URL with
        <span class="calibre"><em class="calibre10">r&lt;version&gt;</em></span>—for example, <a class="ulink" href="http://hadoop.apache.org/docs/r2.5.0/" target="_top">http://hadoop.apache.org/docs/r2.5.0/</a>.</p><p class="calibre2">Be aware that some properties have no effect when set in the
        client configuration. For example, if you set <code class="literal">yarn.nodemanager.resource.memory-mb</code> in your
        <a class="calibre" id="calibre_link-3872"></a>job submission with the expectation that it would change
        the amount of memory available to the node managers running your job,
        you would be disappointed, because this property is honored only if
        set in the node manager’s <em class="calibre10">yarn-site.xml</em> file. In general, you can
        tell the component where a property should be set by its name, so the
        fact that <code class="literal">yarn.nodemanager.resource.memory-mb</code> starts
        with <code class="literal">yarn.nodemanager</code> gives you a
        clue that it can be set only for the node manager daemon. This is not
        a hard and fast rule, however, so in some cases you may need to resort
        to trial and error, or even to reading the source.</p><p class="calibre2">Configuration property names have changed in Hadoop 2 onward, in
        order to give them a more regular naming structure. For example, the
        HDFS properties pertaining to the namenode have been changed to have a
        <code class="literal">dfs.namenode</code> prefix, so <code class="literal">dfs.name.dir</code> is now <code class="literal">dfs.namenode.name.dir</code>. Similarly, MapReduce
        properties have the <code class="literal">mapreduce</code>
        prefix rather than the older <code class="literal">mapred</code>
        prefix, so <code class="literal">mapred.job.name</code> is now
        <code class="literal">mapreduce.job.name</code>.</p><p class="calibre2">This book uses the new property names to avoid deprecation
        warnings. The old property names still work, however, and they are
        often referred to in older documentation. You can find a table listing
        the deprecated property names and their replacements on the <a class="ulink" href="http://bit.ly/deprecated_props" target="_top">Hadoop
        website</a>.</p><p class="calibre2">We discuss many of Hadoop’s most important configuration
        properties throughout this book.</p></div><p class="calibre2"><code class="literal">GenericOptionsParser</code> also
      allows you to set individual <a class="calibre" id="calibre_link-3070"></a><a class="calibre" id="calibre_link-3061"></a><a class="calibre" id="calibre_link-2825"></a>properties. For example:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop ConfigurationPrinter -D color=yellow | grep color</code></strong>
color=yellow</pre><p class="calibre2">Here, the <code class="literal">-D</code> option is used to
      set the configuration property with key <code class="literal">color</code> to the value <code class="literal">yellow</code>. Options specified with <code class="literal">-D</code> take priority over properties from the
      configuration files. This is very useful because you can put defaults
      into configuration files and then override them with the <code class="literal">-D</code> option as needed. A common example of this
      is setting the number of reducers for a MapReduce job via <code class="literal">-D
      mapreduce.job.reduces=<em class="replaceable"><code class="replaceable">n</code></em></code>. This will
      override the number of reducers set on the cluster or set in any
      client-side configuration files.</p><p class="calibre2">The other options that <code class="literal">GenericOptionsParser</code> and <code class="literal">ToolRunner</code> support are listed in <a class="ulink" href="#calibre_link-673" title="Table&nbsp;6-1.&nbsp;GenericOptionsParser and ToolRunner options">Table&nbsp;6-1</a>. You can find more on Hadoop’s
      configuration API in <a class="ulink" href="#calibre_link-41" title="The Configuration API">The Configuration API</a>.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">Do not confuse setting Hadoop properties using the <code class="literal">-D<em class="replaceable"><code class="replaceable"> property=value</code></em></code> option to
        <code class="literal">GenericOptionsParser</code> (and <code class="literal">ToolRunner</code>) with setting JVM system
        properties using the <code class="literal">-D<em class="replaceable"><code class="replaceable">property</code></em>=<em class="replaceable"><code class="replaceable">value</code></em></code>
        option to the <code class="literal">java</code> command. The
        syntax for JVM system properties does not allow any whitespace between
        the <code class="literal">D</code> and the property name,
        whereas <code class="literal">GenericOptionsParser</code> does allow
          whitespace.</p><p class="calibre2">JVM system properties are retrieved <a class="calibre" id="calibre_link-3583"></a>from the <code class="literal">java.lang.System</code> class, but Hadoop
        properties are accessible only from a <code class="literal">Configuration</code> object. So, the following
        command will print nothing, even though the <code class="literal">color</code> system property has been set
        (<a class="calibre" id="calibre_link-1879"></a>via <code class="literal">HADOOP_OPTS</code>),
        because the <code class="literal">System</code> class is not used by <code class="literal">ConfigurationPrinter</code>:</p><pre class="screen2"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">HADOOP_OPTS='-Dcolor=yellow' \
  hadoop ConfigurationPrinter | grep color</code></strong></pre><p class="calibre2">If you want to be able to set configuration through system
        properties, you need to mirror the system properties of interest in
        the configuration file. See
        <a class="ulink" href="#calibre_link-674" title="Variable Expansion">Variable Expansion</a> for further discussion.</p></div><div class="table"><a id="calibre_link-673" class="calibre"></a><div class="table-title">Table&nbsp;6-1.&nbsp;GenericOptionsParser and ToolRunner options</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Option</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">-D
              <em class="replaceable"><code class="calibre44">property</code></em>=<em class="replaceable"><code class="calibre44">value</code></em></code></td><td class="calibre25">Sets the given Hadoop configuration property to the given
              value. Overrides any default or site properties in the
              configuration and any properties set via the <code class="uri">-conf</code> option.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">-conf <em class="replaceable"><code class="calibre44">filename
              ...</code></em></code></td><td class="calibre25">Adds the given files to the list of resources in the
              configuration. This is a convenient way to set site properties
              or to set a number of properties at once.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">-fs
              <em class="replaceable"><code class="calibre44">uri</code></em></code></td><td class="calibre25">Sets the default filesystem to the given URI. Shortcut
              for <code class="uri">-D
              fs.defaultFS=<em class="replaceable"><code class="calibre44">uri</code></em></code>.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">-jt
              <em class="replaceable"><code class="calibre44">host:port</code></em></code></td><td class="calibre25">Sets the YARN resource manager to the given host and
              port. (In Hadoop 1, it sets the jobtracker address, hence the
              option name.) Shortcut for <code class="uri">-D
              yarn.resourcemanager.address=<em class="replaceable"><code class="calibre44">host:port</code></em></code>.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">-files
              <em class="replaceable"><code class="calibre44">file1,file2,...</code></em></code></td><td class="calibre25">Copies the specified files from the local filesystem (or
              any filesystem if a scheme is specified) to the shared
              filesystem used by MapReduce (usually HDFS) and makes them
              available to MapReduce programs in the task’s working directory.
              (See <a class="ulink" href="#calibre_link-54" title="Distributed Cache">Distributed Cache</a> for more on the
              distributed cache mechanism for copying files to machines in the
              cluster.)</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">-archives
              <em class="replaceable"><code class="calibre44">archive1,archive2,...</code></em></code></td><td class="calibre25">Copies the specified archives from the local filesystem
              (or any filesystem if a scheme is specified) to the shared
              filesystem used by MapReduce (usually HDFS), unarchives them,
              and makes them available to MapReduce programs in the task’s
              working directory.</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">-libjars
              <em class="replaceable"><code class="calibre44">jar1,jar2,...</code></em></code></td><td class="calibre28">Copies the specified JAR files from the local filesystem
              (or any filesystem if a scheme is specified) to the shared
              filesystem used by MapReduce (usually HDFS) and adds them to the
              MapReduce task’s classpath. This option is a useful way of
              shipping JAR files that a job is dependent <a class="calibre" id="calibre_link-2467"></a><a class="calibre" id="calibre_link-1445"></a><a class="calibre" id="calibre_link-1193"></a><a class="calibre" id="calibre_link-1788"></a><a class="calibre" id="calibre_link-3702"></a><a class="calibre" id="calibre_link-3704"></a>on.</td></tr></tbody></table></div></div></div></div><div class="book" title="Writing a Unit Test with MRUnit"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4161">Writing a Unit Test with MRUnit</h2></div></div></div><p class="calibre2">The map and reduce <a class="calibre" id="calibre_link-2472"></a><a class="calibre" id="calibre_link-3661"></a><a class="calibre" id="calibre_link-3742"></a><a class="calibre" id="calibre_link-2710"></a>functions in MapReduce are easy to test in isolation, which
    is a consequence of their functional style. <a class="ulink" href="https://mrunit.apache.org/" target="_top">MRUnit</a> is a testing library that
    makes it easy to pass known inputs to a mapper or a reducer and check that
    the outputs are as expected. MRUnit is used in conjunction with a standard
    test execution framework, such as JUnit, so you can run the tests for
    MapReduce jobs in your normal development environment. For example, all of
    the tests described here can be run from within an IDE by following the
    instructions in <a class="ulink" href="#calibre_link-675" title="Setting Up the Development Environment">Setting Up the Development Environment</a>.</p><div class="book" title="Mapper"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4162">Mapper</h3></div></div></div><p class="calibre2">The test for the mapper is <a class="calibre" id="calibre_link-2711"></a><a class="calibre" id="calibre_link-2404"></a>shown in <a class="ulink" href="#calibre_link-676" title="Example&nbsp;6-5.&nbsp;Unit test for MaxTemperatureMapper">Example&nbsp;6-5</a>.</p><div class="example"><a id="calibre_link-676" class="calibre"></a><div class="example-title">Example&nbsp;6-5.&nbsp;Unit test for MaxTemperatureMapper</div><div class="book"><pre class="screen"><code class="k">import</code> <code class="nn">java.io.IOException</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.io.*</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.mrunit.mapreduce.MapDriver</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.junit.*</code><code class="o">;</code>

<code class="k">public</code> <code class="k">class</code> <code class="nc">MaxTemperatureMapperTest</code> <code class="o">{</code>

  <code class="nd">@Test</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">processesValidRecord</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="n">Text</code> <code class="n">value</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="sb">"0043011990999991950051518004+68750+023550FM-12+0382"</code> <code class="o">+</code>
                                  <code class="c2">// Year ^^^^</code>
        <code class="sb">"99999V0203201N00261220001CN9999999N9-00111+99999999999"</code><code class="o">);</code>
                              <code class="c2">// Temperature ^^^^^</code>
    <code class="k">new</code> <code class="n">MapDriver</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">&gt;()</code>
      <code class="o">.</code><code class="na">withMapper</code><code class="o">(</code><code class="k">new</code> <code class="n">MaxTemperatureMapper</code><code class="o">())</code>
      <code class="o">.</code><code class="na">withInput</code><code class="o">(</code><code class="k">new</code> <code class="n">LongWritable</code><code class="o">(</code><code class="mi">0</code><code class="o">),</code> <code class="n">value</code><code class="o">)</code>
      <code class="o">.</code><code class="na">withOutput</code><code class="o">(</code><code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="sb">"1950"</code><code class="o">),</code> <code class="k">new</code> <code class="n">IntWritable</code><code class="o">(-</code><code class="mi">11</code><code class="o">))</code>
      <code class="o">.</code><code class="na">runTest</code><code class="o">();</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The idea of the test is very simple: pass a weather record as
      input to the mapper, and check that the output is the year and
      temperature reading.</p><p class="calibre2">Since we are testing the mapper, we use MRUnit’s <code class="literal">MapDriver</code>, which <a class="calibre" id="calibre_link-2408"></a>we configure with the mapper under test (<code class="literal">MaxTemperatureMapper</code>), the input key and
      value, and the expected output key (a <code class="literal">Text</code> object representing the year, 1950) and
      expected output value (an <code class="literal">IntWritable</code>
      representing the temperature, −1.1°C), before finally calling the
      <code class="literal">runTest()</code> method to execute the test.
      If the expected output values are not emitted by the mapper, MRUnit will
      fail the test. Notice that the input key could be set to any value
      because our mapper ignores it.</p><p class="calibre2">Proceeding in a test-driven fashion, we <a class="calibre" id="calibre_link-2413"></a>create a <code class="literal">Mapper</code>
      implementation that passes the test (see <a class="ulink" href="#calibre_link-677" title="Example&nbsp;6-6.&nbsp;First version of a Mapper that passes MaxTemperatureMapperTest">Example&nbsp;6-6</a>). Because we will be evolving the
      classes in this chapter, each is put in a different package indicating
      its version for ease of exposition. For example, <code class="literal">v1.MaxTemperatureMapper</code> is version 1 of
      <code class="literal">MaxTemperatureMapper</code>. In reality, of
      course, you would evolve classes without repackaging them.</p><div class="example"><a id="calibre_link-677" class="calibre"></a><div class="example-title">Example&nbsp;6-6.&nbsp;First version of a Mapper that passes
        MaxTemperatureMapperTest</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">MaxTemperatureMapper</code>
    <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">&gt;</code> <code class="o">{</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">LongWritable</code> <code class="n">key</code><code class="o">,</code> <code class="n">Text</code> <code class="n">value</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    
    <code class="n">String</code> <code class="n">line</code> <code class="o">=</code> <code class="n">value</code><code class="o">.</code><code class="na">toString</code><code class="o">();</code>
    <code class="n">String</code> <code class="n">year</code> <code class="o">=</code> <code class="n">line</code><code class="o">.</code><code class="na">substring</code><code class="o">(</code><code class="mi">15</code><code class="o">,</code> <code class="mi">19</code><code class="o">);</code>
    <code class="kt">int</code> <code class="n">airTemperature</code> <code class="o">=</code> <code class="n">Integer</code><code class="o">.</code><code class="na">parseInt</code><code class="o">(</code><code class="n">line</code><code class="o">.</code><code class="na">substring</code><code class="o">(</code><code class="mi">87</code><code class="o">,</code> <code class="mi">92</code><code class="o">));</code>
    <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="n">year</code><code class="o">),</code> <code class="k">new</code> <code class="n">IntWritable</code><code class="o">(</code><code class="n">airTemperature</code><code class="o">));</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">This is a very simple implementation that pulls the year and
      temperature fields from the line and writes them to the <code class="literal">Context</code>. Let’s add a test for missing values,
      which in the raw data are represented by a temperature of <code class="literal">+9999</code>:</p><a id="calibre_link-4163" class="calibre"></a><pre class="screen1">  <code class="nd">@Test</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">ignoresMissingTemperatureRecord</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code>
      <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="n">Text</code> <code class="n">value</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="sb">"0043011990999991950051518004+68750+023550FM-12+0382"</code> <code class="o">+</code>
                                  <code class="c2">// Year ^^^^</code>
        <code class="sb">"99999V0203201N00261220001CN9999999N9+99991+99999999999"</code><code class="o">);</code>
                              <code class="c2">// Temperature ^^^^^</code>
    <code class="k">new</code> <code class="n">MapDriver</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">&gt;()</code>
      <code class="o">.</code><code class="na">withMapper</code><code class="o">(</code><code class="k">new</code> <code class="n">MaxTemperatureMapper</code><code class="o">())</code>
      <code class="o">.</code><code class="na">withInput</code><code class="o">(</code><code class="k">new</code> <code class="n">LongWritable</code><code class="o">(</code><code class="mi">0</code><code class="o">),</code> <code class="n">value</code><code class="o">)</code>
      <code class="o">.</code><code class="na">runTest</code><code class="o">();</code>
  <code class="o">}</code></pre><p class="calibre2">A <code class="literal">MapDriver</code> can be used to
      check for zero, one, or more output records, according to the number of
      times that <code class="literal">withOutput()</code> is called. In
      our application, since records with missing temperatures should be
      filtered out, this test asserts that no output is produced for this
      particular input value.</p><p class="calibre2">The new test fails since <code class="literal">+9999</code>
      is not treated as a special case. Rather than putting more logic into
      the mapper, it makes sense to factor out a parser class to encapsulate
      the parsing <a class="calibre" id="calibre_link-2777"></a><a class="calibre" id="calibre_link-2771"></a>logic; see <a class="ulink" href="#calibre_link-678" title="Example&nbsp;6-7.&nbsp;A class for parsing weather records in NCDC format">Example&nbsp;6-7</a>.</p><div class="example"><a id="calibre_link-678" class="calibre"></a><div class="example-title">Example&nbsp;6-7.&nbsp;A class for parsing weather records in NCDC format</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">NcdcRecordParser</code> <code class="o">{</code>
  
  <code class="k">private</code> <code class="k">static</code> <code class="k">final</code> <code class="kt">int</code> <code class="n">MISSING_TEMPERATURE</code> <code class="o">=</code> <code class="mi">9999</code><code class="o">;</code>
  
  <code class="k">private</code> <code class="n">String</code> <code class="n">year</code><code class="o">;</code>
  <code class="k">private</code> <code class="kt">int</code> <code class="n">airTemperature</code><code class="o">;</code>
  <code class="k">private</code> <code class="n">String</code> <code class="n">quality</code><code class="o">;</code>
  
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">parse</code><code class="o">(</code><code class="n">String</code> <code class="n">record</code><code class="o">)</code> <code class="o">{</code>
    <code class="n">year</code> <code class="o">=</code> <code class="n">record</code><code class="o">.</code><code class="na">substring</code><code class="o">(</code><code class="mi">15</code><code class="o">,</code> <code class="mi">19</code><code class="o">);</code>
    <code class="n">String</code> <code class="n">airTemperatureString</code><code class="o">;</code>
    <code class="c2">// Remove leading plus sign as parseInt doesn't like them (pre-Java 7)</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">record</code><code class="o">.</code><code class="na">charAt</code><code class="o">(</code><code class="mi">87</code><code class="o">)</code> <code class="o">==</code> <code class="sb">'+'</code><code class="o">)</code> <code class="o">{</code> 
      <code class="n">airTemperatureString</code> <code class="o">=</code> <code class="n">record</code><code class="o">.</code><code class="na">substring</code><code class="o">(</code><code class="mi">88</code><code class="o">,</code> <code class="mi">92</code><code class="o">);</code>
    <code class="o">}</code> <code class="k">else</code> <code class="o">{</code>
      <code class="n">airTemperatureString</code> <code class="o">=</code> <code class="n">record</code><code class="o">.</code><code class="na">substring</code><code class="o">(</code><code class="mi">87</code><code class="o">,</code> <code class="mi">92</code><code class="o">);</code>
    <code class="o">}</code>
    <code class="n">airTemperature</code> <code class="o">=</code> <code class="n">Integer</code><code class="o">.</code><code class="na">parseInt</code><code class="o">(</code><code class="n">airTemperatureString</code><code class="o">);</code>
    <code class="n">quality</code> <code class="o">=</code> <code class="n">record</code><code class="o">.</code><code class="na">substring</code><code class="o">(</code><code class="mi">92</code><code class="o">,</code> <code class="mi">93</code><code class="o">);</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">parse</code><code class="o">(</code><code class="n">Text</code> <code class="n">record</code><code class="o">)</code> <code class="o">{</code>
    <code class="n">parse</code><code class="o">(</code><code class="n">record</code><code class="o">.</code><code class="na">toString</code><code class="o">());</code>
  <code class="o">}</code>

  <code class="k">public</code> <code class="kt">boolean</code> <code class="nf">isValidTemperature</code><code class="o">()</code> <code class="o">{</code>
    <code class="k">return</code> <code class="n">airTemperature</code> <code class="o">!=</code> <code class="n">MISSING_TEMPERATURE</code> <code class="o">&amp;&amp;</code> <code class="n">quality</code><code class="o">.</code><code class="na">matches</code><code class="o">(</code><code class="sb">"[01459]"</code><code class="o">);</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="n">String</code> <code class="nf">getYear</code><code class="o">()</code> <code class="o">{</code>
    <code class="k">return</code> <code class="n">year</code><code class="o">;</code>
  <code class="o">}</code>

  <code class="k">public</code> <code class="kt">int</code> <code class="nf">getAirTemperature</code><code class="o">()</code> <code class="o">{</code>
    <code class="k">return</code> <code class="n">airTemperature</code><code class="o">;</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The resulting mapper (version 2) is much simpler (see <a class="ulink" href="#calibre_link-679" title="Example&nbsp;6-8.&nbsp;A Mapper that uses a utility class to parse records">Example&nbsp;6-8</a>). It just calls the parser’s
      <code class="literal">parse()</code> method, which parses the fields of
      interest from a line of input, checks whether a valid temperature was
      found using the <code class="literal">isValidTemperature()</code> query
      method, and, if it was, retrieves the year and the temperature using the
      getter methods on the parser. Notice that we check the quality status
      field as well as checking for missing temperatures in
      <code class="literal">isValidTemperature()</code>, to filter out poor
      temperature readings.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Another benefit of creating a parser class is that it makes it
        easy to write related mappers for similar jobs without duplicating
        code. It also gives us the opportunity to write unit tests directly
        against the parser, for more targeted testing.</p></div><div class="example"><a id="calibre_link-679" class="calibre"></a><div class="example-title">Example&nbsp;6-8.&nbsp;A Mapper that uses a utility class to parse records</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">MaxTemperatureMapper</code>
    <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">&gt;</code> <code class="o">{</code>

  <span class="calibre24"><strong class="calibre9"><code class="kc">private</code> <code class="n1">NcdcRecordParser</code> <code class="n1">parser</code> <code class="o1">=</code> <code class="kc">new</code> <code class="n1">NcdcRecordParser</code><code class="o1">();</code></strong></span>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">LongWritable</code> <code class="n">key</code><code class="o">,</code> <code class="n">Text</code> <code class="n">value</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>

    <span class="calibre24"><strong class="calibre9"><code class="n1">parser</code><code class="o1">.</code><code class="na1">parse</code><code class="o1">(</code><code class="n1">value</code><code class="o1">);</code></strong></span>
    <code class="k">if</code> <code class="o">(</code><span class="calibre24"><strong class="calibre9"><code class="n1">parser</code><code class="o1">.</code><code class="na1">isValidTemperature</code><code class="o1">()</code></strong></span><code class="o">)</code> <code class="o">{</code>
      <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="k">new</code> <code class="n">Text</code><code class="o">(</code><span class="calibre24"><strong class="calibre9"><code class="n1">parser</code><code class="o1">.</code><code class="na1">getYear</code><code class="o1">()</code></strong></span><code class="o">),</code>
          <code class="k">new</code> <code class="nf">IntWritable</code><code class="o">(</code><span class="calibre24"><strong class="calibre9"><code class="n1">parser</code><code class="o1">.</code><code class="na1">getAirTemperature</code><code class="o1">()</code></strong></span><code class="o">));</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">With the tests for the mapper now passing, we move on to writing
      the <a class="calibre" id="calibre_link-2712"></a><a class="calibre" id="calibre_link-2405"></a><a class="calibre" id="calibre_link-2414"></a>reducer.</p></div><div class="book" title="Reducer"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4164">Reducer</h3></div></div></div><p class="calibre2">The reducer has to find <a class="calibre" id="calibre_link-2713"></a><a class="calibre" id="calibre_link-3192"></a>the maximum value for a given key. Here’s a simple test
      for this feature, <a class="calibre" id="calibre_link-3195"></a>which uses a <code class="literal">ReduceDriver</code>:</p><a id="calibre_link-4165" class="calibre"></a><pre class="screen1">  <code class="nd">@Test</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">returnsMaximumIntegerInValues</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code>
      <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="k">new</code> <code class="n">ReduceDriver</code><code class="o">&lt;</code><code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">&gt;()</code>
      <code class="o">.</code><code class="na">withReducer</code><code class="o">(</code><code class="k">new</code> <code class="n">MaxTemperatureReducer</code><code class="o">())</code>
      <code class="o">.</code><code class="na">withInput</code><code class="o">(</code><code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="sb">"1950"</code><code class="o">),</code>
          <code class="n">Arrays</code><code class="o">.</code><code class="na">asList</code><code class="o">(</code><code class="k">new</code> <code class="n">IntWritable</code><code class="o">(</code><code class="mi">10</code><code class="o">),</code> <code class="k">new</code> <code class="n">IntWritable</code><code class="o">(</code><code class="mi">5</code><code class="o">)))</code>
      <code class="o">.</code><code class="na">withOutput</code><code class="o">(</code><code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="sb">"1950"</code><code class="o">),</code> <code class="k">new</code> <code class="n">IntWritable</code><code class="o">(</code><code class="mi">10</code><code class="o">))</code>
      <code class="o">.</code><code class="na">runTest</code><code class="o">();</code>
  <code class="o">}</code></pre><p class="calibre2">We construct a list of some <code class="literal">IntWritable</code> values and then verify that
      <code class="literal">MaxTemperatureReducer</code> picks the largest.
      The code in <a class="ulink" href="#calibre_link-680" title="Example&nbsp;6-9.&nbsp;Reducer for the maximum temperature example">Example&nbsp;6-9</a> is for an
      implementation of <code class="literal">MaxTemperatureReducer</code> that passes the
      <a class="calibre" id="calibre_link-2473"></a><a class="calibre" id="calibre_link-3662"></a><a class="calibre" id="calibre_link-3743"></a>test.</p><div class="example"><a id="calibre_link-680" class="calibre"></a><div class="example-title">Example&nbsp;6-9.&nbsp;Reducer for the maximum temperature example</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">MaxTemperatureReducer</code>
    <code class="k">extends</code> <code class="n">Reducer</code><code class="o">&lt;</code><code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">&gt;</code> <code class="o">{</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">reduce</code><code class="o">(</code><code class="n">Text</code> <code class="n">key</code><code class="o">,</code> <code class="n">Iterable</code><code class="o">&lt;</code><code class="n">IntWritable</code><code class="o">&gt;</code> <code class="n">values</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    
    <code class="kt">int</code> <code class="n">maxValue</code> <code class="o">=</code> <code class="n">Integer</code><code class="o">.</code><code class="na">MIN_VALUE</code><code class="o">;</code>
    <code class="k">for</code> <code class="o">(</code><code class="n">IntWritable</code> <code class="n">value</code> <code class="o">:</code> <code class="n">values</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">maxValue</code> <code class="o">=</code> <code class="n">Math</code><code class="o">.</code><code class="na">max</code><code class="o">(</code><code class="n">maxValue</code><code class="o">,</code> <code class="n">value</code><code class="o">.</code><code class="na">get</code><code class="o">());</code>
    <code class="o">}</code>
    <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">key</code><code class="o">,</code> <code class="k">new</code> <code class="n">IntWritable</code><code class="o">(</code><code class="n">maxValue</code><code class="o">));</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div></div></div><div class="book" title="Running Locally on Test Data"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4166">Running Locally on Test Data</h2></div></div></div><p class="calibre2">Now that we have the <a class="calibre" id="calibre_link-2462"></a><a class="calibre" id="calibre_link-3657"></a>mapper and reducer working on controlled inputs, the next
    step is to write a job driver and run it on some test data on a
    development machine.</p><div class="book" title="Running a Job in a Local Job Runner"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4167">Running a Job in a Local Job Runner</h3></div></div></div><p class="calibre2">Using the <code class="literal">Tool</code> interface
      introduced <a class="calibre" id="calibre_link-2255"></a>earlier in the chapter, it’s easy to write a driver to run
      our MapReduce job for finding the maximum temperature by year (see
      <code class="literal">MaxTemperatureDriver</code> in <a class="ulink" href="#calibre_link-681" title="Example&nbsp;6-10.&nbsp;Application to find the maximum temperature">Example&nbsp;6-10</a>).</p><div class="example"><a id="calibre_link-681" class="calibre"></a><div class="example-title">Example&nbsp;6-10.&nbsp;Application to find the maximum temperature</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">MaxTemperatureDriver</code> <code class="k">extends</code> <code class="n">Configured</code> <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="na">length</code> <code class="o">!=</code> <code class="mi">2</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"Usage: %s [generic options] &lt;input&gt; &lt;output&gt;\n"</code><code class="o">,</code>
          <code class="n">getClass</code><code class="o">().</code><code class="na">getSimpleName</code><code class="o">());</code>
      <code class="n">ToolRunner</code><code class="o">.</code><code class="na">printGenericCommandUsage</code><code class="o">(</code><code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">);</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>
    
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Job</code><code class="o">(</code><code class="n">getConf</code><code class="o">(),</code> <code class="sb">"Max temperature"</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setJarByClass</code><code class="o">(</code><code class="n">getClass</code><code class="o">());</code>

    <code class="n">FileInputFormat</code><code class="o">.</code><code class="na">addInputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]));</code>
    <code class="n">FileOutputFormat</code><code class="o">.</code><code class="na">setOutputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">]));</code>
    
    <code class="n">job</code><code class="o">.</code><code class="na">setMapperClass</code><code class="o">(</code><code class="n">MaxTemperatureMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setCombinerClass</code><code class="o">(</code><code class="n">MaxTemperatureReducer</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setReducerClass</code><code class="o">(</code><code class="n">MaxTemperatureReducer</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="n">job</code><code class="o">.</code><code class="na">setOutputKeyClass</code><code class="o">(</code><code class="n">Text</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputValueClass</code><code class="o">(</code><code class="n">IntWritable</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    
    <code class="k">return</code> <code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">;</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="k">new</code> <code class="n">MaxTemperatureDriver</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2"><code class="literal">MaxTemperatureDriver</code> implements
      the <code class="literal">Tool</code> interface, so we get the
      benefit of being able to set the options that <code class="literal">GenericOptionsParser</code> supports. The
      <code class="literal">run()</code> method constructs a <code class="literal">Job</code> object based on the tool’s configuration,
      which it uses to launch a job. Among the possible job configuration
      parameters, we set the input and output file paths; the mapper, reducer,
      and combiner classes; and the output types (the input types are
      determined by the input format, which defaults <a class="calibre" id="calibre_link-3676"></a>to <code class="literal">TextInputFormat</code> and
      has <code class="literal">LongWritable</code> keys and <code class="literal">Text</code> values). It’s also a good idea to set a
      name for the job (<code class="literal">Max temperature</code>) so
      that you can pick it out in the job list during execution and after it
      has completed. By default, the name is the name of the JAR file, which
      normally is not particularly descriptive.</p><p class="calibre2">Now we can run this application against some local files. Hadoop
      comes with a local job runner, a cut-down version of the MapReduce
      execution engine for running MapReduce jobs in a single JVM. It’s
      designed for testing and is very convenient for use in an IDE, since you
      can run it in a debugger to step through the code in your mapper and
      reducer.</p><p class="calibre2">The local job runner is <a class="calibre" id="calibre_link-2531"></a>used if <code class="literal">mapreduce.framework.name</code> is set to <code class="literal">local</code>, which is the default.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-682" id="calibre_link-703">49</a>]</sup></p><p class="calibre2">From the command line, we can run the driver by typing:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">mvn compile</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">export HADOOP_CLASSPATH=target/classes/</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop v2.MaxTemperatureDriver -conf conf/hadoop-local.xml \</code></strong>
<strong class="userinput"><code class="calibre9">  input/ncdc/micro output</code></strong></pre><p class="calibre2">Equivalently, we could use the <code class="literal">-fs</code> and <code class="literal">-jt</code>
      options provided by <code class="literal">GenericOptionsParser</code>:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop v2.MaxTemperatureDriver -fs file:/// -jt local input/ncdc/micro output</code></strong></pre><p class="calibre2">This command executes <code class="literal">MaxTemperatureDriver</code> using input from the
      local <em class="calibre10">input/ncdc/micro</em> directory,
      producing output in the local <em class="calibre10">output</em> directory. Note that although we’ve
      set <code class="literal">-fs</code> so we use the local
      filesystem (<code class="literal">file:///</code>), the local job
      runner will actually work fine against any filesystem, including HDFS
      (and it can be handy to do this if you have a few files that are on
      HDFS).</p><p class="calibre2">We can examine the output on the local <a class="calibre" id="calibre_link-2256"></a>filesystem:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">cat output/part-r-00000</code></strong>
1949    111
1950    22</pre></div><div class="book" title="Testing the Driver"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4168">Testing the Driver</h3></div></div></div><p class="calibre2">Apart from the flexible <a class="calibre" id="calibre_link-2264"></a><a class="calibre" id="calibre_link-3652"></a>configuration options offered by making your application
      implement <code class="literal">Tool</code>, you also make it more
      testable because it allows you to inject an arbitrary <code class="literal">Configuration</code>. You can take advantage of this
      to write a test that uses a local job runner to run a job against known
      input data, which checks that the output is as expected.</p><p class="calibre2">There are two approaches to doing this. The first is to use the
      local job runner and run the job against a test file on the local
      filesystem. The code in <a class="ulink" href="#calibre_link-683" title="Example&nbsp;6-11.&nbsp;A test for MaxTemperatureDriver that uses a local, in-process job runner">Example&nbsp;6-11</a>
      gives an idea of how to do this.</p><div class="example"><a id="calibre_link-683" class="calibre"></a><div class="example-title">Example&nbsp;6-11.&nbsp;A test for MaxTemperatureDriver that uses a local, in-process
          job runner</div><div class="book"><pre class="screen">  <code class="nd">@Test</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">test</code><code class="o">()</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Configuration</code><code class="o">();</code>
    <code class="n">conf</code><code class="o">.</code><code class="na">set</code><code class="o">(</code><code class="sb">"fs.defaultFS"</code><code class="o">,</code> <code class="sb">"file:///"</code><code class="o">);</code>
    <code class="n">conf</code><code class="o">.</code><code class="na">set</code><code class="o">(</code><code class="sb">"mapreduce.framework.name"</code><code class="o">,</code> <code class="sb">"local"</code><code class="o">);</code>
    <code class="n">conf</code><code class="o">.</code><code class="na">setInt</code><code class="o">(</code><code class="sb">"mapreduce.task.io.sort.mb"</code><code class="o">,</code> <code class="mi">1</code><code class="o">);</code>
    
    <code class="n">Path</code> <code class="n">input</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="sb">"input/ncdc/micro"</code><code class="o">);</code>
    <code class="n">Path</code> <code class="n">output</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="sb">"output"</code><code class="o">);</code>
    
    <code class="n">FileSystem</code> <code class="n">fs</code> <code class="o">=</code> <code class="n">FileSystem</code><code class="o">.</code><code class="na">getLocal</code><code class="o">(</code><code class="n">conf</code><code class="o">);</code>
    <code class="n">fs</code><code class="o">.</code><code class="na">delete</code><code class="o">(</code><code class="n">output</code><code class="o">,</code> <code class="k">true</code><code class="o">);</code> <code class="c2">// delete old output</code>
    
    <code class="n">MaxTemperatureDriver</code> <code class="n">driver</code> <code class="o">=</code> <code class="k">new</code> <code class="n">MaxTemperatureDriver</code><code class="o">();</code>
    <code class="n">driver</code><code class="o">.</code><code class="na">setConf</code><code class="o">(</code><code class="n">conf</code><code class="o">);</code>
    
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">driver</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="k">new</code> <code class="n">String</code><code class="o">[]</code> <code class="o">{</code>
        <code class="n">input</code><code class="o">.</code><code class="na">toString</code><code class="o">(),</code> <code class="n">output</code><code class="o">.</code><code class="na">toString</code><code class="o">()</code> <code class="o">});</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">exitCode</code><code class="o">,</code> <code class="n">is</code><code class="o">(</code><code class="mi">0</code><code class="o">));</code>
    
    <code class="n">checkOutput</code><code class="o">(</code><code class="n">conf</code><code class="o">,</code> <code class="n">output</code><code class="o">);</code>
  <code class="o">}</code></pre></div></div><p class="calibre2">The test explicitly <a class="calibre" id="calibre_link-1740"></a>sets <code class="literal">fs.defaultFS</code> and
      <code class="literal">mapreduce.framework.name</code> so it uses
      the local <a class="calibre" id="calibre_link-2532"></a>filesystem and the local job runner. It then runs the
      <code class="literal">MaxTemperatureDriver</code> via its <code class="literal">Tool</code> interface against a small amount of known
      data. At the end of the test, the <code class="literal">checkOutput()</code> method is called to compare the
      actual output with the expected output, line by line.</p><p class="calibre2">The second way of testing the driver <a class="calibre" id="calibre_link-1153"></a><a class="calibre" id="calibre_link-2699"></a><a class="calibre" id="calibre_link-3656"></a>is to run it using a “mini-” cluster. Hadoop has a set of
      testing classes, called <code class="literal">MiniDFSCluster</code>, <code class="literal">MiniMRCluster</code>, and <code class="literal">MiniYARNCluster</code>, that provide a programmatic
      way of creating in-process clusters. Unlike the local job runner, these
      allow testing against the full HDFS, MapReduce, and YARN machinery. Bear
      in mind, too, that node managers in a mini-cluster launch separate JVMs
      to run tasks in, which can make debugging more difficult.</p><div class="note" title="Tip"><h3 class="title3">Tip</h3><p class="calibre2">You can run a mini-cluster from the <a class="calibre" id="calibre_link-1194"></a><a class="calibre" id="calibre_link-1847"></a>command line too, with the following:</p><pre class="screen2"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar \
  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-*-tests.jar \
  minicluster</code></strong></pre></div><p class="calibre2">Mini-clusters are used extensively in Hadoop’s own automated test
      suite, but they can be used for testing user code, too. Hadoop’s
      <code class="literal">ClusterMapReduceTestCase</code> abstract
      <a class="calibre" id="calibre_link-1108"></a>class provides a useful base for writing such a test,
      handles the details of starting and stopping the in-process HDFS and
      YARN clusters in its <code class="literal">setUp()</code> and
      <code class="literal">tearDown()</code> methods, and generates a suitable
      <code class="literal">Configuration</code> object that is set up to work with
      them. Subclasses need only populate data in HDFS (perhaps by copying
      from a local file), run a MapReduce job, and confirm the output is as
      expected. Refer to the <code class="literal">MaxTemperatureDriverMiniTest</code> class in the
      example code that comes with this book for the listing.</p><p class="calibre2">Tests like this serve as regression tests, and are a useful
      repository of input edge cases and their expected results. As you
      encounter more test cases, you can simply add them to the input file and
      update the file of expected output <a class="calibre" id="calibre_link-2463"></a><a class="calibre" id="calibre_link-3658"></a><a class="calibre" id="calibre_link-2265"></a><a class="calibre" id="calibre_link-3653"></a>accordingly.</p></div></div><div class="book" title="Running on a Cluster"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4169">Running on a Cluster</h2></div></div></div><p class="calibre2">Now that we are happy <a class="calibre" id="calibre_link-2464"></a><a class="calibre" id="calibre_link-1133"></a><a class="calibre" id="calibre_link-3659"></a>with the program running on a small test dataset, we are
    ready to try it on the full dataset on a Hadoop cluster. <a class="ulink" href="#calibre_link-1" title="Chapter&nbsp;10.&nbsp;Setting Up a Hadoop Cluster">Chapter&nbsp;10</a> covers how to set up a fully distributed cluster,
    although you can also work through this section on a pseudo-distributed cluster.</p><div class="book" title="Packaging a Job"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-760">Packaging a Job</h3></div></div></div><p class="calibre2">The local job runner <a class="calibre" id="calibre_link-2250"></a><a class="calibre" id="calibre_link-2891"></a>uses a single JVM to run a job, so as long as all the
      classes that your job needs are on its classpath, then things will just
      work.</p><p class="calibre2">In a distributed setting, things are a little more complex. For a
      start, a job’s classes must be packaged into a <em class="calibre10">job JAR
      file</em> to send <a class="calibre" id="calibre_link-2219"></a>to the cluster. Hadoop will find the job JAR automatically
      by searching for the JAR on the driver’s classpath that contains the
      class set in the <code class="literal">setJarByClass()</code>
      method (on <code class="literal">JobConf</code> or <code class="literal">Job</code>). Alternatively, <a class="calibre" id="calibre_link-2228"></a><a class="calibre" id="calibre_link-2211"></a>if you want to set an explicit JAR file by its file path,
      you can use the <code class="literal">setJar()</code> method. (The
      JAR file path may be local or an HDFS file path.)</p><p class="calibre2">Creating a job JAR file is conveniently achieved using a build
      tool such as <a class="calibre" id="calibre_link-887"></a>Ant or Maven. Given the POM in <a class="ulink" href="#calibre_link-671" title="Example&nbsp;6-3.&nbsp;A Maven POM for building and testing a MapReduce application">Example&nbsp;6-3</a>, the following <a class="calibre" id="calibre_link-2656"></a>Maven command will create a JAR file called <em class="calibre10">hadoop-examples.jar</em> in the project directory
      containing all of the compiled classes:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">mvn package -DskipTests</code></strong></pre><p class="calibre2">If you have a single job per JAR, you can specify the main class
      to run in the JAR file’s manifest. If the main class is not in the
      manifest, it must be specified on the command line (as we will see
      shortly when we run the job).</p><p class="calibre2">Any dependent JAR files can be packaged in a <em class="calibre10">lib</em> subdirectory in the job JAR file,
      although there are other ways to include dependencies, discussed later.
      Similarly, resource files can be packaged in a <em class="calibre10">classes</em> subdirectory. (This is analogous to a
      <a class="calibre" id="calibre_link-3772"></a><a class="calibre" id="calibre_link-3779"></a><a class="calibre" id="calibre_link-2179"></a>Java <em class="calibre10">Web application archive</em>, or
      WAR, file, except in that case the JAR files go in a <em class="calibre10">WEB-INF/lib</em> subdirectory and classes go in a
      <em class="calibre10">WEB-INF/classes</em> subdirectory in the
      WAR file.)</p><div class="book" title="The client classpath"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4170">The client classpath</h4></div></div></div><p class="calibre2">The user’s client-side <a class="calibre" id="calibre_link-2220"></a><a class="calibre" id="calibre_link-2892"></a>classpath set by <code class="literal">hadoop jar
        <em class="replaceable"><code class="replaceable">&lt;jar&gt;</code></em></code> is made up of:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">The job JAR file</p></li><li class="listitem"><p class="calibre2">Any JAR files in the <em class="calibre10">lib</em> directory of the job JAR file, and
            the <em class="calibre10">classes</em> directory (if
            present)</p></li><li class="listitem"><p class="calibre2">The classpath defined by <code class="literal">HADOOP_CLASSPATH</code>, if set</p></li></ul></div><p class="calibre2">Incidentally, this explains why <a class="calibre" id="calibre_link-1865"></a>you have to set <code class="literal">HADOOP_CLASSPATH</code> to point to dependent
        classes and libraries if you are running using the local job runner
        without a job JAR (<code class="literal">hadoop
        <em class="replaceable"><code class="replaceable">CLASSNAME</code></em></code>).</p></div><div class="book" title="The task classpath"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4171">The task classpath</h4></div></div></div><p class="calibre2">On a cluster (and <a class="calibre" id="calibre_link-2222"></a><a class="calibre" id="calibre_link-2894"></a>this includes pseudodistributed mode), map and reduce
        tasks run in separate JVMs, and their classpaths are <span class="calibre">not</span> controlled by <code class="literal">HADOOP_CLASSPATH</code>. <code class="literal">HADOOP_CLASSPATH</code> is a client-side
        setting and only sets the classpath for the driver JVM, which submits
        the job.</p><p class="calibre2">Instead, the user’s task classpath is comprised of the
        following:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">The job JAR file</p></li><li class="listitem"><p class="calibre2">Any JAR files contained in the <em class="calibre10">lib</em> directory of the job JAR file, and
            the <em class="calibre10">classes</em> directory (if
            present)</p></li><li class="listitem"><p class="calibre2">Any files added to the distributed cache using the <code class="literal">-libjars</code> option (see <a class="ulink" href="#calibre_link-673" title="Table&nbsp;6-1.&nbsp;GenericOptionsParser and ToolRunner options">Table&nbsp;6-1</a>), or the <code class="literal">addFileToClassPath()</code> method on <code class="literal">DistributedCache</code> (old API), or <code class="literal">Job</code> (new API)</p></li></ul></div></div><div class="book" title="Packaging dependencies"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4172">Packaging dependencies</h4></div></div></div><p class="calibre2">Given these different <a class="calibre" id="calibre_link-2221"></a><a class="calibre" id="calibre_link-2893"></a><a class="calibre" id="calibre_link-1429"></a>ways of controlling what is on the client and task
        classpaths, there are corresponding options for including library
        dependencies for a job:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">Unpack the libraries and repackage them in the job
            JAR.</p></li><li class="listitem"><p class="calibre2">Package the libraries in the <em class="calibre10">lib</em> directory of the job JAR.</p></li><li class="listitem"><p class="calibre2">Keep the libraries separate from the job JAR, and add them
            to the client classpath via <code class="literal">HADOOP_CLASSPATH</code> and to the task
            classpath via <code class="literal">-libjars</code>.</p></li></ul></div><p class="calibre2">The last option, using the distributed cache, is simplest from a
        build point of view because dependencies don’t need rebundling in the
        job JAR. Also, using the distributed cache can mean fewer transfers of
        JAR files around the cluster, since files may be cached on a node
        between tasks. (You can read more about the distributed cache .)</p></div><div class="book" title="Task classpath precedence"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4173">Task classpath precedence</h4></div></div></div><p class="calibre2">User JAR files are added <a class="calibre" id="calibre_link-2223"></a><a class="calibre" id="calibre_link-2895"></a>to the end of both the client classpath and the task
        classpath, which in some cases can cause a dependency conflict with
        Hadoop’s built-in libraries if Hadoop uses a different, incompatible
        version of a library that your code uses. Sometimes you need to be
        able to control the task classpath order so that your classes are
        picked up first. On the client side, you can force Hadoop to put the
        user classpath first in the search order by setting the <code class="literal">HADOOP_USER_CLASSPATH_FIRST</code> environment
        <a class="calibre" id="calibre_link-1881"></a>variable to <code class="literal">true</code>. For
        the task classpath, you can set <code class="literal">mapreduce.job.user.classpath.first</code> to
        <code class="literal">true</code>. Note <a class="calibre" id="calibre_link-2567"></a>that by setting these options you change the class
        loading for Hadoop framework dependencies (but only in your job),
        which could potentially cause the job submission or task to fail, so
        use these options with <a class="calibre" id="calibre_link-2251"></a>caution.</p></div></div><div class="book" title="Launching a Job"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4174">Launching a Job</h3></div></div></div><p class="calibre2">To launch <a class="calibre" id="calibre_link-2246"></a>the job, we need to run the driver, specifying the cluster
      that we want to run the job on with the <code class="literal">-conf</code> option (we equally could have used the
      <code class="literal">-fs</code> and <code class="literal">-jt</code> options):</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">unset HADOOP_CLASSPATH</code></strong>
<strong class="userinput"><code class="calibre9">% hadoop jar hadoop-examples.jar v2.MaxTemperatureDriver \
  -conf conf/hadoop-cluster.xml input/ncdc/all max-temp</code></strong></pre><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">We unset the <code class="literal">HADOOP_CLASSPATH</code>
        environment variable <a class="calibre" id="calibre_link-1866"></a>because we don’t have any third-party dependencies for
        this job. If it were left set to <code class="literal">target/classes/</code> (from earlier in the
        chapter), Hadoop wouldn’t be able to find the job JAR; it would load
        the <code class="literal">MaxTemperatureDriver</code> class from
        <em class="calibre10">target/classes</em> rather than the
        JAR, and the job would fail.</p></div><p class="calibre2">The <code class="literal">waitForCompletion()</code> method on
      <code class="literal">Job</code> launches the job and polls for
      progress, writing a line summarizing the map and reduce’s progress
      whenever either changes. Here’s the output (some lines have been removed
      for clarity):</p><pre class="screen1">14/09/12 06:38:11 INFO input.FileInputFormat: Total input paths to process : 101
14/09/12 06:38:11 INFO impl.YarnClientImpl: Submitted application 
application_1410450250506_0003
14/09/12 06:38:12 INFO mapreduce.Job: Running job: job_1410450250506_0003
14/09/12 06:38:26 INFO mapreduce.Job:  map 0% reduce 0%
...
14/09/12 06:45:24 INFO mapreduce.Job:  map 100% reduce 100%
14/09/12 06:45:24 INFO mapreduce.Job: Job job_1410450250506_0003 completed 
successfully
14/09/12 06:45:24 INFO mapreduce.Job: Counters: 49
    File System Counters
        FILE: Number of bytes read=93995
        FILE: Number of bytes written=10273563
        FILE: Number of read operations=0
        FILE: Number of large read operations=0
        FILE: Number of write operations=0
        HDFS: Number of bytes read=33485855415
        HDFS: Number of bytes written=904
        HDFS: Number of read operations=327
        HDFS: Number of large read operations=0
        HDFS: Number of write operations=16
    Job Counters 
        Launched map tasks=101
        Launched reduce tasks=8
        Data-local map tasks=101
        Total time spent by all maps in occupied slots (ms)=5954495
        Total time spent by all reduces in occupied slots (ms)=74934
        Total time spent by all map tasks (ms)=5954495
        Total time spent by all reduce tasks (ms)=74934
        Total vcore-seconds taken by all map tasks=5954495
        Total vcore-seconds taken by all reduce tasks=74934
        Total megabyte-seconds taken by all map tasks=6097402880
        Total megabyte-seconds taken by all reduce tasks=76732416
    Map-Reduce Framework
        Map input records=1209901509
        Map output records=1143764653
        Map output bytes=10293881877
        Map output materialized bytes=14193
        Input split bytes=14140
        Combine input records=1143764772
        Combine output records=234
        Reduce input groups=100
        Reduce shuffle bytes=14193
        Reduce input records=115
        Reduce output records=100
        Spilled Records=379
        Shuffled Maps =808
        Failed Shuffles=0
        Merged Map outputs=808
        GC time elapsed (ms)=101080
        CPU time spent (ms)=5113180
        Physical memory (bytes) snapshot=60509106176
        Virtual memory (bytes) snapshot=167657209856
        Total committed heap usage (bytes)=68220878848
    Shuffle Errors
        BAD_ID=0
        CONNECTION=0
        IO_ERROR=0
        WRONG_LENGTH=0
        WRONG_MAP=0
        WRONG_REDUCE=0
    File Input Format Counters 
        Bytes Read=33485841275
    File Output Format Counters 
        Bytes Written=90</pre><p class="calibre2">The output includes more useful information. Before the job
      starts, its ID is printed; this is needed whenever you want to refer to
      the job—in logfiles, for example—or when interrogating it via the
      <code class="literal">mapred job</code> command. When the job is
      complete, its statistics (known as counters) are printed out. These are
      very useful for confirming that the job did what you expected. For
      example, for this job, we can see that 1.2 billion records were analyzed
      (“Map input records”), read from around 34 GB of compressed files on
      HDFS (“HDFS: Number of bytes read”). The input was broken into 101
      gzipped files of reasonable size, so there was no problem with not being
      able to split them.</p><p class="calibre2">You can find out more about what the counters mean <a class="calibre" id="calibre_link-2247"></a>in <a class="ulink" href="#calibre_link-684" title="Built-in Counters">Built-in Counters</a>.</p><div class="sidebar"><a id="calibre_link-513" class="calibre"></a><div class="sidebar-title">Job, Task, and Task Attempt IDs</div><p class="calibre2">In Hadoop 2, MapReduce job IDs are <a class="calibre" id="calibre_link-2217"></a>generated from YARN application IDs that are created by
        the YARN resource manager. The format of an application ID<a class="calibre" id="calibre_link-904"></a> is composed of the time that the resource manager (not
        the application) started and an incrementing counter maintained by the
        resource manager to uniquely identify the application to that instance
        of the resource manager. So the application with this ID:</p><a id="calibre_link-4175" class="calibre"></a><pre class="screen2">application_1410450250506_0003</pre><p class="calibre2">is the third (<code class="literal">0003</code>;
        application IDs are 1-based) application run by the resource manager,
        which started at the time represented by the timestamp <code class="literal">1410450250506</code>. The counter is formatted with
        leading zeros to make IDs sort nicely—in directory listings, for
        example. However, when the counter reaches <code class="literal">10000</code>,
        it is <span class="calibre">not</span> reset, resulting in
        longer application IDs (which don’t sort so well).</p><p class="calibre2">The corresponding job ID is created simply by replacing the
        <code class="literal">application</code> prefix of an
        application ID with a <code class="literal">job</code>
        prefix:</p><pre class="screen2">job_1410450250506_0003</pre><p class="calibre2">Tasks belong to a job, and <a class="calibre" id="calibre_link-3625"></a>their IDs are formed by replacing the <code class="literal">job</code> prefix of a job ID with a <code class="literal">task</code> prefix and adding a suffix to identify
        the task within the job. For example:</p><a id="calibre_link-4176" class="calibre"></a><pre class="screen2">task_1410450250506_0003_m_000003</pre><p class="calibre2">is the fourth (<code class="literal">000003</code>; task
        IDs are 0-based) map (<code class="literal">m</code>) task of
        the job with ID <code class="literal">job_1410450250506_0003</code>. The task IDs are
        created for a job when it is initialized, so they do not necessarily
        dictate the order in which the tasks will be executed.</p><p class="calibre2">Tasks may be executed more than once, due to failure (see <a class="ulink" href="#calibre_link-593" title="Task Failure">Task Failure</a>) or speculative execution (see <a class="ulink" href="#calibre_link-497" title="Speculative Execution">Speculative Execution</a>), so to identify different instances
        of a task execution, <a class="calibre" id="calibre_link-3620"></a>task attempts are given unique IDs. For example:</p><a id="calibre_link-4177" class="calibre"></a><pre class="screen2">attempt_1410450250506_0003_m_000003_0</pre><p class="calibre2">is the first (<code class="literal">0</code>; attempt IDs
        are 0-based) attempt at running task <code class="literal">task_1410450250506_0003_m_000003</code>. Task
        attempts are allocated during the job run as needed, so their ordering
        represents the order in which they were created to run.</p></div></div><div class="book" title="The MapReduce Web UI"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4178">The MapReduce Web UI</h3></div></div></div><p class="calibre2">Hadoop comes with a web <a class="calibre" id="calibre_link-2268"></a>UI for viewing information about your jobs. <br>It is useful
      for following a job’s progress while it is running, as well as finding
      job statistics and logs after the job has completed. <br>You can find the UI
      at
      <span class="calibre"><em class="calibre10">http://<em class="replaceable"><code class="replaceable">resource-manager-host</code></em>:8088/</em></span>.</p><div class="book" title="The resource manager page"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4179">The resource manager page</h4></div></div></div><p class="calibre2">A screenshot of the <a class="calibre" id="calibre_link-3217"></a>home page is shown in <a class="ulink" href="#calibre_link-685" title="Figure&nbsp;6-1.&nbsp;Screenshot of the resource manager page">Figure&nbsp;6-1</a>. The “Cluster Metrics” section gives
        a summary of the cluster. This includes the number of applications
        currently running on the cluster (and in various other states), the
        number of resources available on the cluster (“Memory Total”), and
        information about node managers.</p><div class="book"><div class="figure"><a id="calibre_link-685" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4180" class="calibre"></a><img alt="Screenshot of the resource manager page" src="images/000076.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;6-1.&nbsp;Screenshot of the resource manager page</div></div></div><p class="calibre2">The main table shows all the applications that have run or are
        currently running on the cluster. There is a search box that is useful
        for filtering the applications to find the ones you are interested in.
        The main view can show up to 100 entries per page, and the resource
        manager will keep up to 10,000 completed applications in memory at a
        time (set <a class="calibre" id="calibre_link-3888"></a>by <code class="literal">yarn.resourcemanager.max-completed-applications</code>),
        before they are only available from the job history page. Note also
        that the job history is persistent, so you can find jobs there from
        previous runs of the resource manager, too.</p><div class="sidebar"><a id="calibre_link-693" class="calibre"></a><div class="sidebar-title">Job History</div><p class="calibre2"><em class="calibre10">Job history</em> refers <a class="calibre" id="calibre_link-2215"></a>to the events and configuration for a completed
          MapReduce job. It is retained regardless of whether the job was
          successful, in an attempt to provide useful information for the user
          running a job.</p><p class="calibre2">Job history files are stored in HDFS by the MapReduce
          application master, in a directory set by the <code class="literal">mapreduce.jobhistory.done-dir</code>
          property. Job history files are kept for
          one week before being deleted by the system.</p><p class="calibre2">The history log includes job, task, and attempt events, all of
          which are stored in a file in JSON format.
          The history for a particular job may be viewed through the web UI
          for the job history server (which is linked to from the resource
          manager page) or via the command line using <code class="literal">mapred job -history</code> (which you point at
          the job history file).</p></div></div><div class="book" title="The MapReduce job page"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4181">The MapReduce job page</h4></div></div></div><p class="calibre2">Clicking on the <a class="calibre" id="calibre_link-2224"></a>link for the “Tracking UI” takes us to the application
        master’s web UI <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(or to the history page if the application has
        completed). <br>In the case of MapReduce, this takes us to the job page,
        illustrated in <a class="ulink" href="#calibre_link-686" title="Figure&nbsp;6-2.&nbsp;Screenshot of the job page">Figure&nbsp;6-2</a>.</p><div class="figure"><a id="calibre_link-686" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4182" class="calibre"></a><img alt="Screenshot of the job page" src="images/000087.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;6-2.&nbsp;Screenshot of the job page</div></div><p class="calibre2">While the job is running, you can monitor its progress on this
        page. The table at the bottom shows the map progress and the reduce
        progress. “Total” shows the total number of map and reduce tasks for
        this job (a row for each). The other columns then show the state of
        these tasks: “Pending” (waiting to run), “Running,” or “Complete”
        (successfully run).</p><p class="calibre2">The lower part of the table shows the total number of failed and
        killed task attempts for the map or reduce tasks. Task attempts may be
        marked as killed if they are speculative execution duplicates, if the
        node they are running on dies, or if they are killed by a user. See
        <a class="ulink" href="#calibre_link-593" title="Task Failure">Task Failure</a> for background on task failure.</p><p class="calibre2">There also are a number of useful links in the navigation. For
        example, the “Configuration” link is to the consolidated configuration
        file for the job, containing all the properties and their values that
        were in effect during the job run. If you are unsure of what a
        particular property was set to, you can click through to inspect the
        <a class="calibre" id="calibre_link-2269"></a>file.</p></div></div><div class="book" title="Retrieving the Results"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4183">Retrieving the Results</h3></div></div></div><p class="calibre2">Once the job is finished, <a class="calibre" id="calibre_link-2253"></a>there are various ways to retrieve the results. Each
      reducer produces one output file, so there are 30 part files named
      <em class="calibre10">part-r-00000</em> to <em class="calibre10">part-r-00029</em> in the <em class="calibre10">max-temp</em> directory.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">As their names suggest, a good way to think of these “part”
        files is as parts of the <em class="calibre10">max-temp</em>
        “file.”</p><p class="calibre2">If the output is large (which it isn’t in this case), it is
        important to have multiple parts so that more than one reducer can
        work in parallel. Usually, if a file is in this partitioned form, it
        can still be used easily enough—as the input to another MapReduce job,
        for example. In some cases, you can exploit the structure of multiple
        partitions to do a map-side join, for example (see <a class="ulink" href="#calibre_link-687" title="Map-Side Joins">Map-Side Joins</a>).</p></div><p class="calibre2">This job produces a very small amount of output, so it is
      convenient to copy it from HDFS to our development machine. The <code class="literal">-getmerge</code> option to <a class="calibre" id="calibre_link-1846"></a>the <code class="literal">hadoop fs</code> command
      is useful here, as it gets all the files in the directory specified in
      the source pattern and merges them into a single file on the local
      filesystem:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -getmerge max-temp max-temp-local</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">sort max-temp-local | tail</code></strong>
1991       607
1992       605
1993       567
1994       568
1995       567
1996       561
1997       565
1998       568
1999       568
2000       558</pre><p class="calibre2">We sorted the output, as the reduce output partitions are
      unordered (owing to the hash partition function). Doing a bit of
      postprocessing of data from MapReduce is very common, as is feeding it
      into analysis tools such as R, a spreadsheet, or even a relational
      database.</p><p class="calibre2">Another way of retrieving the output if it is small is to use the
      <code class="literal">-cat</code> option to print the output files
      to the console:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -cat max-temp/*</code></strong></pre><p class="calibre2">On closer inspection, we see that some of the results don’t look
      plausible. For instance, the maximum temperature for 1951 (not shown
      here) is 590°C! How do we find out what’s causing this? Is it corrupt
      input data or a bug in the program?</p></div><div class="book" title="Debugging a Job"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4184">Debugging a Job</h3></div></div></div><p class="calibre2">The time-honored <a class="calibre" id="calibre_link-2238"></a><a class="calibre" id="calibre_link-1401"></a>way of debugging programs is via print statements, and
      this is certainly possible in Hadoop. However, there are complications
      to consider: with programs running on tens, hundreds, or thousands of
      nodes, how do we find and examine the output of the debug statements,
      which may be scattered across these nodes? For this particular case,
      where we are looking for (what we think is) an unusual case, we can use
      a debug statement to log to standard error, in conjunction with updating
      the task’s status message to prompt us to look in the error log. The web
      UI makes this easy, as we pass:[will see].</p><p class="calibre2">We also create a custom counter to count the total number of
      records with implausible temperatures in the whole dataset. This gives
      us valuable information about how to deal with the condition. If it
      turns out to be a common occurrence, we might need to learn more about
      the condition and how to extract the temperature in these cases, rather
      than simply dropping the records. In fact, when trying to debug a job,
      you should always ask yourself if you can use a counter to get the
      information you need to find out what’s happening. Even if you need to
      use logging or a status message, it may be useful to use a counter to
      gauge the extent of the problem. (There is more on counters in <a class="ulink" href="#calibre_link-12" title="Counters">Counters</a>.)</p><p class="calibre2">If the amount of log data you produce in the course of debugging
      is large, you have a couple of options. One is to write the information
      to the map’s output, rather than to standard error, for analysis and
      aggregation by the reduce task. This approach usually necessitates
      structural changes to your program, so start with the other technique
      first. The alternative is to write a program (in MapReduce, of course)
      to analyze the logs produced by your job.</p><p class="calibre2">We add our debugging to the mapper (version 3), as opposed to the
      reducer, as we want to find out what the source data causing the
      anomalous output looks like:</p><a id="calibre_link-4185" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">class</code> <code class="nc">MaxTemperatureMapper</code>
    <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">&gt;</code> <code class="o">{</code>

  <span class="calibre24"><strong class="calibre9"><code class="kc">enum</code> <code class="n1">Temperature</code> <code class="o1">{</code>
    <code class="n1">OVER_100</code>
  <code class="o1">}</code></strong></span>
  
  <code class="k">private</code> <code class="n">NcdcRecordParser</code> <code class="n">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="n">NcdcRecordParser</code><code class="o">();</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">LongWritable</code> <code class="n">key</code><code class="o">,</code> <code class="n">Text</code> <code class="n">value</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    
    <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">value</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">isValidTemperature</code><code class="o">())</code> <code class="o">{</code>
      <code class="kt">int</code> <code class="n">airTemperature</code> <code class="o">=</code> <code class="n">parser</code><code class="o">.</code><code class="na">getAirTemperature</code><code class="o">();</code>
      <span class="calibre24"><strong class="calibre9"><code class="kc">if</code> <code class="o1">(</code><code class="n1">airTemperature</code> <code class="o1">&gt;</code> <code class="mi1">1000</code><code class="o1">)</code> <code class="o1">{</code>
        <code class="n1">System</code><code class="o1">.</code><code class="na1">err</code><code class="o1">.</code><code class="na1">println</code><code class="o1">(</code><code class="s">"Temperature over 100 degrees for input: "</code> <code class="o1">+</code> <code class="n1">value</code><code class="o1">);</code>
        <code class="n1">context</code><code class="o1">.</code><code class="na1">setStatus</code><code class="o1">(</code><code class="s">"Detected possibly corrupt record: see logs."</code><code class="o1">);</code>
        <code class="n1">context</code><code class="o1">.</code><code class="na1">getCounter</code><code class="o1">(</code><code class="n1">Temperature</code><code class="o1">.</code><code class="na1">OVER_100</code><code class="o1">).</code><code class="na1">increment</code><code class="o1">(</code><code class="mi1">1</code><code class="o1">);</code>
      <code class="o1">}</code></strong></span>
      <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">getYear</code><code class="o">()),</code> <code class="k">new</code> <code class="n">IntWritable</code><code class="o">(</code><code class="n">airTemperature</code><code class="o">));</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre><p class="calibre2">If the temperature is over 100°C (represented by 1000, because
      temperatures are in tenths of a degree), we print a line to standard
      error with the suspect line, as well as updating the map’s status
      message using the <code class="literal">setStatus()</code> method on
      <code class="literal">Context</code>, directing us to look in the
      log. We also increment a counter, which in Java is represented by a
      field of an enum type. In this program, we have defined a single field,
      <code class="literal">OVER_100</code>, as a way to count the
      number of records with a temperature of over 100°C.</p><p class="calibre2">With this modification, we recompile the code, re-create the JAR
      file, then rerun the job and, while it’s running, go to the tasks
      <a class="calibre" id="calibre_link-1402"></a>page.</p><div class="book" title="The tasks and task attempts pages"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4186">The tasks and task attempts pages</h4></div></div></div><p class="calibre2">The job page has a <a class="calibre" id="calibre_link-1406"></a><a class="calibre" id="calibre_link-3643"></a><a class="calibre" id="calibre_link-3622"></a><a class="calibre" id="calibre_link-1405"></a>number of links for viewing the tasks in a job in more
        detail. <br>For example, clicking on the “Map” link brings us to a page
        that lists information for all of the map tasks. <br>The screenshot in
        <a class="ulink" href="#calibre_link-688" title="Figure&nbsp;6-3.&nbsp;Screenshot of the tasks page">Figure&nbsp;6-3</a> shows this page for the job run with
        our debugging statements in the “Status” column for the task.</p><div class="book"><div class="figure"><a id="calibre_link-688" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4187" class="calibre"></a><img alt="Screenshot of the tasks page" src="images/000004.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;6-3.&nbsp;Screenshot of the tasks page</div></div></div><p class="calibre2">Clicking on the task link takes us to the task attempts page,
        which shows each task attempt for the task. Each task attempt page has
        links to the logfiles and counters. If we follow one of the links to
        the logfiles for the successful task attempt, we can find the suspect
        input record that we logged (the line is wrapped and truncated to fit
        on the page):</p><pre class="screen1">Temperature over 100 degrees for input:
0335999999433181957042302005+37950+139117SAO  +0004RJSN V02011359003150070356999
9994332<span class="calibre24"><strong class="calibre9">01957</strong></span>010100005+35317+139650SAO +000899999V02002359002650076249N0040005<span class="calibre">...</span></pre><p class="calibre2">This record seems to be in a different format from the others.
        For one thing, there are spaces in the line, which are not described
        in the specification.</p><p class="calibre2">When the job has finished, we can look at the value of the
        counter we defined to see how many records over 100°C there are in the
        whole dataset. Counters are accessible via the web UI or the command
        line:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">mapred job -counter job_1410450250506_0006 \</code></strong>
<strong class="userinput"><code class="calibre9">  'v3.MaxTemperatureMapper$Temperature' OVER_100</code></strong>
3</pre><p class="calibre2">The <code class="literal">-counter</code> option takes the
        job ID, counter group name (which is the fully qualified classname
        here), and counter name (the enum name). There are only three
        malformed records in the entire dataset of over a billion records.
        Throwing out bad records is standard for many big data problems,
        although we need to be careful in this case because we are looking for
        an extreme value—the maximum temperature rather than an aggregate
        measure. Still, throwing away three records is probably not going to
        change the result.</p></div><div class="book" title="Handling malformed data"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4188">Handling malformed data</h4></div></div></div><p class="calibre2">Capturing input data that <a class="calibre" id="calibre_link-1404"></a>causes a problem is valuable, as we can use it in a test
        to check that the mapper does the right thing. In this MRUnit test, we
        check that the counter is updated for the malformed input:</p><a id="calibre_link-4189" class="calibre"></a><pre class="screen1">  <code class="nd">@Test</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">parsesMalformedTemperature</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code>
      <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="n">Text</code> <code class="n">value</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="sb">"0335999999433181957042302005+37950+139117SAO  +0004"</code> <code class="o">+</code>
                                  <code class="c2">// Year ^^^^</code>
        <code class="sb">"RJSN V02011359003150070356999999433201957010100005+353"</code><code class="o">);</code>
                              <code class="c2">// Temperature ^^^^^</code>
    <code class="n">Counters</code> <code class="n">counters</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Counters</code><code class="o">();</code>
    <code class="k">new</code> <code class="n">MapDriver</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">&gt;()</code>
      <code class="o">.</code><code class="na">withMapper</code><code class="o">(</code><code class="k">new</code> <code class="n">MaxTemperatureMapper</code><code class="o">())</code>
      <code class="o">.</code><code class="na">withInput</code><code class="o">(</code><code class="k">new</code> <code class="n">LongWritable</code><code class="o">(</code><code class="mi">0</code><code class="o">),</code> <code class="n">value</code><code class="o">)</code>
      <code class="o">.</code><code class="na">withCounters</code><code class="o">(</code><code class="n">counters</code><code class="o">)</code>
      <code class="o">.</code><code class="na">runTest</code><code class="o">();</code>
    <code class="n">Counter</code> <code class="n">c</code> <code class="o">=</code> <code class="n">counters</code><code class="o">.</code><code class="na">findCounter</code><code class="o">(</code><code class="n">MaxTemperatureMapper</code><code class="o">.</code><code class="na">Temperature</code><code class="o">.</code><code class="na">MALFORMED</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">c</code><code class="o">.</code><code class="na">getValue</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="mi">1L</code><code class="o">));</code>
  <code class="o">}</code></pre><p class="calibre2">The record that was causing the problem is of a different format
        than the other lines we’ve seen. <a class="ulink" href="#calibre_link-689" title="Example&nbsp;6-12.&nbsp;Mapper for the maximum temperature example">Example&nbsp;6-12</a> shows a modified program (version
        4) using a parser that ignores each line with a temperature field that
        does not have a leading sign (plus or minus). We’ve also introduced a
        counter to measure the number of records that we are ignoring for this
        <a class="calibre" id="calibre_link-2239"></a>reason.</p><div class="example"><a id="calibre_link-689" class="calibre"></a><div class="example-title">Example&nbsp;6-12.&nbsp;Mapper for the maximum temperature example</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">MaxTemperatureMapper</code>
    <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">&gt;</code> <code class="o">{</code>
  
  <code class="k">enum</code> <code class="n">Temperature</code> <code class="o">{</code>
    <code class="n">MALFORMED</code>
  <code class="o">}</code>

  <code class="k">private</code> <code class="n">NcdcRecordParser</code> <code class="n">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="n">NcdcRecordParser</code><code class="o">();</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">LongWritable</code> <code class="n">key</code><code class="o">,</code> <code class="n">Text</code> <code class="n">value</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    
    <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">value</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">isValidTemperature</code><code class="o">())</code> <code class="o">{</code>
      <code class="kt">int</code> <code class="n">airTemperature</code> <code class="o">=</code> <code class="n">parser</code><code class="o">.</code><code class="na">getAirTemperature</code><code class="o">();</code>
      <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">getYear</code><code class="o">()),</code> <code class="k">new</code> <code class="n">IntWritable</code><code class="o">(</code><code class="n">airTemperature</code><code class="o">));</code>
    <code class="o">}</code> <code class="k">else</code> <code class="k">if</code> <code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">isMalformedTemperature</code><code class="o">())</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="sb">"Ignoring possibly corrupt input: "</code> <code class="o">+</code> <code class="n">value</code><code class="o">);</code>
      <code class="n">context</code><code class="o">.</code><code class="na">getCounter</code><code class="o">(</code><code class="n">Temperature</code><code class="o">.</code><code class="na">MALFORMED</code><code class="o">).</code><code class="na">increment</code><code class="o">(</code><code class="mi">1</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div></div></div><div class="book" title="Hadoop Logs"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-34">Hadoop Logs</h3></div></div></div><p class="calibre2">Hadoop produces logs <a class="calibre" id="calibre_link-2248"></a><a class="calibre" id="calibre_link-2362"></a>in various places, and for various audiences. These are
      summarized in <a class="ulink" href="#calibre_link-690" title="Table&nbsp;6-2.&nbsp;Types of Hadoop logs">Table&nbsp;6-2</a>.</p><div class="table"><a id="calibre_link-690" class="calibre"></a><div class="table-title">Table&nbsp;6-2.&nbsp;Types of Hadoop logs</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Logs</td><td class="calibre20">Primary audience</td><td class="calibre20">Description</td><td class="calibre21">Further information</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">System daemon
              logs</td><td class="calibre23">Administrators</td><td class="calibre23">Each <a class="calibre" id="calibre_link-3584"></a><a class="calibre" id="calibre_link-1325"></a>Hadoop daemon produces a logfile (using log4j) and
              another file that combines standard out and error. <a class="calibre" id="calibre_link-1875"></a>Written in the directory defined by the <code class="uri">HADOOP_LOG_DIR</code> environment variable.</td><td class="calibre25"><a class="ulink" href="#calibre_link-11" title="System logfiles">System logfiles</a> and <a class="ulink" href="#calibre_link-691" title="Logging">Logging</a></td></tr><tr class="calibre26"><td class="calibre23">HDFS audit
              logs</td><td class="calibre23">Administrators</td><td class="calibre23">A log <a class="calibre" id="calibre_link-1924"></a><a class="calibre" id="calibre_link-924"></a>of all HDFS requests, turned off by default.
              Written to the namenode’s log, although this is
              configurable.</td><td class="calibre25"><a class="ulink" href="#calibre_link-692" title="Audit Logging">Audit Logging</a></td></tr><tr class="calibre19"><td class="calibre23">MapReduce job history
              logs</td><td class="calibre23">Users</td><td class="calibre23">A log of the <a class="calibre" id="calibre_link-2216"></a>events (such as task completion) that occur in the
              course of running a job. Saved centrally in HDFS.</td><td class="calibre25"><a class="ulink" href="#calibre_link-693" title="Job History">Job History</a></td></tr><tr class="calibre26"><td class="calibre27">MapReduce task
              logs</td><td class="calibre27">Users</td><td class="calibre27">Each task <a class="calibre" id="calibre_link-3627"></a>child process produces a logfile using log4j
              (called <em class="calibre10">syslog</em>), a file for
              data sent to standard out (<em class="calibre10">stdout</em>), and a file for standard
              error (<em class="calibre10">stderr</em>). Written in
              the <em class="calibre10">userlogs</em> subdirectory
              of the directory <a class="calibre" id="calibre_link-3908"></a>defined by the <code class="uri">YARN_LOG_DIR</code> environment
              variable.</td><td class="calibre28">This section</td></tr></tbody></table></div></div><p class="calibre2">YARN has a service for <em class="calibre10">log aggregation</em>
      <a class="calibre" id="calibre_link-2352"></a><a class="calibre" id="calibre_link-3837"></a>that takes the task logs for completed applications and
      moves them to HDFS, where they are stored in a container file for
      archival purposes. If this service is enabled (by <a class="calibre" id="calibre_link-3856"></a>setting <code class="literal">yarn.log-aggregation-enable</code> to <code class="literal">true</code> on the cluster), then task logs can be
      viewed by clicking on the <em class="calibre10">logs</em> link
      in the task attempt web UI, or by using the <code class="literal">mapred job -logs</code> command.</p><p class="calibre2">By default, log aggregation is not enabled. In this case, task
      logs can be retrieved by visiting the node manager’s web UI at
      <span class="calibre"><em class="calibre10">http://<em class="replaceable"><code class="replaceable">node-manager-host</code></em>:8042/logs/userlogs</em></span>.</p><p class="calibre2">It is straightforward to write to these logfiles. Anything written
      to standard output or standard error is directed to the relevant
      logfile. (Of course, in Streaming, standard output is used for the map
      or reduce output, so it will not show up in the standard output
      log.)</p><p class="calibre2">In Java, you can <a class="calibre" id="calibre_link-2176"></a><a class="calibre" id="calibre_link-3570"></a>write to the task’s <em class="calibre10">syslog</em> file if you wish by using the
      <a class="calibre" id="calibre_link-888"></a>Apache Commons Logging API (or indeed any logging API that
      can write to log4j). This is shown in <a class="ulink" href="#calibre_link-694" title="Example&nbsp;6-13.&nbsp;An identity mapper that writes to standard output and also uses the Apache Commons Logging API">Example&nbsp;6-13</a>.</p><div class="example"><a id="calibre_link-694" class="calibre"></a><div class="example-title">Example&nbsp;6-13.&nbsp;An identity mapper that writes to standard output and also uses
        the Apache Commons Logging API</div><div class="book"><pre class="screen"><code class="k">import</code> <code class="nn">org.apache.commons.logging.Log</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.commons.logging.LogFactory</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.mapreduce.Mapper</code><code class="o">;</code>

<code class="k">public</code> <code class="k">class</code> <code class="nc">LoggingIdentityMapper</code><code class="o">&lt;</code><code class="n">KEYIN</code><code class="o">,</code> <code class="n">VALUEIN</code><code class="o">,</code> <code class="n">KEYOUT</code><code class="o">,</code> <code class="n">VALUEOUT</code><code class="o">&gt;</code>
    <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">KEYIN</code><code class="o">,</code> <code class="n">VALUEIN</code><code class="o">,</code> <code class="n">KEYOUT</code><code class="o">,</code> <code class="n">VALUEOUT</code><code class="o">&gt;</code> <code class="o">{</code>
  
  <code class="k">private</code> <code class="k">static</code> <code class="k">final</code> <code class="n">Log</code> <code class="n">LOG</code> <code class="o">=</code> <code class="n">LogFactory</code><code class="o">.</code><code class="na">getLog</code><code class="o">(</code><code class="n">LoggingIdentityMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
  
  <code class="nd">@Override</code>
  <code class="nd">@SuppressWarnings</code><code class="o">(</code><code class="sb">"unchecked"</code><code class="o">)</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">KEYIN</code> <code class="n">key</code><code class="o">,</code> <code class="n">VALUEIN</code> <code class="n">value</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="c2">// Log to stdout file</code>
    <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="sb">"Map key: "</code> <code class="o">+</code> <code class="n">key</code><code class="o">);</code>
    
    <code class="c2">// Log to syslog file</code>
    <code class="n">LOG</code><code class="o">.</code><code class="na">info</code><code class="o">(</code><code class="sb">"Map key: "</code> <code class="o">+</code> <code class="n">key</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">LOG</code><code class="o">.</code><code class="na">isDebugEnabled</code><code class="o">())</code> <code class="o">{</code>
      <code class="n">LOG</code><code class="o">.</code><code class="na">debug</code><code class="o">(</code><code class="sb">"Map value: "</code> <code class="o">+</code> <code class="n">value</code><code class="o">);</code>
    <code class="o">}</code>
    <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">((</code><code class="n">KEYOUT</code><code class="o">)</code> <code class="n">key</code><code class="o">,</code> <code class="o">(</code><code class="n">VALUEOUT</code><code class="o">)</code> <code class="n">value</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The default log level is <code class="literal">INFO</code>,
      so <code class="literal">DEBUG-</code>level messages do not appear
      in the <em class="calibre10">syslog</em> task logfile.
      However, sometimes you want to see these messages. To enable this,
      <a class="calibre" id="calibre_link-2580"></a>set <code class="literal">mapreduce.map.log.level</code> or <code class="literal">mapreduce.reduce.log.level</code>, as <a class="calibre" id="calibre_link-2607"></a>appropriate. For example, in this case, we could set it
      for the mapper to see the map values in the log as follows:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar hadoop-examples.jar LoggingDriver -conf conf/hadoop-cluster.xml \</code></strong>
<strong class="userinput"><code class="calibre9">  -D mapreduce.map.log.level=DEBUG input/ncdc/sample.txt logging-out</code></strong></pre><p class="calibre2">There are some controls for managing the retention and size of
      task logs. By default, logs are deleted after a minimum of three hours
      (you can set this <a class="calibre" id="calibre_link-3869"></a>using the <code class="literal">yarn.nodemanager.log.retain-seconds</code> property,
      although this is ignored if log aggregation is enabled). You can also
      set a cap on the maximum size of each logfile using the <code class="literal">mapreduce.task.userlog.limit.kb</code> property,
      <a class="calibre" id="calibre_link-2642"></a>which is 0 by default, meaning there is no <a class="calibre" id="calibre_link-2249"></a><a class="calibre" id="calibre_link-2363"></a>cap.</p><div class="note" title="Tip"><h3 class="title3">Tip</h3><p class="calibre2">Sometimes you may need to debug a problem that you suspect is
        occurring in the JVM running a Hadoop command, rather than on the
        cluster. You can send <code class="literal">DEBUG-</code>level
        logs to the console by using an invocation like this:</p><pre class="screen2"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">HADOOP_ROOT_LOGGER=DEBUG,console hadoop fs -text /foo/bar</code></strong></pre></div></div><div class="book" title="Remote Debugging"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4190">Remote Debugging</h3></div></div></div><p class="calibre2">When a task fails <a class="calibre" id="calibre_link-3211"></a><a class="calibre" id="calibre_link-1407"></a><a class="calibre" id="calibre_link-2240"></a>and there is not enough information logged to diagnose the
      error, you may want to resort to running a debugger for that task. This
      is hard to arrange when running the job on a cluster, as you don’t know
      which node is going to process which part of the input, so you can’t set
      up your debugger ahead of the failure. However, there are a few other
      options available:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Reproduce the failure locally</span></dt><dd class="calibre8"><p class="calibre2">Often the failing task fails consistently on a particular
            input. You can try to reproduce the problem locally by downloading
            the file that the task is failing on and running the job locally,
            possibly using a debugger such as <a class="calibre" id="calibre_link-2295"></a><a class="calibre" id="calibre_link-2191"></a>Java’s VisualVM.</p></dd><dt class="calibre7"><span class="term">Use JVM debugging options</span></dt><dd class="calibre8"><p class="calibre2">A common cause of failure is a Java out of memory error in
            the task JVM. You can set <code class="literal">mapred.child.java.opts</code> to <a class="calibre" id="calibre_link-2421"></a>include <code class="literal">-XX:-HeapDumpOnOutOfMemoryError
            -XX:HeapDumpPath=<em class="replaceable"><code class="replaceable">/path/to/dumps</code></em></code>.
            This setting produces a heap dump that can be examined afterward
            with tools such as <em class="calibre10">jhat</em> or the
            Eclipse Memory Analyzer. Note that the JVM options should be added
            to the existing memory settings specified by <code class="literal">mapred.child.java.opts</code>. These are
            explained in more detail in <a class="ulink" href="#calibre_link-35" title="Memory settings in YARN and MapReduce">Memory settings in YARN and MapReduce</a>.</p></dd><dt class="calibre7"><span class="term">Use task profiling</span></dt><dd class="calibre8"><p class="calibre2">Java profilers give a lot of insight into the JVM, and
            Hadoop provides a mechanism to profile a subset of the tasks in a
            job. See <a class="ulink" href="#calibre_link-695" title="Profiling Tasks">Profiling Tasks</a>.</p></dd></dl></div><p class="calibre2">In some cases, it’s useful to keep the intermediate files for a
      failed task attempt for later inspection, particularly if supplementary
      dump or profile <a class="calibre" id="calibre_link-2627"></a>files are created in the task’s working directory. You can
      set <code class="literal">mapreduce.task.files.preserve.failedtasks</code> to
      <code class="literal">true</code> to keep a failed task’s
      files.</p><p class="calibre2">You can keep the intermediate files for successful tasks, too,
      which may be handy if you want to examine a task that isn’t failing. In
      this case, <a class="calibre" id="calibre_link-2628"></a>set the property <code class="literal">mapreduce.task.files.preserve.filepattern</code> to a
      regular expression that matches the IDs of the tasks whose files you
      want to keep.</p><p class="calibre2">Another useful property for <a class="calibre" id="calibre_link-3864"></a>debugging is <code class="literal">yarn.nodemanager.delete.debug-delay-sec</code>, which
      is the number of seconds to wait to delete localized task attempt files,
      such as the script used to launch the task container JVM. If this is set
      on the cluster to a reasonably large value (e.g., <code class="literal">600</code> for 10 minutes), then you have enough time
      to look at the files before they are deleted.</p><p class="calibre2">To examine task attempt files, log into the node that the task
      failed on and look for the directory for that task attempt. It will be
      under one of the local MapReduce directories, as set by the <code class="literal">mapreduce.cluster.local.dir</code> property
      <a class="calibre" id="calibre_link-2529"></a>(covered in more detail in <a class="ulink" href="#calibre_link-696" title="Important Hadoop Daemon Properties">Important Hadoop Daemon Properties</a>). If this property is a
      comma-separated list of directories (to spread load across the physical
      disks on a machine), you may need to look in all of the directories
      before you find the directory for that particular task attempt. The task
      attempt directory is in the following <a class="calibre" id="calibre_link-2465"></a><a class="calibre" id="calibre_link-1134"></a><a class="calibre" id="calibre_link-3660"></a>location:</p><a id="calibre_link-4191" class="calibre"></a><pre class="screen1"><em class="replaceable"><code class="replaceable">mapreduce.cluster.local.dir</code></em>/usercache/<em class="replaceable"><code class="replaceable">user</code></em>/appcache/<em class="replaceable"><code class="replaceable">application-ID</code></em>/output
  /<em class="replaceable"><code class="replaceable">task-attempt-ID</code></em></pre></div></div><div class="book" title="Tuning a Job"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-57">Tuning a Job</h2></div></div></div><p class="calibre2">After a job is working, the <a class="calibre" id="calibre_link-2468"></a><a class="calibre" id="calibre_link-2266"></a><a class="calibre" id="calibre_link-3720"></a>question many developers ask is, “Can I make it run
    faster?”</p><p class="calibre2">There are a few Hadoop-specific “usual suspects” that are worth
    checking to see whether they are responsible for a performance problem.
    You should run through the checklist in <a class="ulink" href="#calibre_link-697" title="Table&nbsp;6-3.&nbsp;Tuning checklist">Table&nbsp;6-3</a>
    before you start trying to profile or optimize at the task level.</p><div class="table"><a id="calibre_link-697" class="calibre"></a><div class="table-title">Table&nbsp;6-3.&nbsp;Tuning checklist</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre36"><col class="c5"><col class="calibre40"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Area</td><td class="calibre20">Best practice</td><td class="calibre21">Further
            information</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">Number of mappers</td><td class="calibre23">How long <a class="calibre" id="calibre_link-2406"></a>are your mappers running for? If they are only
            running for a few seconds on average, you should see whether
            there’s a way to have fewer mappers and make them all run longer—a
            minute or so, as a rule of thumb. The extent to which this is
            possible depends on the input format you are using.</td><td class="calibre25"><a class="ulink" href="#calibre_link-301" title="Small files and CombineFileInputFormat">Small files and CombineFileInputFormat</a></td></tr><tr class="calibre26"><td class="calibre23">Number of
            reducers</td><td class="calibre23">Check <a class="calibre" id="calibre_link-3193"></a>that you are using more than a single reducer.
            Reduce tasks should run for five minutes or so and produce at
            least a block’s worth of data, as a rule of thumb.</td><td class="calibre25"><a class="ulink" href="#calibre_link-555" title="Choosing the Number of Reducers">Choosing the Number of Reducers</a></td></tr><tr class="calibre19"><td class="calibre23">Combiners</td><td class="calibre23">Check <a class="calibre" id="calibre_link-1183"></a>whether your job can take advantage of a combiner to
            reduce the amount of data passing through the shuffle.</td><td class="calibre25"><a class="ulink" href="#calibre_link-539" title="Combiner Functions">Combiner Functions</a></td></tr><tr class="calibre26"><td class="calibre23">Intermediate compression</td><td class="calibre23">Job execution <a class="calibre" id="calibre_link-1216"></a>time can almost always benefit from enabling map
            output compression.</td><td class="calibre25"><a class="ulink" href="#calibre_link-698" title="Compressing map output">Compressing map output</a></td></tr><tr class="calibre19"><td class="calibre23">Custom serialization</td><td class="calibre23">If you are <a class="calibre" id="calibre_link-3366"></a>using your own custom <code class="uri">Writable</code> objects or custom comparators,
            make sure you have implemented <code class="uri">RawComparator</code>.</td><td class="calibre25"><a class="ulink" href="#calibre_link-147" title="Implementing a RawComparator for speed">Implementing a RawComparator for speed</a></td></tr><tr class="calibre26"><td class="calibre27">Shuffle tweaks</td><td class="calibre27">The MapReduce shuffle exposes around a dozen tuning
            parameters for memory management, which may help you wring out the
            last bit of performance.</td><td class="calibre28"><a class="ulink" href="#calibre_link-699" title="Configuration Tuning">Configuration Tuning</a></td></tr></tbody></table></div></div><div class="book" title="Profiling Tasks"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-695">Profiling Tasks</h3></div></div></div><p class="calibre2">Like debugging, <a class="calibre" id="calibre_link-3062"></a><a class="calibre" id="calibre_link-3634"></a>profiling a job running on a distributed system such as
      MapReduce presents some challenges. Hadoop allows you to profile a
      fraction of the tasks in a job and, as each task completes, pulls down
      the profile information to your machine for later analysis with standard
      profiling tools.</p><p class="calibre2">Of course, it’s possible, and somewhat easier, to profile a job
      running in the local job runner. And provided you can run with enough
      input data to exercise the map and reduce tasks, this can be a valuable
      way of improving the performance of your mappers and reducers. There are
      a couple of caveats, however. The local job runner is a very different
      environment from a cluster, and the data flow patterns are very
      different. Optimizing the CPU performance of your code may be pointless
      if your MapReduce job is I/O-bound (as many jobs are). To be sure that
      any tuning is effective, you should compare the new execution time with
      the old one running on a real cluster. Even this is easier said than
      done, since job execution times can vary due to resource contention with
      other jobs and the decisions the scheduler makes regarding task
      placement. To get a good idea of job execution time under these
      circumstances, perform a series of runs (with and without the change)
      and check whether any improvement is statistically significant.</p><p class="calibre2">It’s unfortunately true that some problems (such as excessive
      memory use) can be reproduced only on the cluster, and in these cases
      the ability to profile in situ is indispensable.</p><div class="book" title="The HPROF profiler"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4192">The HPROF profiler</h4></div></div></div><p class="calibre2">There are a number <a class="calibre" id="calibre_link-2057"></a>of configuration properties to control profiling, which
        are also exposed via convenience methods <a class="calibre" id="calibre_link-2229"></a>on <code class="literal">JobConf</code>. Enabling
        profiling is as simple as setting the <a class="calibre" id="calibre_link-2638"></a>property <code class="literal">mapreduce.task.profile</code> to <code class="literal">true</code>:</p><pre class="screen1"><strong class="userinput"><code class="calibre9">% hadoop jar hadoop-examples.jar v4.MaxTemperatureDriver \
  -conf conf/hadoop-cluster.xml \
  -D mapreduce.task.profile=true \
  input/ncdc/all max-temp</code></strong></pre><p class="calibre2">This runs the job as normal, but adds an <code class="literal">-agentlib</code> parameter to the Java command used
        to launch the task containers on the node managers. You can control
        the precise parameter that is added by setting the <code class="literal">mapreduce.task.profile.params</code> property. The
        default uses HPROF, a profiling tool that comes with the JDK that,
        although basic, can give valuable information about a program’s CPU
        and heap usage.</p><p class="calibre2">It doesn’t usually make sense to profile all tasks in the job,
        so by default only those with IDs 0, 1, and 2 are profiled (for both
        maps and reduces). You can change this by <a class="calibre" id="calibre_link-2639"></a><a class="calibre" id="calibre_link-2640"></a>setting <code class="literal">mapreduce.task.profile.maps</code> and <code class="literal">mapreduce.task.profile.reduces</code> to specify
        the range of task IDs to profile.</p><p class="calibre2">The profile output for each task is saved with the task logs in
        the <em class="calibre10">userlogs</em> subdirectory of the
        node manager’s local log directory (alongside the <em class="calibre10">syslog</em>, <em class="calibre10">stdout</em>, and <em class="calibre10">stderr</em> files), and can be retrieved in the
        way described in <a class="ulink" href="#calibre_link-34" title="Hadoop Logs">Hadoop Logs</a>, according to whether
        log aggregation is enabled <a class="calibre" id="calibre_link-2469"></a><a class="calibre" id="calibre_link-2267"></a><a class="calibre" id="calibre_link-3721"></a><a class="calibre" id="calibre_link-3635"></a><a class="calibre" id="calibre_link-3063"></a>or not.</p></div></div></div><div class="book" title="MapReduce Workflows"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4193">MapReduce Workflows</h2></div></div></div><p class="calibre2">So far in this chapter, you <a class="calibre" id="calibre_link-2470"></a><a class="calibre" id="calibre_link-3790"></a>have seen the mechanics of writing a program using
    MapReduce. We haven’t yet considered how to turn a data processing problem
    into the MapReduce model.</p><p class="calibre2">The data processing you have seen so far in this book is to solve a
    fairly simple problem: finding the maximum recorded temperature for given
    years. When the processing gets more complex, this complexity is generally
    manifested by having more MapReduce jobs, rather than having more complex
    map and reduce functions. In other words, as a rule of thumb, think about
    adding <span class="calibre">more</span> jobs, rather than adding
    complexity <span class="calibre">to</span> jobs.</p><p class="calibre2">For more complex problems, it is worth considering a higher-level
    language than MapReduce, such as Pig, Hive, Cascading, Crunch, or Spark.
    One immediate benefit is that it frees you from having to do the
    translation into MapReduce jobs, allowing you to concentrate on the
    analysis you are performing.</p><p class="calibre2">Finally, the book <a class="ulink" href="http://mapreduce.me" target="_top"><span class="calibre">Data-Intensive Text Processing with
    MapReduce</span></a> by <a class="calibre" id="calibre_link-2324"></a><a class="calibre" id="calibre_link-1555"></a><a class="calibre" id="calibre_link-2440"></a>Jimmy Lin and Chris Dyer (Morgan &amp; Claypool Publishers,
    2010) is a great resource for learning more about MapReduce algorithm
    design and is highly recommended.</p><div class="book" title="Decomposing a Problem into MapReduce Jobs"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4194">Decomposing a Problem into MapReduce Jobs</h3></div></div></div><p class="calibre2">Let’s look at an <a class="calibre" id="calibre_link-2455"></a><a class="calibre" id="calibre_link-2241"></a><a class="calibre" id="calibre_link-3793"></a>example of a more complex problem that we want to
      translate into a MapReduce workflow.</p><p class="calibre2">Imagine that we want to find the mean maximum recorded temperature
      for every day of the year and every weather station. In concrete terms,
      to calculate the mean maximum daily temperature recorded by station
      029070-99999, say, on January 1, we take the mean of the maximum daily
      temperatures for this station for January 1, 1901; January 1, 1902; and
      so on, up to January 1, 2000.</p><p class="calibre2">How can we compute this using MapReduce? The computation
      decomposes most naturally into two stages:</p><div class="book"><ol class="orderedlist"><li class="listitem"><p class="calibre2"><span class="calibre"><em class="calibre10">Compute the maximum daily temperature for every
          station-date pair.</em></span></p><p class="calibre2">The MapReduce program in this case is a variant of the maximum
          temperature program, except that the keys in this case are a
          composite station-date pair, rather than just the year.</p></li><li class="listitem"><p class="calibre2"><span class="calibre"><em class="calibre10">Compute the mean of the maximum daily temperatures
          for every station-day-month key.</em></span></p><p class="calibre2">The mapper takes the output from the previous job
          (station-date, maximum temperature) records and projects it into
          (station-day-month, maximum temperature) records by dropping the
          year component. The reduce function then takes the mean of the
          maximum temperatures for each station-day-month key.</p></li></ol></div><p class="calibre2">The output from the first stage looks like this for the station we
      are interested in (the <em class="calibre10">mean_max_daily_temp.sh</em> script in the examples
      provides an implementation in Hadoop Streaming):</p><a id="calibre_link-4195" class="calibre"></a><pre class="screen1">029070-99999	19010101	0
029070-99999	19020101	-94
...</pre><p class="calibre2">The first two fields form the key, and the final column is the
      maximum temperature from all the readings for the given station and
      date. The second stage averages these daily maxima over years to
      yield:</p><a id="calibre_link-4196" class="calibre"></a><pre class="screen1">029070-99999	0101	-68</pre><p class="calibre2">which is interpreted as saying the mean maximum daily temperature
      on January 1 for station 029070-99999 over the century is −6.8°C.</p><p class="calibre2">It’s possible to do this computation in one MapReduce stage, but
      it takes more work on the part of the programmer.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-700" id="calibre_link-704">50</a>]</sup></p><p class="calibre2">The arguments for having more (but simpler) MapReduce stages are
      that doing so leads to more composable and more maintainable mappers and
      reducers. Some of the case studies referred to in <a class="ulink" href="#calibre_link-207" title="Part&nbsp;V.&nbsp;Case Studies">Part&nbsp;V</a> cover real-world problems that were
      solved using MapReduce, and in each case, the data processing task is
      implemented using two or more MapReduce jobs. The details in that
      chapter are invaluable for getting a better idea of how to decompose a
      processing problem into a MapReduce workflow.</p><p class="calibre2">It’s possible to make map and reduce functions even more
      composable than we have done. A mapper commonly performs input format
      parsing, projection (selecting the relevant fields), and filtering
      (removing records that are not of interest). In the mappers you have
      seen so far, we have implemented all of these functions in a single
      mapper. However, there is a case for splitting these into distinct
      mappers and chaining them into a single mapper<a class="calibre" id="calibre_link-1083"></a> using the <code class="literal">ChainMapper</code>
      library class that comes with Hadoop. Combined with a <code class="literal">ChainReducer</code>, you <a class="calibre" id="calibre_link-1085"></a>can run a chain of mappers, followed by a reducer and
      another chain of mappers, in a single MapReduce <a class="calibre" id="calibre_link-2242"></a><a class="calibre" id="calibre_link-2456"></a><a class="calibre" id="calibre_link-3794"></a>job.</p></div><div class="book" title="JobControl"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4197">JobControl</h3></div></div></div><p class="calibre2">When there is more <a class="calibre" id="calibre_link-3795"></a>than one job in a MapReduce workflow, the question arises:
      how do you manage the jobs so they are executed in order? There are
      several approaches, and the main consideration is whether you have a
      linear chain of jobs or a more <a class="calibre" id="calibre_link-1496"></a><a class="calibre" id="calibre_link-1332"></a>complex directed acyclic graph (DAG) of jobs.</p><p class="calibre2">For a linear chain, the simplest approach is to run each job one
      after another, waiting until a job completes successfully before running
      the next:</p><a id="calibre_link-4198" class="calibre"></a><pre class="screen1"><code class="n">JobClient</code><code class="o">.</code><code class="na">runJob</code><code class="o">(</code><code class="n">conf1</code><code class="o">);</code>
<code class="n">JobClient</code><code class="o">.</code><code class="na">runJob</code><code class="o">(</code><code class="n">conf2</code><code class="o">);</code></pre><p class="calibre2">If a job fails, the <code class="literal">runJob()</code> method will
      throw an <code class="literal">IOException</code>, so later jobs
      in the pipeline don’t get executed. Depending on your application, you
      might want to catch the exception and clean up any intermediate data
      that was produced by any previous jobs.</p><p class="calibre2">The approach is similar with the new MapReduce API, except you
      need to examine the Boolean return value of the <code class="literal">waitForCompletion()</code> method on <code class="literal">Job</code>: <code class="literal">true</code>
      means the job succeeded, and <code class="literal">false</code>
      means it failed.</p><p class="calibre2">For anything more complex than a linear chain, there are libraries
      that can help orchestrate your workflow (although they are also suited
      to linear chains, or <a class="calibre" id="calibre_link-2867"></a>even one-off jobs). The simplest is in the <code class="literal">org.apache.hadoop.mapreduce.jobcontrol</code>
      package: <a class="calibre" id="calibre_link-2230"></a>the <code class="literal">JobControl</code> class. (There is an equivalent
      class in the <code class="literal">org.apache.hadoop.mapred.jobcontrol</code> package,
      too.) An instance of <code class="literal">JobControl</code>
      represents a graph of jobs to be run. You add the job configurations,
      then tell the <code class="literal">JobControl</code> instance the
      dependencies between jobs. You run the <code class="literal">JobControl</code> in a thread, and it runs the jobs
      in dependency order. You can poll for progress, and when the jobs have
      finished, you can query for all the jobs’ statuses and the associated
      errors for any failures. If a job fails, <code class="literal">JobControl</code> won’t run its dependencies.</p></div><div class="book" title="Apache Oozie"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4199">Apache Oozie</h3></div></div></div><p class="calibre2">Apache Oozie is a system <a class="calibre" id="calibre_link-3791"></a><a class="calibre" id="calibre_link-891"></a>for running workflows of dependent jobs. It is composed of
      two main parts: a <em class="calibre10">workflow engine</em> that <a class="calibre" id="calibre_link-3789"></a>stores and runs workflows composed of different types of
      Hadoop jobs (MapReduce, Pig, Hive, and so on), and <a class="calibre" id="calibre_link-1255"></a>a <em class="calibre10">coordinator engine</em> that runs
      workflow jobs based on predefined schedules and data availability. Oozie
      has been designed to scale, and it can manage the timely execution of
      thousands of workflows in a Hadoop cluster, each composed of possibly
      dozens of constituent jobs.</p><p class="calibre2">Oozie makes rerunning failed workflows more tractable, since no
      time is wasted running successful parts of a workflow. Anyone who has
      managed a complex batch system knows how difficult it can be to catch up
      from jobs missed due to downtime or failure, and will appreciate this
      feature. (Furthermore, coordinator applications representing a single
      data pipeline may be packaged into a <em class="calibre10">bundle</em> and
      run together as a unit.)</p><p class="calibre2">Unlike <code class="literal">JobControl</code>, which runs
      on the client machine submitting the jobs, Oozie runs as a service in
      the cluster, and clients submit workflow definitions for immediate or
      later execution. In Oozie parlance, a workflow is a DAG of
      <em class="calibre10">action nodes</em> and <em class="calibre10">control-flow
      nodes</em>.</p><p class="calibre2">An action <a class="calibre" id="calibre_link-864"></a>node performs a workflow task, such as moving files in
      HDFS; running a MapReduce, Streaming, Pig, or Hive job; performing a
      Sqoop import; or running an arbitrary shell script or Java program. A
      <a class="calibre" id="calibre_link-1252"></a>control-flow node governs the workflow execution between
      actions by allowing such constructs as conditional logic (so different
      execution branches may be followed depending on the result of an earlier
      action node) or parallel execution. When the workflow completes, Oozie
      can make an HTTP callback to the client to inform it of the workflow
      status. It is also possible to receive callbacks every time the workflow
      enters or exits an action node.</p><div class="book" title="Defining an Oozie workflow"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4200">Defining an Oozie workflow</h4></div></div></div><p class="calibre2">Workflow definitions <a class="calibre" id="calibre_link-892"></a>are written in XML using the Hadoop Process Definition
        Language, the specification for which can be found on the <a class="ulink" href="http://oozie.apache.org/" target="_top">Oozie website</a>. <a class="ulink" href="#calibre_link-701" title="Example&nbsp;6-14.&nbsp;Oozie workflow definition to run the maximum temperature MapReduce job">Example&nbsp;6-14</a> shows a simple Oozie workflow
        definition for running a single MapReduce job.</p><div class="example"><a id="calibre_link-701" class="calibre"></a><div class="example-title">Example&nbsp;6-14.&nbsp;Oozie workflow definition to run the maximum temperature
          MapReduce job</div><div class="book"><pre class="screen"><code class="nt">&lt;workflow-app</code> <code class="na">xmlns=</code><code class="sb">"uri:oozie:workflow:0.1"</code> <code class="na">name=</code><code class="sb">"max-temp-workflow"</code><code class="nt">&gt;</code>
  <code class="nt">&lt;start</code> <code class="na">to=</code><code class="sb">"max-temp-mr"</code><code class="nt">/&gt;</code>
  <code class="nt">&lt;action</code> <code class="na">name=</code><code class="sb">"max-temp-mr"</code><code class="nt">&gt;</code>
    <code class="nt">&lt;map-reduce&gt;</code>
      <code class="nt">&lt;job-tracker&gt;</code>${resourceManager}<code class="nt">&lt;/job-tracker&gt;</code>
      <code class="nt">&lt;name-node&gt;</code>${nameNode}<code class="nt">&lt;/name-node&gt;</code>
      <code class="nt">&lt;prepare&gt;</code>
        <code class="nt">&lt;delete</code> <code class="na">path=</code><code class="sb">"${nameNode}/user/${wf:user()}/output"</code><code class="nt">/&gt;</code>
      <code class="nt">&lt;/prepare&gt;</code>
      <code class="nt">&lt;configuration&gt;</code>
        <code class="nt">&lt;property&gt;</code>
          <code class="nt">&lt;name&gt;</code>mapred.mapper.new-api<code class="nt">&lt;/name&gt;</code>
          <code class="nt">&lt;value&gt;</code>true<code class="nt">&lt;/value&gt;</code>
        <code class="nt">&lt;/property&gt;</code>
        <code class="nt">&lt;property&gt;</code>
          <code class="nt">&lt;name&gt;</code>mapred.reducer.new-api<code class="nt">&lt;/name&gt;</code>
          <code class="nt">&lt;value&gt;</code>true<code class="nt">&lt;/value&gt;</code>
        <code class="nt">&lt;/property&gt;</code>
        <code class="nt">&lt;property&gt;</code>
          <code class="nt">&lt;name&gt;</code>mapreduce.job.map.class<code class="nt">&lt;/name&gt;</code>
          <code class="nt">&lt;value&gt;</code>MaxTemperatureMapper<code class="nt">&lt;/value&gt;</code>
        <code class="nt">&lt;/property&gt;</code>
        <code class="nt">&lt;property&gt;</code>
          <code class="nt">&lt;name&gt;</code>mapreduce.job.combine.class<code class="nt">&lt;/name&gt;</code>
          <code class="nt">&lt;value&gt;</code>MaxTemperatureReducer<code class="nt">&lt;/value&gt;</code>
        <code class="nt">&lt;/property&gt;</code>
        <code class="nt">&lt;property&gt;</code>
          <code class="nt">&lt;name&gt;</code>mapreduce.job.reduce.class<code class="nt">&lt;/name&gt;</code>
          <code class="nt">&lt;value&gt;</code>MaxTemperatureReducer<code class="nt">&lt;/value&gt;</code>
        <code class="nt">&lt;/property&gt;</code>
        <code class="nt">&lt;property&gt;</code>
          <code class="nt">&lt;name&gt;</code>mapreduce.job.output.key.class<code class="nt">&lt;/name&gt;</code>
          <code class="nt">&lt;value&gt;</code>org.apache.hadoop.io.Text<code class="nt">&lt;/value&gt;</code>
        <code class="nt">&lt;/property&gt;</code>
        <code class="nt">&lt;property&gt;</code>
          <code class="nt">&lt;name&gt;</code>mapreduce.job.output.value.class<code class="nt">&lt;/name&gt;</code>
          <code class="nt">&lt;value&gt;</code>org.apache.hadoop.io.IntWritable<code class="nt">&lt;/value&gt;</code>
        <code class="nt">&lt;/property&gt;</code>        
        <code class="nt">&lt;property&gt;</code>
          <code class="nt">&lt;name&gt;</code>mapreduce.input.fileinputformat.inputdir<code class="nt">&lt;/name&gt;</code>
          <code class="nt">&lt;value&gt;</code>/user/${wf:user()}/input/ncdc/micro<code class="nt">&lt;/value&gt;</code>
        <code class="nt">&lt;/property&gt;</code>
        <code class="nt">&lt;property&gt;</code>
          <code class="nt">&lt;name&gt;</code>mapreduce.output.fileoutputformat.outputdir<code class="nt">&lt;/name&gt;</code>
          <code class="nt">&lt;value&gt;</code>/user/${wf:user()}/output<code class="nt">&lt;/value&gt;</code>
        <code class="nt">&lt;/property&gt;</code>
      <code class="nt">&lt;/configuration&gt;</code>
    <code class="nt">&lt;/map-reduce&gt;</code>
    <code class="nt">&lt;ok</code> <code class="na">to=</code><code class="sb">"end"</code><code class="nt">/&gt;</code>
    <code class="nt">&lt;error</code> <code class="na">to=</code><code class="sb">"fail"</code><code class="nt">/&gt;</code>
  <code class="nt">&lt;/action&gt;</code>
  <code class="nt">&lt;kill</code> <code class="na">name=</code><code class="sb">"fail"</code><code class="nt">&gt;</code>
    <code class="nt">&lt;message&gt;</code>MapReduce failed, error message[${wf:errorMessage(wf:lastErrorNode())}]
    <code class="nt">&lt;/message&gt;</code>
  <code class="nt">&lt;/kill&gt;</code>
  <code class="nt">&lt;end</code> <code class="na">name=</code><code class="sb">"end"</code><code class="nt">/&gt;</code>
<code class="nt">&lt;/workflow-app&gt;</code></pre></div></div><p class="calibre2">This workflow has three <a class="calibre" id="calibre_link-1253"></a><a class="calibre" id="calibre_link-865"></a>control-flow nodes and one action node: a <code class="literal">start</code> control node, a <code class="literal">map-reduce</code> action node, a <code class="literal">kill</code> control node, and an <code class="literal">end</code> control node. The nodes and allowed
        transitions between them are shown in <a class="ulink" href="#calibre_link-702" title="Figure&nbsp;6-4.&nbsp;Transition diagram of an Oozie workflow">Figure&nbsp;6-4</a>.</p><div class="figure"><a id="calibre_link-702" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4201" class="calibre"></a><img alt="Transition diagram of an Oozie workflow" src="images/000011.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;6-4.&nbsp;Transition diagram of an Oozie workflow</div></div><p class="calibre2">All workflows must have one <code class="literal">start</code> and one <code class="literal">end</code> node. When the workflow job starts, it
        transitions to the node specified by the <code class="literal">start</code> node (the <code class="literal">max-temp-mr</code> action in this example). A
        workflow job succeeds when it transitions to the <code class="literal">end</code> node. However, if the workflow job
        transitions to a <code class="literal">kill</code> node, it is
        considered to have failed and reports the appropriate error message
        specified by the <code class="literal">message</code> element in
        the workflow definition.</p><p class="calibre2">The bulk of this workflow definition file specifies the <code class="literal">map-reduce</code> action. The first two elements,
        <code class="literal">job-tracker</code> and <code class="literal">name-node</code>, are used to specify the YARN
        resource manager (or jobtracker in Hadoop 1) to submit the job to and
        the namenode (actually a Hadoop filesystem URI) for input and output
        data. Both are parameterized so that the workflow definition is not
        tied to a particular cluster (which makes it easy to test). The
        parameters are specified as workflow job properties at submission
        time, as we shall see later.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">Despite its name, the <code class="literal">job-tracker</code> element is used to specify a
          YARN resource manager address and port.</p></div><p class="calibre2">The optional <code class="literal">prepare</code> element
        runs before the MapReduce job and is used for directory deletion (and
        creation, too, if needed, although that is not shown here). By
        ensuring that the output directory is in a consistent state before
        running a job, Oozie can safely rerun the action if the job
        fails.</p><p class="calibre2">The MapReduce job to run is specified in the <code class="literal">configuration</code> element using nested elements
        for specifying the Hadoop configuration name-value pairs. You can view
        the MapReduce configuration section as a declarative replacement for
        the driver classes that we have used elsewhere in this book for
        running MapReduce programs (such as <a class="ulink" href="#calibre_link-469" title="Example&nbsp;2-5.&nbsp;Application to find the maximum temperature in the weather dataset">Example&nbsp;2-5</a>).</p><p class="calibre2">We have taken advantage of JSP Expression Language <a class="calibre" id="calibre_link-2291"></a>(EL) syntax in several places in the workflow
        definition. Oozie provides a set of functions for interacting with the
        workflow. For example, <code class="literal">${wf:user()}</code>
        returns the name of the user who started the current workflow job, and
        we use it to specify the correct filesystem path. The Oozie
        specification lists all the EL functions that <a class="calibre" id="calibre_link-893"></a>Oozie supports.</p></div><div class="book" title="Packaging and deploying an Oozie workflow application"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4202">Packaging and deploying an Oozie workflow application</h4></div></div></div><p class="calibre2">A workflow <a class="calibre" id="calibre_link-894"></a><a class="calibre" id="calibre_link-2896"></a>application is made up of the workflow definition plus
        all the associated resources (such as MapReduce JAR files, Pig
        scripts, and so on) needed to run it. Applications must adhere to a
        simple directory structure, and are deployed to HDFS so that they can
        be accessed by Oozie. For this workflow application, we’ll put all of
        the files in a base directory called <em class="calibre10">max-temp-workflow</em>, as shown
        diagrammatically here:</p><pre class="screen1">max-temp-workflow/
├── lib/
│&nbsp;&nbsp; └── hadoop-examples.jar
└── workflow.xml</pre><p class="calibre2">The workflow definition file <em class="calibre10">workflow.xml</em> must appear in the top level
        of this directory. JAR files containing the application’s MapReduce
        classes are placed in the <em class="calibre10">lib</em>
        directory.</p><p class="calibre2">Workflow applications that conform to this layout can be built
        with any suitable build tool, such as Ant or Maven; you can find an
        example in the code that accompanies this book. Once an application
        has been built, it should be copied to HDFS using regular Hadoop
        tools. Here is the appropriate command for this application:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -put hadoop-examples/target/max-temp-workflow max-temp-workflow</code></strong></pre></div><div class="book" title="Running an Oozie workflow job"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4203">Running an Oozie workflow job</h4></div></div></div><p class="calibre2">Next, let’s see how to run a <a class="calibre" id="calibre_link-895"></a><a class="calibre" id="calibre_link-2257"></a>workflow job for the application we just uploaded. For
        this we use <a class="calibre" id="calibre_link-2828"></a>the <span class="calibre"><em class="calibre10">oozie</em></span> command-line tool, a
        client program for communicating with an Oozie server. For
        convenience, we export <a class="calibre" id="calibre_link-2830"></a>the <code class="literal">OOZIE_URL</code>
        environment variable to tell the <code class="literal">oozie</code> command which Oozie server to
        use (here we’re using one running locally):</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">export OOZIE_URL="http://localhost:11000/oozie"</code></strong></pre><p class="calibre2">There are lots of subcommands for the <span class="calibre"><em class="calibre10">oozie</em></span>
        tool (type <code class="literal">oozie
        help</code> to get a list), but we’re going to call the <code class="literal">job</code> subcommand with the <code class="literal">-run</code> option to run the
        workflow job:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">oozie job -config ch06-mr-dev/src/main/resources/max-temp-workflow.properties \</code></strong>
<strong class="userinput"><code class="calibre9">  -run</code></strong>
job: 0000001-140911033236814-oozie-oozi-W</pre><p class="calibre2">The <code class="literal">-config</code> option specifies a local Java
        properties file containing definitions for the parameters in the
        workflow XML file (in this case, <code class="literal">nameNode</code> and <code class="literal">resourceManager</code>), as well as <code class="literal">oozie.wf.application.path</code>, which <a class="calibre" id="calibre_link-2829"></a>tells Oozie the location of the workflow application in
        HDFS. Here are the contents of the properties file:</p><a id="calibre_link-4204" class="calibre"></a><pre class="screen1"><code class="na">nameNode</code><code class="o">=</code><code class="sb">hdfs://localhost:8020</code>
<code class="na">resourceManager</code><code class="o">=</code><code class="sb">localhost:8032</code>
<code class="na">oozie.wf.application.path</code><code class="o">=</code><code class="sb">${nameNode}/user/${user.name}/max-temp-workflow</code></pre><p class="calibre2">To get information about the status of the workflow job, we use
        the <code class="literal">-info</code>
        option, specifying the job ID that was printed by the run command
        earlier (type <code class="literal">oozie
        job</code> to get a list of all jobs):</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">oozie job -info 0000001-140911033236814-oozie-oozi-W</code></strong></pre><p class="calibre2">The output shows the status: <code class="literal">RUNNING</code>, <code class="literal">KILLED</code>, or <code class="literal">SUCCEEDED</code>. <br>You can also find all this
        information via Oozie’s web UI
        (<span class="calibre"><em class="calibre10">http://localhost:11000/oozie</em></span>).</p><p class="calibre2">When the job has succeeded, we can inspect the results in the
        usual way:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -cat output/part-*</code></strong>
1949	111
1950	22</pre><p class="calibre2">This example only scratched the surface of writing Oozie
        workflows. The documentation on Oozie’s website has information about
        creating more complex workflows, as well as writing and running
        coordinator <a class="calibre" id="calibre_link-2471"></a><a class="calibre" id="calibre_link-3792"></a>jobs.</p></div></div></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" type="footnote" id="calibre_link-682"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-703">49</a>] </sup>In Hadoop 1, <code class="literal">mapred.job.tracker</code> determines the
          <a class="calibre" id="calibre_link-2426"></a>means of execution: <code class="literal">local</code> for the local job runner, or a
          colon-separated host and port pair for a jobtracker address.</p></div><div class="footnote" type="footnote" id="calibre_link-700"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-704">50</a>] </sup>It’s an interesting exercise to do this. Hint: use <a class="ulink" href="#calibre_link-160" title="Secondary Sort">Secondary Sort</a>.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-331"><section type="chapter" id="calibre_link-4205" title="Chapter&nbsp;7.&nbsp;How MapReduce Works"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;7.&nbsp;How MapReduce Works</h2></div></div></div><p class="calibre2">In this chapter, we look at how MapReduce in Hadoop works in detail.
  This knowledge provides a good foundation for writing more advanced
  MapReduce programs, which we will cover in the following two
  chapters.</p><div class="book" title="Anatomy of a MapReduce Job Run"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-52">Anatomy of a MapReduce Job Run</h2></div></div></div><p class="calibre2">You can run a MapReduce job <a class="calibre" id="calibre_link-2441"></a><a class="calibre" id="calibre_link-2231"></a>with a single method call: <code class="literal">submit()</code> on a <code class="literal">Job</code> object (you can also call <code class="literal">waitForCompletion()</code>, which submits the job if it
    hasn’t been submitted already, then waits for it to finish).<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-490" id="calibre_link-516">51</a>]</sup> This method call conceals a great deal of processing behind
    the scenes. This section uncovers the steps Hadoop takes to run a
    job.</p><p class="calibre2">The whole process is illustrated in <a class="ulink" href="#calibre_link-491" title="Figure&nbsp;7-1.&nbsp;How Hadoop runs a MapReduce job">Figure&nbsp;7-1</a>. At the highest level, there
    are five independent entities:<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-492" id="calibre_link-517">52</a>]</sup></p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">The client, which submits the MapReduce job.</p></li><li class="listitem"><p class="calibre2">The YARN resource manager, which coordinates the allocation of
        compute resources on the cluster.</p></li><li class="listitem"><p class="calibre2">The YARN node managers, which launch and monitor the compute
        containers on machines in the cluster.</p></li><li class="listitem"><p class="calibre2">The MapReduce application master, which coordinates the tasks
        running the MapReduce job. The application master and the MapReduce
        tasks run in containers that are scheduled by the resource manager and
        managed by the node managers.</p></li><li class="listitem"><p class="calibre2">The distributed filesystem (normally HDFS, covered in <a class="ulink" href="#calibre_link-161" title="Chapter&nbsp;3.&nbsp;The Hadoop Distributed Filesystem">Chapter&nbsp;3</a>), which is used for sharing job files between
        the other entities.</p></li></ul></div><div class="book"><div class="figure"><a id="calibre_link-491" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4206" class="calibre"></a><img alt="How Hadoop runs a MapReduce job" src="images/000021.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;7-1.&nbsp;How Hadoop runs a MapReduce job</div></div></div><div class="book" title="Job Submission"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4207">Job Submission</h3></div></div></div><p class="calibre2">The <code class="literal">submit()</code> method on <code class="literal">Job</code> creates <a class="calibre" id="calibre_link-2261"></a><a class="calibre" id="calibre_link-2270"></a>an internal <code class="literal">JobSubmitter</code> instance and calls <code class="literal">submitJobInternal()</code> on it (step 1 in <a class="ulink" href="#calibre_link-491" title="Figure&nbsp;7-1.&nbsp;How Hadoop runs a MapReduce job">Figure&nbsp;7-1</a>). Having submitted the job,
      <code class="literal">waitForCompletion()</code> polls the job’s
      progress once per second and reports the progress to the console if it
      has changed since the last report. When the job completes successfully,
      the job counters are displayed. Otherwise, the error that caused the job
      to fail is logged to the console.</p><p class="calibre2">The job submission process implemented by <code class="literal">JobSubmitter</code> does the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">Asks the <a class="calibre" id="calibre_link-3228"></a>resource manager for a new application ID, used for
          the MapReduce job ID (step 2).</p></li><li class="listitem"><p class="calibre2">Checks the output specification of the job. For example, if
          the output directory has not been specified or it already exists,
          the job is not submitted and an error is thrown to the MapReduce
          program.</p></li><li class="listitem"><p class="calibre2">Computes the input splits for the job. If the splits cannot be
          computed (because the input paths don’t exist, for example), the job
          is not submitted and an error is thrown to the MapReduce
          program.</p></li><li class="listitem"><p class="calibre2">Copies the resources needed to run the job, including the job
          JAR file, the configuration file, and the computed input splits, to
          the shared filesystem in a directory named after the job ID (step
          3). The job JAR is copied with a high replication <a class="calibre" id="calibre_link-2527"></a>factor (controlled by the <code class="literal">mapreduce.client.submit.file.replication</code>
          property, which defaults to 10) so that there are lots of copies
          across the cluster for the node managers to access when they run
          tasks for the job.</p></li><li class="listitem"><p class="calibre2">Submits the job by calling <code class="literal">submitApplication()</code> on the resource
            manager (step 4).</p></li></ul></div></div><div class="book" title="Job Initialization"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4208">Job Initialization</h3></div></div></div><p class="calibre2">When the resource <a class="calibre" id="calibre_link-2245"></a><a class="calibre" id="calibre_link-3227"></a>manager receives a call to its
      <code class="literal">submitApplication()</code> method, it hands off the
      request to the YARN scheduler. The
      scheduler allocates a container, and the resource manager then launches
      the <a class="calibre" id="calibre_link-909"></a>application master’s process there, under the <a class="calibre" id="calibre_link-2801"></a>node manager’s management (steps 5a and 5b).</p><p class="calibre2">The application master for MapReduce jobs is a Java application
      whose main class is <code class="literal">MRAppMaster</code>. It
      initializes the job by creating a number of bookkeeping objects to keep
      track of the job’s progress, as it will receive progress and completion
      reports from the tasks (step 6).
      Next, it retrieves the input splits computed in the client from the
      shared filesystem (step 7).
      It then creates a map task object for each split, as well as a number of
      reduce task objects determined by the <code class="literal">mapreduce.job.reduces</code> property (<a class="calibre" id="calibre_link-2562"></a>set by the <code class="literal">setNumReduceTasks()</code> method on
      <code class="literal">Job</code>). Tasks are given IDs at this point.</p><p class="calibre2">The application master must decide how to run the tasks that make
      up the MapReduce job. If the job is small, the application master may
      choose to run the tasks in the same JVM as itself. This happens when it
      judges that the overhead of allocating and running tasks in new
      containers outweighs the gain to be had in running them in parallel,
      compared to running them sequentially on one node. Such a job is said to
      be <em class="calibre10">uberized</em>, or run as <a class="calibre" id="calibre_link-3723"></a>an <em class="calibre10">uber task</em>.</p><p class="calibre2">What qualifies as a small job? By default, a small job is one that
      has less than 10 mappers, only one reducer, and an input size that is
      less than the size of one HDFS block. (Note that these values may be
      changed for a job by <a class="calibre" id="calibre_link-2565"></a><a class="calibre" id="calibre_link-2566"></a><a class="calibre" id="calibre_link-2564"></a>setting <code class="literal">mapreduce.job.ubertask.maxmaps</code>, <code class="literal">mapreduce.job.ubertask.maxreduces</code>, and
      <code class="literal">mapreduce.job.ubertask.maxbytes</code>.)
      Uber tasks must be enabled explicitly (for an individual job, or across
      the cluster) by <a class="calibre" id="calibre_link-2563"></a>setting <code class="literal">mapreduce.job.ubertask.enable</code> to <code class="literal">true</code>.</p><p class="calibre2">Finally, before any tasks can be run, the application master calls
      the <code class="literal">setupJob()</code> method <a class="calibre" id="calibre_link-2881"></a>on the <code class="literal">OutputCommitter</code>.
      For <code class="literal">FileOutputCommitter</code>, which
      <a class="calibre" id="calibre_link-1642"></a>is the default, it will create the final output directory
      for the job and the temporary working space for the task output. The
      commit protocol is described in more detail in <a class="ulink" href="#calibre_link-493" title="Output Committers">Output Committers</a>.</p></div><div class="book" title="Task Assignment"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4209">Task Assignment</h3></div></div></div><p class="calibre2">If the job does not qualify for running as an uber task, then the
      <a class="calibre" id="calibre_link-914"></a><a class="calibre" id="calibre_link-3642"></a>application master requests containers for all the map and
      reduce tasks in the job from the <a class="calibre" id="calibre_link-3233"></a>resource manager (step 8). Requests for <a class="calibre" id="calibre_link-2401"></a>map tasks are made first and with a higher priority than
      those for reduce tasks, since all the map tasks must complete before the
      sort phase of the reduce can start (see <a class="ulink" href="#calibre_link-46" title="Shuffle and Sort">Shuffle and Sort</a>). Requests for <a class="calibre" id="calibre_link-3189"></a>reduce tasks are not made until 5% of map tasks have completed (see <a class="ulink" href="#calibre_link-494" title="Reduce slow start">Reduce slow start</a>).</p><p class="calibre2">Reduce tasks can run anywhere in the cluster, but requests for map
      tasks have data locality constraints that the scheduler tries to honor
      (see <a class="ulink" href="#calibre_link-495" title="Resource Requests">Resource Requests</a>). In the optimal case, the
      <a class="calibre" id="calibre_link-1340"></a>task is <em class="calibre10">data local</em>—that is, running
      on the same node that the split resides on. Alternatively, the task may
      <a class="calibre" id="calibre_link-3126"></a>be <em class="calibre10">rack local</em>: on the same rack,
      but not the same node, as the split. Some tasks are neither data local
      nor rack local and retrieve their data from a different rack than the
      one they are running on. For a particular job run, you can determine the
      number of tasks that ran at each locality level by looking at the job’s
      counters (see <a class="ulink" href="#calibre_link-496" title="Table&nbsp;9-6.&nbsp;Built-in job counters">Table&nbsp;9-6</a>).</p><p class="calibre2">Requests also specify <a class="calibre" id="calibre_link-2669"></a>memory requirements and CPUs for tasks. By default, each
      map and reduce task is allocated 1,024 MB of memory and one virtual
      core. The values are configurable on a per-job basis (subject to minimum
      and maximum values described in <a class="ulink" href="#calibre_link-35" title="Memory settings in YARN and MapReduce">Memory settings in YARN and MapReduce</a>) via the following <a class="calibre" id="calibre_link-2582"></a><a class="calibre" id="calibre_link-2609"></a><a class="calibre" id="calibre_link-2573"></a><a class="calibre" id="calibre_link-2601"></a>properties: <span class="calibre"><span class="calibre"><code class="literal">mapreduce.map.memory.mb</code>, <code class="literal">mapreduce.reduce.memory.mb</code>, <code class="literal">mapreduce.map.cpu</code></span></span><span class="calibre"><code class="literal">.vcores</code></span> and <code class="literal">mapreduce.reduce.cpu.vcores</code>.</p></div><div class="book" title="Task Execution"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4210">Task Execution</h3></div></div></div><p class="calibre2">Once a task <a class="calibre" id="calibre_link-3629"></a><a class="calibre" id="calibre_link-3234"></a><a class="calibre" id="calibre_link-2512"></a>has been assigned resources for a container on a
      particular node by the resource manager’s scheduler, the <a class="calibre" id="calibre_link-915"></a>application master starts the container by contacting
      the<a class="calibre" id="calibre_link-2805"></a> node manager (steps 9a and 9b).
      The task is executed by a Java application whose main class is <code class="literal">YarnChild</code>. Before it can run the task, it
      localizes the resources that the task needs, including the job
      configuration and JAR file, and any files from the distributed cache
      (step 10; see <a class="ulink" href="#calibre_link-54" title="Distributed Cache">Distributed Cache</a>).
      Finally, it runs the <a class="calibre" id="calibre_link-2402"></a><a class="calibre" id="calibre_link-3190"></a>map or reduce task (step 11).</p><p class="calibre2">The <code class="literal">YarnChild</code> runs in a
      dedicated JVM, so that any bugs in the user-defined map and reduce
      functions (or even in <code class="literal">YarnChild</code>)
      don’t affect the node manager—by causing it to crash or hang, for
      example.</p><p class="calibre2">Each task can perform setup and commit actions, which are run in
      the same JVM as the task itself and are determined by the <code class="literal">OutputCommitter</code> for <a class="calibre" id="calibre_link-2882"></a>the job (see <a class="ulink" href="#calibre_link-493" title="Output Committers">Output Committers</a>). For
      file-based jobs, the commit action moves the task output from a
      temporary location to its final location. The commit protocol ensures
      that when speculative execution is enabled (see <a class="ulink" href="#calibre_link-497" title="Speculative Execution">Speculative Execution</a>), only one of the duplicate tasks is
      committed and the other is aborted.</p><div class="book" title="Streaming"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4211">Streaming</h4></div></div></div><p class="calibre2">Streaming <a class="calibre" id="calibre_link-3641"></a><a class="calibre" id="calibre_link-3552"></a>runs special map and reduce tasks for the purpose of
        launching the user-supplied executable and communicating with it
        (<a class="ulink" href="#calibre_link-498" title="Figure&nbsp;7-2.&nbsp;The relationship of the Streaming executable to the node manager and the task container">Figure&nbsp;7-2</a>).</p><p class="calibre2">The Streaming task communicates with the process (which may be
        written in any language) using standard input and output streams.
        During execution of the task, the Java process passes input key-value
        pairs to the external process, which runs it through the user-defined
        map or reduce function and passes the output key-value pairs back to
        the Java process. From the <a class="calibre" id="calibre_link-2804"></a>node manager’s point of view, it is as if the child
        process ran the map or reduce code itself.</p><div class="book"><div class="figure"><a id="calibre_link-498" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4212" class="calibre"></a><img alt="The relationship of the Streaming executable to the node manager and the task container" src="images/000029.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;7-2.&nbsp;The relationship of the Streaming executable to the node
            manager and the task container</div></div></div></div></div><div class="book" title="Progress and Status Updates"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-13">Progress and Status Updates</h3></div></div></div><p class="calibre2">MapReduce jobs are <a class="calibre" id="calibre_link-2252"></a><a class="calibre" id="calibre_link-3636"></a>long-running batch jobs, taking anything from tens of
      seconds to hours to run. Because this can be a significant length of
      time, it’s important for the user to get feedback on how the job is
      progressing. A job and each of its tasks have a
      <em class="calibre10">status</em>, which includes such things as the state
      of the job or task (e.g., running, successfully completed, failed), the
      progress of maps and reduces, the values of the job’s counters, and a
      status message or description (which may be set by user code). These
      statuses change over the course of the job, so how do they get
      communicated back to the client?</p><p class="calibre2">When a task is running, it keeps track<a class="calibre" id="calibre_link-3064"></a><a class="calibre" id="calibre_link-3528"></a> of its <em class="calibre10">progress</em> (i.e., the
      proportion of the task completed). For <a class="calibre" id="calibre_link-2397"></a><a class="calibre" id="calibre_link-3185"></a>map tasks, this is the proportion of the input that has
      been processed. For reduce tasks, it’s a little more complex, but the
      system can still estimate the proportion of the reduce input processed.
      It does this by dividing the total progress into three parts,
      corresponding to the three phases of the shuffle (see <a class="ulink" href="#calibre_link-46" title="Shuffle and Sort">Shuffle and Sort</a>). For example, if the task has run the
      reducer on half its input, the task’s progress is 5/6, since it has
      completed the copy and sort phases (1/3 each) and is halfway through the
      reduce phase (1/6).</p><div class="sidebar"><a id="calibre_link-502" class="calibre"></a><div class="sidebar-title">What Constitutes Progress in MapReduce?</div><p class="calibre2">Progress is not <a class="calibre" id="calibre_link-2493"></a>always measurable, but nevertheless, it tells Hadoop
        that a task is doing something. For example, a task writing output
        records is making progress, even when it cannot be expressed as a
        percentage of the total number that will be written (because the
        latter figure may not be known, even by the task producing the
        output).</p><p class="calibre2">Progress reporting is important, as Hadoop will not fail a task
        that’s making progress. All of the following operations constitute
        progress:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">Reading an input record (in a mapper or reducer)</p></li><li class="listitem"><p class="calibre2">Writing an output record (in a mapper or reducer)</p></li><li class="listitem"><p class="calibre2">Setting the status <a class="calibre" id="calibre_link-3215"></a><a class="calibre" id="calibre_link-3628"></a>description (via <code class="literal">Reporter</code>’s
            or <code class="literal">TaskAttemptContext</code>’s
            <code class="literal">setStatus()</code> method)</p></li><li class="listitem"><p class="calibre2">Incrementing a counter (using
            <code class="literal">Reporter</code>’s
            <code class="literal">incrCounter()</code> method or
            <code class="literal">Counter</code>’s
            <code class="literal">increment()</code> method)</p></li><li class="listitem"><p class="calibre2">Calling <code class="literal">Reporter</code>’s or
            <code class="literal">TaskAttemptContext</code>’s
            <code class="literal">progress()</code> method</p></li></ul></div></div><p class="calibre2">Tasks also have a set of counters that count various events as the
      task runs (we saw an example in <a class="ulink" href="#calibre_link-499" title="A test run">A test run</a>), which
      are either built into the framework, such as the number of map output
      records written, or defined by users.</p><p class="calibre2">As the map or reduce task runs, the child process communicates
      with its parent <a class="calibre" id="calibre_link-912"></a>application master through the
      <em class="calibre10">umbilical</em> interface. The task reports its
      progress and status (including counters) back to its application master,
      which has an aggregate view of the job, every three seconds over the
      umbilical interface.</p><p class="calibre2">The <a class="calibre" id="calibre_link-3231"></a>resource manager web UI displays all the running
      applications with links to the web UIs of their respective application
      masters, each of which displays further details on the MapReduce job,
      including its progress.</p><p class="calibre2">During the course of the job, the client receives the latest
      status by polling the application master every second (the interval is
      set <a class="calibre" id="calibre_link-2526"></a>via <code class="literal">mapreduce.client.progressmonitor.pollinterval</code>).
      Clients can <a class="calibre" id="calibre_link-2210"></a>also use <code class="literal">Job</code>’s <code class="literal">getStatus()</code> method to obtain a <code class="literal">JobStatus</code> instance, which contains all of the
      status information for the job.</p><p class="calibre2">The process is illustrated in <a class="ulink" href="#calibre_link-500" title="Figure&nbsp;7-3.&nbsp;How status updates are propagated through the MapReduce system">Figure&nbsp;7-3</a>.</p><div class="book"><div class="figure"><a id="calibre_link-500" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4213" class="calibre"></a><img alt="How status updates are propagated through the MapReduce system" src="images/000039.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;7-3.&nbsp;How status updates are propagated through the MapReduce
        system</div></div></div></div><div class="book" title="Job Completion"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4214">Job Completion</h3></div></div></div><p class="calibre2">When the <a class="calibre" id="calibre_link-2235"></a><a class="calibre" id="calibre_link-908"></a>application master receives a notification that the last
      task for a job is complete, it changes the status for the job to
      “successful.” Then, when the <code class="literal">Job</code>
      polls for status, it learns that the job has completed successfully, so
      it prints a message to tell the user and then returns from the
      <code class="literal">waitForCompletion()</code> method. Job statistics and
      counters are printed to the console at this point.</p><p class="calibre2">The application master also sends an HTTP job notification if it
      is configured to do so. This can be configured by clients wishing to
      receive callbacks, <a class="calibre" id="calibre_link-2546"></a>via the <code class="literal">mapreduce.job.end-notification.url</code>
      property.</p><p class="calibre2">Finally, on job completion, the application master and the task
      containers clean up their working state (so intermediate output is
      deleted), and the <code class="literal">OutputCommitter</code>’s
      <code class="literal">commitJob()</code> method is called. Job information
      is archived by the job history server to enable later interrogation by
      users <a class="calibre" id="calibre_link-2442"></a><a class="calibre" id="calibre_link-2232"></a>if desired.</p></div></div><div class="book" title="Failures"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-330">Failures</h2></div></div></div><p class="calibre2">In the real <a class="calibre" id="calibre_link-2474"></a><a class="calibre" id="calibre_link-1606"></a>world, user code is buggy, processes crash, and machines
    fail. One of the major benefits of using Hadoop is its ability to handle
    such failures and allow your job to complete successfully. We need to
    consider the failure of any of the following entities: the task, the
    application master, the node manager, and the resource manager.</p><div class="book" title="Task Failure"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-593">Task Failure</h3></div></div></div><p class="calibre2">Consider first the case of the task <a class="calibre" id="calibre_link-1610"></a><a class="calibre" id="calibre_link-3633"></a>failing. The most common occurrence of this failure is
      when user code in the map or reduce task throws a runtime exception. If
      this happens, the task JVM reports the error back to its parent
      application master before it exits.
      The error ultimately makes it into the user logs. The <a class="calibre" id="calibre_link-916"></a>application master marks the task attempt as
      <em class="calibre10">failed</em>, and frees up the container so its
      resources are available for another task.</p><p class="calibre2">For Streaming tasks, if the Streaming process exits with a nonzero
      exit code, it is marked as failed. This behavior is governed by
      <a class="calibre" id="calibre_link-3541"></a>the <code class="literal">stream.non.zero.exit.is.failure</code> property (the
      default is <code class="literal">true</code>).</p><p class="calibre2">Another failure mode is the sudden exit of the task <a class="calibre" id="calibre_link-2296"></a><a class="calibre" id="calibre_link-2192"></a>JVM—perhaps there is a JVM bug that causes the JVM to exit
      for a particular set of circumstances exposed by the MapReduce user code. In this case, the
      node manager notices that the process has exited and informs the
      application master so it can mark the attempt as failed.</p><p class="calibre2">Hanging tasks are dealt with differently. The application master
      notices that it hasn’t received a progress update for a while and
      proceeds to mark the task as failed. The
      task JVM process will be killed automatically after this period.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-501" id="calibre_link-518">53</a>]</sup> The timeout period after which tasks are considered failed
      is normally 10 minutes and can be configured on a per-job basis (or a
      cluster basis) by <a class="calibre" id="calibre_link-2641"></a>setting the <code class="literal">mapreduce.task.timeout</code> property to a value in
      milliseconds.</p><p class="calibre2">Setting the timeout to a value of zero disables the timeout, so
      long-running tasks are never marked as failed. In this case, a hanging
      task will never free up its container, and over time there may be
      cluster slowdown as a result. This approach should therefore be avoided,
      and making sure that a task is reporting progress periodically should
      suffice (see <a class="ulink" href="#calibre_link-502" title="What Constitutes Progress in MapReduce?">What Constitutes Progress in MapReduce?</a>).</p><p class="calibre2">When the application master is notified of a task attempt that has
      failed, it will reschedule execution of the task.
      The application master will try to avoid rescheduling the task on a node
      manager where it has previously failed. Furthermore, if a task fails
      four times, it will not be retried again.
      This value is configurable. The maximum number of attempts to run a task
      is controlled by the <code class="literal">mapreduce.map.maxattempts</code> property for
      <a class="calibre" id="calibre_link-2403"></a><a class="calibre" id="calibre_link-3191"></a>map tasks <a class="calibre" id="calibre_link-2581"></a><a class="calibre" id="calibre_link-2608"></a>and <code class="literal">mapreduce.reduce.maxattempts</code> for reduce tasks.
      By default, if any task fails four times (or whatever the maximum number
      of attempts is configured to), the whole job fails.</p><p class="calibre2">For some applications, it is undesirable to abort the job if a few
      tasks fail, as it may be possible to use the results of the job despite
      some failures. In this case, the maximum percentage of tasks that are
      allowed to fail without triggering job failure can be set for the job. Map tasks and reduce tasks
      are <a class="calibre" id="calibre_link-2603"></a><a class="calibre" id="calibre_link-2575"></a>controlled independently, using the <code class="literal">mapreduce.map.failures.maxpercent</code> and
      <code class="literal">mapreduce.reduce.failures.maxpercent</code>
      properties.</p><p class="calibre2">A task attempt may also be <em class="calibre10">killed</em>, which is
      different from it failing. A task attempt may be killed because it is a
      speculative duplicate (for more information on this topic, see <a class="ulink" href="#calibre_link-497" title="Speculative Execution">Speculative Execution</a>), or because the node manager it was
      running on failed and the application master marked all the task
      attempts running on it as killed. Killed task attempts do not count against the number of attempts
      to run the task (as set by <code class="literal">mapreduce.map.maxattempts</code> and <code class="literal">mapreduce.reduce.maxattempts</code>), because it
      wasn’t the task’s fault that an attempt was killed.</p><p class="calibre2">Users may also kill or fail task attempts using the web UI or the
      command line (type <code class="literal">mapred job</code> to see
      the options). Jobs may be killed by the same mechanisms.</p></div><div class="book" title="Application Master Failure"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-503">Application Master Failure</h3></div></div></div><p class="calibre2">Just like MapReduce tasks are <a class="calibre" id="calibre_link-1607"></a><a class="calibre" id="calibre_link-907"></a>given several attempts to succeed (in the face of hardware
      or network failures), applications in YARN are retried in the event of
      failure. The maximum number of
      attempts to run a MapReduce application master is controlled by the
      <code class="literal">mapreduce.am.max-attempts</code> property.
      The <a class="calibre" id="calibre_link-2525"></a>default value is 2, so if a MapReduce application master
      fails twice it will not be tried again and the job will fail.</p><p class="calibre2">YARN imposes a limit for the maximum number of <a class="calibre" id="calibre_link-3830"></a>attempts for any YARN application master running on the
      cluster, and individual applications may not exceed this limit. The
      limit is <a class="calibre" id="calibre_link-3882"></a>set by <code class="literal">yarn.resourcemanager.am.max-attempts</code> and
      defaults to 2, so if you want to increase the number of MapReduce
      application master attempts, you will have to increase the YARN setting
      on the cluster, too.</p><p class="calibre2">The way recovery works is as follows. An application master sends
      periodic heartbeats to the resource manager, and in the event of
      application master failure, the resource manager will detect the failure
      and start a new instance of the master running in a new container
      (managed by a node manager). In the case of
      the MapReduce application master, it will use the job history to recover
      the state of the tasks that were already run by the (failed) application
      so they don’t have to be rerun.
      Recovery is enabled by default, but can be disabled by <a class="calibre" id="calibre_link-3853"></a>setting <code class="literal">yarn.app.mapreduce.am.job.recovery.enable</code> to
      <code class="literal">false</code>.</p><p class="calibre2">The MapReduce client polls the application master for progress
      reports, but if its application master fails,
      the client needs to locate the new instance. During job initialization,
      the client asks the <a class="calibre" id="calibre_link-3219"></a>resource manager for the application master’s address, and
      then caches it so it doesn’t overload the resource manager with a
      request every time it needs to poll the application master. If the
      application master fails, however, the client will experience a timeout
      when it issues a status update, at which point the client will go back
      to the resource manager to ask for the new application master’s
      address. This process is
      transparent to the user.</p></div><div class="book" title="Node Manager Failure"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4215">Node Manager Failure</h3></div></div></div><p class="calibre2">If a node manager fails by <a class="calibre" id="calibre_link-1608"></a><a class="calibre" id="calibre_link-2799"></a>crashing or running very slowly, it will stop sending
      heartbeats to the resource manager (or send them very infrequently). The
      resource manager will notice a node manager that has stopped sending
      heartbeats if it hasn’t received one for 10 minutes (this is configured,
      in milliseconds, via <a class="calibre" id="calibre_link-3889"></a>the <code class="literal">yarn.resourcemanager.nm.liveness-monitor.expiry-interval-ms</code>
      property) and remove it from its pool of nodes to schedule containers
      on.</p><p class="calibre2">Any task or <a class="calibre" id="calibre_link-911"></a>application master running on the failed node manager will
      be recovered using the mechanisms described in the previous two
      sections. In addition, the application master arranges for map tasks
      that were run and completed successfully on the failed node manager to
      be rerun if they belong to incomplete jobs, since their intermediate
      output residing on the failed node manager’s local filesystem may not be
      accessible to the reduce task.</p><p class="calibre2">Node managers may be <em class="calibre10">blacklisted</em> if the
      <a class="calibre" id="calibre_link-2794"></a><a class="calibre" id="calibre_link-1007"></a>number of failures for the application is high, even if
      the node manager itself has not failed. Blacklisting is done by the
      application master, and for MapReduce the application master will try to
      reschedule tasks on different nodes if more than three tasks fail on a
      node manager. The user may set the threshold with <a class="calibre" id="calibre_link-2551"></a>the <code class="literal">mapreduce.job.maxtaskfailures.per.tracker</code> job
      property.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Note that the r<a class="calibre" id="calibre_link-3230"></a>esource manager does not do blacklisting across
        applications (at the time of writing), so tasks from new jobs may be
        scheduled on bad nodes even if they have been blacklisted by an
        application master running an earlier job.</p></div></div><div class="book" title="Resource Manager Failure"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4216">Resource Manager Failure</h3></div></div></div><p class="calibre2">Failure of the resource <a class="calibre" id="calibre_link-1609"></a><a class="calibre" id="calibre_link-3225"></a>manager is serious, because without it, neither jobs nor
      task containers can be launched. In the default configuration, the
      resource manager is a single point of failure, since in the (unlikely)
      event of machine failure, all running jobs fail—and can’t be
      recovered.</p><p class="calibre2">To achieve high <a class="calibre" id="calibre_link-1994"></a>availability (HA), it is necessary to run a pair of
      resource managers in an active-standby configuration. If
      the active resource manager fails, then the standby can take over
      without a significant interruption to the client.</p><p class="calibre2">Information about all the running applications is stored in a
      highly available state store (backed by ZooKeeper or HDFS), so that the
      standby can recover the core state of the failed active resource
      manager. Node manager information is not stored in the state store since
      it can be reconstructed relatively quickly by the new resource manager
      as the node managers send their first heartbeats. (Note also that tasks
      are not part of the resource manager’s state, since they are managed by
      the application master. Thus, the amount of state to be stored is
      therefore much more manageable than that of the jobtracker in MapReduce
      1.)</p><p class="calibre2">When the new resource manager starts, it reads the application
      information from the state store, then restarts the <a class="calibre" id="calibre_link-913"></a>application masters for all the applications running on
      the cluster. This does not count as a failed application attempt (so it
      does not count against <code class="literal">yarn.resourcemanager.am.max-attempts</code>), since
      <a class="calibre" id="calibre_link-3883"></a>the application did not fail due to an error in the
      application code, but was forcibly killed by the system.
      In practice, the application master restart is not an issue for
      MapReduce applications since they recover the work done by completed
      tasks (as we saw in <a class="ulink" href="#calibre_link-503" title="Application Master Failure">Application Master Failure</a>).</p><p class="calibre2">The transition of a resource manager from standby to active is
      handled by a <a class="calibre" id="calibre_link-1602"></a>failover controller. The default failover controller is an
      automatic one, which uses ZooKeeper leader election to ensure that there
      is only a single active resource manager at one time. Unlike in HDFS HA
      (see <a class="ulink" href="#calibre_link-21" title="HDFS High Availability">HDFS High Availability</a>), the failover controller does not have
      to be a standalone process, and is embedded in the resource manager by
      default for ease of configuration. It is also possible to configure
      manual failover, but this is not recommended.</p><p class="calibre2">Clients and <a class="calibre" id="calibre_link-2802"></a>node managers must be configured to handle resource
      manager failover, since there are now two possible resource managers to
      communicate with. They try connecting to each resource manager in a
      round-robin fashion until they find the active one. If the active fails,
      then they will retry until the standby becomes <a class="calibre" id="calibre_link-2475"></a>active.</p></div></div><div class="book" title="Shuffle and Sort"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-46">Shuffle and Sort</h2></div></div></div><p class="calibre2">MapReduce makes <a class="calibre" id="calibre_link-2497"></a><a class="calibre" id="calibre_link-3395"></a><a class="calibre" id="calibre_link-3442"></a>the guarantee that the input to every reducer is sorted by
    key. The process by which the system performs the sort—and transfers the
    map outputs to the reducers as inputs—is known as the
    <em class="calibre10">shuffle.</em><sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-504" id="calibre_link-519">54</a>]</sup> In this section, we look at how the shuffle works, as a
    basic understanding will be helpful should you need to optimize a
    MapReduce program. The shuffle is an area of the codebase where
    refinements and improvements are
    continually being made, so the following description necessarily conceals
    many details. In many ways, the shuffle is the heart of MapReduce and is
    where the “magic” happens.</p><div class="book" title="The Map Side"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4217">The Map Side</h3></div></div></div><p class="calibre2">When the map function <a class="calibre" id="calibre_link-2398"></a><a class="calibre" id="calibre_link-3398"></a><a class="calibre" id="calibre_link-2662"></a>starts producing output, it is not simply written to disk.
      The process is more involved, and takes advantage of buffering writes in
      memory and doing some presorting for efficiency reasons. <a class="ulink" href="#calibre_link-505" title="Figure&nbsp;7-4.&nbsp;Shuffle and sort in MapReduce">Figure&nbsp;7-4</a> shows what happens.</p><div class="book"><div class="figure"><a id="calibre_link-505" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4218" class="calibre"></a><img alt="Shuffle and sort in MapReduce" src="images/000047.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;7-4.&nbsp;Shuffle and sort in MapReduce</div></div></div><p class="calibre2">Each map task has a circular memory buffer that it writes the
      output to. The <a class="calibre" id="calibre_link-2633"></a>buffer is 100 MB by default
      (the size can be tuned by changing the <code class="literal">mapreduce</code><code class="literal">.task.io.sort.mb</code> property). When the contents
      of the buffer reach a certain threshold size (<code class="literal">mapreduce.map.sort.spill.percent</code>, <a class="calibre" id="calibre_link-2592"></a>which has the default value 0.80, or 80%), a background
      thread will start to <em class="calibre10">spill</em> the contents to disk.
      Map outputs will continue to be written to the buffer while the spill
      takes place, but if the buffer fills up during this time, the map will
      block until the spill is complete. Spills
      are written in round-robin fashion to the directories specified by the
      <code class="literal">mapreduce.cluster.local.dir</code> property,
      <a class="calibre" id="calibre_link-2530"></a>in a job-specific subdirectory.</p><p class="calibre2">Before it writes to disk, the thread first divides the data into
      partitions corresponding to the reducers that they will ultimately be
      sent to. Within each partition, the background thread performs an
      in-memory sort by key, and if there is a combiner function, it is run on
      the output of the sort. Running the combiner function makes for a more
      compact map output, so there is less data to write to local disk and to
      transfer to the reducer.</p><p class="calibre2">Each time the memory buffer reaches the spill threshold, a new
      spill file is created, so after the map task has written its last output
      record, there could be several spill files. Before the task is finished,
      the spill files are merged into a single partitioned and sorted output
      file. The configuration property <code class="literal">mapreduce.task.io.sort.factor</code> controls the
      <a class="calibre" id="calibre_link-2630"></a>maximum number of streams to merge at once; the default is
      10.</p><p class="calibre2">If there are at least three spill files (set by <a class="calibre" id="calibre_link-2571"></a>the <code class="literal">mapreduce.map.combine.minspills</code> property), the
      combiner is run again before the output file is written. Recall that
      combiners may be run repeatedly over the input without affecting the
      final result. If there are only one or two spills, the potential
      reduction in map output size is not worth the overhead in invoking the
      combiner, so it is not run again for this map output.</p><p class="calibre2">It is often a good idea to compress the <a class="calibre" id="calibre_link-1210"></a>map output as it is written to disk, because doing so
      makes it faster to write to disk, saves disk space, and reduces the
      amount of data to transfer to the reducer. By default, the output is not
      compressed, but it is easy <a class="calibre" id="calibre_link-2585"></a>to enable this by setting <code class="literal">mapreduce.map.output.compress</code> to <code class="literal">true</code>. The compression <a class="calibre" id="calibre_link-2588"></a>library to use is specified by <code class="literal">mapreduce.map.output.compress.codec</code>; see <a class="ulink" href="#calibre_link-506" title="Compression">Compression</a> for more on compression formats.</p><p class="calibre2">The output file’s partitions are made available to the reducers
      over HTTP. The maximum number of worker threads used to serve the file
      partitions is controlled by <a class="calibre" id="calibre_link-2622"></a>the <code class="literal">mapreduce.shuffle.max.threads</code> property; this
      setting is per node manager, not per map task. The default of 0 sets the
      maximum number of threads to twice the number of processors on the
      <a class="calibre" id="calibre_link-2399"></a><a class="calibre" id="calibre_link-3399"></a>machine.</p></div><div class="book" title="The Reduce Side"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4219">The Reduce Side</h3></div></div></div><p class="calibre2">Let’s turn now to the <a class="calibre" id="calibre_link-3186"></a><a class="calibre" id="calibre_link-3400"></a>reduce part of the process. The map output file is sitting
      on the local disk of the machine that ran the map task (note that
      although map outputs always get written to local disk, reduce outputs
      may not be), but now it is needed by the machine that is about to run
      the reduce task for the partition. Moreover, the reduce task needs the
      map output for its particular partition from several map tasks across
      the cluster. The map tasks may finish at different times, so the reduce
      task starts copying their outputs as soon as each completes. This is
      known as the <em class="calibre10">copy phase</em> of the reduce task. The
      reduce task has a small number of copier threads so that it can fetch
      map outputs in parallel. The default is five threads, but this number
      can be changed by <a class="calibre" id="calibre_link-2619"></a>setting the <code class="literal">mapreduce.reduce.shuffle.parallelcopies</code>
      property.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">How do reducers know which machines to fetch map output
        from?</p><p class="calibre2">As map tasks complete successfully, they notify their
        application master using the heartbeat mechanism. Therefore, for a
        given job, the application master knows the mapping between map
        outputs and hosts. A thread in the reducer periodically asks the
        master for map output hosts until it has retrieved them all.</p><p class="calibre2">Hosts do not delete map outputs from disk as soon as the first
        reducer has retrieved them, as the reducer may subsequently fail.
        Instead, they wait until they are told to delete them by the
        application master, which is after the job has completed.</p></div><p class="calibre2">Map outputs are copied to the reduce task JVM’s memory if they are
      small enough (the buffer’s size is controlled <a class="calibre" id="calibre_link-2614"></a>by <code class="literal">mapreduce.reduce.shuffle.input.buffer.percent</code>,
      which specifies the proportion of the heap to use for this purpose);
      otherwise, they are copied to disk.
      When the in-memory buffer reaches a threshold size (controlled by
      <code class="literal">mapreduce.reduce.shuffle.merge.percent</code>) or
      reaches a <a class="calibre" id="calibre_link-2617"></a>threshold number of map outputs (<code class="literal">mapreduce.reduce.merge.inmem.threshold</code>), it is
      <a class="calibre" id="calibre_link-2611"></a>merged and spilled to disk.
      If a combiner is <a class="calibre" id="calibre_link-1182"></a>specified, it will be run during the merge to reduce the
      amount of data written to disk.</p><p class="calibre2">As the copies accumulate on disk, a background thread merges them
      into larger, sorted files. This saves some time merging later on.
      Note that any map outputs that were compressed (by the map task) have to
      be decompressed in memory in order to perform a merge on them.</p><p class="calibre2">When all the map outputs have been copied, the reduce task moves
      into the <em class="calibre10">sort phase</em> (which should properly be
      called the <span class="calibre">merge</span> phase, as the
      sorting was carried out on the map side), which merges the map outputs,
      maintaining their sort ordering. This is done in rounds. For example, if
      there were 50 map outputs and the <em class="calibre10">merge factor</em>
      was 10 (the default, controlled by <a class="calibre" id="calibre_link-2631"></a>the <code class="literal">mapreduce.task.io.sort.factor</code> property, just
      like in the map’s merge), there would be five rounds. Each round would
      merge 10 files into 1, so at the end there would be 5 intermediate
      files.</p><p class="calibre2">Rather than have a final round that merges these five files into a
      single sorted file, the merge saves a trip to disk by directly feeding
      the reduce function in what is the last phase: the <em class="calibre10">reduce
      phase</em>. This final merge can come from a mixture of in-memory
      and on-disk segments.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">The number of files merged in each round is actually more subtle
        than this example suggests. The goal is to merge the minimum number of
        files to get to the merge factor for the final round. So if there were
        40 files, the merge would not merge 10 files in each of the four
        rounds to get 4 files. Instead, the first round would merge only 4
        files, and the subsequent three rounds would merge the full 10 files.
        The 4 merged files and the 6 (as yet unmerged) files make a total of
        10 files for the final round. The process is illustrated in <a class="ulink" href="#calibre_link-507" title="Figure&nbsp;7-5.&nbsp;Efficiently merging 40 file segments with a merge factor of 10">Figure&nbsp;7-5</a>.</p><p class="calibre2">Note that this does not change the number of rounds; it’s just
        an optimization to minimize the amount of data that is written to
        disk, since the final round always merges directly into the
        reduce.</p></div><div class="figure"><a id="calibre_link-507" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4220" class="calibre"></a><img alt="Efficiently merging 40 file segments with a merge factor of 10" src="images/000055.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;7-5.&nbsp;Efficiently merging 40 file segments with a merge factor of
        10</div></div><p class="calibre2">During the reduce phase, the reduce function is invoked for each
      key in the sorted output. The output of this phase is written directly
      to the output filesystem, typically HDFS. In the case of HDFS, because
      the node manager is also running a datanode, the first block replica
      will be written to the local <a class="calibre" id="calibre_link-3187"></a><a class="calibre" id="calibre_link-3401"></a>disk.</p></div><div class="book" title="Configuration Tuning"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-699">Configuration Tuning</h3></div></div></div><p class="calibre2">We are now in a better <a class="calibre" id="calibre_link-3396"></a>position to understand how to tune the shuffle to improve
      MapReduce performance. The relevant settings, which can be used on a
      per-job basis (except where noted), are summarized in Tables <a class="ulink" href="#calibre_link-508" title="Table&nbsp;7-1.&nbsp;Map-side tuning properties">7-1</a> and
      <a class="ulink" href="#calibre_link-509" title="Table&nbsp;7-2.&nbsp;Reduce-side tuning properties">7-2</a>, along with the defaults, which are
      good for general-purpose jobs.</p><p class="calibre2">The general principle is to give the shuffle as much memory as
      possible. However, there is a trade-off, in that you need to make sure
      that your map and reduce functions get enough memory to operate. This is
      why it is best to write your map and reduce functions to use as little
      memory as possible—certainly they should not use an unbounded amount of
      memory (avoid accumulating values in a map, for example).</p><p class="calibre2">The amount of memory given to the JVMs in which the map and reduce
      tasks run is set by the <code class="literal">mapred.child.java.opts</code> property. <a class="calibre" id="calibre_link-2422"></a>You should try to make this as large as possible for the
      amount of memory on your task nodes; the discussion in <a class="ulink" href="#calibre_link-35" title="Memory settings in YARN and MapReduce">Memory settings in YARN and MapReduce</a> goes through the constraints to
      consider.</p><p class="calibre2">On the map side, the best performance can be obtained by avoiding
      multiple spills to disk; one is optimal. If you can estimate the size of
      your map outputs, you can set the <code class="literal">mapreduce.task.io.sort.*</code> properties
      appropriately to minimize the number of spills. In particular, you
      should increase <code class="literal">mapreduce.task.io.sort.mb</code> if <a class="calibre" id="calibre_link-2634"></a>you can. There is a MapReduce counter
      (<code class="literal">SPILLED_RECORDS</code>; see <a class="ulink" href="#calibre_link-12" title="Counters">Counters</a>)
      that counts the total number of records that were spilled to disk over
      the course of a job, which can be useful for tuning. Note that the
      counter includes both map- and reduce-side spills.</p><p class="calibre2">On the reduce side, the best performance is obtained when the
      intermediate data can reside entirely in memory. This does not happen by
      default, since for the general case all the memory is reserved for the
      reduce function. But if your reduce function has light memory
      requirements, setting <code class="literal">mapreduce.reduce.merge.inmem.threshold</code> to 0
      <a class="calibre" id="calibre_link-2612"></a>and <code class="literal">mapreduce.reduce.input.buffer.percent</code> to 1.0
      (<a class="calibre" id="calibre_link-2604"></a>or a lower value; see <a class="ulink" href="#calibre_link-509" title="Table&nbsp;7-2.&nbsp;Reduce-side tuning properties">Table&nbsp;7-2</a>) may bring a performance
      boost.</p><p class="calibre2">In April 2008, Hadoop won the general-purpose terabyte sort
      benchmark (as discussed in <a class="ulink" href="#calibre_link-67" title="A Brief History of Apache Hadoop">A Brief History of Apache Hadoop</a>), and one of
      the optimizations used was keeping the intermediate data in memory on
      the reduce side.</p><p class="calibre2">More generally, Hadoop uses a buffer size of 4 KB by default,
      which is low, so you should increase this across the cluster (by setting
      <code class="literal">io.file.buffer.size</code>; see also <a class="ulink" href="#calibre_link-510" title="Other Hadoop Properties">Other Hadoop Properties</a>).</p><div class="table"><a id="calibre_link-508" class="calibre"></a><div class="table-title">Table&nbsp;7-1.&nbsp;Map-side tuning properties</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Type</td><td class="calibre20">Default
              value</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.task.io.sort.mb</code></td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">100</td><td class="calibre25">The size, <a class="calibre" id="calibre_link-2407"></a><a class="calibre" id="calibre_link-3068"></a>in megabytes, of the memory buffer to use while
              sorting map output.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.map.sort.spill.percent</code></td><td class="calibre23"><code class="uri">float</code></td><td class="calibre23">0.80</td><td class="calibre25">The <a class="calibre" id="calibre_link-2593"></a>threshold usage proportion for both the map output
              memory buffer and the record boundaries index to start the
              process of spilling to disk.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.task.io.sort.factor</code></td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">10</td><td class="calibre25">The <a class="calibre" id="calibre_link-2632"></a>maximum number of streams to merge at once when
              sorting files. This property is also used in the reduce. It’s
              fairly common to increase this to 100.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.map.combine.minspills</code></td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">3</td><td class="calibre25">The <a class="calibre" id="calibre_link-2572"></a>minimum number of spill files needed for the
              combiner to run (if a combiner is specified).</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.map.output.compress</code></td><td class="calibre23"><code class="uri">boolean</code></td><td class="calibre23"><code class="uri">false</code></td><td class="calibre25">Whether to compress <a class="calibre" id="calibre_link-2586"></a>map outputs.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.map.output.compress.codec</code></td><td class="calibre23"><code class="uri">Class</code> name</td><td class="calibre23"><code class="uri">org.apache.hadoop.io.compress.DefaultCodec</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-2589"></a>compression codec to use for map outputs.</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">mapreduce.shuffle.max.threads</code></td><td class="calibre27"><code class="uri">int</code></td><td class="calibre27">0</td><td class="calibre28">The <a class="calibre" id="calibre_link-2623"></a>number of worker threads per node manager for
              serving the map outputs to reducers. This is a cluster-wide
              setting and cannot be set by individual jobs. 0 means use the
              Netty default of twice the number of available
              processors.</td></tr></tbody></table></div></div><div class="table"><a id="calibre_link-509" class="calibre"></a><div class="table-title">Table&nbsp;7-2.&nbsp;Reduce-side tuning properties</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Type</td><td class="calibre20">Default
              value</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.reduce.shuffle.parallelcopies</code></td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">5</td><td class="calibre25">The number <a class="calibre" id="calibre_link-3194"></a><a class="calibre" id="calibre_link-2620"></a><a class="calibre" id="calibre_link-3071"></a>of threads used to copy map outputs to the
              reducer.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.reduce.shuffle.maxfetchfailures</code></td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">10</td><td class="calibre25">The <a class="calibre" id="calibre_link-2616"></a>number of times a reducer tries to fetch a map
              output before reporting the error.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.task.io.sort.factor</code></td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">10</td><td class="calibre25">The maximum number of streams to merge at once when
              sorting files. This property is also used in the map.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.reduce.shuffle.input.buffer.percent</code></td><td class="calibre23"><code class="uri">float</code></td><td class="calibre23">0.70</td><td class="calibre25">The <a class="calibre" id="calibre_link-2615"></a>proportion of total heap size to be allocated to
              the map outputs buffer
              during the copy phase of the shuffle.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.reduce.shuffle.merge.percent</code></td><td class="calibre23"><code class="uri">float</code></td><td class="calibre23">0.66</td><td class="calibre25">The <a class="calibre" id="calibre_link-2618"></a>threshold usage proportion for the map outputs
              buffer (defined by <code class="uri">mapred.job.shuffle</code><code class="uri">.</code><code class="uri">input.buffer.percent</code>) for starting the
              process of merging the outputs and spilling to disk.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.reduce.merge.inmem.threshold</code></td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">1000</td><td class="calibre25">The <a class="calibre" id="calibre_link-2613"></a>threshold number of map outputs for starting the
              process of merging the outputs and spilling to disk. A value of 0 or less means
              there is no threshold, and the
              spill behavior is governed solely by <code class="uri">mapreduce.reduce.shuffle.merge.percent</code>.</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">mapreduce.reduce.input.buffer.percent</code></td><td class="calibre27"><code class="uri">float</code></td><td class="calibre27">0.0</td><td class="calibre28">The <a class="calibre" id="calibre_link-2605"></a>proportion of total heap size to be used for
              retaining map outputs in memory during the reduce. For the
              reduce phase to begin, the size of map outputs in memory must be
              no more than this size. By default, all map outputs are merged
              to disk before the reduce begins, to give the reducers as much
              memory as possible. However, if your reducers require less
              memory, this value may be increased to minimize the number of
              trips to <a class="calibre" id="calibre_link-2498"></a><a class="calibre" id="calibre_link-3397"></a><a class="calibre" id="calibre_link-3443"></a>disk.</td></tr></tbody></table></div></div></div></div><div class="book" title="Task Execution"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4221">Task Execution</h2></div></div></div><p class="calibre2">We saw how the <a class="calibre" id="calibre_link-2513"></a><a class="calibre" id="calibre_link-3630"></a>MapReduce system executes tasks in the context of the
    overall job at the beginning of this chapter, in <a class="ulink" href="#calibre_link-52" title="Anatomy of a MapReduce Job Run">Anatomy of a MapReduce Job Run</a>. In this section, we’ll look at
    some more controls that MapReduce users have over task execution.</p><div class="book" title="The Task Execution Environment"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-589">The Task Execution Environment</h3></div></div></div><p class="calibre2">Hadoop provides information to a map or reduce task about the
      environment in which it is running. For example, a map task can discover
      the name of the file it is processing (see <a class="ulink" href="#calibre_link-511" title="File information in the mapper">File information in the mapper</a>), and a map or reduce task can
      find out the attempt number of the task.
      The properties in <a class="ulink" href="#calibre_link-512" title="Table&nbsp;7-3.&nbsp;Task environment properties">Table&nbsp;7-3</a> can be
      accessed from the job’s configuration, obtained in the old MapReduce API
      by providing an implementation of the
      <code class="literal">configure()</code> method for <code class="literal">Mapper</code> or <code class="literal">Reducer</code>, where the configuration is passed in
      as an argument. In the new API, <a class="calibre" id="calibre_link-2416"></a><a class="calibre" id="calibre_link-3196"></a>these properties can be accessed from the context object
      passed to all methods of the <code class="literal">Mapper</code>
      or <code class="literal">Reducer</code>.</p><div class="table"><a id="calibre_link-512" class="calibre"></a><div class="table-title">Table&nbsp;7-3.&nbsp;Task environment properties</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Type</td><td class="calibre20">Description</td><td class="calibre21">Example</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.job.id</code></td><td class="calibre23"><code class="uri">String</code></td><td class="calibre23">The <a class="calibre" id="calibre_link-2548"></a><a class="calibre" id="calibre_link-2218"></a>job ID (see <a class="ulink" href="#calibre_link-513" title="Job, Task, and Task Attempt IDs">Job, Task, and Task Attempt IDs</a> for a
              description of the format)</td><td class="calibre25"><code class="uri">job_200811201130_0004</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.task.id</code></td><td class="calibre23"><code class="uri">String</code></td><td class="calibre23">The <a class="calibre" id="calibre_link-2629"></a><a class="calibre" id="calibre_link-3626"></a>task ID</td><td class="calibre25"><code class="uri">task_200811201130_0004_m_000003</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.task.attempt.id</code></td><td class="calibre23"><code class="uri">String</code></td><td class="calibre23">The <a class="calibre" id="calibre_link-2626"></a><a class="calibre" id="calibre_link-3621"></a>task attempt ID</td><td class="calibre25"><code class="uri">attempt_200811201130</code><code class="uri">_0004_m_000003_0</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.task.partition</code></td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">The <a class="calibre" id="calibre_link-2637"></a>index of the task within the job</td><td class="calibre25"><code class="uri">3</code></td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">mapreduce.task.ismap</code></td><td class="calibre27"><code class="uri">boolean</code></td><td class="calibre27">Whether <a class="calibre" id="calibre_link-2635"></a>this task is a map task</td><td class="calibre28"><code class="uri">true</code></td></tr></tbody></table></div></div><div class="book" title="Streaming environment variables"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4222">Streaming environment variables</h4></div></div></div><p class="calibre2">Hadoop sets job configuration parameters as environment
        variables for Streaming programs. However, it replaces nonalphanumeric
        characters with underscores to make sure they are valid names. The
        following Python expression illustrates how you can retrieve the value
        of the <code class="literal">mapreduce.job.id</code> property
        from within a Python Streaming script:</p><a id="calibre_link-4223" class="calibre"></a><pre class="screen1"><code class="n">os</code><code class="o">.</code><code class="n">environ</code><code class="p">[</code><code class="sb">"mapreduce_job_id"</code><code class="p">]</code></pre><p class="calibre2">You can also set environment variables for the Streaming
        processes launched by MapReduce by supplying the <code class="literal">-cmdenv</code> option to the Streaming launcher
        program (once for each variable you wish to set). For example, the
        following sets the <code class="literal">MAGIC_PARAMETER</code>
        environment variable:</p><a id="calibre_link-4224" class="calibre"></a><pre class="screen1">-cmdenv MAGIC_PARAMETER=abracadabra</pre></div></div><div class="book" title="Speculative Execution"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-497">Speculative Execution</h3></div></div></div><p class="calibre2">The MapReduce model is to break jobs into tasks and run the tasks
      in parallel to make the overall job execution time smaller than it would
      be if the tasks ran sequentially. This makes the job execution time
      sensitive to slow-running tasks, as it takes only one slow task to make
      the whole job take significantly longer than it would have done
      otherwise. When a job consists of hundreds or thousands of tasks, the
      possibility of a few straggling tasks is very real.</p><p class="calibre2">Tasks may be slow for various reasons, including hardware
      degradation or software misconfiguration, but the causes may be hard to
      detect because the tasks still complete successfully, albeit after a
      longer time than expected. Hadoop doesn’t try to diagnose and fix
      slow-running tasks; instead, it tries to detect when a task is running
      slower than expected and launches another equivalent task as a backup.
      This is <a class="calibre" id="calibre_link-3476"></a><a class="calibre" id="calibre_link-3639"></a>termed <em class="calibre10">speculative execution</em> of
      tasks.</p><p class="calibre2">It’s important to understand that speculative execution does not
      work by launching two duplicate tasks at about the same time so they can
      race each other. This would be wasteful of cluster resources. Rather,
      the scheduler tracks the progress of all tasks of the same type (map and
      reduce) in a job, and only launches speculative duplicates for the small
      proportion that are running significantly slower than the average.
      When a task completes successfully, any duplicate tasks that are running
      are killed since they are no longer needed. So, if the original task
      completes before the speculative task, the speculative task is killed;
      on the other hand, if the speculative task finishes first, the original
      is killed.</p><p class="calibre2">Speculative execution is an optimization, and not a feature to
      make jobs run more reliably.
      If there are bugs that sometimes cause a task to hang or slow down,
      relying on speculative execution to avoid these problems is unwise and
      won’t work reliably, since the same bugs are likely to affect the
      speculative task. You should fix the bug so that the task doesn’t hang
      or slow down.</p><p class="calibre2">Speculative execution is turned on by default. It can be enabled
      or disabled independently for map tasks and reduce tasks, on a
      cluster-wide basis, or on a per-job basis. The relevant properties are
      shown in <a class="ulink" href="#calibre_link-514" title="Table&nbsp;7-4.&nbsp;Speculative execution properties">Table&nbsp;7-4</a>.</p><div class="table"><a id="calibre_link-514" class="calibre"></a><div class="table-title">Table&nbsp;7-4.&nbsp;Speculative execution properties</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Type</td><td class="calibre20">Default value</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.map.</code><code class="uri">speculative</code></td><td class="calibre23"><code class="uri">boolean</code></td><td class="calibre23"><code class="uri">true</code></td><td class="calibre25">Whether <a class="calibre" id="calibre_link-2594"></a>extra instances of map tasks may be launched if a
              task is making slow progress</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.reduce.speculative</code></td><td class="calibre23"><code class="uri">boolean</code></td><td class="calibre23"><code class="uri">true</code></td><td class="calibre25">Whether<a class="calibre" id="calibre_link-2621"></a> extra instances of reduce tasks may be launched
              if a task is making slow progress</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">yarn.app.mapreduce.am.job.speculator.class</code></td><td class="calibre23"><code class="uri"> <code class="literal2">Class</code> </code></td><td class="calibre23"><code class="uri">org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator</code></td><td class="calibre25">The <code class="uri">Speculator</code> class
              implementing <a class="calibre" id="calibre_link-3854"></a>the speculative execution policy (MapReduce 2
              only)</td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">yarn.app.mapreduce.am.job.task.estimator.class</code></td><td class="calibre27"><code class="uri"> <code class="literal2">Class</code> </code></td><td class="calibre27"><code class="uri">org.apache.hadoop.mapreduce.v2.app.speculate.LegacyTaskRuntimeEstimator</code></td><td class="calibre28">An implementation of <code class="uri">TaskRuntimeEstimator</code> used by <code class="uri">Speculator</code> instances that provides
              <a class="calibre" id="calibre_link-3855"></a>estimates for task runtimes (MapReduce 2
              only)</td></tr></tbody></table></div></div><p class="calibre2">Why would you ever want to turn speculative execution off? The
      goal of speculative execution is to reduce job execution time, but this
      comes at the cost of cluster efficiency. On a busy cluster, speculative
      execution can reduce overall throughput, since redundant tasks are being
      executed in an attempt to bring down the execution time for a single
      job. For this reason, some cluster administrators prefer to turn it off
      on the cluster and have users explicitly turn it on for individual jobs.
      This was especially relevant for older versions of Hadoop, when
      speculative execution could be overly aggressive in scheduling
      speculative tasks.</p><p class="calibre2">There is a good case for turning off speculative execution for
      reduce tasks, since any duplicate reduce tasks have to fetch the same
      map outputs as the original task, and this can significantly increase
      network traffic on the cluster.</p><p class="calibre2">Another reason for turning off speculative execution is for
      nonidempotent tasks. However, in many cases it is possible to write
      tasks to be idempotent and use an <code class="literal">OutputCommitter</code> to promote the output
      to its final location when the task succeeds. This technique is
      explained in more detail in the next <a class="calibre" id="calibre_link-3477"></a><a class="calibre" id="calibre_link-3640"></a>section.</p></div><div class="book" title="Output Committers"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-493">Output Committers</h3></div></div></div><p class="calibre2">Hadoop MapReduce uses a <a class="calibre" id="calibre_link-2883"></a>commit protocol to ensure that jobs and tasks either
      succeed or fail cleanly. The behavior is implemented by the <code class="literal">OutputCommitter</code> in use for the job, which is
      set in the old MapReduce API by calling the <code class="literal">setOutputCommitter()</code> on <code class="literal">JobConf</code> or by setting <code class="literal">mapred.output.committer.class</code> in the
      configuration. In the new MapReduce API, the <code class="literal">OutputCommitter</code> is determined by the <code class="literal">OutputFormat</code>, via <a class="calibre" id="calibre_link-2885"></a>its <code class="literal">getOutputCommitter()</code> method.
      The default is <code class="literal">FileOutputCommitter</code>,
      which is appropriate for file-based MapReduce. You can customize an
      existing <code class="literal">OutputCommitter</code> or even
      write a new implementation if you need to do special setup or cleanup
      for jobs or tasks.</p><p class="calibre2">The <code class="literal">OutputCommitter</code> API is as
      follows (in both the old and new MapReduce APIs):</p><a id="calibre_link-4225" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">abstract</code> <code class="k">class</code> <code class="nc">OutputCommitter</code> <code class="o">{</code>

  <code class="k">public</code> <code class="k">abstract</code> <code class="kt">void</code> <code class="nf">setupJob</code><code class="o">(</code><code class="n">JobContext</code> <code class="n">jobContext</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">;</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">commitJob</code><code class="o">(</code><code class="n">JobContext</code> <code class="n">jobContext</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code> <code class="o">}</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">abortJob</code><code class="o">(</code><code class="n">JobContext</code> <code class="n">jobContext</code><code class="o">,</code> <code class="n">JobStatus</code><code class="o">.</code><code class="na">State</code> <code class="n">state</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code> <code class="o">}</code>

  <code class="k">public</code> <code class="k">abstract</code> <code class="kt">void</code> <code class="nf">setupTask</code><code class="o">(</code><code class="n">TaskAttemptContext</code> <code class="n">taskContext</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">;</code>
  <code class="k">public</code> <code class="k">abstract</code> <code class="kt">boolean</code> <code class="nf">needsTaskCommit</code><code class="o">(</code><code class="n">TaskAttemptContext</code> <code class="n">taskContext</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">;</code>
  <code class="k">public</code> <code class="k">abstract</code> <code class="kt">void</code> <code class="nf">commitTask</code><code class="o">(</code><code class="n">TaskAttemptContext</code> <code class="n">taskContext</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">;</code>
  <code class="k">public</code> <code class="k">abstract</code> <code class="kt">void</code> <code class="nf">abortTask</code><code class="o">(</code><code class="n">TaskAttemptContext</code> <code class="n">taskContext</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">;</code>

  <code class="o">}</code>
<code class="o">}</code></pre><p class="calibre2">The <code class="literal">setupJob()</code> method is called before
      the job is run, and is typically used to perform initialization. For
      <code class="literal">FileOutputCommitter</code>, the <a class="calibre" id="calibre_link-1643"></a>method creates the final output directory, <code class="literal">${mapreduce.output.fileoutputformat.outputdir}</code>,
      and a temporary working space for task output,
      <span class="calibre"><em class="calibre10">_temporary</em></span>, as a subdirectory underneath it.</p><p class="calibre2">If the job succeeds, the <code class="literal">commitJob()</code>
      method is called, which in the default file-based implementation deletes
      the temporary working space and creates a hidden empty marker file in
      the output directory called <em class="calibre10">_SUCCESS</em> to indicate to filesystem clients
      that the job completed successfully. If the job did not succeed,
      <code class="literal">abortJob()</code> is called with a state object
      indicating whether the job failed or was killed (by a user, for
      example). In the default implementation, this will delete the job’s
      temporary working space.</p><p class="calibre2">The operations are similar at the task level. The
      <code class="literal">setupTask()</code> method is called before the task is
      run, and the default implementation doesn’t do anything, because
      temporary directories named for task outputs are created when the task
      outputs are written.</p><p class="calibre2">The commit phase for tasks is optional and may be disabled by
      returning <code class="literal">false</code> from
      <code class="literal">needsTaskCommit()</code>. This saves the framework
      from having to run the distributed commit protocol for the task, and
      neither <code class="literal">commitTask()</code> nor
      <code class="literal">abortTask()</code> is called. <code class="literal">FileOutputCommitter</code> will skip the commit phase
      when no output has been written by a task.</p><p class="calibre2">If a task succeeds, <code class="literal">commitTask()</code> is
      called, which in the default implementation moves the temporary task
      output directory (which has the task attempt ID in its name to avoid
      conflicts between task attempts) to the final output path, <code class="literal">${mapreduce.output.fileoutputformat.outputdir}</code>.
      Otherwise, the framework calls <code class="literal">abortTask()</code>,
      which deletes the temporary task output directory.</p><p class="calibre2">The framework ensures that in the event of multiple task attempts
      for a particular task, only one will be committed; the others will be
      aborted. This situation may arise because the first attempt failed for
      some reason—in which case, it would be aborted, and a later, successful
      attempt would be committed. It can also occur if two task attempts were
      running concurrently as speculative duplicates; in this instance, the
      one that finished first would be committed, and the other would be
      aborted.</p><div class="book" title="Task side-effect files"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-706">Task side-effect files</h4></div></div></div><p class="calibre2">The usual way of writing output from map and reduce tasks is by
        <a class="calibre" id="calibre_link-2880"></a>using <code class="literal">OutputCollector</code>
        to collect key-value pairs. Some applications need more flexibility
        than a single key-value pair model, so these applications write output
        files directly from the map or reduce task to a distributed
        filesystem, such as HDFS. (There are other ways to produce multiple
        outputs, too, as described in <a class="ulink" href="#calibre_link-515" title="Multiple Outputs">Multiple Outputs</a>.)</p><p class="calibre2">Care needs to be taken to ensure that multiple instances of the
        same task don’t try to write to the same file. As we saw in the
        previous section, the <code class="literal">OutputCommitter</code> protocol solves this
        problem. If applications write side files in their tasks’ working
        directories, the side files for tasks that successfully complete will
        be promoted to the output directory automatically, whereas failed
        tasks will have their side files deleted.</p><p class="calibre2">A task may find its working directory by retrieving the
        <a class="calibre" id="calibre_link-2636"></a>value of the <code class="literal">mapreduce.task.output.dir</code> property from the
        job configuration. Alternatively, a MapReduce program using the Java
        API may call the <code class="literal">getWorkOutputPath()</code> static method <a class="calibre" id="calibre_link-1645"></a>on <code class="literal">FileOutputFormat</code>
        to get the <code class="literal">Path</code> object representing
        the working directory. The framework creates the working directory
        before executing the task, so you don’t need to create it.</p><p class="calibre2">To take a simple example, imagine a program for converting image
        files from one format to another. One way to do this is to have a
        map-only job, where each map is given a set of images to convert
        (perhaps using <code class="literal">NLineInputFormat</code>;
        see <a class="ulink" href="#calibre_link-473" title="NLineInputFormat">NLineInputFormat</a>). <a class="calibre" id="calibre_link-2790"></a>If a map task writes the converted images into its
        working directory, they will be promoted to the output directory when
        the task successfully <a class="calibre" id="calibre_link-2514"></a><a class="calibre" id="calibre_link-3631"></a><a class="calibre" id="calibre_link-2884"></a>finishes.</p></div></div></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" type="footnote" id="calibre_link-490"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-516">51</a>] </sup>In the old MapReduce API, you <a class="calibre" id="calibre_link-2226"></a>can call <code class="literal">JobClient.submitJob(conf)</code> or <code class="literal">JobClient.runJob(conf)</code>.</p></div><div class="footnote" type="footnote" id="calibre_link-492"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-517">52</a>] </sup>Not discussed in this section are the job history server daemon
        (for retaining job history data) and the shuffle handler auxiliary
        service (for serving map outputs to reduce tasks).</p></div><div class="footnote" type="footnote" id="calibre_link-501"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-518">53</a>] </sup>If a Streaming process hangs, the <a class="calibre" id="calibre_link-2806"></a>node manager will kill it (along with the JVM that
          launched it) only in the following circumstances: <a class="calibre" id="calibre_link-3861"></a>either <code class="literal">yarn.nodemanager.container-executor.class</code>
          is set to <code class="literal">org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</code>,
          or the default container executor is being used and the <code class="literal">setsid</code> command is available on the system
          (so that the task JVM and any processes it launches are in the same
          process group). In any other case, orphaned Streaming processes will
          accumulate on the system, which will impact utilization over
          time.</p></div><div class="footnote" type="footnote" id="calibre_link-504"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-519">54</a>] </sup>The term <span class="calibre">shuffle</span> is actually
        imprecise, since in some contexts it refers to only the part of the
        process where map outputs are fetched by reduce tasks. In this
        section, we take it to mean the whole process, from the point where a
        map produces output to where a reduce consumes input.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-568"><section type="chapter" id="calibre_link-4226" title="Chapter&nbsp;8.&nbsp;MapReduce Types and Formats"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;8.&nbsp;MapReduce Types and Formats</h2></div></div></div><p class="calibre2">MapReduce has a simple model of data processing: inputs and outputs
  for the map and reduce functions are key-value pairs. This chapter looks at
  the MapReduce model in detail, and in particular at how data in various
  formats, from simple text to structured binary objects, can be used with
  this model.</p><div class="book" title="MapReduce Types"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4227">MapReduce Types</h2></div></div></div><p class="calibre2">The map and reduce <a class="calibre" id="calibre_link-2515"></a><a class="calibre" id="calibre_link-2391"></a><a class="calibre" id="calibre_link-3180"></a>functions in Hadoop MapReduce have the following general
    form:</p><a id="calibre_link-4228" class="calibre"></a><pre class="screen1">map: (K1, V1) → list(K2, V2)
reduce: (K2, list(V2)) → list(K3, V3)</pre><p class="calibre2">In general, the map input key and value types (<code class="literal">K1</code> and <code class="literal">V1</code>)
    are different from the map output types (<code class="literal">K2</code> and <code class="literal">V2</code>).
    However, the reduce input must have the same types as the map output,
    although the reduce output types may be different again (<code class="literal">K3</code> and <code class="literal">V3</code>).
    The Java API mirrors this general form:</p><a id="calibre_link-4229" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">class</code> <code class="nc">Mapper</code><code class="o">&lt;</code><code class="n">KEYIN</code><code class="o">,</code> <code class="n">VALUEIN</code><code class="o">,</code> <code class="n">KEYOUT</code><code class="o">,</code> <code class="n">VALUEOUT</code><code class="o">&gt;</code> <code class="o">{</code>
  <code class="k">public</code> <code class="k">class</code> <code class="nc">Context</code> <code class="k">extends</code> <code class="n">MapContext</code><code class="o">&lt;</code><code class="n">KEYIN</code><code class="o">,</code> <code class="n">VALUEIN</code><code class="o">,</code> <code class="n">KEYOUT</code><code class="o">,</code> <code class="n">VALUEOUT</code><code class="o">&gt;</code> <code class="o">{</code>
    <code class="c2">// ...</code>
  <code class="o">}</code>
  <code class="k">protected</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">KEYIN</code> <code class="n">key</code><code class="o">,</code> <code class="n">VALUEIN</code> <code class="n">value</code><code class="o">,</code> 
      <code class="n">Context</code> <code class="n">context</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="c2">// ...</code>
  <code class="o">}</code>
<code class="o">}</code>

<code class="k">public</code> <code class="k">class</code> <code class="nc">Reducer</code><code class="o">&lt;</code><code class="n">KEYIN</code><code class="o">,</code> <code class="n">VALUEIN</code><code class="o">,</code> <code class="n">KEYOUT</code><code class="o">,</code> <code class="n">VALUEOUT</code><code class="o">&gt;</code> <code class="o">{</code>
  <code class="k">public</code> <code class="k">class</code> <code class="nc">Context</code> <code class="k">extends</code> <code class="n">ReducerContext</code><code class="o">&lt;</code><code class="n">KEYIN</code><code class="o">,</code> <code class="n">VALUEIN</code><code class="o">,</code> <code class="n">KEYOUT</code><code class="o">,</code> <code class="n">VALUEOUT</code><code class="o">&gt;</code> <code class="o">{</code>
    <code class="c2">// ...</code>
  <code class="o">}</code>
  <code class="k">protected</code> <code class="kt">void</code> <code class="nf">reduce</code><code class="o">(</code><code class="n">KEYIN</code> <code class="n">key</code><code class="o">,</code> <code class="n">Iterable</code><code class="o">&lt;</code><code class="n">VALUEIN</code><code class="o">&gt;</code> <code class="n">values</code><code class="o">,</code>
      <code class="n">Context</code> <code class="n">context</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="c2">// ...</code>
  <code class="o">}</code>
<code class="o">}</code></pre><p class="calibre2">The context objects are used for emitting key-value pairs, and they
    are parameterized by the output types so that the signature of the
    <code class="literal">write()</code> method is:</p><a id="calibre_link-4230" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="kt">void</code> <code class="nf">write</code><code class="o">(</code><code class="n">KEYOUT</code> <code class="n">key</code><code class="o">,</code> <code class="n">VALUEOUT</code> <code class="n">value</code><code class="o">)</code>
    <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code></pre><p class="calibre2">Since <code class="literal">Mapper</code> and
    <code class="literal">Reducer</code> are separate classes, the type parameters
    have different scopes, and the actual type argument of <code class="literal">KEYIN</code> (say) in the <code class="literal">Mapper</code>
    may <a class="calibre" id="calibre_link-2417"></a><a class="calibre" id="calibre_link-3197"></a>be different from the type of the type parameter of the same
    name (<code class="literal">KEYIN</code>) in the
    <code class="literal">Reducer</code>. For instance, in the maximum temperature
    example from earlier chapters, <code class="literal">KEYIN</code> is
    replaced <a class="calibre" id="calibre_link-2368"></a>by <code class="literal">LongWritable</code> for the
    <code class="literal">Mapper</code> and by <code class="literal">Text</code> for <a class="calibre" id="calibre_link-3667"></a>the <code class="literal">Reducer</code>.</p><p class="calibre2">Similarly, even though the map output types and the reduce input
    types must match, this is not enforced by the Java compiler.</p><p class="calibre2">The type parameters are named differently from the abstract types
    (<code class="literal">KEYIN</code> versus <code class="literal">K1</code>, and so on), but the form is the same.</p><p class="calibre2">If a combiner function <a class="calibre" id="calibre_link-1181"></a>is used, then it has the same form as the reduce function
    (and is an implementation of <code class="literal">Reducer</code>),
    except its output types are the intermediate key and value types (<code class="literal">K2</code> and <code class="literal">V2</code>),
    so they can feed the reduce function:</p><a id="calibre_link-4231" class="calibre"></a><pre class="screen1">map: (K1, V1) → list(K2, V2)
combiner: (K2, list(V2)) → list(K2, V2)
reduce: (K2, list(V2)) → list(K3, V3)</pre><p class="calibre2">Often the combiner and reduce functions are the same, in which case
    <code class="literal">K3</code> is the same as <code class="literal">K2</code>, and <code class="literal">V3</code> is
    the same as <code class="literal">V2</code>.</p><p class="calibre2">The partition function operates on the intermediate key and value
    types (<code class="literal">K2</code> and <code class="literal">V2</code>) and returns the partition index. In
    practice, the partition is determined solely by the key (the value is
    ignored):</p><a id="calibre_link-4232" class="calibre"></a><pre class="screen1">partition: (K2, V2) → integer</pre><p class="calibre2">Or in Java:</p><a id="calibre_link-4233" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">abstract</code> <code class="k">class</code> <code class="nc">Partitioner</code><code class="o">&lt;</code><code class="n">KEY</code><code class="o">,</code> <code class="n">VALUE</code><code class="o">&gt;</code> <code class="o">{</code>
  <code class="k">public</code> <code class="k">abstract</code> <code class="kt">int</code> <code class="nf">getPartition</code><code class="o">(</code><code class="n">KEY</code> <code class="n">key</code><code class="o">,</code> <code class="n">VALUE</code> <code class="n">value</code><code class="o">,</code> <code class="kt">int</code> <code class="n">numPartitions</code><code class="o">);</code>
<code class="o">}</code></pre><div class="sidebar"><a id="calibre_link-4234" class="calibre"></a><div class="sidebar-title">MapReduce Signatures in the Old API</div><p class="calibre2">In the old API (see <a class="ulink" href="#calibre_link-249" title="Appendix&nbsp;D.&nbsp;The Old and New Java MapReduce APIs">Appendix&nbsp;D</a>), the <a class="calibre" id="calibre_link-2488"></a>signatures are very similar and actually name the type
      parameters <code class="literal">K1</code>, <code class="literal">V1</code>, and so on, although the constraints on the
      types are exactly the same in both the old and new <a class="calibre" id="calibre_link-2952"></a>APIs:</p><a id="calibre_link-4235" class="calibre"></a><pre class="screen2"><code class="k">public</code> <code class="k">interface</code> <code class="nc">Mapper</code><code class="o">&lt;</code><code class="n">K1</code><code class="o">,</code> <code class="n">V1</code><code class="o">,</code> <code class="n">K2</code><code class="o">,</code> <code class="n">V2</code><code class="o">&gt;</code> <code class="k">extends</code> <code class="n">JobConfigurable</code><code class="o">,</code> <code class="n">Closeable</code> <code class="o">{</code>
  <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">K1</code> <code class="n">key</code><code class="o">,</code> <code class="n">V1</code> <code class="n">value</code><code class="o">,</code>
      <code class="n">OutputCollector</code><code class="o">&lt;</code><code class="n">K2</code><code class="o">,</code> <code class="n">V2</code><code class="o">&gt;</code> <code class="n">output</code><code class="o">,</code> <code class="n">Reporter</code> <code class="n">reporter</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">;</code>
<code class="o">}</code>

<code class="k">public</code> <code class="k">interface</code> <code class="nc">Reducer</code><code class="o">&lt;</code><code class="n">K2</code><code class="o">,</code> <code class="n">V2</code><code class="o">,</code> <code class="n">K3</code><code class="o">,</code> <code class="n">V3</code><code class="o">&gt;</code> <code class="k">extends</code> <code class="n">JobConfigurable</code><code class="o">,</code> <code class="n">Closeable</code> <code class="o">{</code>
  <code class="kt">void</code> <code class="nf">reduce</code><code class="o">(</code><code class="n">K2</code> <code class="n">key</code><code class="o">,</code> <code class="n">Iterator</code><code class="o">&lt;</code><code class="n">V2</code><code class="o">&gt;</code> <code class="n">values</code><code class="o">,</code>
      <code class="n">OutputCollector</code><code class="o">&lt;</code><code class="n">K3</code><code class="o">,</code> <code class="n">V3</code><code class="o">&gt;</code> <code class="n">output</code><code class="o">,</code> <code class="n">Reporter</code> <code class="n">reporter</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">;</code>
<code class="o">}</code>

<code class="k">public</code> <code class="k">interface</code> <code class="nc">Partitioner</code><code class="o">&lt;</code><code class="n">K2</code><code class="o">,</code> <code class="n">V2</code><code class="o">&gt;</code> <code class="k">extends</code> <code class="n">JobConfigurable</code> <code class="o">{</code>
  <code class="kt">int</code> <code class="nf">getPartition</code><code class="o">(</code><code class="n">K2</code> <code class="n">key</code><code class="o">,</code> <code class="n">V2</code> <code class="n">value</code><code class="o">,</code> <code class="kt">int</code> <code class="n">numPartitions</code><code class="o">);</code>
<code class="o">}</code></pre></div><p class="calibre2">So much for the theory. How does this help you configure MapReduce
    jobs? <a class="ulink" href="#calibre_link-569" title="Table&nbsp;8-1.&nbsp;Configuration of MapReduce types in the new API">Table&nbsp;8-1</a> summarizes the
    configuration options for the new API (and <a class="ulink" href="#calibre_link-570" title="Table&nbsp;8-2.&nbsp;Configuration of MapReduce types in the old API">Table&nbsp;8-2</a> does the same for the old
    API). It is divided into the properties that determine the types and those
    that have to be compatible with the configured types.</p><p class="calibre2">Input types are set by the input format. So, for instance,
    <a class="calibre" id="calibre_link-3677"></a>a <code class="literal">TextInputFormat</code>
    generates keys of type <code class="literal">LongWritable</code> and
    values of type <code class="literal">Text</code>. The other types
    are set explicitly by calling the methods on the <code class="literal">Job</code> (or <code class="literal">JobConf</code> in the old API). If not set explicitly,
    the intermediate types default to the (final) output types, which default
    to <code class="literal">LongWritable</code> and <code class="literal">Text</code>. So, if <code class="literal">K2</code> and <code class="literal">K3</code> are
    the same, you don’t need to call <code class="literal">setMapOutputKeyClass()</code>, because it falls back to
    the type set by calling <code class="literal">setOutputKeyClass()</code>. Similarly, if <code class="literal">V2</code> and <code class="literal">V3</code> are
    the same, you only need to use <code class="literal">setOutputValueClass()</code>.</p><p class="calibre2">It may seem strange that these methods for setting the intermediate
    and final output types exist at all. After all, why can’t the types be
    determined from a combination of the mapper and the reducer? The answer
    has to do with a limitation in Java generics: type erasure means that the
    type information isn’t always present at runtime, so Hadoop has to be
    given it explicitly. This also means that it’s possible to configure a
    MapReduce job with incompatible types, because the configuration isn’t
    checked at compile time. The settings that have to be compatible with the
    MapReduce types are listed in the lower part of <a class="ulink" href="#calibre_link-569" title="Table&nbsp;8-1.&nbsp;Configuration of MapReduce types in the new API">Table&nbsp;8-1</a>. Type conflicts are detected at
    runtime during job execution, and for this reason, it is wise to run a
    test job using a small amount of data to flush out and fix any type
    incompatibilities.</p><div class="table"><a id="calibre_link-569" class="calibre"></a><div class="table-title">Table&nbsp;8-1.&nbsp;Configuration of MapReduce types in the new API</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td rowspan="2" class="calibre45">Property</td><td rowspan="2" class="calibre45">Job setter
            method</td><td colspan="2" class="calibre20">Input
            types</td><td colspan="2" class="calibre20">Intermediate types</td><td colspan="2" class="calibre21">Output types</td></tr><tr class="calibre26"><td class="calibre20"><code class="uri">K1</code></td><td class="calibre20"><code class="uri">V1</code></td><td class="calibre20"><code class="uri">K2</code></td><td class="calibre20"><code class="uri">V2</code></td><td class="calibre20"><code class="uri">K3</code></td><td class="calibre21"><code class="uri">V3</code></td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td colspan="8" class="calibre25"><span class="calibre"><strong class="calibre24">Properties
            for configuring <a class="calibre" id="calibre_link-2549"></a><a class="calibre" id="calibre_link-2590"></a><a class="calibre" id="calibre_link-2591"></a><a class="calibre" id="calibre_link-2553"></a><a class="calibre" id="calibre_link-2556"></a>types:</strong></span></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.job.inputformat.class</code></td><td class="calibre23"><code class="uri">setInputFormatClass()</code></td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.map.output.key.class</code></td><td class="calibre23"><code class="uri">setMapOutputKeyClass()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">•</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.map.output.value.class</code></td><td class="calibre23"><code class="uri">setMapOutputValueClass()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">•</td><td class="calibre23">&nbsp;</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.job.output.key.class</code></td><td class="calibre23"><code class="uri">setOutputKeyClass()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">•</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.job.output.value.class</code></td><td class="calibre23"><code class="uri">setOutputValueClass()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre25">•</td></tr><tr class="calibre19"><td colspan="8" class="calibre25"><span class="calibre"><strong class="calibre24">Properties
            that must be consistent <a class="calibre" id="calibre_link-2550"></a><a class="calibre" id="calibre_link-2545"></a><a class="calibre" id="calibre_link-2558"></a><a class="calibre" id="calibre_link-2554"></a><a class="calibre" id="calibre_link-2552"></a><a class="calibre" id="calibre_link-2560"></a><a class="calibre" id="calibre_link-2557"></a>with the types:</strong></span></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.job.map.class</code></td><td class="calibre23"><code class="uri">setMapperClass()</code></td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre23">&nbsp;</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.job.combine.class</code></td><td class="calibre23"><code class="uri">setCombinerClass()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre23">&nbsp;</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.job.partitioner.class</code></td><td class="calibre23"><code class="uri">setPartitionerClass()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre23">&nbsp;</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.job.output.key.comparator.class</code></td><td class="calibre23"><code class="uri">setSortComparatorClass()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">•</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.job.output.group.comparator.class</code></td><td class="calibre23"><code class="uri">setGroupingComparatorClass()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">•</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.job.reduce.class</code></td><td class="calibre23"><code class="uri">setReducerClass()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre25">•</td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">mapreduce.job.outputformat.class</code></td><td class="calibre27"><code class="uri">setOutputFormatClass()</code></td><td class="calibre27">&nbsp;</td><td class="calibre27">&nbsp;</td><td class="calibre27">&nbsp;</td><td class="calibre27">&nbsp;</td><td class="calibre27">•</td><td class="calibre28">•</td></tr></tbody></table></div></div><div class="table"><a id="calibre_link-570" class="calibre"></a><div class="table-title">Table&nbsp;8-2.&nbsp;Configuration of MapReduce types in the old API</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td rowspan="2" class="calibre45">Property</td><td rowspan="2" class="calibre45">JobConf setter
            method</td><td colspan="2" class="calibre20">Input
            types</td><td colspan="2" class="calibre20">Intermediate types</td><td colspan="2" class="calibre21">Output types</td></tr><tr class="calibre26"><td class="calibre20"><code class="uri">K1</code></td><td class="calibre20"><code class="uri">V1</code></td><td class="calibre20"><code class="uri">K2</code></td><td class="calibre20"><code class="uri">V2</code></td><td class="calibre20"><code class="uri">K3</code></td><td class="calibre21"><code class="uri">V3</code></td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td colspan="8" class="calibre25"><span class="calibre"><strong class="calibre24">Properties
            for configuring <a class="calibre" id="calibre_link-2425"></a><a class="calibre" id="calibre_link-2428"></a><a class="calibre" id="calibre_link-2429"></a><a class="calibre" id="calibre_link-2432"></a><a class="calibre" id="calibre_link-2434"></a>types:</strong></span></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapred.input.format.class</code></td><td class="calibre23"><code class="uri">setInputFormat()</code></td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapred.mapoutput.key.class</code></td><td class="calibre23"><code class="uri">setMapOutputKeyClass()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">•</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapred.mapoutput.value.class</code></td><td class="calibre23"><code class="uri">setMapOutputValueClass()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">•</td><td class="calibre23">&nbsp;</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapred.output.key.class</code></td><td class="calibre23"><code class="uri">setOutputKeyClass()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">•</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapred.output.value.class</code></td><td class="calibre23"><code class="uri">setOutputValueClass()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre25">•</td></tr><tr class="calibre19"><td colspan="8" class="calibre25"><span class="calibre"><strong class="calibre24">Properties
            that must be consistent <a class="calibre" id="calibre_link-2430"></a><a class="calibre" id="calibre_link-2427"></a><a class="calibre" id="calibre_link-2424"></a><a class="calibre" id="calibre_link-2436"></a><a class="calibre" id="calibre_link-2433"></a><a class="calibre" id="calibre_link-2435"></a><a class="calibre" id="calibre_link-2437"></a><a class="calibre" id="calibre_link-2431"></a>with the types:</strong></span></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapred.mapper.class</code></td><td class="calibre23"><code class="uri">setMapperClass()</code></td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre23">&nbsp;</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapred.map.runner.class</code></td><td class="calibre23"><code class="uri">setMapRunnerClass()</code></td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre23">&nbsp;</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapred.combiner.class</code></td><td class="calibre23"><code class="uri">setCombinerClass()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre23">&nbsp;</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapred.partitioner.class</code></td><td class="calibre23"><code class="uri">setPartitionerClass()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre23">&nbsp;</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapred.output.key.comparator.class</code></td><td class="calibre23"><code class="uri">setOutputKeyComparatorClass()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">•</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapred.output.value.groupfn.class</code></td><td class="calibre23"><code class="uri">setOutputValueGroupingComparator()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">•</td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre25">&nbsp;</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapred.reducer.class</code></td><td class="calibre23"><code class="uri">setReducerClass()</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre23">•</td><td class="calibre25">•</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">mapred.output.format.class</code></td><td class="calibre27"><code class="uri">setOutputFormat()</code></td><td class="calibre27">&nbsp;</td><td class="calibre27">&nbsp;</td><td class="calibre27">&nbsp;</td><td class="calibre27">&nbsp;</td><td class="calibre27">•</td><td class="calibre28">•</td></tr></tbody></table></div></div><div class="book" title="The Default MapReduce Job"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-471">The Default MapReduce Job</h3></div></div></div><p class="calibre2">What <a class="calibre" id="calibre_link-2516"></a><a class="calibre" id="calibre_link-2243"></a><a class="calibre" id="calibre_link-2457"></a>happens when you run MapReduce without setting a mapper or
      a reducer? Let’s try it by running this minimal MapReduce
      program:</p><a id="calibre_link-4236" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">class</code> <code class="nc">MinimalMapReduce</code> <code class="k">extends</code> <code class="n">Configured</code> <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="na">length</code> <code class="o">!=</code> <code class="mi">2</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"Usage: %s [generic options] &lt;input&gt; &lt;output&gt;\n"</code><code class="o">,</code>
          <code class="n">getClass</code><code class="o">().</code><code class="na">getSimpleName</code><code class="o">());</code>
      <code class="n">ToolRunner</code><code class="o">.</code><code class="na">printGenericCommandUsage</code><code class="o">(</code><code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">);</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>
    
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Job</code><code class="o">(</code><code class="n">getConf</code><code class="o">());</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setJarByClass</code><code class="o">(</code><code class="n">getClass</code><code class="o">());</code>
    <code class="n">FileInputFormat</code><code class="o">.</code><code class="na">addInputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]));</code>
    <code class="n">FileOutputFormat</code><code class="o">.</code><code class="na">setOutputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">]));</code>
    <code class="k">return</code> <code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">;</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="k">new</code> <code class="n">MinimalMapReduce</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre><p class="calibre2">The only configuration that we set is an input path and an output
      path. We run it over a subset of our weather data with the
      following:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop MinimalMapReduce "input/ncdc/all/190{1,2}.gz" output</code></strong></pre><p class="calibre2">We do get some output: one file named <em class="calibre10">part-r-00000</em> in the output directory. Here’s
      what the first few lines look like (truncated to fit the page):</p><pre class="screen1">0→0029029070999991901010106004+64333+023450FM-12+000599999V0202701N01591<span class="calibre">...</span>
0→0035029070999991902010106004+64333+023450FM-12+000599999V0201401N01181<span class="calibre">...</span>
135→0029029070999991901010113004+64333+023450FM-12+000599999V0202901N00821<span class="calibre">...</span>
141→0035029070999991902010113004+64333+023450FM-12+000599999V0201401N01181<span class="calibre">...</span>
270→0029029070999991901010120004+64333+023450FM-12+000599999V0209991C00001<span class="calibre">...</span>
282→0035029070999991902010120004+64333+023450FM-12+000599999V0201401N01391<span class="calibre">...</span></pre><p class="calibre2">Each line is an integer followed by a tab character, followed by
      the original weather data record. Admittedly, it’s not a very useful
      program, but understanding how it produces its output does provide some
      insight into the defaults that Hadoop uses when running MapReduce jobs.
      <a class="ulink" href="#calibre_link-571" title="Example&nbsp;8-1.&nbsp;A minimal MapReduce driver, with the defaults explicitly set">Example&nbsp;8-1</a> shows a program that has
      exactly the same effect as <code class="literal">MinimalMapReduce</code>, but explicitly sets the job
      settings to their defaults.</p><div class="example"><a id="calibre_link-571" class="calibre"></a><div class="example-title">Example&nbsp;8-1.&nbsp;A minimal MapReduce driver, with the defaults explicitly
        set</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">MinimalMapReduceWithDefaults</code> <code class="k">extends</code> <code class="n">Configured</code> <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="n">JobBuilder</code><code class="o">.</code><code class="na">parseInputAndOutput</code><code class="o">(</code><code class="k">this</code><code class="o">,</code> <code class="n">getConf</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">job</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>
    
    <span class="calibre24"><strong class="calibre9"><code class="n1">job</code><code class="o1">.</code><code class="na1">setInputFormatClass</code><code class="o1">(</code><code class="n1">TextInputFormat</code><code class="o1">.</code><code class="na1">class</code><code class="o1">);</code>
    
    <code class="n1">job</code><code class="o1">.</code><code class="na1">setMapperClass</code><code class="o1">(</code><code class="n1">Mapper</code><code class="o1">.</code><code class="na1">class</code><code class="o1">);</code>
    
    <code class="n1">job</code><code class="o1">.</code><code class="na1">setMapOutputKeyClass</code><code class="o1">(</code><code class="n1">LongWritable</code><code class="o1">.</code><code class="na1">class</code><code class="o1">);</code>
    <code class="n1">job</code><code class="o1">.</code><code class="na1">setMapOutputValueClass</code><code class="o1">(</code><code class="n1">Text</code><code class="o1">.</code><code class="na1">class</code><code class="o1">);</code>
    
    <code class="n1">job</code><code class="o1">.</code><code class="na1">setPartitionerClass</code><code class="o1">(</code><code class="n1">HashPartitioner</code><code class="o1">.</code><code class="na1">class</code><code class="o1">);</code>
    
    <code class="n1">job</code><code class="o1">.</code><code class="na1">setNumReduceTasks</code><code class="o1">(</code><code class="mi1">1</code><code class="o1">);</code>
    <code class="n1">job</code><code class="o1">.</code><code class="na1">setReducerClass</code><code class="o1">(</code><code class="n1">Reducer</code><code class="o1">.</code><code class="na1">class</code><code class="o1">);</code>

    <code class="n1">job</code><code class="o1">.</code><code class="na1">setOutputKeyClass</code><code class="o1">(</code><code class="n1">LongWritable</code><code class="o1">.</code><code class="na1">class</code><code class="o1">);</code>
    <code class="n1">job</code><code class="o1">.</code><code class="na1">setOutputValueClass</code><code class="o1">(</code><code class="n1">Text</code><code class="o1">.</code><code class="na1">class</code><code class="o1">);</code>

    <code class="n1">job</code><code class="o1">.</code><code class="na1">setOutputFormatClass</code><code class="o1">(</code><code class="n1">TextOutputFormat</code><code class="o1">.</code><code class="na1">class</code><code class="o1">);</code></strong></span>
    
    <code class="k">return</code> <code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">;</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="k">new</code> <code class="n">MinimalMapReduceWithDefaults</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">We’ve simplified the first few lines of the
      <code class="literal">run()</code> method by extracting the logic for
      printing usage and setting the input and output paths into a helper
      method. Almost all MapReduce drivers take these two arguments (input and
      output), so reducing the
      boilerplate code here is a good thing. Here are the relevant
      methods <a class="calibre" id="calibre_link-2225"></a>in the <code class="literal">JobBuilder</code> class for reference:</p><a id="calibre_link-4237" class="calibre"></a><pre class="screen1">  <code class="k">public</code> <code class="k">static</code> <code class="n">Job</code> <code class="nf">parseInputAndOutput</code><code class="o">(</code><code class="n">Tool</code> <code class="n">tool</code><code class="o">,</code> <code class="n">Configuration</code> <code class="n">conf</code><code class="o">,</code>
      <code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    
    <code class="k">if</code> <code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="na">length</code> <code class="o">!=</code> <code class="mi">2</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">printUsage</code><code class="o">(</code><code class="n">tool</code><code class="o">,</code> <code class="sb">"&lt;input&gt; &lt;output&gt;"</code><code class="o">);</code>
      <code class="k">return</code> <code class="k">null</code><code class="o">;</code>
    <code class="o">}</code>
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Job</code><code class="o">(</code><code class="n">conf</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setJarByClass</code><code class="o">(</code><code class="n">tool</code><code class="o">.</code><code class="na">getClass</code><code class="o">());</code>
    <code class="n">FileInputFormat</code><code class="o">.</code><code class="na">addInputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]));</code>
    <code class="n">FileOutputFormat</code><code class="o">.</code><code class="na">setOutputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">]));</code>
    <code class="k">return</code> <code class="n">job</code><code class="o">;</code>
  <code class="o">}</code>

  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">printUsage</code><code class="o">(</code><code class="n">Tool</code> <code class="n">tool</code><code class="o">,</code> <code class="n">String</code> <code class="n">extraArgsUsage</code><code class="o">)</code> <code class="o">{</code>
    <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"Usage: %s [genericOptions] %s\n\n"</code><code class="o">,</code>
        <code class="n">tool</code><code class="o">.</code><code class="na">getClass</code><code class="o">().</code><code class="na">getSimpleName</code><code class="o">(),</code> <code class="n">extraArgsUsage</code><code class="o">);</code>
    <code class="n">GenericOptionsParser</code><code class="o">.</code><code class="na">printGenericCommandUsage</code><code class="o">(</code><code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">);</code>
  <code class="o">}</code></pre><p class="calibre2">Going back to <code class="literal">MinimalMapReduceWithDefaults</code> in <a class="ulink" href="#calibre_link-571" title="Example&nbsp;8-1.&nbsp;A minimal MapReduce driver, with the defaults explicitly set">Example&nbsp;8-1</a>, although there are many other
      default job settings, the ones bolded are those most central to running
      a job. Let’s go through them in turn.</p><p class="calibre2">The default input format is <code class="literal">TextInputFormat</code>, which produces keys of type
      <code class="literal">LongWritable</code> (the offset of the
      beginning of the line in the file) and values of type <code class="literal">Text</code> (the line of text). This explains where
      the integers in the final output come from: they are the line
      offsets.</p><p class="calibre2">The default mapper is just the <code class="literal">Mapper</code> class, which writes the input key and
      value unchanged to the output:</p><a id="calibre_link-4238" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">class</code> <code class="nc">Mapper</code><code class="o">&lt;</code><code class="n">KEYIN</code><code class="o">,</code> <code class="n">VALUEIN</code><code class="o">,</code> <code class="n">KEYOUT</code><code class="o">,</code> <code class="n">VALUEOUT</code><code class="o">&gt;</code> <code class="o">{</code>

  <code class="k">protected</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">KEYIN</code> <code class="n">key</code><code class="o">,</code> <code class="n">VALUEIN</code> <code class="n">value</code><code class="o">,</code> 
      <code class="n">Context</code> <code class="n">context</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">((</code><code class="n">KEYOUT</code><code class="o">)</code> <code class="n">key</code><code class="o">,</code> <code class="o">(</code><code class="n">VALUEOUT</code><code class="o">)</code> <code class="n">value</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre><p class="calibre2"><code class="literal">Mapper</code> is a generic type, which
      allows it to work with any key or value types. In this case, the map
      input and output key is of type <code class="literal">LongWritable</code>, and the map input and output
      value is of type <code class="literal">Text</code>.</p><p class="calibre2">The default partitioner is <code class="literal">HashPartitioner</code>, which hashes a record’s key
      to determine which partition the record belongs in. Each partition is
      processed by a reduce task, so the number of partitions is equal to the
      number of reduce tasks for the job:</p><a id="calibre_link-4239" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">class</code> <code class="nc">HashPartitioner</code><code class="o">&lt;</code><code class="n">K</code><code class="o">,</code> <code class="n">V</code><code class="o">&gt;</code> <code class="k">extends</code> <code class="n">Partitioner</code><code class="o">&lt;</code><code class="n">K</code><code class="o">,</code> <code class="n">V</code><code class="o">&gt;</code> <code class="o">{</code>

  <code class="k">public</code> <code class="kt">int</code> <code class="nf">getPartition</code><code class="o">(</code><code class="n">K</code> <code class="n">key</code><code class="o">,</code> <code class="n">V</code> <code class="n">value</code><code class="o">,</code>
      <code class="kt">int</code> <code class="n">numReduceTasks</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">return</code> <code class="o">(</code><code class="n">key</code><code class="o">.</code><code class="na">hashCode</code><code class="o">()</code> <code class="o">&amp;</code> <code class="n">Integer</code><code class="o">.</code><code class="na">MAX_VALUE</code><code class="o">)</code> <code class="o">%</code> <code class="n">numReduceTasks</code><code class="o">;</code>
  <code class="o">}</code>
<code class="o">}</code></pre><p class="calibre2">The key’s hash code is turned into a nonnegative integer by
      bitwise ANDing it with the largest integer value. It is then reduced
      modulo the number of partitions to find the index of the partition that
      the record belongs in.</p><p class="calibre2">By default, there is a single reducer, and therefore a single
      partition; the action of the partitioner is irrelevant in this case
      since everything goes into one partition. However, it is important to
      understand the behavior of <code class="literal">HashPartitioner</code> when <a class="calibre" id="calibre_link-1887"></a>you have more than one reduce task. Assuming the key’s
      hash function is a good one, the records will be allocated evenly across
      reduce tasks, with all records that share the same key being processed
      by the same reduce task.</p><p class="calibre2">You may have noticed that we didn’t set the number of map tasks.
      The reason for this is that the number is equal to the number of splits
      that the input is turned into, which is driven by the size of the input
      and the file’s block size (if the file is in HDFS). The options for
      controlling split size are discussed in <a class="ulink" href="#calibre_link-572" title="FileInputFormat input splits">FileInputFormat input splits</a>.</p><div class="sidebar"><a id="calibre_link-555" class="calibre"></a><div class="sidebar-title">Choosing the Number of Reducers</div><p class="calibre2">The single reducer default is something of a gotcha for new
        users to Hadoop. Almost
        all real-world jobs should set this to a larger number; otherwise, the
        job will be very slow since all the intermediate data flows through a
        single reduce task.</p><p class="calibre2">Choosing the number of reducers for a job is more of an art than
        a science. Increasing the number of reducers makes the reduce phase
        shorter, since you get more parallelism. However, if you take this too
        far, you can have lots of small files, which is suboptimal. One rule
        of thumb is to aim for reducers that each run for five minutes or so,
        and which produce at least one HDFS block’s worth of output.</p></div><p class="calibre2">The default reducer is <code class="literal">Reducer</code>,
      again a generic type, which simply writes all its input to its
      output:</p><a id="calibre_link-4240" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">class</code> <code class="nc">Reducer</code><code class="o">&lt;</code><code class="n">KEYIN</code><code class="o">,</code> <code class="n">VALUEIN</code><code class="o">,</code> <code class="n">KEYOUT</code><code class="o">,</code> <code class="n">VALUEOUT</code><code class="o">&gt;</code> <code class="o">{</code>

  <code class="k">protected</code> <code class="kt">void</code> <code class="nf">reduce</code><code class="o">(</code><code class="n">KEYIN</code> <code class="n">key</code><code class="o">,</code> <code class="n">Iterable</code><code class="o">&lt;</code><code class="n">VALUEIN</code><code class="o">&gt;</code> <code class="n">values</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code>
      <code class="n">Context</code> <code class="n">context</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="k">for</code> <code class="o">(</code><code class="n">VALUEIN</code> <code class="nd">value:</code> <code class="n">values</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">((</code><code class="n">KEYOUT</code><code class="o">)</code> <code class="n">key</code><code class="o">,</code> <code class="o">(</code><code class="n">VALUEOUT</code><code class="o">)</code> <code class="n">value</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre><p class="calibre2">For this job, the output key is <code class="literal">LongWritable</code> and the output value is <code class="literal">Text</code>. In fact, all the keys for this MapReduce
      program are <code class="literal">LongWritable</code> and all the
      values are <code class="literal">Text</code>, since these are the
      input keys and values, and the map and reduce functions are both
      identity functions, which by definition preserve type. Most MapReduce
      programs, however, don’t use the same key or value types throughout, so
      you need to configure the job to declare the types you are using, as
      described in the previous section.</p><p class="calibre2">Records are sorted by the MapReduce system before being presented
      to the reducer. In this case, the keys are sorted numerically, which has
      the effect of interleaving the lines from the input files into one
      combined output file.</p><p class="calibre2">The default output format is <code class="literal">TextOutputFormat</code>, which writes out records,
      one per line, by converting keys and values to strings and separating
      them with a tab character. This is why the output is tab-separated: it
      is a feature of <code class="literal">TextOutputFormat</code>.</p><div class="book" title="The default Streaming job"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4241">The default Streaming job</h4></div></div></div><p class="calibre2">In Streaming, the <a class="calibre" id="calibre_link-3548"></a>default job is similar, but not identical, to the Java
        equivalent. The basic form is:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \</code></strong>
<strong class="userinput"><code class="calibre9">  -input input/ncdc/sample.txt \</code></strong>
<strong class="userinput"><code class="calibre9">  -output output \</code></strong>
<strong class="userinput"><code class="calibre9">  -mapper /bin/cat</code></strong></pre><p class="calibre2">When we specify a non-Java mapper and the default text mode is
        in effect (<code class="literal">-io text</code>), Streaming
        does something special. It doesn’t pass the key to the mapper process;
        it just passes the value. (For other input formats, the same effect
        can be achieved by <a class="calibre" id="calibre_link-3539"></a>setting <code class="literal">stream.map.input.ignoreKey</code> to <code class="literal">true</code>.) This is actually very useful because
        the key is just the line offset in the file and the value is the line,
        which is all most applications are interested in. The overall effect
        of this job is to perform a sort of the input.</p><p class="calibre2">With more of the defaults spelled out, the command looks like
        this (notice that Streaming uses the old MapReduce API
        classes):</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \</code></strong>
<strong class="userinput"><code class="calibre9">  -input input/ncdc/sample.txt \</code></strong>
<strong class="userinput"><code class="calibre9">  -output output \</code></strong>
<strong class="userinput"><code class="calibre9">  -inputformat org.apache.hadoop.mapred.TextInputFormat \</code></strong>
<strong class="userinput"><code class="calibre9">  -mapper /bin/cat \</code></strong>
<strong class="userinput"><code class="calibre9">  -partitioner org.apache.hadoop.mapred.lib.HashPartitioner \</code></strong>
<strong class="userinput"><code class="calibre9">  -numReduceTasks 1 \</code></strong>
<strong class="userinput"><code class="calibre9">  -reducer org.apache.hadoop.mapred.lib.IdentityReducer \</code></strong>
<strong class="userinput"><code class="calibre9">  -outputformat org.apache.hadoop.mapred.TextOutputFormat</code></strong>
<strong class="userinput"><code class="calibre9">  -io text</code></strong></pre><p class="calibre2">The <code class="literal">-mapper</code> and <code class="literal">-reducer</code>
        arguments take a command or a Java class. A combiner may optionally be
        specified using the <code class="literal">-combiner</code>
        argument.</p></div><div class="book" title="Keys and values in Streaming"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4242">Keys and values in Streaming</h4></div></div></div><p class="calibre2">A Streaming application can control the separator that is used
        when a key-value pair is turned into a series of bytes and sent to the
        map or reduce process over standard input. The default is a tab
        character, but it is useful to be able to change it in the case that
        the keys or values themselves contain tab characters.</p><p class="calibre2">Similarly, when the map or reduce writes out key-value pairs,
        they may be separated by a configurable separator. Furthermore, the
        key from the output can be composed of more than the first field: it
        can be made up of the first <span class="calibre">n</span>
        fields (defined <a class="calibre" id="calibre_link-3542"></a><a class="calibre" id="calibre_link-3543"></a>by <code class="literal">stream.num.map.output.key.fields</code> or <code class="literal">stream.num.reduce.output.key.fields</code>), with
        the value being the remaining fields. For example, if the output from
        a Streaming process was <code class="literal">a,b,c</code> (with
        a comma as the separator), and <span class="calibre">n</span>
        was 2, the key would be parsed as <code class="literal">a,b</code> and the value as <code class="literal">c</code>.</p><p class="calibre2">Separators may be configured independently for maps and reduces.
        The properties are listed in <a class="ulink" href="#calibre_link-573" title="Table&nbsp;8-3.&nbsp;Streaming separator properties">Table&nbsp;8-3</a> and shown in a diagram of
        the data flow path in <a class="ulink" href="#calibre_link-574" title="Figure&nbsp;8-1.&nbsp;Where separators are used in a Streaming MapReduce job">Figure&nbsp;8-1</a>.</p><p class="calibre2">These settings do not have any bearing on the input and output
        formats. For example, if <code class="literal">stream.reduce.output.field.separator</code> were
        <a class="calibre" id="calibre_link-3546"></a>set to be a colon, say, and the reduce stream process
        wrote the line <code class="literal">a:b</code> to standard out,
        the Streaming reducer would know to extract the key as <code class="literal">a</code> and the value as <code class="literal">b</code>. With the standard <code class="literal">TextOutputFormat</code>, this record would be
        written to the output file with a tab separating <code class="literal">a</code> and <code class="literal">b</code>.
        You can change the separator that <code class="literal">TextOutputFormat</code> uses by setting <code class="literal">mapreduce.output.textoutputformat.separator</code>.</p><div class="table"><a id="calibre_link-573" class="calibre"></a><div class="table-title">Table&nbsp;8-3.&nbsp;Streaming separator properties</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Type</td><td class="calibre20">Default
                value</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">stream.map.input.field.separator</code></td><td class="calibre23"><code class="uri">String</code></td><td class="calibre23"><code class="uri">\t</code></td><td class="calibre25">The separator <a class="calibre" id="calibre_link-3538"></a>to use when passing the input key and value
                strings to the stream map process as a stream of bytes</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">stream.map.output.field.separator</code></td><td class="calibre23"><code class="uri">String</code></td><td class="calibre23"><code class="uri">\t</code></td><td class="calibre25">The separator <a class="calibre" id="calibre_link-3540"></a>to use when splitting the output from the stream
                map process into key and value strings for the map
                output</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">stream.num.map.output.key.fields</code></td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23"><code class="uri">1</code></td><td class="calibre25">The number of fields separated by <code class="uri">stream.map.</code><code class="uri">output.field.separator</code> to treat as the map output key</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">stream.reduce.input.field.separator</code></td><td class="calibre23"><code class="uri">String</code></td><td class="calibre23"><code class="uri">\t</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-3545"></a>separator to use when passing the input key and
                value strings to the stream reduce process as a stream of
                bytes</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">stream.reduce.output.field.separator</code></td><td class="calibre23"><code class="uri">String</code></td><td class="calibre23"><code class="uri">\t</code></td><td class="calibre25">The separator to use when splitting the output from the
                stream reduce process into key and value strings for the final
                reduce output</td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">stream.num.reduce.output.key.fields</code></td><td class="calibre27"><code class="uri">int</code></td><td class="calibre27"><code class="uri">1</code></td><td class="calibre28">The number of fields separated <a class="calibre" id="calibre_link-2244"></a><a class="calibre" id="calibre_link-2458"></a><a class="calibre" id="calibre_link-3549"></a>by <code class="uri">stream.reduce.output.field.</code><code class="uri">separator</code> to treat as the reduce output
                key</td></tr></tbody></table></div></div><div class="book"><div class="figure"><a id="calibre_link-574" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4243" class="calibre"></a><img alt="Where separators are used in a Streaming MapReduce job" src="images/000064.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;8-1.&nbsp;Where separators are used in a Streaming MapReduce
          job</div></div></div></div></div></div><div class="book" title="Input Formats"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4244">Input Formats</h2></div></div></div><p class="calibre2">Hadoop can <a class="calibre" id="calibre_link-2481"></a>process many different types of data formats, from flat text
    files to databases. In this section, we explore the different formats
    available.</p><div class="book" title="Input Splits and Records"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-549">Input Splits and Records</h3></div></div></div><p class="calibre2">As we saw in <a class="ulink" href="#calibre_link-462" title="Chapter&nbsp;2.&nbsp;MapReduce">Chapter&nbsp;2</a>, an <a class="calibre" id="calibre_link-2105"></a><a class="calibre" id="calibre_link-2111"></a>input split is a chunk of the input that is processed by a
      single map. Each map processes a single split. Each split is divided
      into records, and the map processes each record—a key-value pair—in
      turn. Splits and records are logical: there is nothing that requires
      them to be tied to files, for example, although in their most common
      incarnations, they are. In a database context, a split might correspond
      to a range of rows from a table and a record to a row in that range
      (this is <a class="calibre" id="calibre_link-1398"></a>precisely the case with <code class="literal">DBInputFormat</code>, which is an input format for
        reading data from a relational database).</p><p class="calibre2">Input splits are represented by the <a class="calibre" id="calibre_link-2125"></a>Java class <code class="literal">InputSplit</code>
      (which, like all of the classes mentioned in this section, is in
      <a class="calibre" id="calibre_link-2866"></a>the <code class="literal">org.apache.hadoop.mapreduce</code> package):<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-575" id="calibre_link-603">55</a>]</sup></p><a id="calibre_link-4245" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">abstract</code> <code class="k">class</code> <code class="nc">InputSplit</code> <code class="o">{</code>
  <code class="k">public</code> <code class="k">abstract</code> <code class="kt">long</code> <code class="nf">getLength</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code><code class="o">;</code>
  <code class="k">public</code> <code class="k">abstract</code> <code class="n">String</code><code class="o">[]</code> <code class="nf">getLocations</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code>
      <code class="n">InterruptedException</code><code class="o">;</code>
<code class="o">}</code></pre><p class="calibre2">An <code class="literal">InputSplit</code> has a length in
      bytes and a set of storage locations, which are just hostname strings.
      Notice that a split doesn’t contain the input data; it is just a
      reference to the data. The storage locations are used by the MapReduce
      system to place map tasks as close to the split’s data as possible, and
      the size is used to order the splits so that the largest get processed
      first, in an attempt to minimize the job runtime (this is an instance of
      a greedy approximation algorithm).</p><p class="calibre2">As a MapReduce application writer, you don’t need to deal with
      <code class="literal">InputSplit</code>s directly, as they are
      created by an <code class="literal">InputFormat</code> (an
      <code class="literal">InputFormat</code> is responsible <a class="calibre" id="calibre_link-2121"></a>for creating the input splits and dividing them into
      records). Before we see some concrete examples of <code class="literal">InputFormat</code>s, let’s briefly examine how it is
      used in MapReduce. Here’s the interface:</p><a id="calibre_link-4246" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">abstract</code> <code class="k">class</code> <code class="nc">InputFormat</code><code class="o">&lt;</code><code class="n">K</code><code class="o">,</code> <code class="n">V</code><code class="o">&gt;</code> <code class="o">{</code>
  <code class="k">public</code> <code class="k">abstract</code> <code class="n">List</code><code class="o">&lt;</code><code class="n">InputSplit</code><code class="o">&gt;</code> <code class="nf">getSplits</code><code class="o">(</code><code class="n">JobContext</code> <code class="n">context</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code><code class="o">;</code>
  
  <code class="k">public</code> <code class="k">abstract</code> <code class="n">RecordReader</code><code class="o">&lt;</code><code class="n">K</code><code class="o">,</code> <code class="n">V</code><code class="o">&gt;</code>
      <code class="n">createRecordReader</code><code class="o">(</code><code class="n">InputSplit</code> <code class="n">split</code><code class="o">,</code> <code class="n">TaskAttemptContext</code> <code class="n">context</code><code class="o">)</code> 
          <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code><code class="o">;</code>
<code class="o">}</code></pre><p class="calibre2">The client running the job calculates the splits for the job by
      calling <code class="literal">getSplits()</code>, then sends them
      to the application master, which uses their storage locations to
      schedule map tasks that will process them on the cluster. The map task
      passes the split to the <code class="literal">createRecordReader()</code> method on <code class="literal">InputFormat</code> to obtain a <code class="literal">RecordReader</code> for that split. A <code class="literal">RecordReader</code> is little <a class="calibre" id="calibre_link-3172"></a>more than an iterator over records, and the map task uses
      one to generate record key-value pairs, which it passes to the map
      function. We can see this by looking at the <code class="literal">Mapper</code>’s <code class="literal">run()</code>
      method:</p><a id="calibre_link-4247" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="kt">void</code> <code class="nf">run</code><code class="o">(</code><code class="n">Context</code> <code class="n">context</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
  <code class="n">setup</code><code class="o">(</code><code class="n">context</code><code class="o">);</code>
  <code class="k">while</code> <code class="o">(</code><code class="n">context</code><code class="o">.</code><code class="na">nextKeyValue</code><code class="o">())</code> <code class="o">{</code>
    <code class="n">map</code><code class="o">(</code><code class="n">context</code><code class="o">.</code><code class="na">getCurrentKey</code><code class="o">(),</code> <code class="n">context</code><code class="o">.</code><code class="na">getCurrentValue</code><code class="o">(),</code> <code class="n">context</code><code class="o">);</code>
  <code class="o">}</code>
  <code class="n">cleanup</code><code class="o">(</code><code class="n">context</code><code class="o">);</code>
<code class="o">}</code></pre><p class="calibre2">After running <code class="literal">setup()</code>, the <code class="literal">nextKeyValue()</code> is called repeatedly on the
      <code class="literal">Context</code> (which delegates to the
      identically named method on the <code class="literal">RecordReader</code>) to populate the key and value
      objects for the mapper. The key and value are retrieved from the
      <code class="literal">RecordReader</code> by way of the <code class="literal">Context</code> and are passed to the <code class="literal">map()</code> method for it to do its work. When the
      reader gets to the end of the stream, the <code class="literal">nextKeyValue()</code> method returns <code class="literal">false</code>, and the map task runs its
      <code class="literal">cleanup()</code> method and then completes.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">Although it’s not shown in the code snippet, for reasons of
        efficiency, <code class="literal">RecordReader</code> implementations will
        return the <span class="calibre">same</span> key and value
        objects on each call to <code class="literal">getCurrentKey()</code> and
        <code class="literal">getCurrentValue()</code>. Only the contents of these
        objects are changed by the reader’s
        <code class="literal">nextKeyValue()</code> method. This can be a surprise
        to users, who might expect keys and values to be immutable and not to
        be reused. This causes problems when a reference to a key or value
        object is retained outside the <code class="literal">map()</code> method,
        as its value can change without warning. If you need to do this, make
        a copy of the object you want to hold on to. For example, for a
        <code class="literal">Text</code> object, you can use its copy
        constructor: <code class="literal">new
        Text(value)</code>.</p><p class="calibre2">The situation is similar with reducers. In this case, the value
        objects in the reducer’s iterator are reused, so you need to copy any
        that you need to retain between calls to the iterator (see <a class="ulink" href="#calibre_link-576" title="Example&nbsp;9-11.&nbsp;Reducer for joining tagged station records with tagged weather records">Example&nbsp;9-11</a>).</p></div><p class="calibre2">Finally, note that the <code class="literal">Mapper</code>’s
      <code class="literal">run()</code> method is public and may be customized by
      users. <code class="literal">MultithreadedMapper</code> is an
      <a class="calibre" id="calibre_link-2729"></a>implementation that runs mappers concurrently in a
      configurable number of threads (set <a class="calibre" id="calibre_link-2595"></a>by <code class="literal">mapreduce.mapper.multithreadedmapper.threads</code>).
      For most data processing tasks, it confers no advantage over the default
      implementation. However, for mappers that spend a long time processing
      each record—because they contact external servers, for example—it allows
      multiple mappers to run in one JVM with little <a class="calibre" id="calibre_link-2112"></a>contention.</p><div class="book" title="FileInputFormat"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4248">FileInputFormat</h4></div></div></div><p class="calibre2"><code class="literal">FileInputFormat</code> is the base
        <a class="calibre" id="calibre_link-1638"></a><a class="calibre" id="calibre_link-2117"></a>class for all implementations of <code class="literal">InputFormat</code> that use <a class="calibre" id="calibre_link-2122"></a>files as their data source (see <a class="ulink" href="#calibre_link-577" title="Figure&nbsp;8-2.&nbsp;InputFormat class hierarchy">Figure&nbsp;8-2</a>). It provides two things: a
        place to define which files are included as the input to a job, and an
        implementation for generating splits for the input files. The job of
        dividing splits into records is performed by subclasses.</p><div class="figure"><a id="calibre_link-577" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4249" class="calibre"></a><img alt="InputFormat class hierarchy" src="images/000072.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;8-2.&nbsp;InputFormat class hierarchy</div></div></div><div class="book" title="FileInputFormat input paths"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4250">FileInputFormat input paths</h4></div></div></div><p class="calibre2">The input to a job is specified as a collection of paths, which
        offers great flexibility in constraining the input. <code class="literal">FileInputFormat</code> offers four static
        convenience methods for setting <a class="calibre" id="calibre_link-2212"></a>a <code class="literal">Job</code>’s input
        paths:</p><a id="calibre_link-4251" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">addInputPath</code><code class="o">(</code><code class="n">Job</code> <code class="n">job</code><code class="o">,</code> <code class="n">Path</code> <code class="n">path</code><code class="o">)</code>
<code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">addInputPaths</code><code class="o">(</code><code class="n">Job</code> <code class="n">job</code><code class="o">,</code> <code class="n">String</code> <code class="n">commaSeparatedPaths</code><code class="o">)</code>
<code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">setInputPaths</code><code class="o">(</code><code class="n">Job</code> <code class="n">job</code><code class="o">,</code> <code class="n">Path</code><code class="o">...</code> <code class="n">inputPaths</code><code class="o">)</code>
<code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">setInputPaths</code><code class="o">(</code><code class="n">Job</code> <code class="n">job</code><code class="o">,</code> <code class="n">String</code> <code class="n">commaSeparatedPaths</code><code class="o">)</code></pre><p class="calibre2">The <code class="literal">addInputPath()</code> and
        <code class="literal">addInputPaths()</code> methods add a path
        or paths to the list of inputs. You can call these methods repeatedly
        to build the list of paths. The <code class="literal">setInputPaths()</code> methods set the entire list
        of paths in one go (replacing any paths set on the <code class="literal">Job</code> in previous calls).</p><p class="calibre2">A path may represent a file, a directory, or, by using a glob, a
        collection of files and directories. A path representing a directory
        includes all the files in the directory as input to the job. See <a class="ulink" href="#calibre_link-578" title="File patterns">File patterns</a> for more on using globs.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">The contents of a directory specified as an input path are not
          processed recursively. In fact, the directory should only contain
          files. If the directory contains a subdirectory, it will be
          interpreted as a file, which will cause an error. The way to handle
          this case is to use a file glob or a filter to select only the files
          <a class="calibre" id="calibre_link-2535"></a>in the directory based on a name pattern.
          Alternatively, <code class="literal">mapreduce.input.fileinputformat.input.dir.recursive</code>
          can be set to <code class="literal">true</code> to force the
          input directory to be read recursively.</p></div><p class="calibre2">The add and set methods allow files to be specified by inclusion
        only. To exclude certain files from the input, you can set a filter
        using the <code class="literal">setInputPathFilter()</code>
        method on <code class="literal">FileInputFormat</code>. Filters
        are discussed in more detail in <a class="ulink" href="#calibre_link-182" title="PathFilter">PathFilter</a>.</p><p class="calibre2">Even if you don’t set a filter, <code class="literal">FileInputFormat</code> uses a default filter that
        excludes hidden files (those whose names begin with a dot or an
        underscore). If you set a filter by calling <code class="literal">setInputPathFilter()</code>, it acts in addition to
        the default filter. In other words, only nonhidden files that are
        accepted by your filter get through.</p><p class="calibre2">Paths and filters can be set through configuration properties,
        too (<a class="ulink" href="#calibre_link-579" title="Table&nbsp;8-4.&nbsp;Input path and filter properties">Table&nbsp;8-4</a>), which can be handy
        for Streaming jobs. Setting paths is done with the <code class="literal">-input</code> option for the Streaming interface,
        so setting paths directly usually is not needed.</p><div class="table"><a id="calibre_link-579" class="calibre"></a><div class="table-title">Table&nbsp;8-4.&nbsp;Input path and filter properties</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Type</td><td class="calibre20">Default
                value</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.input.fileinputformat.inputdir</code></td><td class="calibre23">Comma-separated
                paths</td><td class="calibre23">None</td><td class="calibre25">The input <a class="calibre" id="calibre_link-2536"></a>files for a job. Paths that contain commas
                should have those commas escaped by a backslash character. For
                example, the glob <code class="uri">{a,b}</code>
                would be escaped as <code class="uri">{a\,b}</code>.</td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">mapreduce.input.pathFilter.class</code></td><td class="calibre27"><code class="uri">PathFilter</code> classname</td><td class="calibre27">None</td><td class="calibre28">The <a class="calibre" id="calibre_link-2542"></a>filter to apply to the input files for a
                job.</td></tr></tbody></table></div></div></div><div class="book" title="FileInputFormat input splits"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-572">FileInputFormat input splits</h4></div></div></div><p class="calibre2">Given a set of files, how does <code class="literal">FileInputFormat</code> turn them into splits?
        <code class="literal">FileInputFormat</code> splits only large
        files—here, “large” means larger than an HDFS block. The split size is
        normally the size of an HDFS block, which is appropriate for most
        applications; however, it is possible to control this value by setting
        various Hadoop properties, as shown in <a class="ulink" href="#calibre_link-580" title="Table&nbsp;8-5.&nbsp;Properties for controlling split size">Table&nbsp;8-5</a>.</p><div class="table"><a id="calibre_link-580" class="calibre"></a><div class="table-title">Table&nbsp;8-5.&nbsp;Properties for controlling split size</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Type</td><td class="calibre20">Default value</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.input.fileinputformat.split.minsize</code></td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">1</td><td class="calibre25">The smallest <a class="calibre" id="calibre_link-2538"></a>valid size in bytes for a file split</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.input.fileinputformat.split.maxsize</code>
                <sup class="calibre5">[<a class="firstname" href="#calibre_link-581" id="calibre_link-582">a</a>]</sup></td><td class="calibre23"><code class="uri">long</code></td><td class="calibre23"><code class="uri">Long.MAX_VALUE</code>
                (i.e., 9223372036854775807)</td><td class="calibre25">The largest <a class="calibre" id="calibre_link-2537"></a>valid size in bytes for a file split</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">dfs.blocksize</code></td><td class="calibre27"><code class="uri">long</code></td><td class="calibre27">128 MB (i.e., 134217728)</td><td class="calibre28">The size <a class="calibre" id="calibre_link-1450"></a><a class="calibre" id="calibre_link-1018"></a>of a block in HDFS in bytes</td></tr></tbody><tbody class="calibre22"><tr class="calibre19"><td colspan="4" class="calibre28"><div class="footnote1" id="calibre_link-581"><p class="calibre2"><sup class="calibre37">[<a class="firstname" href="#calibre_link-582">a</a>] </sup>This property is not present in the old MapReduce
                    API (with the exception of <code class="literal2">CombineFileInputFormat</code>).
                    Instead, it is calculated indirectly as the size of the
                    total input for the job, divided by the guide number of
                    map tasks specified by <code class="literal2">mapreduce.job.maps</code> (or the
                    <code class="literal2">setNumMapTasks()</code> method on
                    <code class="literal2">JobConf</code>). Because the
                    number of map tasks defaults to 1, this makes the maximum
                    split size the size of the input.</p></div></td></tr></tbody></table></div></div><p class="calibre2">The minimum split size is usually 1 byte, although some formats
        have a lower bound on the split size. (For example, sequence files
        insert sync entries every so often in the stream, so the minimum split
        size has to be large enough to ensure that every split has a sync
        point to allow the reader to resynchronize with a record boundary. See
        <a class="ulink" href="#calibre_link-583" title="Reading a SequenceFile">Reading a SequenceFile</a>.)</p><p class="calibre2">Applications may impose a minimum split size. By setting this to
        a value larger than the block size, they can force splits to be larger
        than a block. There is no good reason for doing this when using HDFS,
        because doing so will increase the number of blocks that are not local
        to a map task.</p><p class="calibre2">The maximum split size defaults to the maximum value that can be
        represented by a Java <code class="literal">long</code> type. It
        has an effect only when it is less than the block size, forcing splits
        to be smaller than a block.</p><p class="calibre2">The split size is calculated by the following formula (see the
        <code class="literal">computeSplitSize()</code> method in <code class="literal">FileInputFormat</code>):</p><a id="calibre_link-4252" class="calibre"></a><pre class="screen1"><span class="calibre"><code class="n">max</code></span><code class="o">(</code><code class="n">minimumSize</code><code class="o">,</code> <span class="calibre"><code class="n">min</code></span><code class="o">(</code><code class="n">maximumSize</code><code class="o">,</code> <code class="n">blockSize</code><code class="o">))</code></pre><p class="calibre2">and by default:</p><a id="calibre_link-4253" class="calibre"></a><pre class="screen1">minimumSize &lt; blockSize &lt; maximumSize</pre><p class="calibre2">so the split size is <code class="literal">blockSize</code>. Various settings for these
        parameters and how they affect the <a class="calibre" id="calibre_link-2116"></a>final split size are illustrated in <a class="ulink" href="#calibre_link-584" title="Table&nbsp;8-6.&nbsp;Examples of how to control the split size">Table&nbsp;8-6</a>.</p><div class="table"><a id="calibre_link-584" class="calibre"></a><div class="table-title">Table&nbsp;8-6.&nbsp;Examples of how to control the split size</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Minimum split
                size</td><td class="calibre20">Maximum split size</td><td class="calibre20">Block
                size</td><td class="calibre20">Split
                size</td><td class="calibre21">Comment</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">1 (default)</td><td class="calibre23"><code class="uri">Long.MAX_VALUE</code>
                (default)</td><td class="calibre23">128 MB (default)</td><td class="calibre23">128 MB</td><td class="calibre25">By default, the split size is the same as the default
                block size.</td></tr><tr class="calibre26"><td class="calibre23">1 (default)</td><td class="calibre23"><code class="uri">Long.MAX_VALUE</code>
                (default)</td><td class="calibre23">256 MB</td><td class="calibre23">256 MB</td><td class="calibre25">The most natural way to increase the split size is to
                have larger blocks in HDFS, either by setting <code class="uri">dfs.blocksize</code> or by configuring this
                on a per-file basis at file construction time.</td></tr><tr class="calibre19"><td class="calibre23">256 MB</td><td class="calibre23"><code class="uri">Long.MAX_VALUE</code>
                (default)</td><td class="calibre23">128 MB (default)</td><td class="calibre23">256 MB</td><td class="calibre25">Making the minimum split size greater than the block
                size increases the split size, but at the cost of
                locality.</td></tr><tr class="calibre26"><td class="calibre27">1 (default)</td><td class="calibre27">64 MB</td><td class="calibre27">128 MB (default)</td><td class="calibre27">64 MB</td><td class="calibre28">Making the maximum split size less than the block size
                decreases the split <a class="calibre" id="calibre_link-1639"></a><a class="calibre" id="calibre_link-2118"></a>size.</td></tr></tbody></table></div></div></div><div class="book" title="Small files and CombineFileInputFormat"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-301">Small files and CombineFileInputFormat</h4></div></div></div><p class="calibre2">Hadoop works better with a <a class="calibre" id="calibre_link-2114"></a><a class="calibre" id="calibre_link-1178"></a>small number of large files than a large number of small
        files. One reason for this is that <code class="literal">FileInputFormat</code> generates splits in such a
        way that each split is all or part of a single file. If the file is
        very small (“small” means significantly smaller than an HDFS block)
        and there are a lot of them, each map task will process very little
        input, and there will be a lot of them (one per file), each of which
        imposes extra bookkeeping overhead. Compare a 1 GB file broken into
        eight 128 MB blocks with 10,000 or so 100 KB files. The 10,000 files
        use one map each, and the job time can be tens or hundreds of times
        slower than the equivalent one with a single input file and eight map
        tasks.</p><p class="calibre2">The situation is alleviated somewhat by <code class="literal">CombineFileInputFormat</code>, which was designed
        to work well with small files. Where <code class="literal">FileInputFormat</code> creates a split per file,
        <code class="literal">Combine</code><code class="literal">FileInputFormat</code> packs many files
        into each split so that each mapper has more to process. Crucially,
        <code class="literal">CombineFileInputFormat</code> takes node
        and rack locality into account when deciding which blocks to place in
        the same split, so it does not compromise the speed at which it can
        process the input in a typical MapReduce job.</p><p class="calibre2">Of course, if possible, it is still a good idea to avoid the
        many small files case, because MapReduce works best when it can
        operate at the transfer rate of the disks in the cluster, and
        processing many small files increases the number of seeks that are
        needed to run a job. Also, storing large numbers of small files in
        HDFS is wasteful of the namenode’s memory. One technique for avoiding
        the many small files case is to merge small files into larger files by
        using a sequence file, as in <a class="ulink" href="#calibre_link-585" title="Example&nbsp;8-4.&nbsp;A MapReduce program for packaging a collection of small files as a single SequenceFile">Example&nbsp;8-4</a>; with this approach,
        the keys can act as filenames (or a constant such as <code class="literal">NullWritable</code>, if not needed) and the values
        as file contents. But if you already have a large number of small
        files in HDFS, then <code class="literal">CombineFileInputFormat</code> is worth
        trying.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2"><code class="literal">CombineFileInputFormat</code>
          isn’t just good for small files. It can bring benefits when
          processing large files, too, since it will generate one split per
          node, which may be made up of multiple blocks. Essentially, <code class="literal">CombineFileInputFormat</code>
          decouples the amount of data that a mapper consumes from the block
          size of the files in HDFS.</p></div></div><div class="book" title="Preventing splitting"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4254">Preventing splitting</h4></div></div></div><p class="calibre2">Some applications <a class="calibre" id="calibre_link-2120"></a>don’t want files to be split, as this allows a single
        mapper to process each input file in its entirety. For example, a
        simple way to check if all the records in a file are sorted is to go
        through the records in order, checking whether each record is not less
        than the preceding one. Implemented as a map task, this algorithm will
        work only if one map processes the whole file.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-586" id="calibre_link-604">56</a>]</sup></p><p class="calibre2">There are a couple of ways to ensure that an existing file is
        not split. The first (quick-and-dirty) way is to increase the minimum
        split size to be larger than the largest file in your system. Setting
        it to its maximum value, <code class="literal">Long.MAX_VALUE</code>, has this effect. The second
        is to subclass the concrete subclass of <code class="literal">FileInputFormat</code> that you want to use, to
        override the <code class="literal">isSplitable()</code>
        method<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-587" id="calibre_link-605">57</a>]</sup> to return <code class="literal">false</code>. For
        example, here’s a nonsplittable <code class="literal">TextInputFormat</code>:</p><a id="calibre_link-4255" class="calibre"></a><pre class="screen1"><code class="k">import</code> <code class="nn">org.apache.hadoop.fs.Path</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.mapreduce.JobContext</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.mapreduce.lib.input.TextInputFormat</code><code class="o">;</code>

<code class="k">public</code> <code class="k">class</code> <code class="nc">NonSplittableTextInputFormat</code> <code class="k">extends</code> <code class="n">TextInputFormat</code> <code class="o">{</code>
  <code class="nd">@Override</code>
  <code class="k">protected</code> <code class="kt">boolean</code> <code class="nf">isSplitable</code><code class="o">(</code><code class="n">JobContext</code> <code class="n">context</code><code class="o">,</code> <code class="n">Path</code> <code class="n">file</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">return</code> <code class="k">false</code><code class="o">;</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div><div class="book" title="File information in the mapper"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-511">File information in the mapper</h4></div></div></div><p class="calibre2">A mapper processing a <a class="calibre" id="calibre_link-2119"></a><a class="calibre" id="calibre_link-2415"></a>file input split can find information about the split by
        calling the <code class="literal">getInputSplit()</code> method on the
        <code class="literal">Mapper</code>’s <code class="literal">Context</code> object. When the input format
        derives from <code class="literal">FileInputFormat</code>, the
        <code class="literal">InputSplit</code> returned by this method
        can be cast to a <code class="literal">FileSplit</code> to
        access the file information listed in <a class="ulink" href="#calibre_link-588" title="Table&nbsp;8-7.&nbsp;File split properties">Table&nbsp;8-7</a>.</p><p class="calibre2">In the old MapReduce API, and the Streaming interface, the same
        file split information is made available through properties that can
        be read from the mapper’s configuration.
        (In the old MapReduce API this is achieved by implementing
        <code class="literal">configure()</code> in your <code class="literal">Mapper</code> implementation to get access to the
        <code class="literal">JobConf</code>
        object.)</p><p class="calibre2">In addition to the properties in <a class="ulink" href="#calibre_link-588" title="Table&nbsp;8-7.&nbsp;File split properties">Table&nbsp;8-7</a>, all mappers and reducers have access
        to the properties listed in <a class="ulink" href="#calibre_link-589" title="The Task Execution Environment">The Task Execution Environment</a>.</p><div class="table"><a id="calibre_link-588" class="calibre"></a><div class="table-title">Table&nbsp;8-7.&nbsp;File split properties</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre36"><col class="c4"><col class="c3"><col class="calibre38"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">FileSplit method</td><td class="calibre20">Property name</td><td class="calibre20">Type</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">getPath()</code></td><td class="calibre23"><code class="uri">mapreduce.map.input.file</code></td><td class="calibre23"><code class="uri">Path</code>/<code class="uri">String</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-2576"></a>path of the input file being processed</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">getStart()</code></td><td class="calibre23"><code class="uri">mapreduce.map.input.start</code></td><td class="calibre23"><code class="uri">long</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-2578"></a>byte offset of the start of the split from the
                beginning of the file</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">getLength()</code></td><td class="calibre27"><code class="uri">mapreduce.map.input.length</code></td><td class="calibre27"><code class="uri">long</code></td><td class="calibre28">The <a class="calibre" id="calibre_link-2577"></a>length of the split in bytes</td></tr></tbody></table></div></div><p class="calibre2">In the next section, we’ll see how to use a <code class="literal">FileSplit</code> when we need to access the split’s
        filename.</p></div><div class="book" title="Processing a whole file as a record"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-265">Processing a whole file as a record</h4></div></div></div><p class="calibre2">A related requirement <a class="calibre" id="calibre_link-3174"></a><a class="calibre" id="calibre_link-1633"></a>that sometimes crops up is for mappers to have access to
        the full contents of a file. Not splitting the file gets you part of
        the way there, but you also need to have a <code class="literal">Record</code><span class="calibre"><code class="literal">Reader</code></span> that delivers the file
        contents as the value of the record. The listing for <code class="literal">WholeFileInputFormat</code> in <a class="ulink" href="#calibre_link-590" title="Example&nbsp;8-2.&nbsp;An InputFormat for reading a whole file as a record">Example&nbsp;8-2</a> shows a way of doing this.</p><div class="example"><a id="calibre_link-590" class="calibre"></a><div class="example-title">Example&nbsp;8-2.&nbsp;An InputFormat for reading a whole file as a record</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">WholeFileInputFormat</code>
    <code class="k">extends</code> <code class="n">FileInputFormat</code><code class="o">&lt;</code><code class="n">NullWritable</code><code class="o">,</code> <code class="n">BytesWritable</code><code class="o">&gt;</code> <code class="o">{</code>
  
  <code class="nd">@Override</code>
  <code class="k">protected</code> <code class="kt">boolean</code> <code class="nf">isSplitable</code><code class="o">(</code><code class="n">JobContext</code> <code class="n">context</code><code class="o">,</code> <code class="n">Path</code> <code class="n">file</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">return</code> <code class="k">false</code><code class="o">;</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="n">RecordReader</code><code class="o">&lt;</code><code class="n">NullWritable</code><code class="o">,</code> <code class="n">BytesWritable</code><code class="o">&gt;</code> <code class="n">createRecordReader</code><code class="o">(</code>
      <code class="n">InputSplit</code> <code class="n">split</code><code class="o">,</code> <code class="n">TaskAttemptContext</code> <code class="n">context</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code>
      <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="n">WholeFileRecordReader</code> <code class="n">reader</code> <code class="o">=</code> <code class="k">new</code> <code class="n">WholeFileRecordReader</code><code class="o">();</code>
    <code class="n">reader</code><code class="o">.</code><code class="na">initialize</code><code class="o">(</code><code class="n">split</code><code class="o">,</code> <code class="n">context</code><code class="o">);</code>
    <code class="k">return</code> <code class="n">reader</code><code class="o">;</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2"><code class="literal">WholeFileInputFormat</code> defines
        a format where the keys are not used, represented by <code class="literal">NullWritable</code>, and the values are the file
        contents, <a class="calibre" id="calibre_link-1035"></a>represented by <code class="literal">BytesWritable</code> instances. It defines two
        methods. First, the format is careful to specify that input files
        should never be split, by overriding <code class="literal">isSplitable()</code> to return <code class="literal">false</code>. Second, we implement <code class="literal">createRecordReader()</code> to return a custom
        implementation of <code class="literal">RecordReader</code>, which <a class="calibre" id="calibre_link-3173"></a>appears in <a class="ulink" href="#calibre_link-591" title="Example&nbsp;8-3.&nbsp;The RecordReader used by WholeFileInputFormat for reading a whole file as a record">Example&nbsp;8-3</a>.</p><div class="example"><a id="calibre_link-591" class="calibre"></a><div class="example-title">Example&nbsp;8-3.&nbsp;The RecordReader used by WholeFileInputFormat for reading a
          whole file as a record</div><div class="book"><pre class="screen"><code class="k">class</code> <code class="nc">WholeFileRecordReader</code> <code class="k">extends</code> <code class="n">RecordReader</code><code class="o">&lt;</code><code class="n">NullWritable</code><code class="o">,</code> <code class="n">BytesWritable</code><code class="o">&gt;</code> <code class="o">{</code>
  
  <code class="k">private</code> <code class="n">FileSplit</code> <code class="n">fileSplit</code><code class="o">;</code>
  <code class="k">private</code> <code class="n">Configuration</code> <code class="n">conf</code><code class="o">;</code>
  <code class="k">private</code> <code class="n">BytesWritable</code> <code class="n">value</code> <code class="o">=</code> <code class="k">new</code> <code class="n">BytesWritable</code><code class="o">();</code>
  <code class="k">private</code> <code class="kt">boolean</code> <code class="n">processed</code> <code class="o">=</code> <code class="k">false</code><code class="o">;</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">initialize</code><code class="o">(</code><code class="n">InputSplit</code> <code class="n">split</code><code class="o">,</code> <code class="n">TaskAttemptContext</code> <code class="n">context</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="k">this</code><code class="o">.</code><code class="na">fileSplit</code> <code class="o">=</code> <code class="o">(</code><code class="n">FileSplit</code><code class="o">)</code> <code class="n">split</code><code class="o">;</code>
    <code class="k">this</code><code class="o">.</code><code class="na">conf</code> <code class="o">=</code> <code class="n">context</code><code class="o">.</code><code class="na">getConfiguration</code><code class="o">();</code>
  <code class="o">}</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">boolean</code> <code class="nf">nextKeyValue</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(!</code><code class="n">processed</code><code class="o">)</code> <code class="o">{</code>
      <code class="kt">byte</code><code class="o">[]</code> <code class="n">contents</code> <code class="o">=</code> <code class="k">new</code> <code class="kt">byte</code><code class="o">[(</code><code class="kt">int</code><code class="o">)</code> <code class="n">fileSplit</code><code class="o">.</code><code class="na">getLength</code><code class="o">()];</code>
      <code class="n">Path</code> <code class="n">file</code> <code class="o">=</code> <code class="n">fileSplit</code><code class="o">.</code><code class="na">getPath</code><code class="o">();</code>
      <code class="n">FileSystem</code> <code class="n">fs</code> <code class="o">=</code> <code class="n">file</code><code class="o">.</code><code class="na">getFileSystem</code><code class="o">(</code><code class="n">conf</code><code class="o">);</code>
      <code class="n">FSDataInputStream</code> <code class="n">in</code> <code class="o">=</code> <code class="k">null</code><code class="o">;</code>
      <code class="k">try</code> <code class="o">{</code>
        <code class="n">in</code> <code class="o">=</code> <code class="n">fs</code><code class="o">.</code><code class="na">open</code><code class="o">(</code><code class="n">file</code><code class="o">);</code>
        <code class="n">IOUtils</code><code class="o">.</code><code class="na">readFully</code><code class="o">(</code><code class="n">in</code><code class="o">,</code> <code class="n">contents</code><code class="o">,</code> <code class="mi">0</code><code class="o">,</code> <code class="n">contents</code><code class="o">.</code><code class="na">length</code><code class="o">);</code>
        <code class="n">value</code><code class="o">.</code><code class="na">set</code><code class="o">(</code><code class="n">contents</code><code class="o">,</code> <code class="mi">0</code><code class="o">,</code> <code class="n">contents</code><code class="o">.</code><code class="na">length</code><code class="o">);</code>
      <code class="o">}</code> <code class="k">finally</code> <code class="o">{</code>
        <code class="n">IOUtils</code><code class="o">.</code><code class="na">closeStream</code><code class="o">(</code><code class="n">in</code><code class="o">);</code>
      <code class="o">}</code>
      <code class="n">processed</code> <code class="o">=</code> <code class="k">true</code><code class="o">;</code>
      <code class="k">return</code> <code class="k">true</code><code class="o">;</code>
    <code class="o">}</code>
    <code class="k">return</code> <code class="k">false</code><code class="o">;</code>
  <code class="o">}</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="n">NullWritable</code> <code class="nf">getCurrentKey</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="k">return</code> <code class="n">NullWritable</code><code class="o">.</code><code class="na">get</code><code class="o">();</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="n">BytesWritable</code> <code class="nf">getCurrentValue</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code>
      <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="k">return</code> <code class="n">value</code><code class="o">;</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">float</code> <code class="nf">getProgress</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="k">return</code> <code class="n">processed</code> <code class="o">?</code> <code class="mi">1.0f</code> <code class="o">:</code> <code class="mi">0.0f</code><code class="o">;</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">close</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="c2">// do nothing</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2"><code class="literal">WholeFileRecordReader</code> is
        responsible for <a class="calibre" id="calibre_link-1647"></a>taking a <code class="literal">FileSplit</code>
        and converting it into a single record, with a <code class="literal">null</code>
        key and a value containing the bytes of the file. Because there is
        only a single record, <code class="literal">WholeFileRecordReader</code> has either processed
        it or not, so it maintains a Boolean called <code class="literal">processed</code>. If the file has not been
        processed when the <code class="literal">nextKeyValue()</code> method is
        called, then we open the file, create a byte array whose length is the
        length of the file, and use the <a class="calibre" id="calibre_link-2151"></a>Hadoop <code class="literal">IOUtils</code> class
        to slurp the file into the byte array. Then we set the array on the
        <code class="literal">BytesWritable</code> instance that was
        passed into the <code class="literal">next()</code> method, and
        return <code class="literal">true</code> to signal that a record
        has been read.</p><p class="calibre2">The other methods are straightforward bookkeeping methods for
        accessing the current key and value types and getting the progress of
        the reader, and a <code class="literal">close()</code> method,
        which is invoked by the MapReduce framework when the reader is
        done.</p><p class="calibre2">To demonstrate how <code class="literal">WholeFileInputFormat</code> can be used, consider a
        MapReduce job for packaging small files into sequence files, where the
        key is the original filename and the value is the content of the file.
        The listing is in <a class="ulink" href="#calibre_link-585" title="Example&nbsp;8-4.&nbsp;A MapReduce program for packaging a collection of small files as a single SequenceFile">Example&nbsp;8-4</a>.</p><div class="example"><a id="calibre_link-585" class="calibre"></a><div class="example-title">Example&nbsp;8-4.&nbsp;A MapReduce program for packaging a collection of small files
          as a single SequenceFile</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">SmallFilesToSequenceFileConverter</code> <code class="k">extends</code> <code class="n">Configured</code>
    <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>
  
  <code class="k">static</code> <code class="k">class</code> <code class="nc">SequenceFileMapper</code>
      <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">NullWritable</code><code class="o">,</code> <code class="n">BytesWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">BytesWritable</code><code class="o">&gt;</code> <code class="o">{</code>
    
    <code class="k">private</code> <code class="n">Text</code> <code class="n">filenameKey</code><code class="o">;</code>
    
    <code class="nd">@Override</code>
    <code class="k">protected</code> <code class="kt">void</code> <code class="nf">setup</code><code class="o">(</code><code class="n">Context</code> <code class="n">context</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code>
        <code class="n">InterruptedException</code> <code class="o">{</code>
      <code class="n">InputSplit</code> <code class="n">split</code> <code class="o">=</code> <code class="n">context</code><code class="o">.</code><code class="na">getInputSplit</code><code class="o">();</code>
      <code class="n">Path</code> <code class="n">path</code> <code class="o">=</code> <code class="o">((</code><code class="n">FileSplit</code><code class="o">)</code> <code class="n">split</code><code class="o">).</code><code class="na">getPath</code><code class="o">();</code>
      <code class="n">filenameKey</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="n">path</code><code class="o">.</code><code class="na">toString</code><code class="o">());</code>
    <code class="o">}</code>
    
    <code class="nd">@Override</code>
    <code class="k">protected</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">NullWritable</code> <code class="n">key</code><code class="o">,</code> <code class="n">BytesWritable</code> <code class="n">value</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
        <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
      <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">filenameKey</code><code class="o">,</code> <code class="n">value</code><code class="o">);</code>
    <code class="o">}</code>
    
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="n">JobBuilder</code><code class="o">.</code><code class="na">parseInputAndOutput</code><code class="o">(</code><code class="k">this</code><code class="o">,</code> <code class="n">getConf</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">job</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>
    
    <code class="n">job</code><code class="o">.</code><code class="na">setInputFormatClass</code><code class="o">(</code><code class="n">WholeFileInputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputFormatClass</code><code class="o">(</code><code class="n">SequenceFileOutputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputKeyClass</code><code class="o">(</code><code class="n">Text</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputValueClass</code><code class="o">(</code><code class="n">BytesWritable</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="n">job</code><code class="o">.</code><code class="na">setMapperClass</code><code class="o">(</code><code class="n">SequenceFileMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="k">return</code> <code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">;</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="k">new</code> <code class="n">SmallFilesToSequenceFileConverter</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">Because the input format is a <code class="literal">WholeFileInputFormat</code>, the mapper only has to
        find the filename for the input
        file split. It does this by casting the <code class="literal">InputSplit</code> from the context to a <code class="literal">FileSplit</code>, which has a method to retrieve
        the file path. The path is stored in a <code class="literal">Text</code> object for the key. The reducer is the
        identity (not explicitly set), and the output <a class="calibre" id="calibre_link-3346"></a>format is a <code class="literal">SequenceFileOutputFormat</code>.</p><p class="calibre2">Here’s a run on a few small files. We’ve chosen to use two
        reducers, so we get two output sequence files:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar hadoop-examples.jar SmallFilesToSequenceFileConverter \</code></strong>
<strong class="userinput"><code class="calibre9">  -conf conf/hadoop-localhost.xml -D mapreduce.job.reduces=2 \</code></strong>
<strong class="userinput"><code class="calibre9">  input/smallfiles output</code></strong></pre><p class="calibre2">Two part files are created, each of which is a sequence file. We
        can inspect these with the <code class="literal">-text</code>
        option to the filesystem shell:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -conf conf/hadoop-localhost.xml -text output/part-r-00000</code></strong>
hdfs://localhost/user/tom/input/smallfiles/a	61 61 61 61 61 61 61 61 61 61
hdfs://localhost/user/tom/input/smallfiles/c	63 63 63 63 63 63 63 63 63 63
hdfs://localhost/user/tom/input/smallfiles/e	
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -conf conf/hadoop-localhost.xml -text output/part-r-00001</code></strong>
hdfs://localhost/user/tom/input/smallfiles/b	62 62 62 62 62 62 62 62 62 62
hdfs://localhost/user/tom/input/smallfiles/d	64 64 64 64 64 64 64 64 64 64
hdfs://localhost/user/tom/input/smallfiles/f	66 66 66 66 66 66 66 66 66 66</pre><p class="calibre2">The input files were named <em class="calibre10">a</em>, <em class="calibre10">b</em>,
        <em class="calibre10">c</em>, <em class="calibre10">d</em>, <em class="calibre10">e</em>,
        and <em class="calibre10">f</em>, and each contained 10
        characters of the corresponding letter (so, for example, <em class="calibre10">a</em> contained 10 “a” characters), except
        <em class="calibre10">e</em>, which was empty. We can see
        this in the textual rendering of the sequence files, which prints the
        filename followed by the hex representation of the file.</p><div class="note" title="Tip"><h3 class="title3">Tip</h3><p class="calibre2">There’s at least one way we could improve this program. As
          mentioned earlier, having one mapper per file is inefficient, so
          subclassing <code class="literal">CombineFileInputFormat</code> instead of <code class="literal">FileInputFormat</code> would be a <a class="calibre" id="calibre_link-1634"></a><a class="calibre" id="calibre_link-3175"></a><a class="calibre" id="calibre_link-2106"></a>better approach.</p></div></div></div><div class="book" title="Text Input"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4256">Text Input</h3></div></div></div><p class="calibre2">Hadoop excels <a class="calibre" id="calibre_link-2108"></a>at processing unstructured text. In this section, we
      discuss the different <code class="literal">InputFormat</code>s
      that Hadoop provides to process text.</p><div class="book" title="TextInputFormat"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-836">TextInputFormat</h4></div></div></div><p class="calibre2"><code class="literal">TextInputFormat</code> is the
        <a class="calibre" id="calibre_link-3672"></a><a class="calibre" id="calibre_link-3675"></a>default <code class="literal">InputFormat</code>.
        Each record is a line of input. The key, a <code class="literal">LongWritable</code>, is the byte offset within the
        file of the beginning of the line. The value is the contents of the
        line, excluding any line terminators (e.g., newline or carriage
        return), and is packaged as a <code class="literal">Text</code> object. So, a file containing the
        following text:</p><pre class="screen1">On the top of the Crumpetty Tree
The Quangle Wangle sat,
But his face you could not see,
On account of his Beaver Hat.</pre><p class="calibre2">is divided into one split of four records. The records are
        interpreted as the following key-value pairs:</p><a id="calibre_link-4257" class="calibre"></a><pre class="screen1">(0, On the top of the Crumpetty Tree)
(33, The Quangle Wangle sat,)
(57, But his face you could not see,)
(89, On account of his Beaver Hat.)</pre><p class="calibre2">Clearly, the keys are <span class="calibre">not</span>
        line numbers. This would be impossible to implement in general, in
        that a file is broken into splits at byte, not line, boundaries.
        Splits are processed independently. Line numbers are really a
        sequential notion. You have to keep a count of lines as you consume
        them, so knowing the line number within a split would be possible, but
        not within the file.</p><p class="calibre2">However, the offset within the file of each line is known by
        each split independently of the other splits, since each split knows
        the size of the preceding splits and just adds this onto the offsets
        within the split to produce a global file offset. The offset is
        usually sufficient for applications that need a unique identifier for
        each line. Combined with the file’s name, it is unique within the
        filesystem. Of course, if all the lines are a fixed width, calculating
        the line number is simply a matter of dividing the offset by the
        width.</p><div class="sidebar"><a id="calibre_link-4258" class="calibre"></a><div class="sidebar-title">The Relationship Between Input Splits and HDFS Blocks</div><p class="calibre2">The logical records that <code class="literal">FileInputFormat</code>s define <a class="calibre" id="calibre_link-1015"></a><a class="calibre" id="calibre_link-2113"></a>usually do not fit neatly into HDFS blocks. For
          example, a <code class="literal">TextInputFormat</code>’s
          logical records are lines, which will cross HDFS boundaries more
          often than not. This has no bearing on the functioning of your
          program—lines are not missed or broken, for example—but it’s worth
          knowing about because it does mean that data-local maps (that is,
          maps that are running on the same host as their input data) will
          perform some remote reads. The slight overhead this causes is not
          normally significant.</p><p class="calibre2"><a class="ulink" href="#calibre_link-592" title="Figure&nbsp;8-3.&nbsp;Logical records and HDFS blocks for TextInputFormat">Figure&nbsp;8-3</a> shows an
          example. A single file is broken into lines, and the line boundaries
          do not correspond with the HDFS block boundaries. Splits honor
          logical record boundaries (in this case, lines), so we see that the
          first split contains line 5, even though it spans the first and
          second block. The second split starts at line 6.</p><div class="book"><div class="figure1"><a id="calibre_link-592" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4259" class="calibre"></a><img alt="Logical records and HDFS blocks for TextInputFormat" src="images/000082.png" class="calibre29"></div></div><div class="figure-title1">Figure&nbsp;8-3.&nbsp;Logical records and HDFS blocks for TextInputFormat</div></div></div></div><div class="book" title="Controlling the maximum line length"><div class="titlepage1"><div class="book"><div class="book"><h5 class="title8" id="calibre_link-4260">Controlling the maximum line length</h5></div></div></div><p class="calibre2">If you are using <a class="calibre" id="calibre_link-3668"></a><a class="calibre" id="calibre_link-2325"></a>one of the text input formats discussed here, you can
          set a maximum expected line length to safeguard against corrupted
          files. Corruption in a file can manifest itself as a very long line,
          which can cause out-of-memory errors and then task failure. By
          <a class="calibre" id="calibre_link-2541"></a>setting <code class="literal">mapreduce.input.linerecordreader.line.maxlength</code>
          to a value in bytes that fits in memory (and is comfortably greater
          than the length of lines in your input data), you ensure that the
          record reader will skip the (long) corrupt lines without the task
          failing.</p></div></div><div class="book" title="KeyValueTextInputFormat"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-598">KeyValueTextInputFormat</h4></div></div></div><p class="calibre2"><code class="literal">TextInputFormat</code>’s keys, being
        <a class="calibre" id="calibre_link-2305"></a><a class="calibre" id="calibre_link-3669"></a>simply the offsets within the file, are not normally
        very useful. It is common for each line in a file to be a key-value
        pair, separated by a delimiter such as a tab character. For example,
        this is the kind of output produced by <code class="literal">TextOutputFormat</code>, Hadoop’s default <code class="literal">OutputFormat</code>. To interpret such files
        correctly, <code class="literal">KeyValueTextInputFormat</code>
        is appropriate.</p><p class="calibre2"><span class="calibre">You can specify the separator via the <code class="literal">mapreduce.input.keyvaluelinerecord</code></span><span class="calibre"><code class="literal">reader.key.value.separator</code>
        </span>property. <a class="calibre" id="calibre_link-2539"></a>It is a tab character by default. Consider the following
        input file, where <span class="calibre">→</span> represents a
        (horizontal) tab character:</p><pre class="screen1">line1→On the top of the Crumpetty Tree
line2→The Quangle Wangle sat,
line3→But his face you could not see,
line4→On account of his Beaver Hat.</pre><p class="calibre2">Like in the <code class="literal">TextInputFormat</code>
        case, the input is in a single split comprising four records, although
        this time the keys are the <code class="literal">Text</code>
        sequences before the tab in each line:</p><a id="calibre_link-4261" class="calibre"></a><pre class="screen1">(line1, On the top of the Crumpetty Tree)
(line2, The Quangle Wangle sat,)
(line3, But his face you could not see,)
(line4, On account of his Beaver Hat.)</pre></div><div class="book" title="NLineInputFormat"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-473">NLineInputFormat</h4></div></div></div><p class="calibre2">With <code class="literal">TextInputFormat</code> and
        <code class="literal">KeyValueTextInputFormat</code>, each
        <a class="calibre" id="calibre_link-2791"></a><a class="calibre" id="calibre_link-3670"></a>mapper receives a variable number of lines of input. The
        number depends on the size of the split and the length of the lines.
        If you want your mappers to receive a fixed number of lines of input,
        then <code class="literal">NLineInputFormat</code> is the
        <code class="literal">InputFormat</code> to use. Like with
        <code class="literal">TextInputFormat</code>, the keys are the
        byte offsets within the file and the values are the lines
        themselves.</p><p class="calibre2"><span class="calibre">N</span> refers to the number of
        lines of input that each mapper receives. With <span class="calibre">N</span> set to 1
        (the default), each mapper receives exactly one line of
        input. <a class="calibre" id="calibre_link-2540"></a>The <code class="literal">mapre</code><span class="calibre"><code class="literal">duce.input.lineinputformat.linespermap</code>
        property </span>controls the value of <span class="calibre">N</span>. <span class="calibre">By way of
        example,</span> consider these four lines again:</p><pre class="screen1">On the top of the Crumpetty Tree
The Quangle Wangle sat,
But his face you could not see,
On account of his Beaver Hat.</pre><p class="calibre2">If, for example, <span class="calibre">N</span> is 2, then
        each split contains two lines. One mapper will receive the first two
        key-value pairs:</p><a id="calibre_link-4262" class="calibre"></a><pre class="screen1">(0, On the top of the Crumpetty Tree)
(33, The Quangle Wangle sat,)</pre><p class="calibre2">And another mapper will receive the second two key-value
        pairs:</p><a id="calibre_link-4263" class="calibre"></a><pre class="screen1">(57, But his face you could not see,)
(89, On account of his Beaver Hat.)</pre><p class="calibre2">The keys and values are the same as those that <code class="literal">TextInputFormat</code> produces. The difference is
        in the way the splits are constructed.</p><p class="calibre2">Usually, having a map task for a small number of lines of input
        is inefficient (due to the overhead in task setup), but there are
        applications that take a small amount of input data and run an
        extensive (i.e., CPU-intensive) computation for it, then emit their
        output. Simulations are a good example. By creating an input file that
        specifies input parameters, one per line, you can perform a
        <em class="calibre10">parameter sweep</em>: run a set of simulations in
        parallel to find how a model varies as the parameter changes.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">If you have long-running simulations, you may fall afoul of
          task timeouts. When a task doesn’t report progress for more than 10
          minutes, the application master assumes it has failed and aborts the
          process (see <a class="ulink" href="#calibre_link-593" title="Task Failure">Task Failure</a>).</p><p class="calibre2">The best way to guard against this is to report progress
          periodically, by writing a status message or incrementing a counter,
          for example. See <a class="ulink" href="#calibre_link-502" title="What Constitutes Progress in MapReduce?">What Constitutes Progress in MapReduce?</a>.</p></div><p class="calibre2">Another example is using Hadoop to bootstrap data loading from
        multiple data sources, such as
        databases. You create a “seed” input file that lists the data sources,
        one per line. Then each mapper is allocated a single data source, and
        it loads the data from that source into HDFS. The job doesn’t need the
        reduce phase, so the number of reducers should be set to zero (by
        calling <code class="literal">setNumReduceTasks()</code> on <code class="literal">Job</code>). Furthermore, MapReduce jobs can be run to process the
        data loaded into HDFS. See <a class="ulink" href="#calibre_link-464" title="Appendix&nbsp;C.&nbsp;Preparing the NCDC Weather Data">Appendix&nbsp;C</a> for an
        example.</p></div><div class="book" title="XML"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4264">XML</h4></div></div></div><p class="calibre2">Most XML parsers <a class="calibre" id="calibre_link-3674"></a><a class="calibre" id="calibre_link-3822"></a>operate on whole XML documents, so if a large XML
        document is made up of multiple input splits, it is a challenge to
        parse these individually. Of course, you can process the entire XML
        document in one mapper (if it is not too large) using the technique in
        <a class="ulink" href="#calibre_link-265" title="Processing a whole file as a record">Processing a whole file as a record</a>.</p><p class="calibre2">Large XML documents that are composed of a series of “records”
        (XML document fragments) can be broken into these records using simple
        string or regular-expression matching to find the start and end tags
        of records. This alleviates the problem when the document is split by
        the framework because the next start tag of a record is easy to find
        by simply scanning from the start of the split, just like <code class="literal">TextInputFormat</code> finds newline
        boundaries.</p><p class="calibre2">Hadoop comes with a class for this purpose <a class="calibre" id="calibre_link-3554"></a>called <code class="literal">StreamXmlRecordReader</code> (which is in
        <a class="calibre" id="calibre_link-2869"></a>the <code class="literal">org.apache.hadoop.streaming.mapreduce</code>
        package, although it can be used outside of Streaming). You can use it
        by setting your input format to <code class="literal">StreamInputFormat</code> and setting the <code class="literal">stream.recordreader.class</code> property
        <a class="calibre" id="calibre_link-3544"></a>to <code class="literal">org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader</code>.
        The reader is configured by setting job configuration properties to
        tell it the patterns for the start and end tags (see the class
        documentation for details).<sup class="calibre6">[<a class="firstname" href="#calibre_link-594" id="calibre_link-606">58</a>]</sup></p><p class="calibre2">To take an example, Wikipedia provides dumps of its content in
        XML form, which are appropriate for processing in parallel with
        MapReduce using this approach. The data is contained in one large XML
        wrapper document, which contains a series of elements, such as
        <code class="literal">page</code> elements that contain a page’s
        content and associated metadata. Using <code class="literal">StreamXmlRecordReader</code>, the <code class="literal">page</code> elements can be interpreted as records
        for processing by a <a class="calibre" id="calibre_link-2109"></a>mapper.</p></div></div><div class="book" title="Binary Input"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4265">Binary Input</h3></div></div></div><p class="calibre2">Hadoop MapReduce is <a class="calibre" id="calibre_link-2101"></a>not restricted to processing textual data. It has support
      for binary formats, too.</p><div class="book" title="SequenceFileInputFormat"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-270">SequenceFileInputFormat</h4></div></div></div><p class="calibre2">Hadoop’s sequence <a class="calibre" id="calibre_link-991"></a><a class="calibre" id="calibre_link-3344"></a>file format stores sequences of binary key-value pairs.
        Sequence files are well suited as a format for MapReduce data because
        they are splittable (they have sync points so that readers can
        synchronize with record boundaries from an arbitrary point in the
        file, such as the start of a split), they support compression as a
        part of the format, and they can store arbitrary types using a variety
        of serialization frameworks. (These topics are covered in <a class="ulink" href="#calibre_link-141" title="SequenceFile">SequenceFile</a>.)</p><p class="calibre2">To use data from sequence files as the input to MapReduce, you
        can use <code class="literal">SequenceFileInputFormat</code>.
        The keys and values are determined by the sequence file, and you need
        to make sure that your map input types correspond. For example, if
        your sequence file has <code class="literal">IntWritable</code>
        keys and <code class="literal">Text</code> values, like the one
        created in <a class="ulink" href="#calibre_link-226" title="Chapter&nbsp;5.&nbsp;Hadoop I/O">Chapter&nbsp;5</a>, then the map signature would be
        <code class="literal">Mapper&lt;IntWritable, Text, K,
        V&gt;</code>, where <code class="literal">K</code> and
        <code class="literal">V</code> are the types of the map’s output
        keys and values.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Although its name doesn’t give it away, <code class="literal">SequenceFileInputFormat</code> can read map files
          as well as sequence files. If it finds a directory where it was
          expecting a sequence file, <code class="literal">SequenceFileInputFormat</code> assumes that it is
          reading a map file and uses its datafile. This is why there is no
          <code class="literal">MapFileInputFormat</code> class.</p></div></div><div class="book" title="SequenceFileAsTextInputFormat"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4266">SequenceFileAsTextInputFormat</h4></div></div></div><p class="calibre2"><code class="literal">SequenceFileAsTextInputFormat</code>
        is a <a class="calibre" id="calibre_link-990"></a><a class="calibre" id="calibre_link-3343"></a>variant of <code class="literal">SequenceFileInputFormat</code> that converts the
        sequence file’s keys and values to <code class="literal">Text</code> objects. The conversion is performed by
        calling <code class="literal">toString()</code> on the keys and
        values. This format makes sequence files suitable input for
        Streaming.</p></div><div class="book" title="SequenceFileAsBinaryInputFormat"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4267">SequenceFileAsBinaryInputFormat</h4></div></div></div><p class="calibre2"><code class="literal">SequenceFileAsBinaryInputFormat</code> is a
        <a class="calibre" id="calibre_link-988"></a><a class="calibre" id="calibre_link-3341"></a>variant of <code class="literal">SequenceFileInputFormat</code> that retrieves the
        sequence file’s keys and values as opaque binary objects. They are
        encapsulated as <code class="literal">BytesWritable</code>
        objects, and the application is free to interpret the underlying byte
        array as it pleases. In combination with a process that creates
        sequence files with <code class="literal">SequenceFile.Writer</code>’s <code class="literal">appendRaw()</code> method or <code class="literal">SequenceFileAsBinaryOutputFormat</code>, this
        provides a way to use any binary data types with MapReduce (packaged
        as a sequence file), although plugging into Hadoop’s serialization
        mechanism is normally a cleaner alternative (see <a class="ulink" href="#calibre_link-256" title="Serialization Frameworks">Serialization Frameworks</a>).</p></div><div class="book" title="FixedLengthInputFormat"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4268">FixedLengthInputFormat</h4></div></div></div><p class="calibre2"><code class="literal">FixedLengthInputFormat</code> is for
        <a class="calibre" id="calibre_link-986"></a><a class="calibre" id="calibre_link-1699"></a>reading fixed-width binary records from a file, when the
        records are not separated by delimiters. The record size must be set
        via <code class="literal">fixedlengthinputformat.record.length</code>.</p></div></div><div class="book" title="Multiple Inputs"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-729">Multiple Inputs</h3></div></div></div><p class="calibre2">Although the input <a class="calibre" id="calibre_link-2107"></a><a class="calibre" id="calibre_link-2714"></a>to a MapReduce job may consist of multiple input files
      (constructed by a combination of file globs, filters, and plain paths),
      all of the input is interpreted by a single <code class="literal">InputFormat</code> and a single <code class="literal">Mapper</code>. What often happens, however, is that
      the data format evolves over time, so you have to write your mapper to
      cope with all of your legacy formats. Or you may have data sources that
      provide the same type of data but in different formats. This arises in
      the case of performing joins of different datasets; see <a class="ulink" href="#calibre_link-595" title="Reduce-Side Joins">Reduce-Side Joins</a>. For instance, one might be tab-separated
      plain text, and the other a binary sequence file. Even if they are in
      the same format, they may have different representations, and therefore
      need to be parsed differently.</p><p class="calibre2">These cases are handled elegantly by <a class="calibre" id="calibre_link-2721"></a>using the <code class="literal">MultipleInputs</code> class, which allows you to
      specify which <code class="literal">InputFormat</code> and
      <code class="literal">Mapper</code> to use on a per-path basis.
      For example, if we had weather data from the UK Met Office<sup class="calibre6">[<a class="firstname" href="#calibre_link-596" id="calibre_link-607">59</a>]</sup> that we wanted to combine with the <a class="calibre" id="calibre_link-2778"></a><a class="calibre" id="calibre_link-2772"></a>NCDC data for our maximum temperature analysis, we might
      set up the input as follows:</p><a id="calibre_link-4269" class="calibre"></a><pre class="screen1">    <code class="n">MultipleInputs</code><code class="o">.</code><code class="na">addInputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">ncdcInputPath</code><code class="o">,</code>
        <code class="n">TextInputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">,</code> <code class="n">MaxTemperatureMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">MultipleInputs</code><code class="o">.</code><code class="na">addInputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">metOfficeInputPath</code><code class="o">,</code>
        <code class="n">TextInputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">,</code> <code class="n">MetOfficeMaxTemperatureMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code></pre><p class="calibre2">This code replaces the usual calls to <code class="literal">FileInputFormat.addInputPath()</code> and <code class="literal">job.setMapperClass()</code>. Both the Met Office and
      NCDC data are text based, so we use <code class="literal">Text</code><code class="literal">InputFormat</code> for each. But the line
      format of the two data sources is different, so we use two different
      mappers. The <code class="literal">MaxTemperatureMapper</code>
      reads NCDC input data and extracts the year and temperature fields. The
      <code class="literal">MetOfficeMaxTemperatureMapper</code> reads
      Met Office input data and extracts the year and temperature fields. The
      important thing is that the map outputs have the same types, since the
      reducers (which are all of the same type) see the aggregated map outputs
      and are not aware of the different mappers used to produce them.</p><p class="calibre2">The <code class="literal">MultipleInputs</code> class has an
      overloaded version of <code class="literal">addInputPath()</code>
      that doesn’t take a mapper:</p><a id="calibre_link-4270" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">addInputPath</code><code class="o">(</code><code class="n">Job</code> <code class="n">job</code><code class="o">,</code> <code class="n">Path</code> <code class="n">path</code><code class="o">,</code>
                                <code class="n">Class</code><code class="o">&lt;?</code> <code class="k">extends</code> <code class="n">InputFormat</code><code class="o">&gt;</code> <code class="n">inputFormatClass</code><code class="o">)</code></pre><p class="calibre2">This is useful when you only have one mapper (set using the
      <code class="literal">Job</code>’s <code class="literal">setMapperClass()</code> method) but multiple input
      formats.</p></div><div class="book" title="Database Input (and Output)"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-602">Database Input (and Output)</h3></div></div></div><p class="calibre2"><code class="literal">DBInputFormat</code> is an input
      <a class="calibre" id="calibre_link-1366"></a><a class="calibre" id="calibre_link-2102"></a><a class="calibre" id="calibre_link-2875"></a>format for reading data from a relational database, using
      JDBC. Because it doesn’t have any sharding capabilities, you need to be
      careful not to overwhelm the database from which you are reading by
      running too many mappers. For this reason, it is best used for loading
      relatively small datasets, perhaps for joining with larger datasets from
      HDFS using <code class="literal">MultipleInputs</code>. The
      corresponding output <a class="calibre" id="calibre_link-1399"></a>format is <code class="literal">DBOutputFormat</code>, which is useful for dumping
      job outputs (of modest size) into a database.</p><p class="calibre2">For an alternative way of moving data between relational databases
      and HDFS, consider using Sqoop, which is described in <a class="ulink" href="#calibre_link-391" title="Chapter&nbsp;15.&nbsp;Sqoop">Chapter&nbsp;15</a>.</p><p class="calibre2">HBase’s <code class="literal">TableInputFormat</code> is
      <a class="calibre" id="calibre_link-3586"></a><a class="calibre" id="calibre_link-1906"></a>designed to allow a MapReduce program to operate on data
      stored in an HBase table. <code class="literal">TableOutputFormat</code> is for <a class="calibre" id="calibre_link-3590"></a>writing MapReduce outputs into an <a class="calibre" id="calibre_link-2482"></a>HBase table.</p></div></div><div class="book" title="Output Formats"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4271">Output Formats</h2></div></div></div><p class="calibre2">Hadoop has output <a class="calibre" id="calibre_link-2489"></a>data formats that correspond to the input formats covered in
    the previous section. The <code class="literal">OutputFormat</code>
    class <a class="calibre" id="calibre_link-2886"></a>hierarchy appears in <a class="ulink" href="#calibre_link-597" title="Figure&nbsp;8-4.&nbsp;OutputFormat class hierarchy">Figure&nbsp;8-4</a>.</p><div class="book"><div class="figure"><a id="calibre_link-597" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4272" class="calibre"></a><img alt="OutputFormat class hierarchy" src="images/000000.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;8-4.&nbsp;OutputFormat class hierarchy</div></div></div><div class="book" title="Text Output"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4273">Text Output</h3></div></div></div><p class="calibre2">The default output <a class="calibre" id="calibre_link-2879"></a><a class="calibre" id="calibre_link-3673"></a><a class="calibre" id="calibre_link-3681"></a>format, <code class="literal">TextOutputFormat</code>, writes records as lines of
      text. Its keys and values may be of any type, since <code class="literal">TextOutputFormat</code> turns them to strings by
      calling <code class="literal">toString()</code> on them. Each
      key-value pair is separated by a tab character, although that may be
      changed using the <code class="literal">mapreduce.output.textoutputformat.separator</code>
      property. <a class="calibre" id="calibre_link-2600"></a>The counterpart to <code class="literal">TextOutputFormat</code> for reading in this case is
      <code class="literal">KeyValueText</code><code class="literal">InputFormat</code>, since it breaks lines
      into key-value pairs based on a configurable separator (see <a class="ulink" href="#calibre_link-598" title="KeyValueTextInputFormat">KeyValueTextInputFormat</a>).</p><p class="calibre2">You can suppress the key or the value from the output (or both,
      making this output format <a class="calibre" id="calibre_link-2809"></a><a class="calibre" id="calibre_link-3671"></a>equivalent to <code class="literal">NullOutputFormat</code>, which emits nothing) using
      <a class="calibre" id="calibre_link-2812"></a>a <code class="literal">NullWritable</code> type.
      This also causes no separator to be written, which makes the output
      suitable for reading in using <code class="literal">TextInputFormat</code>.</p></div><div class="book" title="Binary Output"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4274">Binary Output</h3></div></div></div><div class="book" title="SequenceFileOutputFormat"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4275">SequenceFileOutputFormat</h4></div></div></div><p class="calibre2">As the name<a class="calibre" id="calibre_link-2874"></a><a class="calibre" id="calibre_link-992"></a><a class="calibre" id="calibre_link-3347"></a> indicates, <code class="literal">SequenceFileOutputFormat</code> writes sequence
        files for its output. This is a good choice of output if it forms the
        input to a further MapReduce job, since it is compact and is readily
        compressed. Compression is controlled via the static methods on <code class="literal">SequenceFileOutputFormat</code>, as described in
        <a class="ulink" href="#calibre_link-599" title="Using Compression in MapReduce">Using Compression in MapReduce</a>. For an example of how
        to use <code class="literal">SequenceFileOutputFormat</code>,
        see <a class="ulink" href="#calibre_link-271" title="Sorting">Sorting</a>.</p></div><div class="book" title="SequenceFileAsBinaryOutputFormat"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4276">SequenceFileAsBinaryOutputFormat</h4></div></div></div><p class="calibre2"><code class="literal">SequenceFileAsBinaryOutputFormat</code>—the
        counterpart <a class="calibre" id="calibre_link-3342"></a><a class="calibre" id="calibre_link-989"></a>to <code class="literal">SequenceFileAsBinaryInputFormat</code>—writes keys
        and values in raw binary format into a sequence file container.</p></div><div class="book" title="MapFileOutputFormat"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4277">MapFileOutputFormat</h4></div></div></div><p class="calibre2"><code class="literal">MapFileOutputFormat</code> writes
        <a class="calibre" id="calibre_link-2410"></a><a class="calibre" id="calibre_link-987"></a>map files as output. The keys in a MapFile must be added
        in order, so you need to ensure that your reducers emit keys in sorted
        order.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">The reduce <span class="calibre">input</span> keys are
          guaranteed to be sorted, but the output keys are under the control
          of the reduce function, and there is nothing in the general
          MapReduce contract that states that the reduce <span class="calibre">output</span> keys have to be ordered in any way.
          The extra constraint of sorted reduce output keys is just needed for
          <code class="literal">MapFileOutputFormat</code>.</p></div></div></div><div class="book" title="Multiple Outputs"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-515">Multiple Outputs</h3></div></div></div><p class="calibre2"><code class="literal">FileOutputFormat</code> and its
      <a class="calibre" id="calibre_link-2877"></a><a class="calibre" id="calibre_link-2717"></a>subclasses generate a set of files in the output
      directory. There is one file per reducer, and files are named by the
      partition number: <em class="calibre10">part-r-00000</em>,
      <em class="calibre10">part-r-00001</em>, and so on. Sometimes
      there is a need to have more control over the naming of the files or to
      produce multiple files per reducer. MapReduce comes with the <code class="literal">MultipleOutputs</code> class to help you do
      this.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-600" id="calibre_link-608">60</a>]</sup></p><div class="book" title="An example: Partitioning data"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4278">An example: Partitioning data</h4></div></div></div><p class="calibre2">Consider the problem of <a class="calibre" id="calibre_link-2950"></a><a class="calibre" id="calibre_link-2719"></a>partitioning the weather dataset by weather station. We
        would like to run a job whose output is one file per station, with
        each file containing all the records for that station.</p><p class="calibre2">One way of doing this is to have a reducer for each weather
        station. To arrange this, we need to do two things. First, write a
        partitioner that puts records from the same weather station into the
        same partition. Second, set the number of reducers on the job to be
        the number of weather stations. The partitioner would look like
        this:</p><a id="calibre_link-4279" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">class</code> <code class="nc">StationPartitioner</code> <code class="k">extends</code> <code class="n">Partitioner</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">&gt;</code> <code class="o">{</code>
  
  <code class="k">private</code> <code class="n">NcdcRecordParser</code> <code class="n">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="n">NcdcRecordParser</code><code class="o">();</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">getPartition</code><code class="o">(</code><code class="n">LongWritable</code> <code class="n">key</code><code class="o">,</code> <code class="n">Text</code> <code class="n">value</code><code class="o">,</code> <code class="kt">int</code> <code class="n">numPartitions</code><code class="o">)</code> <code class="o">{</code>
    <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">value</code><code class="o">);</code>
    <code class="k">return</code> <code class="nf">getPartition</code><code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">getStationId</code><code class="o">());</code>
  <code class="o">}</code>

  <code class="k">private</code> <code class="kt">int</code> <code class="nf">getPartition</code><code class="o">(</code><code class="n">String</code> <code class="n">stationId</code><code class="o">)</code> <code class="o">{</code>
    <code class="o">...</code>
  <code class="o">}</code>

<code class="o">}</code></pre><p class="calibre2">The <code class="literal">getPartition(String)</code>
        method, whose implementation is not shown, turns the station ID into a
        partition index. To do this, it needs a list of all the station IDs;
        it then just returns the index of the station ID in the list.</p><p class="calibre2">There are two drawbacks to this approach. The first is that
        since the number of partitions needs to be known before the job is
        run, so does the number of weather stations. Although the NCDC
        provides metadata about its stations, there is no guarantee that the
        IDs encountered in the data will match those in the metadata. A
        station that appears in the metadata but not in the data wastes a
        reduce task. Worse, a station that appears in the data but not in the
        metadata doesn’t get a reduce task; it has to be thrown away. One way
        of mitigating this problem would be to write a job to extract the
        unique station IDs, but it’s a shame that we need an extra job to do
        this.</p><p class="calibre2">The second drawback is more subtle. It is generally a bad idea
        to allow the number of partitions to be rigidly fixed by the
        application, since this can lead to small or uneven-sized partitions.
        Having many reducers doing a small amount of work isn’t an efficient
        way of organizing a job; it’s much better to get reducers to do more
        work and have fewer of them, as the overhead in running a task is then
        reduced. Uneven-sized partitions can be difficult to avoid, too.
        Different weather stations will have gathered a widely varying amount
        of data; for example, compare a station that opened one year ago to
        one that has been gathering data for a century. If a few reduce tasks
        take significantly longer than the others, they will dominate the job
        execution time and cause it to be longer than it needs to be.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">There are two special cases when it does make sense to allow
          the application to set the number of partitions (or equivalently,
          the number of reducers):</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Zero reducers</span></dt><dd class="calibre8"><p class="calibre2">This is a vacuous case: there are no partitions, as the
                application needs to run only map tasks.</p></dd><dt class="calibre7"><span class="term">One reducer</span></dt><dd class="calibre8"><p class="calibre2">It can be convenient to run small jobs to combine the
                output of previous jobs into a single file. This should be
                attempted only when the amount of data is small enough to be
                processed comfortably by one reducer.</p></dd></dl></div></div><p class="calibre2">It is much better to let the cluster drive the number of
        partitions for a job, the idea being that the more cluster resources
        there are available, the faster the job can complete. This is why the
        <a class="calibre" id="calibre_link-1888"></a>default <code class="literal">HashPartitioner</code> works so well: it works with
        any number of partitions and ensures each partition has a good mix of
        keys, leading to more evenly sized partitions.</p><p class="calibre2">If we go back to using <code class="literal">HashPartitioner</code>, each partition will contain
        multiple stations, so to create a file per station, we need to arrange
        for each reducer to write multiple files. This is <a class="calibre" id="calibre_link-2720"></a><a class="calibre" id="calibre_link-2951"></a>where <code class="literal">MultipleOutputs</code>
        comes in.</p></div><div class="book" title="MultipleOutputs"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4280">MultipleOutputs</h4></div></div></div><p class="calibre2"><code class="literal">MultipleOutputs</code> allows
        <a class="calibre" id="calibre_link-2724"></a><a class="calibre" id="calibre_link-2715"></a>you to write data to files whose names are derived from
        the output keys and values, or in fact from an arbitrary string. This
        allows each reducer (or mapper in a map-only job) to create more than
        a single file. Filenames are of the form <em class="calibre10"><em class="replaceable"><code class="replaceable">name</code></em>-m-<em class="replaceable"><code class="replaceable">nnnnn</code></em></em>
        for map outputs and <em class="calibre10"><em class="replaceable"><code class="replaceable">name</code></em>-r-<em class="replaceable"><code class="replaceable">nnnnn</code></em></em>
        for reduce outputs, where <em class="calibre10"><em class="replaceable"><code class="replaceable">name</code></em></em> is an
        arbitrary name that is set by the program and <em class="calibre10"><em class="replaceable"><code class="replaceable">nnnnn</code></em></em> is an
        integer designating the part number, starting from
        <span class="calibre"><em class="calibre10">00000</em></span>. The part number ensures that outputs
        written from different partitions (mappers or reducers) do not collide
        in the case of the same name.</p><p class="calibre2">The program in <a class="ulink" href="#calibre_link-601" title="Example&nbsp;8-5.&nbsp;Partitioning whole dataset into files named by the station ID using MultipleOutputs">Example&nbsp;8-5</a> shows how to use
        <code class="literal">MultipleOutputs</code> to partition the
        dataset by station.</p><div class="example"><a id="calibre_link-601" class="calibre"></a><div class="example-title">Example&nbsp;8-5.&nbsp;Partitioning whole dataset into files named by the station ID
          using MultipleOutputs</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">PartitionByStationUsingMultipleOutputs</code> <code class="k">extends</code> <code class="n">Configured</code>
    <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>
  
  <code class="k">static</code> <code class="k">class</code> <code class="nc">StationMapper</code>
      <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">Text</code><code class="o">&gt;</code> <code class="o">{</code>
  
    <code class="k">private</code> <code class="n">NcdcRecordParser</code> <code class="n">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="n">NcdcRecordParser</code><code class="o">();</code>
    
    <code class="nd">@Override</code>
    <code class="k">protected</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">LongWritable</code> <code class="n">key</code><code class="o">,</code> <code class="n">Text</code> <code class="n">value</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
        <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
      <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">value</code><code class="o">);</code>
      <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">getStationId</code><code class="o">()),</code> <code class="n">value</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
  
  <span class="calibre24"><strong class="calibre9"><code class="kc">static</code> <code class="kc">class</code> <code class="nc1">MultipleOutputsReducer</code>
      <code class="kc">extends</code> <code class="n1">Reducer</code><code class="o1">&lt;</code><code class="n1">Text</code><code class="o1">,</code> <code class="n1">Text</code><code class="o1">,</code> <code class="n1">NullWritable</code><code class="o1">,</code> <code class="n1">Text</code><code class="o1">&gt;</code> <code class="o1">{</code>
    
    <code class="kc">private</code> <code class="n1">MultipleOutputs</code><code class="o1">&lt;</code><code class="n1">NullWritable</code><code class="o1">,</code> <code class="n1">Text</code><code class="o1">&gt;</code> <code class="n1">multipleOutputs</code><code class="o1">;</code>

    <code class="nd1">@Override</code>
    <code class="kc">protected</code> <code class="kt1">void</code> <code class="nf1">setup</code><code class="o1">(</code><code class="n1">Context</code> <code class="n1">context</code><code class="o1">)</code>
        <code class="kc">throws</code> <code class="n1">IOException</code><code class="o1">,</code> <code class="n1">InterruptedException</code> <code class="o1">{</code>
      <code class="n1">multipleOutputs</code> <code class="o1">=</code> <code class="kc">new</code> <code class="n1">MultipleOutputs</code><code class="o1">&lt;</code><code class="n1">NullWritable</code><code class="o1">,</code> <code class="n1">Text</code><code class="o1">&gt;(</code><code class="n1">context</code><code class="o1">);</code>
    <code class="o1">}</code>

    <code class="nd1">@Override</code>
    <code class="kc">protected</code> <code class="kt1">void</code> <code class="nf1">reduce</code><code class="o1">(</code><code class="n1">Text</code> <code class="n1">key</code><code class="o1">,</code> <code class="n1">Iterable</code><code class="o1">&lt;</code><code class="n1">Text</code><code class="o1">&gt;</code> <code class="n1">values</code><code class="o1">,</code> <code class="n1">Context</code> <code class="n1">context</code><code class="o1">)</code>
        <code class="kc">throws</code> <code class="n1">IOException</code><code class="o1">,</code> <code class="n1">InterruptedException</code> <code class="o1">{</code>
      <code class="kc">for</code> <code class="o1">(</code><code class="n1">Text</code> <code class="n1">value</code> <code class="o1">:</code> <code class="n1">values</code><code class="o1">)</code> <code class="o1">{</code>
        <code class="n1">multipleOutputs</code><code class="o1">.</code><code class="na1">write</code><code class="o1">(</code><code class="n1">NullWritable</code><code class="o1">.</code><code class="na1">get</code><code class="o1">(),</code> <code class="n1">value</code><code class="o1">,</code> <code class="n1">key</code><code class="o1">.</code><code class="na1">toString</code><code class="o1">());</code>
      <code class="o1">}</code>
    <code class="o1">}</code>
    
    <code class="nd1">@Override</code>
    <code class="kc">protected</code> <code class="kt1">void</code> <code class="nf1">cleanup</code><code class="o1">(</code><code class="n1">Context</code> <code class="n1">context</code><code class="o1">)</code>
        <code class="kc">throws</code> <code class="n1">IOException</code><code class="o1">,</code> <code class="n1">InterruptedException</code> <code class="o1">{</code>
      <code class="n1">multipleOutputs</code><code class="o1">.</code><code class="na1">close</code><code class="o1">();</code>
    <code class="o1">}</code>
  <code class="o1">}</code></strong></span>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="n">JobBuilder</code><code class="o">.</code><code class="na">parseInputAndOutput</code><code class="o">(</code><code class="k">this</code><code class="o">,</code> <code class="n">getConf</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">job</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>
    
    <code class="n">job</code><code class="o">.</code><code class="na">setMapperClass</code><code class="o">(</code><code class="n">StationMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setMapOutputKeyClass</code><code class="o">(</code><code class="n">Text</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setReducerClass</code><code class="o">(</code><code class="n">MultipleOutputsReducer</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputKeyClass</code><code class="o">(</code><code class="n">NullWritable</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="k">return</code> <code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">;</code>
  <code class="o">}</code>
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="k">new</code> <code class="n">PartitionByStationUsingMultipleOutputs</code><code class="o">(),</code>
        <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">In the reducer, which is where we generate the output, we
        construct an instance of <code class="literal">MultipleOutputs</code> in the <code class="literal">setup()</code> method and assign it to an instance
        variable. We then use the <code class="literal">MultipleOutputs</code> instance in the
        <code class="literal">reduce()</code> method to write to the
        output, in place of the context. The <code class="literal">write()</code> method takes the key and value, as
        well as a name. We use the station identifier for the name, so the
        overall effect is to produce output files with the naming scheme
        <em class="calibre10"><em class="replaceable"><code class="replaceable">station_identifier</code></em>-r-<em class="replaceable"><code class="replaceable">nnnnn</code></em></em>.</p><p class="calibre2">In one run, the first few output files were named as
        follows:</p><pre class="screen1">output/010010-99999-r-00027
output/010050-99999-r-00013
output/010100-99999-r-00015
output/010280-99999-r-00014
output/010550-99999-r-00000
output/010980-99999-r-00011
output/011060-99999-r-00025
output/012030-99999-r-00029
output/012350-99999-r-00018
output/012620-99999-r-00004</pre><p class="calibre2">The base path specified in the <code class="literal">write()</code>
        method of <code class="literal">MultipleOutputs</code> is
        interpreted relative to the output directory, and because it may
        contain file path separator characters (<code class="literal">/</code>), it’s possible to create subdirectories
        of arbitrary depth. For example, the following modification partitions
        the data by station and year so that each year’s data is contained in
        a directory named by the station ID (such as <em class="calibre10">029070-99999/1901/part-r-00000</em>):</p><a id="calibre_link-4281" class="calibre"></a><pre class="screen1">    <code class="nd">@Override</code>
    <code class="k">protected</code> <code class="kt">void</code> <code class="nf">reduce</code><code class="o">(</code><code class="n">Text</code> <code class="n">key</code><code class="o">,</code> <code class="n">Iterable</code><code class="o">&lt;</code><code class="n">Text</code><code class="o">&gt;</code> <code class="n">values</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
        <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
      <code class="k">for</code> <code class="o">(</code><code class="n">Text</code> <code class="n">value</code> <code class="o">:</code> <code class="n">values</code><code class="o">)</code> <code class="o">{</code>
        <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">value</code><code class="o">);</code>
        <code class="n">String</code> <code class="n">basePath</code> <code class="o">=</code> <code class="n">String</code><code class="o">.</code><code class="na">format</code><code class="o">(</code><code class="sb">"%s/%s/part"</code><code class="o">,</code>
            <code class="n">parser</code><code class="o">.</code><code class="na">getStationId</code><code class="o">(),</code> <code class="n">parser</code><code class="o">.</code><code class="na">getYear</code><code class="o">());</code>
        <code class="n">multipleOutputs</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">NullWritable</code><code class="o">.</code><code class="na">get</code><code class="o">(),</code> <code class="n">value</code><code class="o">,</code> <code class="n">basePath</code><code class="o">);</code>
      <code class="o">}</code>
    <code class="o">}</code></pre><p class="calibre2"><code class="literal">MultipleOutputs</code> delegates to
        the mapper’s <code class="literal">OutputFormat</code>. In this
        example it’s a <code class="literal">TextOutputFormat</code>,
        but more complex setups are possible. For example, you can create
        named outputs, each with its own <code class="literal">OutputFormat</code> and key and value types (which
        may differ from the output types of the mapper or reducer).
        Furthermore, the mapper or reducer (or both) may write to multiple
        output files for each record processed. Consult the Java documentation
        for more <a class="calibre" id="calibre_link-2878"></a><a class="calibre" id="calibre_link-2718"></a><a class="calibre" id="calibre_link-2725"></a><a class="calibre" id="calibre_link-2716"></a>information.</p></div></div><div class="book" title="Lazy Output"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4282">Lazy Output</h3></div></div></div><p class="calibre2"><code class="literal">FileOutputFormat</code> subclasses
      <a class="calibre" id="calibre_link-2315"></a><a class="calibre" id="calibre_link-2876"></a>will create output (<em class="calibre10">part-r-<em class="replaceable"><code class="replaceable">nnnnn</code></em></em>)
      files, even if they are empty. Some applications prefer that empty files
      not be created, which is where <code class="literal">LazyOutputFormat</code> helps. It is a wrapper output
      format that ensures that the output file is created only when the first
      record is emitted for a given partition. To use it, call its
      <code class="literal">setOutputFormatClass()</code> method with the <code class="literal">JobConf</code> and the underlying output
      format.</p><p class="calibre2">Streaming supports a <code class="literal">-lazyOutput</code> option to enable <code class="literal">LazyOutputFormat</code>.</p></div><div class="book" title="Database Output"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4283">Database Output</h3></div></div></div><p class="calibre2">The output formats for writing to relational databases and to
      HBase are <a class="calibre" id="calibre_link-2490"></a><a class="calibre" id="calibre_link-2887"></a>mentioned in <a class="ulink" href="#calibre_link-602" title="Database Input (and Output)">Database Input (and Output)</a>.</p></div></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" type="footnote" id="calibre_link-575"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-603">55</a>] </sup>But see the classes in <code class="literal">org.apache.hadoop.mapred</code> for the old
          MapReduce API counterparts.</p></div><div class="footnote" type="footnote" id="calibre_link-586"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-604">56</a>] </sup>This is how the mapper in <code class="literal">SortValidator.RecordStatsChecker</code> is
            implemented.</p></div><div class="footnote" type="footnote" id="calibre_link-587"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-605">57</a>] </sup>In the method name <code class="literal">isSplitable()</code>,
            “splitable” has a single “t.” It is usually spelled “splittable,”
            which is the spelling I have used in this book.</p></div><div class="footnote" id="calibre_link-594"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-606">58</a>] </sup>See <a class="ulink" href="http://mahout.apache.org/" target="_top">Mahout’s <code class="literal">XmlInputFormat</code></a> for an improved
            XML input format.</p></div><div class="footnote" id="calibre_link-596"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-607">59</a>] </sup>Met Office data is generally available only to the research
          and academic community. However, there is a small amount of monthly
          weather station data available at <a class="ulink" href="http://www.metoffice.gov.uk/climate/uk/stationdata/" target="_top">http://www.metoffice.gov.uk/climate/uk/stationdata/</a>.</p></div><div class="footnote" type="footnote" id="calibre_link-600"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-608">60</a>] </sup>The old MapReduce API includes two classes for producing
          multiple outputs: <code class="literal">MultipleOutputFormat</code> and <code class="literal">MultipleOutputs</code>. In a <a class="calibre" id="calibre_link-2723"></a>nutshell, <code class="literal">MultipleOutputs</code> is more fully featured,
          but <code class="literal">MultipleOutput</code><code class="literal">Format</code> has more control over the
          output directory structure and file naming. <code class="literal">MultipleOutputs</code> in the new API combines
          the best features of the two multiple output classes in the old API.
          The code on this book’s website includes old API equivalents of the
          examples in this section using both <code class="literal">MultipleOutputs</code> and <code class="literal">MultipleOutputFormat</code>.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-261"><section type="chapter" id="calibre_link-4284" title="Chapter&nbsp;9.&nbsp;MapReduce Features"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;9.&nbsp;MapReduce Features</h2></div></div></div><p class="calibre2">This chapter looks at some of the more advanced features of MapReduce,
  including counters and sorting and joining datasets.</p><div class="book" title="Counters"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-12">Counters</h2></div></div></div><p class="calibre2">There are often <a class="calibre" id="calibre_link-1262"></a><a class="calibre" id="calibre_link-2450"></a>things that you would like to know about the data you are
    analyzing but that are peripheral to the analysis you are performing. For
    example, if you were counting invalid records and discovered that the
    proportion of invalid records in the whole dataset was very high, you might be prompted
    to check why so many records were being marked as invalid—perhaps there is
    a bug in the part of the program that detects invalid records? Or if the
    data was of poor quality and genuinely did have very many invalid records,
    after discovering this, you might decide to increase the size of the
    dataset so that the number of good records was large enough for meaningful
    analysis.</p><p class="calibre2">Counters are a useful channel for gathering statistics about the
    job: for quality control or for application-level statistics. They are
    also useful for problem diagnosis. If you are tempted to put a log message
    into your map or reduce task, it is often better to see whether you can
    use a counter instead to record that a particular condition occurred. In
    addition to counter values being much easier to retrieve than log output
    for large distributed jobs, you get a record of the number of times that
    condition occurred, which is more work to obtain from a set of
    logfiles.</p><div class="book" title="Built-in Counters"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-684">Built-in Counters</h3></div></div></div><p class="calibre2">Hadoop maintains <a class="calibre" id="calibre_link-1263"></a><a class="calibre" id="calibre_link-1028"></a>some built-in counters for every job, and these report
      various metrics. For example, there are counters for the number of bytes
      and records processed, which allow you to confirm that the expected
      amount of input was consumed and the expected amount of output was
      produced.</p><p class="calibre2">Counters are divided into groups, and there are several groups for
      the built-in counters, listed in <a class="ulink" href="#calibre_link-708" title="Table&nbsp;9-1.&nbsp;Built-in counter groups">Table&nbsp;9-1</a>.</p><div class="table"><a id="calibre_link-708" class="calibre"></a><div class="table-title">Table&nbsp;9-1.&nbsp;Built-in counter groups</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Group</td><td class="calibre20">Name/Enum</td><td class="calibre21">Reference</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">MapReduce task counters</td><td class="calibre23"><code class="uri">org.apache.hadoop.mapreduce.TaskCounter</code></td><td class="calibre25"><a class="ulink" href="#calibre_link-45" title="Table&nbsp;9-2.&nbsp;Built-in MapReduce task counters">Table&nbsp;9-2</a></td></tr><tr class="calibre26"><td class="calibre23">Filesystem <a class="calibre" id="calibre_link-1668"></a>counters</td><td class="calibre23"><code class="uri">org.apache.hadoop.mapreduce.FileSystemCounter</code></td><td class="calibre25"><a class="ulink" href="#calibre_link-709" title="Table&nbsp;9-3.&nbsp;Built-in filesystem task counters">Table&nbsp;9-3</a></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">FileInputFormat</code> <a class="calibre" id="calibre_link-1640"></a>counters</td><td class="calibre23"><code class="uri">org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter</code></td><td class="calibre25"><a class="ulink" href="#calibre_link-710" title="Table&nbsp;9-4.&nbsp;Built-in FileInputFormat task counters">Table&nbsp;9-4</a></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">FileOutputFormat</code> <a class="calibre" id="calibre_link-1646"></a>counters</td><td class="calibre23"><code class="uri">org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter</code></td><td class="calibre25"><a class="ulink" href="#calibre_link-711" title="Table&nbsp;9-5.&nbsp;Built-in FileOutputFormat task counters">Table&nbsp;9-5</a></td></tr><tr class="calibre19"><td class="calibre27">Job <a class="calibre" id="calibre_link-2213"></a>counters</td><td class="calibre27"><code class="uri">org.apache.hadoop.mapreduce.JobCounter</code></td><td class="calibre28"><a class="ulink" href="#calibre_link-496" title="Table&nbsp;9-6.&nbsp;Built-in job counters">Table&nbsp;9-6</a></td></tr></tbody></table></div></div><p class="calibre2">Each group either <a class="calibre" id="calibre_link-1030"></a><a class="calibre" id="calibre_link-3623"></a>contains <em class="calibre10">task counters</em> (which are
      updated as a task progresses) or <em class="calibre10">job counters</em>
      (which are updated as a job progresses). We look at both types in the
      following sections.</p><div class="book" title="Task counters"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4285">Task counters</h4></div></div></div><p class="calibre2">Task counters gather information about tasks over the course of
        their execution, and the results are aggregated over all the tasks in
        a job. The <code class="literal">MAP_INPUT_RECORDS</code>
        counter, for example, counts the input records read by each map task
        and aggregates over all map tasks in a job, so that the final figure
        is the total number of input records for the whole job.</p><p class="calibre2">Task counters are maintained by each task attempt, and
        periodically sent to the application master so they can be globally
        aggregated. (This is described in <a class="ulink" href="#calibre_link-13" title="Progress and Status Updates">Progress and Status Updates</a>.) Task counters are sent in full
        every time, rather than sending the counts since the last
        transmission, since this guards against errors due to lost messages.
        Furthermore, during a
        job run, counters may go down if a task fails.</p><p class="calibre2">Counter values are definitive only once a job has successfully
        completed. However, some counters provide useful diagnostic
        information as a task is progressing, and it can be useful to monitor
        them with the web UI. For example, <code class="literal">PHYSICAL_MEMORY_BYTES</code>, <code class="literal">VIRTUAL_MEMORY_BYTES</code>, and <code class="literal">COMMITTED_HEAP_BYTES</code> provide an indication
        of how memory usage varies over the course of a particular task
        attempt.</p><p class="calibre2">The built-in task counters include those in the MapReduce task
        counters group (<a class="ulink" href="#calibre_link-45" title="Table&nbsp;9-2.&nbsp;Built-in MapReduce task counters">Table&nbsp;9-2</a>) and those in the file-related counters groups
        (Tables <a class="ulink" href="#calibre_link-709" title="Table&nbsp;9-3.&nbsp;Built-in filesystem task counters">9-3</a>, <a class="ulink" href="#calibre_link-710" title="Table&nbsp;9-4.&nbsp;Built-in FileInputFormat task counters">9-4</a>, and <a class="ulink" href="#calibre_link-711" title="Table&nbsp;9-5.&nbsp;Built-in FileOutputFormat task counters">9-5</a>).</p><div class="table"><a id="calibre_link-45" class="calibre"></a><div class="table-title">Table&nbsp;9-2.&nbsp;Built-in MapReduce task counters</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Counter</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">Map input records
                (<code class="uri">MAP_INPUT_RECORDS</code>)</td><td class="calibre25">The number of input <a class="calibre" id="calibre_link-2644"></a>records consumed by all the maps in the job.
                Incremented every time a record is read from a <code class="uri">RecordReader</code> and passed to the map’s
                <code class="uri">map()</code> method by the
                framework.</td></tr><tr class="calibre26"><td class="calibre23">Split raw bytes (<code class="uri">SPLIT_RAW_BYTES</code>)</td><td class="calibre25">The number <a class="calibre" id="calibre_link-3481"></a>of bytes of input-split objects read by maps.
                These objects represent the split metadata (that is, the
                offset and length within a file) rather than the split data
                itself, so the total size should be small.</td></tr><tr class="calibre19"><td class="calibre23">Map output records (<code class="uri">MAP_OUTPUT_RECORDS</code>)</td><td class="calibre25">The number of <a class="calibre" id="calibre_link-2647"></a>map output records produced by all the maps in
                the job. Incremented
                every time the <code class="uri">collect()</code>
                method is called on a map’s <code class="uri">OutputCollector</code>.</td></tr><tr class="calibre26"><td class="calibre23">Map output bytes (<code class="uri">MAP_OUTPUT_BYTES</code>)</td><td class="calibre25">The number of <a class="calibre" id="calibre_link-2645"></a>bytes of uncompressed output produced by all the
                maps in the job. Incremented every time the <code class="uri">collect()</code> method is called on a
                map’s <code class="uri">OutputCollector</code>.</td></tr><tr class="calibre19"><td class="calibre23">Map output materialized bytes (<code class="uri">MAP_OUTPUT_MATERIALIZED_BYTES</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-2646"></a>number of bytes of map output actually written
                to disk. If map output compression is enabled, this is
                reflected in the counter value.</td></tr><tr class="calibre26"><td class="calibre23">Combine input records (<code class="uri">COMBINE_INPUT_RECORDS</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-1184"></a>number of input records consumed by all the
                combiners (if any) in the job. Incremented every time a value
                is read from the combiner’s iterator over values. Note that
                this count is the number of values consumed by the combiner,
                not the number of distinct key groups (which would not be a
                useful metric, since there is not necessarily one group per
                key for a combiner; see <a class="ulink" href="#calibre_link-539" title="Combiner Functions">Combiner Functions</a>,
                and also <a class="ulink" href="#calibre_link-46" title="Shuffle and Sort">Shuffle and Sort</a>).</td></tr><tr class="calibre19"><td class="calibre23">Combine output records
                (<code class="uri">COMBINE_OUTPUT_RECORDS</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-1185"></a>number of output records produced by all the
                combiners (if any) in the job. Incremented every time the
                <code class="uri">collect()</code> method is called
                on a combiner’s <code class="uri">OutputCollector</code>.</td></tr><tr class="calibre26"><td class="calibre23">Reduce input groups (<code class="uri">REDUCE_INPUT_GROUPS</code>)</td><td class="calibre25">The number <a class="calibre" id="calibre_link-3198"></a>of distinct key groups consumed by all the
                reducers in the job. Incremented every time the reducer’s
                <code class="uri">reduce()</code> method is called
                by the framework.</td></tr><tr class="calibre19"><td class="calibre23">Reduce input records
                (<code class="uri">REDUCE_INPUT_RECORDS</code>)</td><td class="calibre25">The number <a class="calibre" id="calibre_link-3199"></a>of input records consumed by all the reducers in
                the job. Incremented every time a value is read from the
                reducer’s iterator over values. If reducers consume all of
                their inputs, this count should be the same as the count for
                map output records.</td></tr><tr class="calibre26"><td class="calibre23">Reduce output records
                (<code class="uri">REDUCE_OUTPUT_RECORDS</code>)</td><td class="calibre25">The number of reduce <a class="calibre" id="calibre_link-3200"></a>output records produced by all the maps in the
                job. Incremented every
                time the <code class="uri">collect()</code> method
                is called on a reducer’s <code class="uri">OutputCollector</code>.</td></tr><tr class="calibre19"><td class="calibre23">Reduce shuffle bytes (<code class="uri">REDUCE_SHUFFLE_BYTES</code>)</td><td class="calibre25">The number of <a class="calibre" id="calibre_link-3201"></a>bytes of map output copied by the shuffle to
                reducers.</td></tr><tr class="calibre26"><td class="calibre23">Spilled records (<code class="uri">SPILLED_RECORDS</code>)</td><td class="calibre25">The number of <a class="calibre" id="calibre_link-3478"></a>records spilled to disk in all map and reduce
                tasks in the job.</td></tr><tr class="calibre19"><td class="calibre23">CPU milliseconds (<code class="uri">CPU_MILLISECONDS</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-1278"></a>cumulative CPU time for a task in milliseconds,
                as reported by <em class="calibre10">/proc/cpuinfo</em>.</td></tr><tr class="calibre26"><td class="calibre23">Physical memory bytes (<code class="uri">PHYSICAL_MEMORY_BYTES</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-2988"></a>physical memory being used by a task in bytes,
                as reported by <em class="calibre10">/proc/meminfo</em>.</td></tr><tr class="calibre19"><td class="calibre23">Virtual memory bytes (<code class="uri">VIRTUAL_MEMORY_BYTES</code>)</td><td class="calibre25">The virtual memory <a class="calibre" id="calibre_link-3766"></a>being used by a task in bytes, as reported by
                <em class="calibre10">/proc/meminfo</em>.</td></tr><tr class="calibre26"><td class="calibre23">Committed heap bytes (<code class="uri">COMMITTED_HEAP_BYTES</code>)</td><td class="calibre25">The total <a class="calibre" id="calibre_link-1199"></a>amount of memory available in the JVM in bytes,
                as reported by <code class="uri">Runtime.getRuntime().totalMemory()</code>.</td></tr><tr class="calibre19"><td class="calibre23">GC time milliseconds (<code class="uri">GC_TIME_MILLIS</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-1785"></a>elapsed time for garbage collection in tasks in
                milliseconds, as reported by <code class="uri">GarbageCollectorMXBean.getCollectionTime()</code>.</td></tr><tr class="calibre26"><td class="calibre23">Shuffled maps (<code class="uri">SHUFFLED_MAPS</code>)</td><td class="calibre25">The number <a class="calibre" id="calibre_link-3402"></a>of map output files transferred to reducers by
                the shuffle (see <a class="ulink" href="#calibre_link-46" title="Shuffle and Sort">Shuffle and Sort</a>).</td></tr><tr class="calibre19"><td class="calibre23">Failed shuffle (<code class="uri">FAILED_SHUFFLE</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-1600"></a>number of map output copy failures during the
                shuffle.</td></tr><tr class="calibre26"><td class="calibre27">Merged map outputs (<code class="uri">MERGED_MAP_OUTPUTS</code>)</td><td class="calibre28">The number <a class="calibre" id="calibre_link-2671"></a>of map outputs that have been merged on the
                reduce side of the shuffle.</td></tr></tbody></table></div></div><div class="table"><a id="calibre_link-709" class="calibre"></a><div class="table-title">Table&nbsp;9-3.&nbsp;Built-in filesystem task counters</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Counter</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><span class="calibre">Filesystem</span> bytes
                read (<code class="uri">BYTES_READ</code>)</td><td class="calibre25">The number of <a class="calibre" id="calibre_link-1669"></a><a class="calibre" id="calibre_link-1036"></a>bytes read by the filesystem by map and reduce
                tasks. There is a counter for each filesystem, and <span class="calibre">Filesystem</span> may be Local, HDFS, S3,
                etc.</td></tr><tr class="calibre26"><td class="calibre23"><span class="calibre">Filesystem</span> bytes
                written (<code class="uri">BYTES_WRITTEN</code>)</td><td class="calibre25">The number <a class="calibre" id="calibre_link-1037"></a>of bytes written by the filesystem by map and
                reduce tasks.</td></tr><tr class="calibre19"><td class="calibre23"><span class="calibre">Filesystem</span> read ops
                (<code class="uri">READ_OPS</code>)</td><td class="calibre25">The number <a class="calibre" id="calibre_link-3171"></a>of read operations (e.g., open, file status) by
                the filesystem by map and reduce tasks.</td></tr><tr class="calibre26"><td class="calibre23"><span class="calibre">Filesystem</span> large
                read ops (<code class="uri">LARGE_READ_OPS</code>)</td><td class="calibre25">The number <a class="calibre" id="calibre_link-2313"></a>of large read operations (e.g., list directory
                for a large directory) by the filesystem by map and reduce
                tasks.</td></tr><tr class="calibre19"><td class="calibre27"><span class="calibre">Filesystem</span> write ops
                (<code class="uri">WRITE_OPS</code>)</td><td class="calibre28">The <a class="calibre" id="calibre_link-3811"></a>number of write operations (e.g., create,
                append) by the filesystem by map and reduce tasks.</td></tr></tbody></table></div></div><div class="table"><a id="calibre_link-710" class="calibre"></a><div class="table-title">Table&nbsp;9-4.&nbsp;Built-in FileInputFormat task counters</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Counter</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre27">Bytes read (<code class="uri">BYTES_READ</code>)</td><td class="calibre28">The number of bytes read by map tasks via the
                <code class="uri">FileInputFormat</code>.</td></tr></tbody></table></div></div><div class="table"><a id="calibre_link-711" class="calibre"></a><div class="table-title">Table&nbsp;9-5.&nbsp;Built-in FileOutputFormat task counters</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Counter</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre27">Bytes written (<code class="uri">BYTES_WRITTEN</code>)</td><td class="calibre28">The number of bytes written by map tasks (for map-only
                jobs) or reduce tasks via the <a class="calibre" id="calibre_link-1264"></a><a class="calibre" id="calibre_link-1031"></a><a class="calibre" id="calibre_link-3624"></a><code class="uri">FileOutputFormat</code>.</td></tr></tbody></table></div></div></div><div class="book" title="Job counters"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4286">Job counters</h4></div></div></div><p class="calibre2">Job counters (<a class="ulink" href="#calibre_link-496" title="Table&nbsp;9-6.&nbsp;Built-in job counters">Table&nbsp;9-6</a>) are
        maintained <a class="calibre" id="calibre_link-2214"></a><a class="calibre" id="calibre_link-1029"></a>by the application master, so they don’t need to be sent
        across the network, unlike all other counters, including user-defined
        ones. They measure job-level statistics, not values that change while
        a task is running. For example, <code class="literal">TOTAL_LAUNCHED_MAPS</code> counts the number of map
        tasks that were launched over the course of a job (including tasks
        that failed).</p><div class="table"><a id="calibre_link-496" class="calibre"></a><div class="table-title">Table&nbsp;9-6.&nbsp;Built-in job counters</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Counter</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">Launched map tasks (<code class="uri">TOTAL_LAUNCHED_MAPS</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-3707"></a>number of map tasks that were launched. Includes
                tasks that were started speculatively (see <a class="ulink" href="#calibre_link-497" title="Speculative Execution">Speculative Execution</a>).</td></tr><tr class="calibre26"><td class="calibre23">Launched reduce tasks (<code class="uri">TOTAL_LAUNCHED_REDUCES</code>)</td><td class="calibre25">The number <a class="calibre" id="calibre_link-3708"></a>of reduce tasks that were launched. Includes
                tasks that were started speculatively.</td></tr><tr class="calibre19"><td class="calibre23">Launched uber tasks (<code class="uri">TOTAL_LAUNCHED_UBERTASKS</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-3709"></a>number of uber tasks (see <a class="ulink" href="#calibre_link-52" title="Anatomy of a MapReduce Job Run">Anatomy of a MapReduce Job Run</a>) that were
                launched.</td></tr><tr class="calibre26"><td class="calibre23">Maps in uber tasks (<code class="uri">NUM_UBER_SUBMAPS</code>)</td><td class="calibre25">The number <a class="calibre" id="calibre_link-2818"></a>of maps in uber tasks.</td></tr><tr class="calibre19"><td class="calibre23">Reduces in uber tasks (<code class="uri">NUM_UBER_SUBREDUCES</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-2819"></a>number of reduces in uber tasks.</td></tr><tr class="calibre26"><td class="calibre23">Failed map tasks (<code class="uri">NUM_FAILED_MAPS</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-2813"></a>number of map tasks that failed. See <a class="ulink" href="#calibre_link-593" title="Task Failure">Task Failure</a> for potential causes.</td></tr><tr class="calibre19"><td class="calibre23">Failed reduce tasks (<code class="uri">NUM_FAILED_REDUCES</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-2814"></a>number of reduce tasks that failed.</td></tr><tr class="calibre26"><td class="calibre23">Failed uber tasks (<code class="uri">NUM_FAILED_UBERTASKS</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-2815"></a>number of uber tasks that failed.</td></tr><tr class="calibre19"><td class="calibre23">Killed map tasks (<code class="uri">NUM_KILLED_MAPS</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-2816"></a>number of map tasks that were killed. See <a class="ulink" href="#calibre_link-593" title="Task Failure">Task Failure</a> for potential causes.</td></tr><tr class="calibre26"><td class="calibre23">Killed reduce tasks (<code class="uri">NUM_KILLED_REDUCES</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-2817"></a>number of reduce tasks that were killed.</td></tr><tr class="calibre19"><td class="calibre23">Data-local map tasks (<code class="uri">DATA_LOCAL_MAPS</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-1396"></a>number of map tasks that ran on the same node as
                their input data.</td></tr><tr class="calibre26"><td class="calibre23">Rack-local map tasks (<code class="uri">RACK_LOCAL_MAPS</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-3130"></a>number of map tasks that ran on a node in the
                same rack as their input data, but were not
                data-local.</td></tr><tr class="calibre19"><td class="calibre23">Other local map tasks (<code class="uri">OTHER_LOCAL_MAPS</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-2872"></a>number of map tasks that ran on a node in a
                different rack to their input data. Inter-rack bandwidth is
                scarce, and Hadoop tries to place map tasks close to their
                input data, so this count should be low. See <a class="ulink" href="#calibre_link-187" title="Figure&nbsp;2-2.&nbsp;Data-local (a), rack-local (b), and off-rack (c) map tasks">Figure&nbsp;2-2</a>.</td></tr><tr class="calibre26"><td class="calibre23">Total time in map tasks (<code class="uri">MILLIS_MAPS</code>)</td><td class="calibre25">The <a class="calibre" id="calibre_link-2696"></a>total time taken running map tasks, in
                milliseconds. Includes tasks that were started speculatively.
                See also <a class="calibre" id="calibre_link-3758"></a><a class="calibre" id="calibre_link-2660"></a>corresponding counters for measuring core and
                memory usage (<code class="uri">VCORES_MILLIS_MAPS</code> and
                <code class="uri">MB_MILLIS_MAPS</code>).</td></tr><tr class="calibre19"><td class="calibre27">Total time in reduce tasks (<code class="uri">MILLIS_REDUCES</code>)</td><td class="calibre28">The <a class="calibre" id="calibre_link-2697"></a>total time taken running reduce tasks, in
                milliseconds. Includes tasks that were started speculatively.
                <a class="calibre" id="calibre_link-3759"></a><a class="calibre" id="calibre_link-2661"></a>See also corresponding counters for measuring
                core and memory usage (<code class="uri">VCORES_MILLIS_REDUCES</code> and
                <code class="uri">MB_MILLIS_REDUCES</code>).</td></tr></tbody></table></div></div></div></div><div class="book" title="User-Defined Java Counters"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4287">User-Defined Java Counters</h3></div></div></div><p class="calibre2">MapReduce allows <a class="calibre" id="calibre_link-2177"></a><a class="calibre" id="calibre_link-1271"></a>user code to define a set of counters, which are then
      incremented as desired in the mapper or reducer. Counters are defined by
      a Java enum, which serves to group related counters. A job may define an
      arbitrary number of enums, each with an arbitrary number of fields. The
      name of the enum is the group name, and the enum’s fields are the
      counter names. Counters are global: the MapReduce framework aggregates
      them across all maps and reduces to produce a grand total at the end of
      the job.</p><p class="calibre2">We created some counters in <a class="ulink" href="#calibre_link-79" title="Chapter&nbsp;6.&nbsp;Developing a MapReduce Application">Chapter&nbsp;6</a> for
      counting malformed records in the weather dataset. The program in <a class="ulink" href="#calibre_link-712" title="Example&nbsp;9-1.&nbsp;Application to run the maximum temperature job, including counting missing and malformed fields and quality codes">Example&nbsp;9-1</a> extends that example to count
      the number of missing records and the distribution of temperature
      quality codes.</p><div class="example"><a id="calibre_link-712" class="calibre"></a><div class="example-title">Example&nbsp;9-1.&nbsp;Application to run the maximum temperature job, including
        counting missing and malformed fields and quality codes</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">MaxTemperatureWithCounters</code> <code class="k">extends</code> <code class="n">Configured</code> <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>
  
  <code class="k">enum</code> <code class="n">Temperature</code> <code class="o">{</code>
    <code class="n">MISSING</code><code class="o">,</code>
    <code class="n">MALFORMED</code>
  <code class="o">}</code>
  
  <code class="k">static</code> <code class="k">class</code> <code class="nc">MaxTemperatureMapperWithCounters</code>
      <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">&gt;</code> <code class="o">{</code>
    
    <code class="k">private</code> <code class="n">NcdcRecordParser</code> <code class="n">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="n">NcdcRecordParser</code><code class="o">();</code>
  
    <code class="nd">@Override</code>
    <code class="k">protected</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">LongWritable</code> <code class="n">key</code><code class="o">,</code> <code class="n">Text</code> <code class="n">value</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
        <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
      
      <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">value</code><code class="o">);</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">isValidTemperature</code><code class="o">())</code> <code class="o">{</code>
        <code class="kt">int</code> <code class="n">airTemperature</code> <code class="o">=</code> <code class="n">parser</code><code class="o">.</code><code class="na">getAirTemperature</code><code class="o">();</code>
        <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">getYear</code><code class="o">()),</code>
            <code class="k">new</code> <code class="nf">IntWritable</code><code class="o">(</code><code class="n">airTemperature</code><code class="o">));</code>
      <code class="o">}</code> <code class="k">else</code> <code class="k">if</code> <code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">isMalformedTemperature</code><code class="o">())</code> <code class="o">{</code>
        <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="sb">"Ignoring possibly corrupt input: "</code> <code class="o">+</code> <code class="n">value</code><code class="o">);</code>
        <code class="n">context</code><code class="o">.</code><code class="na">getCounter</code><code class="o">(</code><code class="n">Temperature</code><code class="o">.</code><code class="na">MALFORMED</code><code class="o">).</code><code class="na">increment</code><code class="o">(</code><code class="mi">1</code><code class="o">);</code>
      <code class="o">}</code> <code class="k">else</code> <code class="k">if</code> <code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">isMissingTemperature</code><code class="o">())</code> <code class="o">{</code>
        <code class="n">context</code><code class="o">.</code><code class="na">getCounter</code><code class="o">(</code><code class="n">Temperature</code><code class="o">.</code><code class="na">MISSING</code><code class="o">).</code><code class="na">increment</code><code class="o">(</code><code class="mi">1</code><code class="o">);</code>
      <code class="o">}</code>
      
      <code class="c2">// dynamic counter</code>
      <code class="n">context</code><code class="o">.</code><code class="na">getCounter</code><code class="o">(</code><code class="sb">"TemperatureQuality"</code><code class="o">,</code> <code class="n">parser</code><code class="o">.</code><code class="na">getQuality</code><code class="o">()).</code><code class="na">increment</code><code class="o">(</code><code class="mi">1</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="n">JobBuilder</code><code class="o">.</code><code class="na">parseInputAndOutput</code><code class="o">(</code><code class="k">this</code><code class="o">,</code> <code class="n">getConf</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">job</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>
    
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputKeyClass</code><code class="o">(</code><code class="n">Text</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputValueClass</code><code class="o">(</code><code class="n">IntWritable</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="n">job</code><code class="o">.</code><code class="na">setMapperClass</code><code class="o">(</code><code class="n">MaxTemperatureMapperWithCounters</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setCombinerClass</code><code class="o">(</code><code class="n">MaxTemperatureReducer</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setReducerClass</code><code class="o">(</code><code class="n">MaxTemperatureReducer</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="k">return</code> <code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">;</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="k">new</code> <code class="n">MaxTemperatureWithCounters</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The best way to see what this program does is to run it over the
      complete dataset:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar hadoop-examples.jar MaxTemperatureWithCounters \
  input/ncdc/all output-counters</code></strong></pre><p class="calibre2">When the job has successfully completed, it prints out the
      counters at the end (this is done by the job client). Here are the ones
      we are interested in:</p><pre class="screen1">Air Temperature Records
  Malformed=3
  Missing=66136856
TemperatureQuality
  0=1
  1=973422173
  2=1246032
  4=10764500
  5=158291879
  6=40066
  9=66136858</pre><p class="calibre2">Notice that the counters for temperature have been made more
      readable by using a resource bundle named after the enum (using an
      underscore as a separator for nested classes)—in this case <em class="calibre10">MaxTemperatureWithCounters_Temperature.properties</em>,
      which contains the display name mappings.</p><div class="book" title="Dynamic counters"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4288">Dynamic counters</h4></div></div></div><p class="calibre2">The code makes use of a <a class="calibre" id="calibre_link-1556"></a><a class="calibre" id="calibre_link-1266"></a>dynamic counter—one that isn’t defined by a Java enum.
        Because a Java enum’s fields are defined at compile time, you can’t
        create new counters on the fly using enums. Here we want to count the
        distribution of temperature quality codes, and though the format
        specification defines the values that the temperature quality code
        <span class="calibre"><em class="calibre10">can</em></span> take, it is more convenient to use a dynamic
        counter to emit the values that it <span class="calibre"><em class="calibre10">actually</em></span>
        takes. The method we use on the <code class="literal">Context</code> object takes a group and counter
        name using <code class="literal">String</code> names:</p><a id="calibre_link-4289" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="n">Counter</code> <code class="nf">getCounter</code><code class="o">(</code><code class="n">String</code> <code class="n">groupName</code><code class="o">,</code> <code class="n">String</code> <code class="n">counterName</code><code class="o">)</code></pre><p class="calibre2">The two ways of creating and accessing counters—using enums and
        using strings—are actually equivalent because Hadoop turns enums into
        strings to send counters over RPC. Enums are slightly easier to work
        with, provide type safety, and are suitable for most jobs. For the odd
        occasion when you need to create counters dynamically, you can use the
        <code class="literal">String</code> interface.</p></div><div class="book" title="Retrieving counters"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4290">Retrieving counters</h4></div></div></div><p class="calibre2">In addition to <a class="calibre" id="calibre_link-1269"></a>using the web UI and the command line (using <code class="literal">mapred job -counter</code>), you can retrieve
        counter values using the Java API. You can do this while the job is
        running, although it is more usual to get counters at the end of a job
        run, when they are stable. <a class="ulink" href="#calibre_link-713" title="Example&nbsp;9-2.&nbsp;Application to calculate the proportion of records with missing temperature fields">Example&nbsp;9-2</a>
        shows a program that calculates the proportion of records that have
        missing temperature fields.</p><div class="example"><a id="calibre_link-713" class="calibre"></a><div class="example-title">Example&nbsp;9-2.&nbsp;Application to calculate the proportion of records with
          missing temperature fields</div><div class="book"><pre class="screen"><code class="k">import</code> <code class="nn">org.apache.hadoop.conf.Configured</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.mapreduce.*</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.util.*</code><code class="o">;</code>

<code class="k">public</code> <code class="k">class</code> <code class="nc">MissingTemperatureFields</code> <code class="k">extends</code> <code class="n">Configured</code> <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="na">length</code> <code class="o">!=</code> <code class="mi">1</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">JobBuilder</code><code class="o">.</code><code class="na">printUsage</code><code class="o">(</code><code class="k">this</code><code class="o">,</code> <code class="sb">"&lt;job ID&gt;"</code><code class="o">);</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>
    <code class="n">String</code> <code class="n">jobID</code> <code class="o">=</code> <code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">];</code>
    <code class="n">Cluster</code> <code class="n">cluster</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Cluster</code><code class="o">(</code><code class="n">getConf</code><code class="o">());</code>
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="n">cluster</code><code class="o">.</code><code class="na">getJob</code><code class="o">(</code><code class="n">JobID</code><code class="o">.</code><code class="na">forName</code><code class="o">(</code><code class="n">jobID</code><code class="o">));</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">job</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"No job with ID %s found.\n"</code><code class="o">,</code> <code class="n">jobID</code><code class="o">);</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>
    <code class="k">if</code> <code class="o">(!</code><code class="n">job</code><code class="o">.</code><code class="na">isComplete</code><code class="o">())</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"Job %s is not complete.\n"</code><code class="o">,</code> <code class="n">jobID</code><code class="o">);</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>

    <code class="n">Counters</code> <code class="n">counters</code> <code class="o">=</code> <code class="n">job</code><code class="o">.</code><code class="na">getCounters</code><code class="o">();</code>
    <code class="kt">long</code> <code class="n">missing</code> <code class="o">=</code> <code class="n">counters</code><code class="o">.</code><code class="na">findCounter</code><code class="o">(</code>
        <code class="n">MaxTemperatureWithCounters</code><code class="o">.</code><code class="na">Temperature</code><code class="o">.</code><code class="na">MISSING</code><code class="o">).</code><code class="na">getValue</code><code class="o">();</code>
    <code class="kt">long</code> <code class="n">total</code> <code class="o">=</code> <code class="n">counters</code><code class="o">.</code><code class="na">findCounter</code><code class="o">(</code><code class="n">TaskCounter</code><code class="o">.</code><code class="na">MAP_INPUT_RECORDS</code><code class="o">).</code><code class="na">getValue</code><code class="o">();</code>

    <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"Records with missing temperature fields: %.2f%%\n"</code><code class="o">,</code>
        <code class="mi">100.0</code> <code class="o">*</code> <code class="n">missing</code> <code class="o">/</code> <code class="n">total</code><code class="o">);</code>
    <code class="k">return</code> <code class="mi">0</code><code class="o">;</code>
  <code class="o">}</code>
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="k">new</code> <code class="n">MissingTemperatureFields</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">First we retrieve a <code class="literal">Job</code>
        object from a <code class="literal">Cluster</code> by calling
        the <code class="literal">getJob()</code> method with the job
        ID. We check whether there is actually a job with the given ID by
        checking if it is <code class="literal">null</code>. There may
        not be, either because the ID was incorrectly specified or because the
        job is no longer in the job history.</p><p class="calibre2">After confirming that the job has completed, we call the
        <code class="literal">Job</code>’s <code class="literal">getCounters()</code> method, which <a class="calibre" id="calibre_link-1274"></a>returns a <code class="literal">Counters</code>
        object encapsulating all the counters for the job. The <code class="literal">Counters</code> class provides various
        methods for finding the names and values of counters. We use the
        <code class="literal">findCounter()</code> method, which takes
        an enum to find the number of records that had a missing temperature
        field and also the total number of records processed (from a built-in
        counter).</p><p class="calibre2">Finally, we print the proportion of records that had a missing
        temperature field. Here’s what we get for the whole <a class="calibre" id="calibre_link-2178"></a><a class="calibre" id="calibre_link-1272"></a><a class="calibre" id="calibre_link-1270"></a>weather dataset:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar hadoop-examples.jar MissingTemperatureFields job_1410450250506_0007</code></strong>
Records with missing temperature fields: 5.47%</pre></div></div><div class="book" title="User-Defined Streaming Counters"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4291">User-Defined Streaming Counters</h3></div></div></div><p class="calibre2">A Streaming <a class="calibre" id="calibre_link-3553"></a><a class="calibre" id="calibre_link-1273"></a>MapReduce program can increment counters by sending a
      specially formatted line to the standard error stream, which is co-opted
      as a control channel in this case. The line must have the following
      format:</p><pre class="screen1">reporter:counter:<em class="replaceable"><code class="replaceable">group</code></em>,<em class="replaceable"><code class="replaceable">counter</code></em>,<em class="replaceable"><code class="replaceable">amount</code></em></pre><p class="calibre2">This snippet in <a class="calibre" id="calibre_link-3098"></a>Python shows how to increment the “Missing” counter in the
      “Temperature” group by 1:</p><pre class="screen1"><code class="n">sys</code><code class="o">.</code><code class="n">stderr</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="sb">"reporter:counter:Temperature,Missing,1</code><code class="se">\n</code><code class="sb">"</code><code class="p">)</code></pre><p class="calibre2">In a similar way, a status message may be sent with a line
      formatted like <a class="calibre" id="calibre_link-2451"></a>this:</p><pre class="screen1">reporter:status:<em class="replaceable"><code class="replaceable">message</code></em></pre></div></div><div class="book" title="Sorting"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-271">Sorting</h2></div></div></div><p class="calibre2">The ability to sort <a class="calibre" id="calibre_link-3426"></a><a class="calibre" id="calibre_link-2501"></a><a class="calibre" id="calibre_link-3432"></a>data is at the heart of MapReduce. Even if your application
    isn’t concerned with sorting per se, it may be able to use the sorting
    stage that MapReduce provides to organize its data. In this section, we
    examine different ways of sorting datasets and how you can control the sort
    order in MapReduce. Sorting Avro data is covered separately, in <a class="ulink" href="#calibre_link-714" title="Sorting Using Avro MapReduce">Sorting Using Avro MapReduce</a>.</p><div class="book" title="Preparation"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4292">Preparation</h3></div></div></div><p class="calibre2">We are going to <a class="calibre" id="calibre_link-3439"></a>sort the weather dataset by temperature. Storing
      temperatures as <code class="literal">Text</code> objects doesn’t
      work for sorting purposes, because signed integers don’t sort lexicographically.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-715" id="calibre_link-740">61</a>]</sup> Instead, we are going to store the data using sequence
      files whose <code class="literal">IntWritable</code> keys represent the
      temperatures (and sort correctly) and whose <code class="literal">Text</code> values are the lines of data.</p><p class="calibre2">The MapReduce job in <a class="ulink" href="#calibre_link-716" title="Example&nbsp;9-3.&nbsp;A MapReduce program for transforming the weather data into SequenceFile format">Example&nbsp;9-3</a> is a
      map-only job that also filters the input to remove records that don’t
      have a valid temperature reading. Each map creates a single
      block-compressed sequence file as output. It is invoked with the
      following command:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar hadoop-examples.jar SortDataPreprocessor input/ncdc/all \
  input/ncdc/all-seq</code></strong></pre><div class="example"><a id="calibre_link-716" class="calibre"></a><div class="example-title">Example&nbsp;9-3.&nbsp;A MapReduce program for transforming the weather data into
        SequenceFile format</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">SortDataPreprocessor</code> <code class="k">extends</code> <code class="n">Configured</code> <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>
  
  <code class="k">static</code> <code class="k">class</code> <code class="nc">CleanerMapper</code>
      <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">&gt;</code> <code class="o">{</code>
  
    <code class="k">private</code> <code class="n">NcdcRecordParser</code> <code class="n">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="n">NcdcRecordParser</code><code class="o">();</code>
    
    <code class="nd">@Override</code>
    <code class="k">protected</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">LongWritable</code> <code class="n">key</code><code class="o">,</code> <code class="n">Text</code> <code class="n">value</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
        <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
      
      <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">value</code><code class="o">);</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">isValidTemperature</code><code class="o">())</code> <code class="o">{</code>
        <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="k">new</code> <code class="n">IntWritable</code><code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">getAirTemperature</code><code class="o">()),</code> <code class="n">value</code><code class="o">);</code>
      <code class="o">}</code>
    <code class="o">}</code>
  <code class="o">}</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="n">JobBuilder</code><code class="o">.</code><code class="na">parseInputAndOutput</code><code class="o">(</code><code class="k">this</code><code class="o">,</code> <code class="n">getConf</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">job</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>

    <code class="n">job</code><code class="o">.</code><code class="na">setMapperClass</code><code class="o">(</code><code class="n">CleanerMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputKeyClass</code><code class="o">(</code><code class="n">IntWritable</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputValueClass</code><code class="o">(</code><code class="n">Text</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setNumReduceTasks</code><code class="o">(</code><code class="mi">0</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputFormatClass</code><code class="o">(</code><code class="n">SequenceFileOutputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">SequenceFileOutputFormat</code><code class="o">.</code><code class="na">setCompressOutput</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">true</code><code class="o">);</code>
    <code class="n">SequenceFileOutputFormat</code><code class="o">.</code><code class="na">setOutputCompressorClass</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">GzipCodec</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">SequenceFileOutputFormat</code><code class="o">.</code><code class="na">setOutputCompressionType</code><code class="o">(</code><code class="n">job</code><code class="o">,</code>
        <code class="n">CompressionType</code><code class="o">.</code><code class="na">BLOCK</code><code class="o">);</code>

    <code class="k">return</code> <code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">;</code>
  <code class="o">}</code>
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="k">new</code> <code class="n">SortDataPreprocessor</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div></div><div class="book" title="Partial Sort"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4293">Partial Sort</h3></div></div></div><p class="calibre2">In <a class="ulink" href="#calibre_link-471" title="The Default MapReduce Job">The Default MapReduce Job</a>, we <a class="calibre" id="calibre_link-3436"></a><a class="calibre" id="calibre_link-2942"></a>saw that, by default, MapReduce will sort input records by
      their keys. <a class="ulink" href="#calibre_link-717" title="Example&nbsp;9-4.&nbsp;A MapReduce program for sorting a SequenceFile with IntWritable keys using the default HashPartitioner">Example&nbsp;9-4</a> is
      a variation for sorting sequence files with <code class="literal">IntWritable</code> keys.</p><div class="example"><a id="calibre_link-717" class="calibre"></a><div class="example-title">Example&nbsp;9-4.&nbsp;A MapReduce program for sorting a SequenceFile with IntWritable
        keys using the default HashPartitioner</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">SortByTemperatureUsingHashPartitioner</code> <code class="k">extends</code> <code class="n">Configured</code>
    <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="n">JobBuilder</code><code class="o">.</code><code class="na">parseInputAndOutput</code><code class="o">(</code><code class="k">this</code><code class="o">,</code> <code class="n">getConf</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">job</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>
    
    <code class="n">job</code><code class="o">.</code><code class="na">setInputFormatClass</code><code class="o">(</code><code class="n">SequenceFileInputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputKeyClass</code><code class="o">(</code><code class="n">IntWritable</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputFormatClass</code><code class="o">(</code><code class="n">SequenceFileOutputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">SequenceFileOutputFormat</code><code class="o">.</code><code class="na">setCompressOutput</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">true</code><code class="o">);</code>
    <code class="n">SequenceFileOutputFormat</code><code class="o">.</code><code class="na">setOutputCompressorClass</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">GzipCodec</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">SequenceFileOutputFormat</code><code class="o">.</code><code class="na">setOutputCompressionType</code><code class="o">(</code><code class="n">job</code><code class="o">,</code>
        <code class="n">CompressionType</code><code class="o">.</code><code class="na">BLOCK</code><code class="o">);</code>
    
    <code class="k">return</code> <code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">;</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="k">new</code> <code class="n">SortByTemperatureUsingHashPartitioner</code><code class="o">(),</code>
        <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><div class="sidebar"><a id="calibre_link-4294" class="calibre"></a><div class="sidebar-title">Controlling Sort Order</div><p class="calibre2">The sort order for keys is <a class="calibre" id="calibre_link-3430"></a>controlled by a <code class="literal">RawComparator</code>, which is <a class="calibre" id="calibre_link-3136"></a>found as follows:</p><div class="book"><ol class="orderedlist1"><li class="listitem"><p class="calibre2">If the property <code class="literal">mapreduce.job.output.key.comparator.class</code>
            is set, <a class="calibre" id="calibre_link-2555"></a>either explicitly or by calling
            <code class="literal">setSortComparatorClass()</code> on <code class="literal">Job</code>, then an instance of that class is
            used. (In the old API, the equivalent method is
            <code class="literal">setOutputKeyComparatorClass()</code> on <code class="literal">JobConf</code>.)</p></li><li class="listitem"><p class="calibre2">Otherwise, keys must be a <a class="calibre" id="calibre_link-3804"></a>subclass of <code class="literal">WritableComparable</code>, and the registered
            comparator for the key class is used.</p></li><li class="listitem"><p class="calibre2">If there is no registered comparator, then a <code class="literal">RawComparator</code> is used. The
            <code class="literal">RawComparator</code> deserializes the byte streams
            being compared into objects and delegates to the <code class="literal">Writable</code><code class="literal">Comparable</code>’s
            <code class="literal">compareTo()</code> method.</p></li></ol></div><p class="calibre2">These rules reinforce the importance of registering optimized
        versions of <code class="literal">RawComparator</code>s for your
        own custom <code class="literal">Writable</code> classes (which
        is covered in <a class="ulink" href="#calibre_link-147" title="Implementing a RawComparator for speed">Implementing a RawComparator for speed</a>), and also
        show that it’s straightforward to override the sort order by setting your own
        comparator (we do this in <a class="ulink" href="#calibre_link-160" title="Secondary Sort">Secondary Sort</a>).</p></div><p class="calibre2">Suppose we run this program using 30 reducers:<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-718" id="calibre_link-742">62</a>]</sup></p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar hadoop-examples.jar SortByTemperatureUsingHashPartitioner \</code></strong>
<strong class="userinput"><code class="calibre9">  -D mapreduce.job.reduces=30 input/ncdc/all-seq output-hashsort</code></strong></pre><p class="calibre2">This command produces 30 output files, each of which is sorted.
      However, there is no easy way to combine the files (by concatenation,
      for example, in the case of plain-text files) to produce a globally
      sorted file.</p><p class="calibre2">For many applications, this doesn’t matter. For example, having a
      partially sorted set of files is fine when you want to do lookups by
      key. The <code class="literal">SortByTemperatureToMapFile</code> and
      <code class="literal">LookupRecordsByTemperature</code> classes in this book’s
      example code explore this idea. By using a map file instead of a
      sequence file, it’s possible to first find the relevant partition that a
      key belongs in (using the partitioner), then to do an efficient lookup
      of the record within the map file <a class="calibre" id="calibre_link-2943"></a><a class="calibre" id="calibre_link-3437"></a>partition.</p></div><div class="book" title="Total Sort"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-80">Total Sort</h3></div></div></div><p class="calibre2">How can you produce a globally <a class="calibre" id="calibre_link-3444"></a>sorted file using Hadoop? The naive answer is to use a
      single partition.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-719" id="calibre_link-744">63</a>]</sup> But this is incredibly inefficient for large files,
      because one machine has to process all of the output, so you are
      throwing away the benefits of the parallel architecture that MapReduce
      provides.</p><p class="calibre2">Instead, it is possible to produce a set of sorted files that, if
      concatenated, would form a globally sorted file. The secret to doing
      this is to use a partitioner that respects the total order of the
      output. For example, if we had four partitions, we could put keys for
      temperatures less than –10°C in the first partition, those between –10°C
      and 0°C in the second, those between 0°C and 10°C in the third, and
      those over 10°C in the fourth.</p><p class="calibre2">Although this approach works, you have to choose your partition
      sizes carefully to ensure that they are fairly even, so job times aren’t
      dominated by a single reducer. For the partitioning scheme just
      described, the relative sizes of the partitions are as follows:</p><div class="informaltable"><table class="calibre31"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><tbody class="calibre22"><tr class="calibre11"><td class="calibre32"><span class="calibre24"><strong class="calibre24">Temperature
              range</strong></span></td><td class="calibre32">&lt; –10°C</td><td class="calibre32">[–10°C, 0°C)</td><td class="calibre32">[0°C, 10°C)</td><td class="calibre33">&gt;= 10°C</td></tr><tr class="calibre13"><td class="calibre34"><span class="calibre24"><strong class="calibre24">Proportion of
              records</strong></span></td><td class="calibre34">11%</td><td class="calibre34">13%</td><td class="calibre34">17%</td><td class="calibre35">59%</td></tr></tbody></table></div><p class="calibre2">These partitions are not very even. To construct more even
      partitions, we need to have a better understanding of the temperature
      distribution for the whole dataset. It’s fairly easy to write a
      MapReduce job to count the number of records that fall into a collection
      of temperature buckets. For example, <a class="ulink" href="#calibre_link-720" title="Figure&nbsp;9-1.&nbsp;Temperature distribution for the weather dataset">Figure&nbsp;9-1</a> shows the distribution for buckets
      of size 1°C, where each point on the plot corresponds to one
      bucket.</p><p class="calibre2">Although we could use this information to construct a very even
      set of partitions, the fact that we needed to run a job that used the
      entire dataset to construct them is not ideal. It’s possible to get a
      fairly even set of partitions by <em class="calibre10">sampling</em> the key
      space. The idea behind sampling is that you look at a small subset of
      the keys to approximate the key distribution, which is then used to
      construct partitions. Luckily, we don’t have to write the code to do
      this ourselves, as Hadoop comes with a selection of samplers.</p><p class="calibre2">The <code class="literal">InputSampler</code> class
      <a class="calibre" id="calibre_link-2124"></a>defines a nested <code class="literal">Sampler</code> interface whose implementations
      return a sample of keys given an
      <code class="literal">InputFormat</code> and <code class="literal">Job</code>:</p><a id="calibre_link-4295" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">interface</code> <code class="nc">Sampler</code><code class="o">&lt;</code><code class="n">K</code><code class="o">,</code> <code class="n">V</code><code class="o">&gt;</code> <code class="o">{</code>
  <code class="n">K</code><code class="o">[]</code> <code class="nf">getSample</code><code class="o">(</code><code class="n">InputFormat</code><code class="o">&lt;</code><code class="n">K</code><code class="o">,</code> <code class="n">V</code><code class="o">&gt;</code> <code class="n">inf</code><code class="o">,</code> <code class="n">Job</code> <code class="n">job</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code><code class="o">;</code>
<code class="o">}</code></pre><div class="book"><div class="figure"><a id="calibre_link-720" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4296" class="calibre"></a><img alt="Temperature distribution for the weather dataset" src="images/000007.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;9-1.&nbsp;Temperature distribution for the weather dataset</div></div></div><p class="calibre2">This interface usually is not called directly by clients. Instead,
      the <code class="literal">writePartitionFile()</code> static
      method on <code class="literal">InputSampler</code> is used, which
      creates a sequence file to store the keys that define the
      partitions:</p><a id="calibre_link-4297" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">static</code> <code class="o">&lt;</code><code class="n">K</code><code class="o">,</code> <code class="n">V</code><code class="o">&gt;</code> <code class="kt">void</code> <code class="n">writePartitionFile</code><code class="o">(</code><code class="n">Job</code> <code class="n">job</code><code class="o">,</code> <code class="n">Sampler</code><code class="o">&lt;</code><code class="n">K</code><code class="o">,</code> <code class="n">V</code><code class="o">&gt;</code> <code class="n">sampler</code><code class="o">)</code> 
    <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">ClassNotFoundException</code><code class="o">,</code> <code class="n">InterruptedException</code></pre><p class="calibre2">The sequence file is <a class="calibre" id="calibre_link-3706"></a>used by <code class="literal">TotalOrderPartitioner</code> to create partitions for
      the sort job. <a class="ulink" href="#calibre_link-721" title="Example&nbsp;9-5.&nbsp;A MapReduce program for sorting a SequenceFile with IntWritable keys using the TotalOrderPartitioner to globally sort the data">Example&nbsp;9-5</a> puts it all
      together.</p><div class="example"><a id="calibre_link-721" class="calibre"></a><div class="example-title">Example&nbsp;9-5.&nbsp;A MapReduce program for sorting a SequenceFile with IntWritable
        keys using the TotalOrderPartitioner to globally sort the data</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">SortByTemperatureUsingTotalOrderPartitioner</code> <code class="k">extends</code> <code class="n">Configured</code>
    <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="n">JobBuilder</code><code class="o">.</code><code class="na">parseInputAndOutput</code><code class="o">(</code><code class="k">this</code><code class="o">,</code> <code class="n">getConf</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">job</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>
    
    <code class="n">job</code><code class="o">.</code><code class="na">setInputFormatClass</code><code class="o">(</code><code class="n">SequenceFileInputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputKeyClass</code><code class="o">(</code><code class="n">IntWritable</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputFormatClass</code><code class="o">(</code><code class="n">SequenceFileOutputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">SequenceFileOutputFormat</code><code class="o">.</code><code class="na">setCompressOutput</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">true</code><code class="o">);</code>
    <code class="n">SequenceFileOutputFormat</code><code class="o">.</code><code class="na">setOutputCompressorClass</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">GzipCodec</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">SequenceFileOutputFormat</code><code class="o">.</code><code class="na">setOutputCompressionType</code><code class="o">(</code><code class="n">job</code><code class="o">,</code>
        <code class="n">CompressionType</code><code class="o">.</code><code class="na">BLOCK</code><code class="o">);</code>

    <code class="n">job</code><code class="o">.</code><code class="na">setPartitionerClass</code><code class="o">(</code><code class="n">TotalOrderPartitioner</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="n">InputSampler</code><code class="o">.</code><code class="na">Sampler</code><code class="o">&lt;</code><code class="n">IntWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">&gt;</code> <code class="n">sampler</code> <code class="o">=</code>
        <code class="k">new</code> <code class="n">InputSampler</code><code class="o">.</code><code class="na">RandomSampler</code><code class="o">&lt;</code><code class="n">IntWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">&gt;(</code><code class="mi">0.1</code><code class="o">,</code> <code class="mi">10000</code><code class="o">,</code> <code class="mi">10</code><code class="o">);</code>
    
    <code class="n">InputSampler</code><code class="o">.</code><code class="na">writePartitionFile</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">sampler</code><code class="o">);</code>

    <code class="c2">// Add to DistributedCache</code>
    <code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="n">job</code><code class="o">.</code><code class="na">getConfiguration</code><code class="o">();</code>
    <code class="n">String</code> <code class="n">partitionFile</code> <code class="o">=</code> <code class="n">TotalOrderPartitioner</code><code class="o">.</code><code class="na">getPartitionFile</code><code class="o">(</code><code class="n">conf</code><code class="o">);</code>
    <code class="n">URI</code> <code class="n">partitionUri</code> <code class="o">=</code> <code class="k">new</code> <code class="n">URI</code><code class="o">(</code><code class="n">partitionFile</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">addCacheFile</code><code class="o">(</code><code class="n">partitionUri</code><code class="o">);</code>

    <code class="k">return</code> <code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">;</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code>
        <code class="k">new</code> <code class="nf">SortByTemperatureUsingTotalOrderPartitioner</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">We use a <code class="literal">RandomSampler</code>, which
      chooses keys with a uniform probability—here, 0.1. There are also
      parameters for the maximum number of samples to take and the maximum
      number of splits to sample (here, 10,000 and 10, respectively; these
      settings are the defaults when <code class="literal">InputSampler</code> is run as an application), and
      the sampler stops when the first of these limits is met. Samplers run on
      the client, making it important to limit the number of splits that are
      downloaded so the sampler runs quickly. In practice, the time taken to
      run the sampler is a small fraction of the overall job time.</p><p class="calibre2">The <code class="literal">InputSampler</code> writes a
      partition file that we need to share with the tasks running on the
      cluster by adding it to the distributed cache (see <a class="ulink" href="#calibre_link-54" title="Distributed Cache">Distributed Cache</a>).</p><p class="calibre2">On one run, the sampler chose –5.6°C, 13.9°C, and 22.0°C as
      partition boundaries (for four partitions), which translates into more
      even partition sizes than the earlier choice:</p><div class="informaltable"><table class="calibre31"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><tbody class="calibre22"><tr class="calibre11"><td class="calibre32"><span class="calibre24"><strong class="calibre24">Temperature
              range</strong></span></td><td class="calibre32">&lt; –5.6°C</td><td class="calibre32">[–5.6°C, 13.9°C)</td><td class="calibre32">[13.9°C, 22.0°C)</td><td class="calibre33">&gt;= 22.0°C</td></tr><tr class="calibre13"><td class="calibre34"><span class="calibre24"><strong class="calibre24">Proportion of
              records</strong></span></td><td class="calibre34">29%</td><td class="calibre34">24%</td><td class="calibre34">23%</td><td class="calibre35">24%</td></tr></tbody></table></div><p class="calibre2">Your input data determines the best sampler to use. For example,
      <code class="literal">SplitSampler</code>, which samples only the
      first <span class="calibre">n</span> records in a split, is not so
      good for sorted data,<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-722" id="calibre_link-747">64</a>]</sup> because it doesn’t select keys from throughout the
      split.</p><p class="calibre2">On the other hand, <code class="literal">IntervalSampler</code> chooses keys at regular
      intervals through the split and makes a better choice for sorted data.
      <code class="literal">RandomSampler</code> is a good
      general-purpose sampler. If none of these suits your application (and
      remember that the point of sampling is to produce partitions that are
      <span class="calibre">approximately</span> equal in size), you can
      write your own implementation of the <code class="literal">Sampler</code> interface.</p><p class="calibre2">One of the nice properties of <code class="literal">InputSampler</code> and <code class="literal">TotalOrderPartitioner</code> is that you are free to
      choose the number of partitions—that is, the number of reducers.
      However, <code class="literal">TotalOrderPartitioner</code> will
      work only if the partition boundaries are distinct. One problem with
      choosing a high number is that you may get collisions if you have a
      small key space.</p><p class="calibre2">Here’s how we run it:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar hadoop-examples.jar SortByTemperatureUsingTotalOrderPartitioner \</code></strong>
<strong class="userinput"><code class="calibre9">  -D mapreduce.job.reduces=30 input/ncdc/all-seq output-totalsort</code></strong></pre><p class="calibre2">The program produces 30 output partitions, each of which is
      internally sorted; in addition, for these partitions, all the keys in
      partition <span class="calibre">i</span> are less than the keys in
      <a class="calibre" id="calibre_link-3445"></a>partition <span class="calibre">i</span> + 1.</p></div><div class="book" title="Secondary Sort"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-160">Secondary Sort</h3></div></div></div><p class="calibre2">The MapReduce <a class="calibre" id="calibre_link-3440"></a><a class="calibre" id="calibre_link-3304"></a>framework sorts the records by key before they reach the
      reducers. For any particular key, however, the values are <span class="calibre">not</span> sorted. The order in which the values
      appear is not even stable from one run to the next, because they come
      from different map tasks, which may finish at different times from run
      to run. Generally speaking, most MapReduce programs are written so as
      not to depend on the order in which the values appear to the reduce
      function. However, it is possible to impose an order on the values by
      sorting and grouping the keys in a particular way.</p><p class="calibre2">To illustrate the idea, consider the MapReduce program for
      calculating the maximum temperature for each year. If we arranged for
      the values (temperatures) to be sorted in descending order, we wouldn’t
      have to iterate through them to find the maximum; instead, we could take
      the first for each year and ignore the rest. (This approach isn’t the
      most efficient way to solve this particular problem, but it illustrates
      how secondary sort works in general.)</p><p class="calibre2">To achieve this, we change our keys to be composite: a combination
      of year and temperature. We want
      the sort order for keys to be by year (ascending) and then by
      temperature (descending):</p><a id="calibre_link-4298" class="calibre"></a><pre class="screen1">1900 35°C
1900 34°C
1900 34°C
...
1901 36°C
1901 35°C</pre><p class="calibre2">If all we did was change the key, this wouldn’t help, because then
      records for the same year would have different keys and therefore would
      not (in general) go to the same reducer. For example, (1900, 35°C) and
      (1900, 34°C) could go to different reducers. By setting a partitioner to
      partition by the year part of the key, we can guarantee that records for
      the same year go to the same reducer. This still isn’t enough to achieve
      our goal, however. A partitioner
      ensures only that one reducer receives all the records for a year; it
      doesn’t change the fact that the reducer groups by key within the
      partition:</p><div class="informalfigure"><div class="book"><a id="calibre_link-4299" class="calibre"></a><img alt="image with no caption" src="images/000016.png" class="calibre29"></div></div><p class="calibre2">The final piece of the puzzle is the setting to control the
      grouping. If we group values in the reducer by the year part of the key,
      we will see all the records for the same year in one reduce group. And
      because they are sorted by temperature in descending order, the first is
      the maximum temperature:</p><div class="informalfigure"><div class="book"><a id="calibre_link-4300" class="calibre"></a><img alt="image with no caption" src="images/000024.png" class="calibre29"></div></div><p class="calibre2">To summarize, there is a recipe here to get the effect of sorting
      by value:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">Make the key a composite of the natural key and the natural
          value.</p></li><li class="listitem"><p class="calibre2">The sort comparator should order by the composite key (i.e.,
          the natural key <span class="calibre">and</span> natural
          value).</p></li><li class="listitem"><p class="calibre2">The partitioner and grouping comparator for the composite key
          should consider only the natural key for partitioning and
          grouping.</p></li></ul></div><div class="book" title="Java code"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4301">Java code</h4></div></div></div><p class="calibre2">Putting this all <a class="calibre" id="calibre_link-2173"></a>together results in the code in <a class="ulink" href="#calibre_link-723" title="Example&nbsp;9-6.&nbsp;Application to find the maximum temperature by sorting temperatures in the key">Example&nbsp;9-6</a>. This program uses the
        plain-text input again.</p><div class="example"><a id="calibre_link-723" class="calibre"></a><div class="example-title">Example&nbsp;9-6.&nbsp;Application to find the maximum temperature by sorting
          temperatures in the key</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">MaxTemperatureUsingSecondarySort</code>
    <code class="k">extends</code> <code class="n">Configured</code> <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>
  
  <code class="k">static</code> <code class="k">class</code> <code class="nc">MaxTemperatureMapper</code>
      <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">IntPair</code><code class="o">,</code> <code class="n">NullWritable</code><code class="o">&gt;</code> <code class="o">{</code>
  
    <code class="k">private</code> <code class="n">NcdcRecordParser</code> <code class="n">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="n">NcdcRecordParser</code><code class="o">();</code>
    
    <code class="nd">@Override</code>
    <code class="k">protected</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">LongWritable</code> <code class="n">key</code><code class="o">,</code> <code class="n">Text</code> <code class="n">value</code><code class="o">,</code>
        <code class="n">Context</code> <code class="n">context</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
      
      <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">value</code><code class="o">);</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">isValidTemperature</code><code class="o">())</code> <code class="o">{</code>
        <span class="calibre24"><strong class="calibre9"><code class="n1">context</code><code class="o1">.</code><code class="na1">write</code><code class="o1">(</code><code class="kc">new</code> <code class="n1">IntPair</code><code class="o1">(</code><code class="n1">parser</code><code class="o1">.</code><code class="na1">getYearInt</code><code class="o1">(),</code>
            <code class="n1">parser</code><code class="o1">.</code><code class="na1">getAirTemperature</code><code class="o1">()),</code> <code class="n1">NullWritable</code><code class="o1">.</code><code class="na1">get</code><code class="o1">());</code></strong></span>
      <code class="o">}</code>
    <code class="o">}</code>
  <code class="o">}</code>
  
  <code class="k">static</code> <code class="k">class</code> <code class="nc">MaxTemperatureReducer</code>
      <code class="k">extends</code> <code class="n">Reducer</code><code class="o">&lt;</code><code class="n">IntPair</code><code class="o">,</code> <code class="n">NullWritable</code><code class="o">,</code> <code class="n">IntPair</code><code class="o">,</code> <code class="n">NullWritable</code><code class="o">&gt;</code> <code class="o">{</code>
  
    <code class="nd">@Override</code>
    <code class="k">protected</code> <code class="kt">void</code> <code class="nf">reduce</code><code class="o">(</code><code class="n">IntPair</code> <code class="n">key</code><code class="o">,</code> <code class="n">Iterable</code><code class="o">&lt;</code><code class="n">NullWritable</code><code class="o">&gt;</code> <code class="n">values</code><code class="o">,</code>
        <code class="n">Context</code> <code class="n">context</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
      
      <span class="calibre24"><strong class="calibre9"><code class="n1">context</code><code class="o1">.</code><code class="na1">write</code><code class="o1">(</code><code class="n1">key</code><code class="o1">,</code> <code class="n1">NullWritable</code><code class="o1">.</code><code class="na1">get</code><code class="o1">());</code></strong></span>
    <code class="o">}</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="k">class</code> <code class="nc">FirstPartitioner</code>
      <code class="k">extends</code> <code class="n">Partitioner</code><code class="o">&lt;</code><code class="n">IntPair</code><code class="o">,</code> <code class="n">NullWritable</code><code class="o">&gt;</code> <code class="o">{</code>

    <code class="nd">@Override</code>
    <code class="k">public</code> <code class="kt">int</code> <code class="nf">getPartition</code><code class="o">(</code><code class="n">IntPair</code> <code class="n">key</code><code class="o">,</code> <code class="n">NullWritable</code> <code class="n">value</code><code class="o">,</code> <code class="kt">int</code> <code class="n">numPartitions</code><code class="o">)</code> <code class="o">{</code>
      <code class="c2">// multiply by 127 to perform some mixing</code>
      <code class="k">return</code> <code class="n">Math</code><code class="o">.</code><code class="na">abs</code><code class="o">(</code><code class="n">key</code><code class="o">.</code><code class="na">getFirst</code><code class="o">()</code> <code class="o">*</code> <code class="mi">127</code><code class="o">)</code> <code class="o">%</code> <code class="n">numPartitions</code><code class="o">;</code>
    <code class="o">}</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="k">class</code> <code class="nc">KeyComparator</code> <code class="k">extends</code> <code class="n">WritableComparator</code> <code class="o">{</code>
    <code class="k">protected</code> <code class="nf">KeyComparator</code><code class="o">()</code> <code class="o">{</code>
      <code class="k">super</code><code class="o">(</code><code class="n">IntPair</code><code class="o">.</code><code class="na">class</code><code class="o">,</code> <code class="k">true</code><code class="o">);</code>
    <code class="o">}</code>
    <code class="nd">@Override</code>
    <code class="k">public</code> <code class="kt">int</code> <code class="nf">compare</code><code class="o">(</code><code class="n">WritableComparable</code> <code class="n">w1</code><code class="o">,</code> <code class="n">WritableComparable</code> <code class="n">w2</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">IntPair</code> <code class="n">ip1</code> <code class="o">=</code> <code class="o">(</code><code class="n">IntPair</code><code class="o">)</code> <code class="n">w1</code><code class="o">;</code>
      <code class="n">IntPair</code> <code class="n">ip2</code> <code class="o">=</code> <code class="o">(</code><code class="n">IntPair</code><code class="o">)</code> <code class="n">w2</code><code class="o">;</code>
      <code class="kt">int</code> <code class="n">cmp</code> <code class="o">=</code> <code class="n">IntPair</code><code class="o">.</code><code class="na">compare</code><code class="o">(</code><code class="n">ip1</code><code class="o">.</code><code class="na">getFirst</code><code class="o">(),</code> <code class="n">ip2</code><code class="o">.</code><code class="na">getFirst</code><code class="o">());</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">cmp</code> <code class="o">!=</code> <code class="mi">0</code><code class="o">)</code> <code class="o">{</code>
        <code class="k">return</code> <code class="n">cmp</code><code class="o">;</code>
      <code class="o">}</code>
      <code class="k">return</code> <code class="o">-</code><code class="n">IntPair</code><code class="o">.</code><code class="na">compare</code><code class="o">(</code><code class="n">ip1</code><code class="o">.</code><code class="na">getSecond</code><code class="o">(),</code> <code class="n">ip2</code><code class="o">.</code><code class="na">getSecond</code><code class="o">());</code> <code class="c2">//reverse</code>
    <code class="o">}</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="k">class</code> <code class="nc">GroupComparator</code> <code class="k">extends</code> <code class="n">WritableComparator</code> <code class="o">{</code>
    <code class="k">protected</code> <code class="nf">GroupComparator</code><code class="o">()</code> <code class="o">{</code>
      <code class="k">super</code><code class="o">(</code><code class="n">IntPair</code><code class="o">.</code><code class="na">class</code><code class="o">,</code> <code class="k">true</code><code class="o">);</code>
    <code class="o">}</code>
    <code class="nd">@Override</code>
    <code class="k">public</code> <code class="kt">int</code> <code class="nf">compare</code><code class="o">(</code><code class="n">WritableComparable</code> <code class="n">w1</code><code class="o">,</code> <code class="n">WritableComparable</code> <code class="n">w2</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">IntPair</code> <code class="n">ip1</code> <code class="o">=</code> <code class="o">(</code><code class="n">IntPair</code><code class="o">)</code> <code class="n">w1</code><code class="o">;</code>
      <code class="n">IntPair</code> <code class="n">ip2</code> <code class="o">=</code> <code class="o">(</code><code class="n">IntPair</code><code class="o">)</code> <code class="n">w2</code><code class="o">;</code>
      <code class="k">return</code> <code class="n">IntPair</code><code class="o">.</code><code class="na">compare</code><code class="o">(</code><code class="n">ip1</code><code class="o">.</code><code class="na">getFirst</code><code class="o">(),</code> <code class="n">ip2</code><code class="o">.</code><code class="na">getFirst</code><code class="o">());</code>
    <code class="o">}</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="n">JobBuilder</code><code class="o">.</code><code class="na">parseInputAndOutput</code><code class="o">(</code><code class="k">this</code><code class="o">,</code> <code class="n">getConf</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">job</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>
    
    <code class="n">job</code><code class="o">.</code><code class="na">setMapperClass</code><code class="o">(</code><code class="n">MaxTemperatureMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <span class="calibre24"><strong class="calibre9"><code class="n1">job</code><code class="o1">.</code><code class="na1">setPartitionerClass</code><code class="o1">(</code><code class="n1">FirstPartitioner</code><code class="o1">.</code><code class="na1">class</code><code class="o1">);</code></strong></span>
    <span class="calibre24"><strong class="calibre9"><code class="n1">job</code><code class="o1">.</code><code class="na1">setSortComparatorClass</code><code class="o1">(</code><code class="n1">KeyComparator</code><code class="o1">.</code><code class="na1">class</code><code class="o1">);</code></strong></span>
    <span class="calibre24"><strong class="calibre9"><code class="n1">job</code><code class="o1">.</code><code class="na1">setGroupingComparatorClass</code><code class="o1">(</code><code class="n1">GroupComparator</code><code class="o1">.</code><code class="na1">class</code><code class="o1">);</code></strong></span>
    <code class="n">job</code><code class="o">.</code><code class="na">setReducerClass</code><code class="o">(</code><code class="n">MaxTemperatureReducer</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputKeyClass</code><code class="o">(</code><code class="n">IntPair</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputValueClass</code><code class="o">(</code><code class="n">NullWritable</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    
    <code class="k">return</code> <code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">;</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="k">new</code> <code class="n">MaxTemperatureUsingSecondarySort</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">In the mapper, we create a key representing the year and
        temperature, using an <code class="literal">IntPair</code> <code class="literal">Writable</code> implementation. (<code class="literal">IntPair</code> is like the <code class="literal">TextPair</code> class we developed in <a class="ulink" href="#calibre_link-724" title="Implementing a Custom Writable">Implementing a Custom Writable</a>.) We don’t need to carry any
        information in the value, because we can get the first (maximum)
        temperature in the reducer from the key, so we use a <code class="literal">NullWritable</code>. The reducer emits the first
        key, which, due to the secondary sorting, is an <code class="literal">IntPair</code> for the year and its maximum
        temperature. <code class="literal">IntPair</code>’s
        <code class="literal">toString()</code> method creates a tab-separated
        string, so the output is a set of tab-separated year-temperature
        pairs.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Many applications need to access all the sorted values, not
          just the first value as we have provided here. To do this, you need
          to populate the value fields since in the reducer you can retrieve
          only the first key. This necessitates some unavoidable duplication
          of information between key and value.</p></div><p class="calibre2">We set the partitioner to partition by the first field of the
        key (the year) using a custom partitioner called <code class="literal">FirstPartitioner</code>. To sort keys by year
        (ascending) and temperature (descending), we use a custom sort
        comparator, using <code class="literal">setSortComparatorClass()</code>, that extracts the
        fields and performs the appropriate comparisons. Similarly, to group
        keys by year, we set a custom comparator, using <code class="literal">setGroupingComparatorClass()</code>, to extract the
        first field of the key for comparison.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-725" id="calibre_link-748">65</a>]</sup></p><p class="calibre2">Running this program gives the maximum temperatures for each
        <a class="calibre" id="calibre_link-2174"></a>year:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar hadoop-examples.jar MaxTemperatureUsingSecondarySort \</code></strong>
<strong class="userinput"><code class="calibre9">  input/ncdc/all output-secondarysort</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -cat output-secondarysort/part-* | sort | head</code></strong>
1901	317
1902	244
1903	289
1904	256
1905	283
1906	294
1907	283
1908	289
1909	278
1910	294</pre></div><div class="book" title="Streaming"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-741">Streaming</h4></div></div></div><p class="calibre2">To do a secondary sort in <a class="calibre" id="calibre_link-3550"></a>Streaming, we can take advantage of a couple of library
        classes that Hadoop provides. Here’s the driver that we can use to do
        a secondary sort:</p><a id="calibre_link-4302" class="calibre"></a><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
  -D stream.num.map.output.key.fields=2 \
  -D mapreduce.partition.keypartitioner.options=-k1,1 \
  -D mapreduce.job.output.key.comparator.class=\
org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \
  -D mapreduce.partition.keycomparator.options="-k1n -k2nr" \
  -files secondary_sort_map.py,secondary_sort_reduce.py \
  -input input/ncdc/all \
  -output output-secondarysort-streaming \
  -mapper ch09-mr-features/src/main/python/secondary_sort_map.py \
  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \
  -reducer ch09-mr-features/src/main/python/secondary_sort_reduce.py</code></strong></pre><p class="calibre2">Our map function (<a class="ulink" href="#calibre_link-726" title="Example&nbsp;9-7.&nbsp;Map function for secondary sort in Python">Example&nbsp;9-7</a>)
        emits records with year and temperature fields. We want to treat the
        combination of both of these fields as the key, so we set <code class="literal">stream.num.map.output.key.fields</code> to 2. This
        means that values will be empty, just like in the Java case.</p><div class="example"><a id="calibre_link-726" class="calibre"></a><div class="example-title">Example&nbsp;9-7.&nbsp;Map function for secondary sort in Python</div><div class="book"><pre class="screen"><code class="c1">#!/usr/bin/env python</code>

<code class="k">import</code> <code class="nn">re</code>
<code class="k">import</code> <code class="nn">sys</code>

<code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">sys</code><code class="o">.</code><code class="n">stdin</code><code class="p">:</code>
  <code class="n">val</code> <code class="o">=</code> <code class="n">line</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code>
  <code class="p">(</code><code class="n">year</code><code class="p">,</code> <code class="n">temp</code><code class="p">,</code> <code class="n">q</code><code class="p">)</code> <code class="o">=</code> <code class="p">(</code><code class="n">val</code><code class="p">[</code><code class="mi">15</code><code class="p">:</code><code class="mi">19</code><code class="p">],</code> <code class="nb">int</code><code class="p">(</code><code class="n">val</code><code class="p">[</code><code class="mi">87</code><code class="p">:</code><code class="mi">92</code><code class="p">]),</code> <code class="n">val</code><code class="p">[</code><code class="mi">92</code><code class="p">:</code><code class="mi">93</code><code class="p">])</code>
  <code class="k">if</code> <code class="n">temp</code> <code class="o">==</code> <code class="mi">9999</code><code class="p">:</code>
    <code class="n">sys</code><code class="o">.</code><code class="n">stderr</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="sb">"reporter:counter:Temperature,Missing,1</code><code class="se">\n</code><code class="sb">"</code><code class="p">)</code>
  <code class="k">elif</code> <code class="n">re</code><code class="o">.</code><code class="n">match</code><code class="p">(</code><code class="sb">"[01459]"</code><code class="p">,</code> <code class="n">q</code><code class="p">):</code>
    <code class="k">print</code> <code class="sb">"</code><code class="si">%s</code><code class="se">\t</code><code class="si">%s</code><code class="sb">"</code> <code class="o">%</code> <code class="p">(</code><code class="n">year</code><code class="p">,</code> <code class="n">temp</code><code class="p">)</code></pre></div></div><p class="calibre2">However, we don’t want to partition by the entire key, so we use
        <code class="literal">KeyFieldBasedPartitioner</code>, which
        allows us to partition by a part of the key. The specification
        <code class="literal">mapreduce.partition.keypartitioner.options</code>
        configures the partitioner. The value <code class="literal">-k1,1</code> instructs the partitioner to use
        only the first field of the key, where fields are assumed to be
        separated by a string defined by the <span class="calibre"><code class="literal">mapreduce.map.output</code></span><span class="calibre"><code class="literal">.key.field.separator</code></span> property (a
        tab character by default).</p><p class="calibre2">Next, we want a comparator that sorts the year field in
        ascending order and the temperature field in descending order, so that
        the reduce function can simply return the first record in each group.
        Hadoop provides <code class="literal">KeyFieldBasedComparator</code>, which is ideal for
        this purpose. The comparison order is defined by a specification that
        is like the one used for GNU <em class="calibre10">sort</em>.
        It is set using the <code class="literal">mapreduce.partition.keycomparator.options</code>
        property. The value <code class="literal">-k1n -k2nr</code> used
        in this example means “sort by the first field in numerical order,
        then by the second field in reverse numerical order.” Like its
        partitioner cousin, <code class="literal">KeyFieldBasedPartitioner</code>, it
        uses the map output key separator to split a key into fields.</p><p class="calibre2">In the Java version, we had to set the grouping comparator;
        however, in Streaming, groups are not demarcated in any way, so in the
        reduce function we have to detect the group boundaries ourselves by
        looking for when the year changes (<a class="ulink" href="#calibre_link-727" title="Example&nbsp;9-8.&nbsp;Reduce function for secondary sort in Python">Example&nbsp;9-8</a>).</p><div class="example"><a id="calibre_link-727" class="calibre"></a><div class="example-title">Example&nbsp;9-8.&nbsp;Reduce function for secondary sort in Python</div><div class="book"><pre class="screen"><code class="c1">#!/usr/bin/env python</code>

<code class="k">import</code> <code class="nn">sys</code>

<code class="n">last_group</code> <code class="o">=</code> <code class="nb">None</code>
<code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">sys</code><code class="o">.</code><code class="n">stdin</code><code class="p">:</code>
  <code class="n">val</code> <code class="o">=</code> <code class="n">line</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code>
  <code class="p">(</code><code class="n">year</code><code class="p">,</code> <code class="n">temp</code><code class="p">)</code> <code class="o">=</code> <code class="n">val</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="sb">"</code><code class="se">\t</code><code class="sb">"</code><code class="p">)</code>
  <code class="n">group</code> <code class="o">=</code> <code class="n">year</code>
  <code class="k">if</code> <code class="n">last_group</code> <code class="o">!=</code> <code class="n">group</code><code class="p">:</code>
    <code class="k">print</code> <code class="n">val</code>
    <code class="n">last_group</code> <code class="o">=</code> <code class="n">group</code></pre></div></div><p class="calibre2">When we run the Streaming program, we get the same output as the
        Java version.</p><p class="calibre2">Finally, note that <code class="literal">KeyFieldBasedPartitioner</code> and <code class="literal">KeyFieldBasedComparator</code> are not confined to
        use in Streaming programs; they are applicable to Java MapReduce
        programs, <a class="calibre" id="calibre_link-2502"></a><a class="calibre" id="calibre_link-3305"></a><a class="calibre" id="calibre_link-3441"></a><a class="calibre" id="calibre_link-3551"></a><a class="calibre" id="calibre_link-3433"></a>too.</p></div></div></div><div class="book" title="Joins"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-262">Joins</h2></div></div></div><p class="calibre2">MapReduce can perform <a class="calibre" id="calibre_link-2276"></a><a class="calibre" id="calibre_link-2483"></a>joins between large datasets, but writing the code to do
    joins from scratch is fairly involved. Rather than writing MapReduce
    programs, you might consider using a higher-level framework such as Pig,
    Hive, Cascading, Cruc, or Spark, in which join operations are a core part
    of the implementation.</p><p class="calibre2">Let’s briefly consider the problem we are trying to solve. We have
    two datasets—for example, the weather stations database and the weather
    records—and we want to reconcile the two. Let’s say we want to see each
    station’s history, with the station’s metadata inlined in each output row.
    This is illustrated in <a class="ulink" href="#calibre_link-728" title="Figure&nbsp;9-2.&nbsp;Inner join of two datasets">Figure&nbsp;9-2</a>.</p><p class="calibre2">How we implement the join depends on how large the datasets are and
    how they are partitioned. If one dataset is large (the weather records)
    but the other one is small enough to be distributed to each node in the
    cluster (as the station metadata is), the join can be effected by a
    MapReduce job that brings the records for each station together (a partial
    sort on station ID, for example). The mapper or reducer uses the smaller
    dataset to look up the station metadata for a station ID, so it can be
    written out with each record. See <a class="ulink" href="#calibre_link-150" title="Side Data Distribution">Side Data Distribution</a>
    for a discussion of this approach, where we focus on the mechanics of
    distributing the data to nodes in the cluster.</p><div class="book"><div class="figure"><a id="calibre_link-728" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4303" class="calibre"></a><img alt="Inner join of two datasets" src="images/000032.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;9-2.&nbsp;Inner join of two datasets</div></div></div><p class="calibre2">If the join is performed by the mapper it is called a
    <em class="calibre10">map-side join</em>, whereas if it is performed by the
    reducer it is called a <em class="calibre10">reduce-side join</em>.</p><p class="calibre2">If both datasets are too large for either to be copied to each node
    in the cluster, we can still join them using MapReduce with a map-side or
    reduce-side join, depending on how the data is structured. One common
    example of this case is a user database and a log of some user activity
    (such as access logs). For a popular service, it is not feasible to
    distribute the user database (or the logs) to all the MapReduce
    nodes.</p><div class="book" title="Map-Side Joins"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-687">Map-Side Joins</h3></div></div></div><p class="calibre2">A map-side join between large <a class="calibre" id="calibre_link-2279"></a><a class="calibre" id="calibre_link-2395"></a>inputs works by performing the join before the data
      reaches the map function. For this to work, though, the inputs to each
      map must be partitioned and sorted in a particular way. Each input
      dataset must be divided into the same number of partitions, and it must
      be sorted by the same key (the join key) in each source. All the records
      for a particular key must reside in the same partition. This may sound
      like a strict requirement (and it is), but it actually fits the
      description of the output of a MapReduce job.</p><p class="calibre2">A map-side join can be used to join the outputs of several jobs
      that had the same number of reducers, the same keys, and output files
      that are not splittable (by virtue of being smaller than an HDFS block
      or being gzip compressed, for example). In the context of the weather
      example, if we ran a partial sort on the stations file by station ID,
      and another identical sort on the records, again by station ID and with
      the same number of reducers, then the two outputs would satisfy the
      conditions for running a map-side join.</p><p class="calibre2">You use <a class="calibre" id="calibre_link-1204"></a>a <code class="literal">CompositeInputFormat</code>
      from <a class="calibre" id="calibre_link-2868"></a>the <code class="literal">org.apache.hadoop.mapreduce.join</code> package to
      run a map-side join. The input sources and join type (inner or outer)
      for <span class="calibre"><code class="literal">Composite</code></span><span class="calibre"><code class="literal">Input</code></span><code class="literal">Format</code> are configured through a join
      expression that is written according to a simple grammar. The package
      documentation has details and examples.</p><p class="calibre2">The <code class="literal">org.apache.hadoop.examples.Join</code> example is a
      general-purpose command-line program for running a map-side join, since
      it allows you to run a MapReduce job for any specified mapper and
      reducer over multiple inputs that are joined with a given <a class="calibre" id="calibre_link-2280"></a><a class="calibre" id="calibre_link-2396"></a>join operation.</p></div><div class="book" title="Reduce-Side Joins"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-595">Reduce-Side Joins</h3></div></div></div><p class="calibre2">A reduce-side join <a class="calibre" id="calibre_link-2285"></a><a class="calibre" id="calibre_link-3183"></a>is more general than a map-side join, in that the input
      datasets don’t have to be structured in any particular way, but it is
      less efficient because both datasets have to go through the MapReduce
      shuffle. The basic idea is that the mapper tags each record with its
      source and uses the join key as the map output key, so that the records
      with the same key are brought together in the reducer. We use several
      ingredients to make this work in practice:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Multiple inputs</span></dt><dd class="calibre8"><p class="calibre2">The input sources for the datasets generally have different
            formats, so it is very convenient to <a class="calibre" id="calibre_link-2722"></a>use the <code class="literal">MultipleInputs</code> class (see <a class="ulink" href="#calibre_link-729" title="Multiple Inputs">Multiple Inputs</a>) to separate the logic for parsing and
            tagging each source.</p></dd><dt class="calibre7"><span class="term">Secondary sort</span></dt><dd class="calibre8"><p class="calibre2">As described, the reducer will see the records from both
            sources that have the same key, but they are not guaranteed to be
            in any particular order. However, to perform the join, it is
            important to have the data from one source before that from the
            other. For the weather data join, the station record must be the
            first of the values seen for each key, so the reducer can fill in
            the weather records with the station name and emit them
            straightaway. Of course, it would be possible to receive the
            records in any order if we buffered them in memory, but this
            should be avoided because the number of records in any group may
            be very large and exceed the amount of memory available to the
            reducer.</p><p class="calibre2">We saw in <a class="ulink" href="#calibre_link-160" title="Secondary Sort">Secondary Sort</a> how to impose an
            order on the values for each key that the reducers see, so we use
            this technique here.</p></dd></dl></div><p class="calibre2">To tag each record, we use <code class="literal">TextPair</code> (discussed in <a class="ulink" href="#calibre_link-226" title="Chapter&nbsp;5.&nbsp;Hadoop I/O">Chapter&nbsp;5</a>) for the keys (to store the station ID) and the
      tag. The only requirement for the tag values is that they sort in such a
      way that the station records come before the weather records. This can
      be achieved by tagging station records as <code class="literal">0</code> and weather records as <code class="literal">1</code>. The mapper classes to do this are shown in
      Examples <a class="ulink" href="#calibre_link-730" title="Example&nbsp;9-9.&nbsp;Mapper for tagging station records for a reduce-side join">9-9</a> and <a class="ulink" href="#calibre_link-731" title="Example&nbsp;9-10.&nbsp;Mapper for tagging weather records for a reduce-side join">9-10</a>.</p><div class="example"><a id="calibre_link-730" class="calibre"></a><div class="example-title">Example&nbsp;9-9.&nbsp;Mapper for tagging station records for a reduce-side
        join</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">JoinStationMapper</code>
    <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">TextPair</code><code class="o">,</code> <code class="n">Text</code><code class="o">&gt;</code> <code class="o">{</code>
  <code class="k">private</code> <code class="n">NcdcStationMetadataParser</code> <code class="n">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="n">NcdcStationMetadataParser</code><code class="o">();</code>

  <code class="nd">@Override</code>
  <code class="k">protected</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">LongWritable</code> <code class="n">key</code><code class="o">,</code> <code class="n">Text</code> <code class="n">value</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">value</code><code class="o">))</code> <code class="o">{</code>
      <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="k">new</code> <code class="n">TextPair</code><code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">getStationId</code><code class="o">(),</code> <code class="sb">"0"</code><code class="o">),</code>
          <code class="k">new</code> <code class="nf">Text</code><code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">getStationName</code><code class="o">()));</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><div class="example"><a id="calibre_link-731" class="calibre"></a><div class="example-title">Example&nbsp;9-10.&nbsp;Mapper for tagging weather records for a reduce-side
        join</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">JoinRecordMapper</code>
    <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">TextPair</code><code class="o">,</code> <code class="n">Text</code><code class="o">&gt;</code> <code class="o">{</code>
  <code class="k">private</code> <code class="n">NcdcRecordParser</code> <code class="n">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="n">NcdcRecordParser</code><code class="o">();</code>
  
  <code class="nd">@Override</code>
  <code class="k">protected</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">LongWritable</code> <code class="n">key</code><code class="o">,</code> <code class="n">Text</code> <code class="n">value</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">value</code><code class="o">);</code>
    <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="k">new</code> <code class="n">TextPair</code><code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">getStationId</code><code class="o">(),</code> <code class="sb">"1"</code><code class="o">),</code> <code class="n">value</code><code class="o">);</code>
  <code class="o">}</code>

<code class="o">}</code></pre></div></div><p class="calibre2">The reducer knows that it will receive the station record first,
      so it extracts its name from the value and writes it out as a part of
      every output record (<a class="ulink" href="#calibre_link-576" title="Example&nbsp;9-11.&nbsp;Reducer for joining tagged station records with tagged weather records">Example&nbsp;9-11</a>).</p><div class="example"><a id="calibre_link-576" class="calibre"></a><div class="example-title">Example&nbsp;9-11.&nbsp;Reducer for joining tagged station records with tagged weather
        records</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">JoinReducer</code> <code class="k">extends</code> <code class="n">Reducer</code><code class="o">&lt;</code><code class="n">TextPair</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">Text</code><code class="o">&gt;</code> <code class="o">{</code>

  <code class="nd">@Override</code>
  <code class="k">protected</code> <code class="kt">void</code> <code class="nf">reduce</code><code class="o">(</code><code class="n">TextPair</code> <code class="n">key</code><code class="o">,</code> <code class="n">Iterable</code><code class="o">&lt;</code><code class="n">Text</code><code class="o">&gt;</code> <code class="n">values</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="n">Iterator</code><code class="o">&lt;</code><code class="n">Text</code><code class="o">&gt;</code> <code class="n">iter</code> <code class="o">=</code> <code class="n">values</code><code class="o">.</code><code class="na">iterator</code><code class="o">();</code>
    <code class="n">Text</code> <code class="n">stationName</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="n">iter</code><code class="o">.</code><code class="na">next</code><code class="o">());</code>
    <code class="k">while</code> <code class="o">(</code><code class="n">iter</code><code class="o">.</code><code class="na">hasNext</code><code class="o">())</code> <code class="o">{</code>
      <code class="n">Text</code> <code class="n">record</code> <code class="o">=</code> <code class="n">iter</code><code class="o">.</code><code class="na">next</code><code class="o">();</code>
      <code class="n">Text</code> <code class="n">outValue</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="n">stationName</code><code class="o">.</code><code class="na">toString</code><code class="o">()</code> <code class="o">+</code> <code class="sb">"\t"</code> <code class="o">+</code> <code class="n">record</code><code class="o">.</code><code class="na">toString</code><code class="o">());</code>
      <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">key</code><code class="o">.</code><code class="na">getFirst</code><code class="o">(),</code> <code class="n">outValue</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The code assumes that every station ID in the weather records has
      exactly one matching record in the station dataset. If this were not the
      case, we would need to generalize the code to put the tag into the value
      objects, by using another <code class="literal">TextPair</code>.
      The <code class="literal">reduce()</code> method would then be able to tell
      which entries were station names and detect (and handle) missing or
      duplicate entries before processing the weather records.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">Because objects in the reducer’s values iterator are reused (for
        efficiency purposes), it is vital that the code makes a copy of the
        first <code class="literal">Text</code> object from the <code class="literal">values</code> iterator:</p><a id="calibre_link-4304" class="calibre"></a><pre class="programlisting"><code class="n">Text</code> <code class="n">stationName</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="n">iter</code><code class="o">.</code><code class="na">next</code><code class="o">());</code></pre><p class="calibre2">If the copy is not made, the <code class="literal">stationName</code> reference will refer to the
        value just read when it is turned into a string, which is a
        bug.</p></div><p class="calibre2">Tying the job together is the driver class, shown in <a class="ulink" href="#calibre_link-732" title="Example&nbsp;9-12.&nbsp;Application to join weather records with station names">Example&nbsp;9-12</a>. The essential point here is that
      we partition and group on the first part of the key, the station ID,
      which we do with a custom <code class="literal">Partitioner</code>
      (<code class="literal">KeyPartitioner</code>) and a <a class="calibre" id="calibre_link-2953"></a>custom group comparator, <code class="literal">First</code><code class="literal">Comparator</code> (from <code class="literal">TextPair</code>).</p><div class="example"><a id="calibre_link-732" class="calibre"></a><div class="example-title">Example&nbsp;9-12.&nbsp;Application to join weather records with station names</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">JoinRecordWithStationName</code> <code class="k">extends</code> <code class="n">Configured</code> <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="k">class</code> <code class="nc">KeyPartitioner</code> <code class="k">extends</code> <code class="n">Partitioner</code><code class="o">&lt;</code><code class="n">TextPair</code><code class="o">,</code> <code class="n">Text</code><code class="o">&gt;</code> <code class="o">{</code>
    <code class="nd">@Override</code>
    <code class="k">public</code> <code class="kt">int</code> <code class="nf">getPartition</code><code class="o">(</code><span class="calibre24"><strong class="calibre9"><code class="n1">TextPair</code> <code class="n1">key</code></strong></span><code class="o">,</code> <code class="n">Text</code> <code class="n">value</code><code class="o">,</code> <code class="kt">int</code> <code class="n">numPartitions</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">return</code> <code class="o">(</code><span class="calibre24"><strong class="calibre9"><code class="n1">key</code><code class="o1">.</code><code class="na1">getFirst</code><code class="o1">().</code><code class="na1">hashCode</code><code class="o1">()</code></strong></span> <code class="o">&amp;</code> <code class="n">Integer</code><code class="o">.</code><code class="na">MAX_VALUE</code><code class="o">)</code> <code class="o">%</code> <code class="n">numPartitions</code><code class="o">;</code>
    <code class="o">}</code>
  <code class="o">}</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="na">length</code> <code class="o">!=</code> <code class="mi">3</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">JobBuilder</code><code class="o">.</code><code class="na">printUsage</code><code class="o">(</code><code class="k">this</code><code class="o">,</code> <code class="sb">"&lt;ncdc input&gt; &lt;station input&gt; &lt;output&gt;"</code><code class="o">);</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>
    
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Job</code><code class="o">(</code><code class="n">getConf</code><code class="o">(),</code> <code class="sb">"Join weather records with station names"</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setJarByClass</code><code class="o">(</code><code class="n">getClass</code><code class="o">());</code>
    
    <code class="n">Path</code> <code class="n">ncdcInputPath</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]);</code>
    <code class="n">Path</code> <code class="n">stationInputPath</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">]);</code>
    <code class="n">Path</code> <code class="n">outputPath</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">2</code><code class="o">]);</code>
    
    <code class="n">MultipleInputs</code><code class="o">.</code><code class="na">addInputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">ncdcInputPath</code><code class="o">,</code>
        <code class="n">TextInputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">,</code> <code class="n">JoinRecordMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">MultipleInputs</code><code class="o">.</code><code class="na">addInputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">stationInputPath</code><code class="o">,</code>
        <code class="n">TextInputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">,</code> <code class="n">JoinStationMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">FileOutputFormat</code><code class="o">.</code><code class="na">setOutputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">outputPath</code><code class="o">);</code>
    
    <span class="calibre24"><strong class="calibre9"><code class="n1">job</code><code class="o1">.</code><code class="na1">setPartitionerClass</code><code class="o1">(</code><code class="n1">KeyPartitioner</code><code class="o1">.</code><code class="na1">class</code><code class="o1">);</code>
    <code class="n1">job</code><code class="o1">.</code><code class="na1">setGroupingComparatorClass</code><code class="o1">(</code><code class="n1">TextPair</code><code class="o1">.</code><code class="na1">FirstComparator</code><code class="o1">.</code><code class="na1">class</code><code class="o1">);</code></strong></span>
    
    <code class="n">job</code><code class="o">.</code><code class="na">setMapOutputKeyClass</code><code class="o">(</code><code class="n">TextPair</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    
    <code class="n">job</code><code class="o">.</code><code class="na">setReducerClass</code><code class="o">(</code><code class="n">JoinReducer</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="n">job</code><code class="o">.</code><code class="na">setOutputKeyClass</code><code class="o">(</code><code class="n">Text</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    
    <code class="k">return</code> <code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">;</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="k">new</code> <code class="n">JoinRecordWithStationName</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">Running the program on the sample data yields the following
      <a class="calibre" id="calibre_link-2484"></a><a class="calibre" id="calibre_link-2286"></a><a class="calibre" id="calibre_link-3184"></a>output:</p><pre class="screen1">011990-99999    SIHCCAJAVRI                     0067011990999991950051507004<span class="calibre">...</span>
011990-99999    SIHCCAJAVRI                     0043011990999991950051512004<span class="calibre">...</span>
011990-99999    SIHCCAJAVRI                     0043011990999991950051518004<span class="calibre">...</span>
012650-99999    TYNSET-HANSMOEN                 0043012650999991949032412004<span class="calibre">...</span>
012650-99999    TYNSET-HANSMOEN                 0043012650999991949032418004<span class="calibre">...</span></pre></div></div><div class="book" title="Side Data Distribution"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-150">Side Data Distribution</h2></div></div></div><p class="calibre2"><em class="calibre10">Side data</em> can be <a class="calibre" id="calibre_link-3403"></a><a class="calibre" id="calibre_link-2499"></a>defined as extra read-only data needed by a job to process
    the main dataset. The challenge is to make side data available to all the
    map or reduce tasks (which are spread across the cluster) in a convenient
    and efficient fashion.</p><div class="book" title="Using the Job Configuration"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4305">Using the Job Configuration</h3></div></div></div><p class="calibre2">You can set arbitrary key-value pairs in <a class="calibre" id="calibre_link-3406"></a>the job configuration using the various setter <a class="calibre" id="calibre_link-1233"></a>methods on <code class="literal">Configuration</code> (or <code class="literal">JobConf</code> in the old MapReduce API). This is
      very useful when you need to pass a small piece of metadata to your
      tasks.</p><p class="calibre2">In the task, you can retrieve the data from the configuration
      returned by <code class="literal">Context</code>’s <code class="literal">getConfiguration()</code> method. (In the old API,
      it’s a little more involved: override the
      <code class="literal">configure()</code> method in the <code class="literal">Mapper</code> or <code class="literal">Reducer</code> and use a getter method on the
      <code class="literal">JobConf</code> object
      passed in to retrieve the data. It’s very common to store the data in an
      instance field so it can be used in the <code class="literal">map()</code>
      or <code class="literal">reduce()</code> method.)</p><p class="calibre2">Usually a primitive type is sufficient to encode your metadata,
      but for arbitrary objects you can either handle the serialization
      yourself (if you have an existing mechanism for turning objects to
      strings and back) or use Hadoop’s <code class="literal">Stringifier</code> class. The <code class="literal">Default</code><code class="literal">String</code><code class="literal">ifier</code> uses Hadoop’s
      <a class="calibre" id="calibre_link-1412"></a><a class="calibre" id="calibre_link-3357"></a>serialization framework to serialize objects (see <a class="ulink" href="#calibre_link-733" title="Serialization">Serialization</a>).</p><p class="calibre2">You shouldn’t use this mechanism for transferring more than a few
      kilobytes of data, because it can put pressure on the memory usage in
      MapReduce components. The job configuration is always read by the
      client, the application master, and the task JVM, and each time the
      configuration is read, all of its entries are read into memory, even if
      they are not used.</p></div><div class="book" title="Distributed Cache"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-54">Distributed Cache</h3></div></div></div><p class="calibre2">Rather than <a class="calibre" id="calibre_link-3404"></a><a class="calibre" id="calibre_link-1519"></a>serializing side data in the job configuration, it is
      preferable to distribute datasets using Hadoop’s distributed cache
      mechanism. This provides a service for copying files and archives to the
      task nodes in time for the tasks to use them when they run. To save
      network bandwidth, files are normally copied to any particular node once
        per job.</p><div class="book" title="Usage"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4306">Usage</h4></div></div></div><p class="calibre2">For tools that use <code class="literal">GenericOptionsParser</code> (this<a class="calibre" id="calibre_link-1789"></a> includes many of the programs in this book; see <a class="ulink" href="#calibre_link-247" title="GenericOptionsParser, Tool, and ToolRunner">GenericOptionsParser, Tool, and ToolRunner</a>), you can specify
        the files to be distributed as a comma-separated list of URIs as the
        argument to the <code class="literal">-files</code> option. Files can be on the
        local filesystem, on HDFS, or on another Hadoop-readable filesystem
        (such as S3). If no scheme is supplied, then the files are assumed to
        be local. (This is true even when the default filesystem is not the
        local filesystem.)</p><p class="calibre2">You can also copy archive files (JAR files, ZIP files, tar
        files, and gzipped tar files) to your tasks using the <code class="literal">-archives</code> option; these are unarchived on
        the task node. The <code class="literal">-libjars</code> option will add JAR files to
        the classpath of the mapper and reducer tasks. This is useful if you
        haven’t bundled library JAR files in your job JAR file.</p><p class="calibre2">Let’s see how to use the distributed cache to share a metadata
        file for station names. The command we will run is:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar hadoop-examples.jar \</code></strong>
<strong class="userinput"><code class="calibre9">  MaxTemperatureByStationNameUsingDistributedCacheFile \</code></strong>
<strong class="userinput"><code class="calibre9">  -files input/ncdc/metadata/stations-fixed-width.txt input/ncdc/all output</code></strong></pre><p class="calibre2">This command will copy the local file <em class="calibre10">stations-fixed-width.txt</em> (no scheme is
        supplied, so the path is automatically interpreted as a local file) to
        the task nodes, so we can use it to look up station names. The listing
        for <code class="literal">MaxTemperatureByStationNameUsing</code><code class="literal">DistributedCacheFile</code> appears in
        <a class="ulink" href="#calibre_link-734" title="Example&nbsp;9-13.&nbsp;Application to find the maximum temperature by station, showing station names from a lookup table passed as a distributed cache file">Example&nbsp;9-13</a>.</p><div class="example"><a id="calibre_link-734" class="calibre"></a><div class="example-title">Example&nbsp;9-13.&nbsp;Application to find the maximum temperature by station,
          showing station names from a lookup table passed as a distributed
          cache file</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">MaxTemperatureByStationNameUsingDistributedCacheFile</code>
    <code class="k">extends</code> <code class="n">Configured</code> <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>
  
  <code class="k">static</code> <code class="k">class</code> <code class="nc">StationTemperatureMapper</code>
      <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">&gt;</code> <code class="o">{</code>

    <code class="k">private</code> <code class="n">NcdcRecordParser</code> <code class="n">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="n">NcdcRecordParser</code><code class="o">();</code>
    
    <code class="nd">@Override</code>
    <code class="k">protected</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">LongWritable</code> <code class="n">key</code><code class="o">,</code> <code class="n">Text</code> <code class="n">value</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
        <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
      
      <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">value</code><code class="o">);</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">isValidTemperature</code><code class="o">())</code> <code class="o">{</code>
        <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="k">new</code> <code class="n">Text</code><code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">getStationId</code><code class="o">()),</code>
            <code class="k">new</code> <code class="nf">IntWritable</code><code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">getAirTemperature</code><code class="o">()));</code>
      <code class="o">}</code>
    <code class="o">}</code>
  <code class="o">}</code>
  
  <code class="k">static</code> <code class="k">class</code> <code class="nc">MaxTemperatureReducerWithStationLookup</code>
      <code class="k">extends</code> <code class="n">Reducer</code><code class="o">&lt;</code><code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">IntWritable</code><code class="o">&gt;</code> <code class="o">{</code>
    
    <span class="calibre24"><strong class="calibre9"><code class="kc">private</code> <code class="n1">NcdcStationMetadata</code> <code class="n1">metadata</code><code class="o1">;</code></strong></span>
    
    <span class="calibre24"><strong class="calibre9"><code class="nd1">@Override</code>
    <code class="kc">protected</code> <code class="kt1">void</code> <code class="nf1">setup</code><code class="o1">(</code><code class="n1">Context</code> <code class="n1">context</code><code class="o1">)</code>
        <code class="kc">throws</code> <code class="n1">IOException</code><code class="o1">,</code> <code class="n1">InterruptedException</code> <code class="o1">{</code>
      <code class="n1">metadata</code> <code class="o1">=</code> <code class="kc">new</code> <code class="n1">NcdcStationMetadata</code><code class="o1">();</code>
      <code class="n1">metadata</code><code class="o1">.</code><code class="na1">initialize</code><code class="o1">(</code><code class="kc">new</code> <code class="n1">File</code><code class="o1">(</code><code class="s">"stations-fixed-width.txt"</code><code class="o1">));</code>
    <code class="o1">}</code></strong></span>

    <code class="nd">@Override</code>
    <code class="k">protected</code> <code class="kt">void</code> <code class="nf">reduce</code><code class="o">(</code><code class="n">Text</code> <code class="n">key</code><code class="o">,</code> <code class="n">Iterable</code><code class="o">&lt;</code><code class="n">IntWritable</code><code class="o">&gt;</code> <code class="n">values</code><code class="o">,</code>
        <code class="n">Context</code> <code class="n">context</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
      
      <span class="calibre24"><strong class="calibre9"><code class="n1">String</code> <code class="n1">stationName</code> <code class="o1">=</code> <code class="n1">metadata</code><code class="o1">.</code><code class="na1">getStationName</code><code class="o1">(</code><code class="n1">key</code><code class="o1">.</code><code class="na1">toString</code><code class="o1">());</code></strong></span>
      
      <code class="kt">int</code> <code class="n">maxValue</code> <code class="o">=</code> <code class="n">Integer</code><code class="o">.</code><code class="na">MIN_VALUE</code><code class="o">;</code>
      <code class="k">for</code> <code class="o">(</code><code class="n">IntWritable</code> <code class="n">value</code> <code class="o">:</code> <code class="n">values</code><code class="o">)</code> <code class="o">{</code>
        <code class="n">maxValue</code> <code class="o">=</code> <code class="n">Math</code><code class="o">.</code><code class="na">max</code><code class="o">(</code><code class="n">maxValue</code><code class="o">,</code> <code class="n">value</code><code class="o">.</code><code class="na">get</code><code class="o">());</code>
      <code class="o">}</code>
      <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="k">new</code> <code class="n">Text</code><code class="o">(</code><span class="calibre24"><strong class="calibre9"><code class="n1">stationName</code></strong></span><code class="o">),</code> <code class="k">new</code> <code class="n">IntWritable</code><code class="o">(</code><code class="n">maxValue</code><code class="o">));</code>
    <code class="o">}</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="n">JobBuilder</code><code class="o">.</code><code class="na">parseInputAndOutput</code><code class="o">(</code><code class="k">this</code><code class="o">,</code> <code class="n">getConf</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">job</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>
    
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputKeyClass</code><code class="o">(</code><code class="n">Text</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputValueClass</code><code class="o">(</code><code class="n">IntWritable</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="n">job</code><code class="o">.</code><code class="na">setMapperClass</code><code class="o">(</code><code class="n">StationTemperatureMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setCombinerClass</code><code class="o">(</code><code class="n">MaxTemperatureReducer</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setReducerClass</code><code class="o">(</code><code class="n">MaxTemperatureReducerWithStationLookup</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    
    <code class="k">return</code> <code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">;</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code>
        <code class="k">new</code> <code class="nf">MaxTemperatureByStationNameUsingDistributedCacheFile</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The program finds the maximum temperature by weather station, so
        the mapper (<code class="literal">StationTemperatureMapper</code>) simply
        emits (station ID, temperature) pairs. For the combiner, we reuse
        <code class="literal">MaxTemperatureReducer</code> (from
        Chapters <a class="ulink" href="#calibre_link-462" title="Chapter&nbsp;2.&nbsp;MapReduce">2</a> and <a class="ulink" href="#calibre_link-79" title="Chapter&nbsp;6.&nbsp;Developing a MapReduce Application">6</a>) to pick the maximum temperature for any given group
        of map outputs on the map side. The reducer (<code class="literal">MaxTemperatureReducerWithStationLookup</code>) is
        different from the combiner, since in addition to finding the maximum
        temperature, it uses the cache file to look up the station
        name.</p><p class="calibre2">We use the reducer’s <code class="literal">setup()</code>
        method to retrieve the cache file using its original name, relative to
        the working directory of the task.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">You can use the distributed cache for copying files that do
          not fit in memory. Hadoop map files are very useful in this regard,
          since they serve as an on-disk lookup format (see <a class="ulink" href="#calibre_link-735" title="MapFile">MapFile</a>). Because map files are collections of files
          with a defined directory structure, you should put them into an
          archive format (JAR, ZIP, tar, or gzipped tar) and add them to the
          cache using the <code class="literal">-archives</code>
          option.</p></div><p class="calibre2">Here’s a snippet of the output, showing some maximum
        temperatures for a few weather stations:</p><pre class="screen1">PEATS RIDGE WARATAH          	372
STRATHALBYN RACECOU          	410
SHEOAKS AWS                  	399
WANGARATTA AERO              	409
MOOGARA                      	334
MACKAY AERO                  	331</pre></div><div class="book" title="How it works"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4307">How it works</h4></div></div></div><p class="calibre2">When you launch a job, Hadoop copies the files specified by the
        <code class="literal">-files</code>, <code class="literal">-archives</code>, and <code class="literal">-libjars</code> options to the distributed
        filesystem (normally HDFS). Then, before a task is run, the node
        manager copies the files from the distributed filesystem to a local
        disk—the cache—so the task can access the files. The files are said to
        be <em class="calibre10">localized</em> at this point. From the task’s
        point of view, the files are just there, symbolically linked from the
        task’s working directory. In addition, files specified by <code class="literal">-libjars</code> are added to
        the task’s classpath before it is launched.</p><p class="calibre2">The node manager also maintains a reference count for the number
        of tasks using each file in the cache. Before the task has run, the
        file’s reference count is incremented by 1; then, after the task has
        run, the count is decreased by 1. Only when the file is not being used
        (when the count reaches zero) is it eligible for deletion. Files are
        deleted to make room for a new file when the node’s cache exceeds a
        certain size—10 GB by default—using a least-recently used policy. The
        cache size may be changed by setting the configuration property
        <code class="literal">yarn.nodemanager.localizer.cache.target-size-mb</code>.</p><p class="calibre2">Although this design doesn’t guarantee that subsequent tasks
        from the same job running on the same node will find the file they
        need in the cache, it is very likely that they will: tasks from a job
        are usually scheduled to run at around the same time, so there isn’t
        the opportunity for enough other jobs to run to cause the original
        task’s file to be deleted from the cache.</p></div><div class="book" title="The distributed cache API"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4308">The distributed cache API</h4></div></div></div><p class="calibre2">Most applications don’t need to use the distributed cache API,
        because they can use the cache via <code class="literal">GenericOptionsParser</code>, as we saw in <a class="ulink" href="#calibre_link-734" title="Example&nbsp;9-13.&nbsp;Application to find the maximum temperature by station, showing station names from a lookup table passed as a distributed cache file">Example&nbsp;9-13</a>.
        However, if <code class="literal">Generic</code><code class="literal">OptionsParser</code> is not being used,
        then the API in <code class="literal">Job</code> can be used to
        put objects into the distributed cache.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-736" id="calibre_link-749">66</a>]</sup> Here are the pertinent methods in <code class="literal">Job</code>:</p><a id="calibre_link-4309" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="kt">void</code> <code class="nf">addCacheFile</code><code class="o">(</code><code class="n">URI</code> <code class="n">uri</code><code class="o">)</code>
<code class="k">public</code> <code class="kt">void</code> <code class="nf">addCacheArchive</code><code class="o">(</code><code class="n">URI</code> <code class="n">uri</code><code class="o">)</code>
<code class="k">public</code> <code class="kt">void</code> <code class="nf">setCacheFiles</code><code class="o">(</code><code class="n">URI</code><code class="o">[]</code> <code class="n">files</code><code class="o">)</code>
<code class="k">public</code> <code class="kt">void</code> <code class="nf">setCacheArchives</code><code class="o">(</code><code class="n">URI</code><code class="o">[]</code> <code class="n">archives</code><code class="o">)</code>
<code class="k">public</code> <code class="kt">void</code> <code class="nf">addFileToClassPath</code><code class="o">(</code><code class="n">Path</code> <code class="n">file</code><code class="o">)</code>
<code class="k">public</code> <code class="kt">void</code> <code class="nf">addArchiveToClassPath</code><code class="o">(</code><code class="n">Path</code> <code class="n">archive</code><code class="o">)</code></pre><p class="calibre2">Recall that there are two types of objects that can be placed in
        the cache: files and archives. Files are left intact on the task node,
        whereas archives are unarchived on the task node. For each type of
        object, there are three methods: an
        <code class="literal">addCache<em class="replaceable"><code class="replaceable">XXXX</code></em>()</code>
        method to add the file or archive to the distributed cache, a
        <code class="literal">setCache<em class="replaceable"><code class="replaceable">XXXX</code></em>s()</code>
        method to set the entire list of files or archives to be added to the
        cache in a single call (replacing those set in any previous calls),
        and an
        <code class="literal">add<em class="replaceable"><code class="replaceable">XXXX</code></em>ToClassPath()</code>
        method to add the file or archive to the MapReduce task’s classpath.
        <a class="ulink" href="#calibre_link-737" title="Table&nbsp;9-7.&nbsp;Distributed cache API">Table&nbsp;9-7</a> compares these <a class="calibre" id="calibre_link-2209"></a>API methods to the <code class="literal">GenericOptionsParser</code> options described in
        <a class="ulink" href="#calibre_link-673" title="Table&nbsp;6-1.&nbsp;GenericOptionsParser and ToolRunner options">Table&nbsp;6-1</a>.</p><div class="table"><a id="calibre_link-737" class="calibre"></a><div class="table-title">Table&nbsp;9-7.&nbsp;Distributed cache API</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Job API method</td><td class="calibre20">GenericOptionsParser equivalent</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">addCacheFile(URI uri)</code> <code class="uri">setCacheFiles(URI[]
                files)</code></td><td class="calibre23"><code class="uri">-files
                <em class="replaceable"><code class="calibre44">file1,file2,...</code></em></code></td><td class="calibre25">Add files to the distributed cache to be copied to the
                task node.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">addCacheArchive(URI uri)</code>
                <code class="uri">setCacheArchives(URI[]
                files)</code></td><td class="calibre23"><code class="uri">-archives
                <em class="replaceable"><code class="calibre44">archive1,archive2,...</code></em></code></td><td class="calibre25">Add archives to the distributed cache to be copied to
                the task node and unarchived there.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">addFileToClassPath(Path
                file)</code></td><td class="calibre23"><code class="uri">-libjars
                <em class="replaceable"><code class="calibre44">jar1,jar2,...</code></em></code></td><td class="calibre25">Add files to the distributed cache to be added to the
                MapReduce task’s classpath. The files are not unarchived, so
                this is a useful way to add JAR files to the
                classpath.</td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">addArchiveToClassPath(Path
                archive)</code></td><td class="calibre27">None</td><td class="calibre28">Add archives to the distributed cache to be unarchived
                and added to the MapReduce task’s classpath. This can be
                useful when you want to add a directory of files to the
                classpath, since you can create an archive containing the
                files. Alternatively, you could create a JAR file and use
                <code class="uri">addFileToClassPath()</code>, which
                works equally well.</td></tr></tbody></table></div></div><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">The URIs referenced in the add or set methods must be files in
          a shared filesystem that exist when the job is run. On the other
          hand, the filenames specified as a <code class="literal">GenericOptionsParser</code> option (e.g.,
          <code class="literal">-files</code>) may refer to local files,
          in which case they get copied to the default shared filesystem
          (normally HDFS) on your behalf.</p><p class="calibre2">This is the key difference between using the Java API directly
          and using <code class="literal">GenericOptionsParser</code>:
          the Java API does <span class="calibre">not</span> copy the
          file specified in the add or set method to the shared filesystem,
          whereas the <code class="literal">GenericOptionsParser</code>
          does.</p></div><p class="calibre2">Retrieving distributed cache files from the task works in the
        same way as before: you access the localized file directly by name, as
        we did in <a class="ulink" href="#calibre_link-734" title="Example&nbsp;9-13.&nbsp;Application to find the maximum temperature by station, showing station names from a lookup table passed as a distributed cache file">Example&nbsp;9-13</a>.
        This works because MapReduce will always create a symbolic link from
        the task’s working directory to every file or archive added to the
        distributed cache.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-738" id="calibre_link-750">67</a>]</sup> Archives are unarchived so you can access the files in
        them using the nested <a class="calibre" id="calibre_link-2500"></a><a class="calibre" id="calibre_link-3405"></a><a class="calibre" id="calibre_link-1520"></a>path.</p></div></div></div><div class="book" title="MapReduce Library Classes"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4310">MapReduce Library Classes</h2></div></div></div><p class="calibre2">Hadoop comes with a library of mappers and <a class="calibre" id="calibre_link-2485"></a>reducers for commonly used functions. They are listed with
    brief descriptions in <a class="ulink" href="#calibre_link-739" title="Table&nbsp;9-8.&nbsp;MapReduce library classes">Table&nbsp;9-8</a>. For
    further information on how to use them, consult their Java
    documentation.</p><div class="table"><a id="calibre_link-739" class="calibre"></a><div class="table-title">Table&nbsp;9-8.&nbsp;MapReduce library classes</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="c4"><col class="c5"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Classes</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">ChainMapper</code>, <code class="uri">ChainReducer</code></td><td class="calibre25">Run a chain of <a class="calibre" id="calibre_link-1084"></a><a class="calibre" id="calibre_link-1086"></a>mappers in a single mapper and a reducer followed by
            a chain of mappers in a single reducer, respectively.
            (Symbolically, <code class="uri">M+RM*</code>, where
            <code class="uri">M</code> is a mapper and <code class="uri">R</code> is a reducer.) This can substantially
            reduce the amount of disk I/O incurred compared to running
            multiple MapReduce jobs.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">FieldSelectionMapReduce</code>
            (old API): <code class="uri">FieldSelectionMapper</code>
            and <code class="uri">FieldSelectionReducer</code> (new
            API)</td><td class="calibre25">A mapper and <a class="calibre" id="calibre_link-1617"></a><a class="calibre" id="calibre_link-1616"></a><a class="calibre" id="calibre_link-1618"></a>reducer that can select fields (like the Unix
            <code class="uri">cut</code> command) from the input
            keys and values and emit them as output keys and values.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">IntSumReducer</code>, <code class="uri">LongSumReducer</code></td><td class="calibre25">Reducers <a class="calibre" id="calibre_link-2136"></a><a class="calibre" id="calibre_link-2365"></a>that sum integer values to produce a total for every
            key.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">InverseMapper</code></td><td class="calibre25">A mapper <a class="calibre" id="calibre_link-2140"></a>that swaps keys and values.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">MultithreadedMapRunner</code>
            (old API), <code class="uri">MultithreadedMapper</code>
            (new API)</td><td class="calibre25">A mapper <a class="calibre" id="calibre_link-2731"></a><a class="calibre" id="calibre_link-2730"></a>(or map runner in the old API) that runs mappers
            concurrently in separate threads. Useful for mappers that are not
            CPU-bound.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">TokenCounterMapper</code></td><td class="calibre25">A <a class="calibre" id="calibre_link-3696"></a>mapper that tokenizes the input value into
            <a class="calibre" id="calibre_link-3559"></a>words (using Java’s <code class="uri">StringTokenizer</code>) and emits each
            word along with a count of 1.</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">RegexMapper</code></td><td class="calibre28">A mapper <a class="calibre" id="calibre_link-3206"></a>that finds matches of a regular expression in the
            input value and emits the matches along with a count of 1.</td></tr></tbody></table></div></div></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" type="footnote" id="calibre_link-715"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-740">61</a>] </sup>One commonly used workaround for this problem—particularly in
          text-based Streaming applications—is to add an offset to eliminate
          all negative numbers and to left pad with zeros so all numbers are
          the same number of characters. However, see <a class="ulink" href="#calibre_link-741" title="Streaming">Streaming</a> for another approach.</p></div><div class="footnote" type="footnote" id="calibre_link-718"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-742">62</a>] </sup>See <a class="ulink" href="#calibre_link-743" title="Sorting and merging SequenceFiles">Sorting and merging SequenceFiles</a> for how
          to do the same thing using the sort program example that comes with
          Hadoop.</p></div><div class="footnote" type="footnote" id="calibre_link-719"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-744">63</a>] </sup>A better answer is to use <a class="calibre" id="calibre_link-3007"></a><a class="calibre" id="calibre_link-1306"></a><a class="calibre" id="calibre_link-3464"></a>Pig (<a class="ulink" href="#calibre_link-745" title="Sorting Data">Sorting Data</a>), Hive (<a class="ulink" href="#calibre_link-746" title="Sorting and Aggregating">Sorting and Aggregating</a>), Crunch, or Spark, all of which
          can sort with a single command.</p></div><div class="footnote" type="footnote" id="calibre_link-722"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-747">64</a>] </sup>In some applications, it’s common for some of the input to
          already be sorted, or at least partially sorted. For example, the
          weather dataset is ordered by time, which may introduce certain
          biases, making the <code class="literal">RandomSampler</code>
          a safer choice.</p></div><div class="footnote" type="footnote" id="calibre_link-725"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-748">65</a>] </sup>For simplicity, these custom comparators as shown are not
            optimized; see <a class="ulink" href="#calibre_link-147" title="Implementing a RawComparator for speed">Implementing a RawComparator for speed</a> for
            the steps we would need to take to make them faster.</p></div><div class="footnote" type="footnote" id="calibre_link-736"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-749">66</a>] </sup><span class="calibre">If you are using the
            old MapReduce API, the <a class="calibre" id="calibre_link-1521"></a>same methods can be found in
            <code class="literal">org.</code><code class="literal">apache</code><code class="literal">.hadoop</code></span><code class="literal">.file</code><code class="literal">cache.DistributedCache.</code></p></div><div class="footnote" type="footnote" id="calibre_link-738"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-750">67</a>] </sup>In Hadoop 1, localized files were not always symlinked, so
            it was sometimes necessary to retrieve localized file paths using
            methods on <code class="literal">JobContext</code>. This limitation was
            removed in Hadoop 2.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-809"><div class="book" type="part" id="calibre_link-4311" title="Part&nbsp;III.&nbsp;Hadoop Operations"><div class="book"><div class="book"><div class="book"><h1 class="title6">Part&nbsp;III.&nbsp;Hadoop Operations</h1></div></div></div></div></div>

<div class="calibre1" id="calibre_link-1"><section type="chapter" id="calibre_link-4312" title="Chapter&nbsp;10.&nbsp;Setting Up a Hadoop Cluster"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;10.&nbsp;Setting Up a Hadoop Cluster</h2></div></div></div><p class="calibre2">This chapter explains how to set up Hadoop to run on a cluster of
  machines. Running HDFS, MapReduce, and YARN on a single machine is great for
  learning about these systems, but to do useful work, they need to run on
  multiple nodes.</p><p class="calibre2">There are a few options when it comes to getting a Hadoop cluster,
  from building your own, to running on rented hardware or using an offering
  that provides Hadoop as a hosted service in the cloud. The number of hosted
  options is too large to list here, but even if you choose to build a Hadoop
  cluster yourself, there are still a number of <a class="calibre" id="calibre_link-1144"></a>installation options:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Apache tarballs</span></dt><dd class="calibre8"><p class="calibre2">The Apache Hadoop project and related projects provide binary
        (and source) tarballs for each release. Installation from binary
        tarballs gives you the most flexibility but entails the most amount of
        work, since you need to decide on where the installation files,
        configuration files, and logfiles are located on the filesystem, set
        their file permissions correctly, and so on.</p></dd><dt class="calibre7"><span class="term">Packages</span></dt><dd class="calibre8"><p class="calibre2">RPM and Debian packages are available from the <a class="ulink" href="http://bigtop.apache.org/" target="_top">Apache Bigtop project</a>, as well
        as from all the Hadoop vendors. Packages bring a number of advantages
        over tarballs: they provide a consistent filesystem layout, they are
        tested together as a stack (so you know that the versions of Hadoop
        and Hive, say, will work together), and they work well with
        configuration management tools like Puppet.</p></dd><dt class="calibre7"><span class="term">Hadoop cluster management tools</span></dt><dd class="calibre8"><p class="calibre2">Cloudera Manager and Apache Ambari are examples of dedicated
        tools for installing and managing a Hadoop cluster over its whole
        lifecycle. They provide a simple web UI, and are the recommended way
        to set up a Hadoop cluster for most users and operators. These tools
        encode a lot of operator knowledge about running Hadoop. For example,
        they use heuristics based on the hardware profile (among other
        factors) to choose good defaults for Hadoop configuration settings.
        For more complex setups, like HA, or secure Hadoop, the management
        tools provide well-tested wizards for getting a working cluster in a
        short amount of time. Finally, they add extra features that the other
        installation options don’t offer, such as unified monitoring and log
        search, and rolling upgrades (so you can upgrade the cluster without
        experiencing downtime).</p></dd></dl></div><p class="calibre2">This chapter and the next give you enough information to set up and
  operate your own basic cluster, but even if you are using Hadoop cluster
  management tools or a service in which a lot of the routine setup and
  maintenance are done for you, these chapters still offer valuable
  information about how Hadoop works from an operations point of view. For
  more in-depth information, I highly recommend <span class="calibre"><a class="ulink" href="http://shop.oreilly.com/product/0636920025085.do" target="_top">Hadoop
  Operations</a></span> by <a class="calibre" id="calibre_link-3260"></a>Eric Sammer (O’Reilly, 2012).</p><div class="book" title="Cluster Specification"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4313">Cluster Specification</h2></div></div></div><p class="calibre2">Hadoop is designed <a class="calibre" id="calibre_link-1151"></a>to run on commodity hardware. That means that you are not
    tied to expensive, proprietary offerings from a single vendor; rather, you
    can choose standardized, commonly available hardware from any of a large
    range of vendors to build your cluster.</p><p class="calibre2">“Commodity” does not mean “low-end.” Low-end machines often have
    cheap components, which have higher failure rates than more expensive (but
    still commodity-class) machines. When you are operating tens, hundreds, or
    thousands of machines, cheap components turn out to be a false economy, as
    the higher failure rate incurs a greater maintenance cost. On the other
    hand, large database-class machines are not recommended either, since they
    don’t score well on the price/performance curve. And even though you would
    need fewer of them to build a cluster of comparable performance to one
    built of mid-range commodity hardware, when one did fail, it would have a
    bigger impact on the cluster because a larger proportion of the cluster
    hardware would be unavailable.</p><p class="calibre2">Hardware specifications rapidly become obsolete, but for the sake of
    illustration, a typical choice of machine for running an HDFS datanode and
    a YARN node manager in 2014 would have had the following
    specifications:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Processor</span></dt><dd class="calibre8"><p class="calibre2">Two hex/octo-core 3 GHz CPUs</p></dd><dt class="calibre7"><span class="term">Memory</span></dt><dd class="calibre8"><p class="calibre2">64−512 GB ECC RAM<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-23" id="calibre_link-58">68</a>]</sup></p></dd><dt class="calibre7"><span class="term">Storage</span></dt><dd class="calibre8"><p class="calibre2">12−24 × 1−4 TB SATA disks</p></dd><dt class="calibre7"><span class="term">Network</span></dt><dd class="calibre8"><p class="calibre2">Gigabit Ethernet with link aggregation</p></dd></dl></div><p class="calibre2">Although the hardware specification for your cluster will assuredly
    be different, Hadoop is designed to use multiple cores and disks, so it
    will be able to take full advantage of more powerful hardware.</p><div class="sidebar"><a id="calibre_link-4314" class="calibre"></a><div class="sidebar-title">Why Not Use RAID?</div><p class="calibre2">HDFS clusters do not benefit from <a class="calibre" id="calibre_link-3131"></a><a class="calibre" id="calibre_link-3202"></a>using RAID (redundant array of independent disks) for
      <a class="calibre" id="calibre_link-1389"></a>datanode storage (although RAID is recommended for the
      namenode’s disks, to protect against corruption of its metadata). The
      redundancy that RAID provides is not needed, since HDFS handles it by
      replication between nodes.</p><p class="calibre2">Furthermore, RAID striping (RAID 0), which is commonly used to
      increase performance, turns out to be <span class="calibre">slower</span> than the <a class="calibre" id="calibre_link-2202"></a><a class="calibre" id="calibre_link-2293"></a>JBOD (just a bunch of disks) configuration used by HDFS,
      which round-robins HDFS blocks between all disks. This is because RAID 0
      read and write operations are limited by the speed of the
      slowest-responding disk in the RAID array. In JBOD, disk operations are
      independent, so the average speed of operations is greater than that of
      the slowest disk. Disk performance often shows considerable variation in
      practice, even for disks of the same model. In some <a class="ulink" href="http://markmail.org/message/xmzc45zi25htr7ry" target="_top">benchmarking carried
      out on a Yahoo! cluster</a>, JBOD performed 10% faster than RAID 0
      in one test (Gridmix) and 30% better in another (HDFS write
      throughput).</p><p class="calibre2">Finally, if a disk fails in a JBOD configuration, HDFS can
      continue to operate without the failed disk, whereas with RAID, failure
      of a single disk causes the whole array (and hence the node) to become
      unavailable.</p></div><div class="book" title="Cluster Sizing"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4315">Cluster Sizing</h3></div></div></div><p class="calibre2">How large should your <a class="calibre" id="calibre_link-1149"></a>cluster be? There isn’t an exact answer to this question,
      but the beauty of Hadoop is that you can start with a small cluster
      (say, 10 nodes) and grow it as your storage and computational needs
      grow. In many ways, a better question is this: how fast does your
      cluster need to grow? You can get a good feel for this by considering
      storage capacity.</p><p class="calibre2">For example, if your data grows by 1 TB a day and you have
      three-way HDFS replication, you need an additional 3 TB of raw storage
      per day. Allow some room for intermediate files and logfiles (around
      30%, say), and this is in the range of one (2014-vintage) machine per
      week. In practice, you wouldn’t buy a new machine each week and add it
      to the cluster. The value of doing a back-of-the-envelope calculation
      like this is that it gives you a feel for how big your cluster should
      be. In this example, a cluster that holds two years’ worth of data needs
      100 machines.</p><div class="book" title="Master node scenarios"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4316">Master node scenarios</h4></div></div></div><p class="calibre2">Depending on the size of the <a class="calibre" id="calibre_link-2745"></a>cluster, there are various configurations for running
        the master daemons: the namenode, secondary namenode, <a class="calibre" id="calibre_link-3220"></a>resource manager, and history server. For a small
        cluster (on the order of 10 nodes), it is usually acceptable to run
        the namenode and the resource manager on a single master machine (as
        long as at least one copy of the namenode’s metadata is stored on a
        remote filesystem). However, as the cluster gets larger, there are
        good reasons to separate them.</p><p class="calibre2">The namenode has high memory <a class="calibre" id="calibre_link-2666"></a><a class="calibre" id="calibre_link-2758"></a>requirements, as it holds file and block metadata for
        the entire namespace in memory. The secondary namenode, although idle
        most of the time, has a comparable memory footprint to the primary
        when it creates a checkpoint. (This is explained in detail in <a class="ulink" href="#calibre_link-24" title="The filesystem image and edit log">The filesystem image and edit log</a>.) For <a class="calibre" id="calibre_link-1672"></a>filesystems with a large number of files, there may not
        be enough physical memory on one machine to run both the primary and
        secondary namenode.</p><p class="calibre2">Aside from simple resource requirements, the main reason to run
        masters on separate machines is for high availability. Both <a class="calibre" id="calibre_link-1933"></a>HDFS and <a class="calibre" id="calibre_link-3833"></a>YARN support configurations where they can run masters
        in active-standby pairs. If the active master fails, then the standby,
        running on separate hardware, takes over with little or no
        interruption to the service. In the case of HDFS, the standby performs
        the checkpointing function of the secondary namenode (so you don’t
        need to run a standby and a secondary namenode).</p><p class="calibre2">Configuring and running Hadoop HA is not covered in this book.
        Refer to the Hadoop website or vendor documentation for <a class="calibre" id="calibre_link-1150"></a>details.</p></div></div><div class="book" title="Network Topology"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-191">Network Topology</h3></div></div></div><p class="calibre2">A common Hadoop cluster <a class="calibre" id="calibre_link-1129"></a><a class="calibre" id="calibre_link-2787"></a>architecture consists of a two-level network topology, as
      illustrated in <a class="ulink" href="#calibre_link-25" title="Figure&nbsp;10-1.&nbsp;Typical two-level network architecture for a Hadoop cluster">Figure&nbsp;10-1</a>. Typically there are
      30 to 40 servers per rack (only 3 are shown in the diagram), with a 10
      Gb switch for the rack and an uplink to a core switch or router (at
      least 10 Gb or better). The salient point is that the aggregate
      bandwidth between nodes on the same rack is much greater than that
      between nodes on different racks.</p><div class="figure"><a id="calibre_link-25" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4317" class="calibre"></a><img alt="Typical two-level network architecture for a Hadoop cluster" src="images/000041.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;10-1.&nbsp;Typical two-level network architecture for a Hadoop
        cluster</div></div><div class="book" title="Rack awareness"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4318">Rack awareness</h4></div></div></div><p class="calibre2">To get maximum <a class="calibre" id="calibre_link-3127"></a>performance out of Hadoop, it is important to configure
        Hadoop so that it knows the topology of your network. If your cluster
        runs on a single rack, then there is nothing more to do, since this is
        the default. However, for multirack clusters, you need to map nodes to
        racks. This allows Hadoop to prefer within-rack transfers (where there
        is more bandwidth available) to off-rack transfers when placing
        MapReduce tasks on nodes. HDFS
        will also be able to place replicas more intelligently to trade off
        performance and resilience.</p><p class="calibre2">Network locations such as nodes and racks are represented in a
        tree, which reflects the network “distance” between locations. The
        namenode uses the network location when determining where to place
        block replicas (see <a class="ulink" href="#calibre_link-26" title="Network Topology and Hadoop">Network Topology and Hadoop</a>); the
        MapReduce scheduler uses network location to determine where the
        closest replica is for input to a map task.</p><p class="calibre2">For the network in <a class="ulink" href="#calibre_link-25" title="Figure&nbsp;10-1.&nbsp;Typical two-level network architecture for a Hadoop cluster">Figure&nbsp;10-1</a>, the rack
        topology is described by two network locations—say, <em class="calibre10">/switch1/rack1</em> and <em class="calibre10">/switch1/rack2</em>. Because there is only one
        top-level switch in this cluster, the locations can be simplified to
        <em class="calibre10">/rack1</em> and <em class="calibre10">/rack2</em>.</p><p class="calibre2">The Hadoop configuration must specify a map between node
        addresses and network locations. The map is described by a Java
        interface, <code class="literal">DNSToSwitchMapping</code>,
        <a class="calibre" id="calibre_link-1535"></a>whose signature
        is:</p><a id="calibre_link-4319" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">interface</code> <code class="nc">DNSToSwitchMapping</code> <code class="o">{</code>
  <code class="k">public</code> <code class="n">List</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="nf">resolve</code><code class="o">(</code><code class="n">List</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">names</code><code class="o">);</code>
<code class="o">}</code></pre><p class="calibre2">The <code class="literal">names</code> parameter is a list
        of IP addresses, and the return value is a list of corresponding
        network location strings. The <code class="literal">net.topology.node.switch.mapping.impl</code>
        configuration <a class="calibre" id="calibre_link-2783"></a>property defines an implementation of the <code class="literal">DNSToSwitchMapping</code> interface that the
        namenode and the resource manager use to resolve worker node network
        locations.</p><p class="calibre2">For the network in our example, we would map <em class="calibre10">node1</em>, <em class="calibre10">node2</em>, and <em class="calibre10">node3</em> to <em class="calibre10">/rack1</em>, and <em class="calibre10">node4</em>, <em class="calibre10">node5</em>, and <em class="calibre10">node6</em> to <em class="calibre10">/rack2</em>.</p><p class="calibre2">Most installations don’t need to implement the interface
        themselves, however, since the default implementation is <code class="literal">ScriptBasedMapping</code>, which <a class="calibre" id="calibre_link-3294"></a>runs a user-defined script to determine the mapping. The
        script’s location is controlled by the <a class="calibre" id="calibre_link-2784"></a>property <code class="literal">net.topology.script.file.name</code>. The
        script must accept a variable number of arguments that are the
        hostnames or IP addresses to be mapped, and it must emit the
        corresponding network locations to standard output, separated by
        whitespace. The <a class="ulink" href="http://wiki.apache.org/hadoop/topology_rack_awareness_scripts" target="_top">Hadoop
        wiki</a> has an example.</p><p class="calibre2">If no script location is specified, the default behavior is to
        map all nodes to a single <a class="calibre" id="calibre_link-1152"></a><a class="calibre" id="calibre_link-1130"></a><a class="calibre" id="calibre_link-2788"></a>network location, <a class="calibre" id="calibre_link-3128"></a>called <em class="calibre10">/default-rack</em>.</p></div></div></div><div class="book" title="Cluster Setup and Installation"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-344">Cluster Setup and Installation</h2></div></div></div><p class="calibre2">This section describes how to install and configure a basic Hadoop
    cluster from scratch using the Apache Hadoop distribution on a Unix
    operating system. It provides background information on the things you
    need to think about when setting up Hadoop. For a production installation,
    most users and operators should consider one of the Hadoop cluster
    management tools listed at the beginning of this chapter.</p><div class="book" title="Installing Java"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4320">Installing Java</h3></div></div></div><p class="calibre2">Hadoop runs on both <a class="calibre" id="calibre_link-1146"></a><a class="calibre" id="calibre_link-2166"></a>Unix and Windows operating systems, and requires Java to
      be installed. For a production installation, you should select a
      combination of operating system, Java, and Hadoop that has been
      certified by the vendor of the Hadoop distribution you are using. There
      is also a page on the <a class="ulink" href="http://wiki.apache.org/hadoop/HadoopJavaVersions" target="_top">Hadoop
      wiki</a> that lists combinations that community members have run
      with success.</p></div><div class="book" title="Creating Unix User Accounts"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4321">Creating Unix User Accounts</h3></div></div></div><p class="calibre2">It’s good practice to create <a class="calibre" id="calibre_link-1141"></a><a class="calibre" id="calibre_link-3744"></a><a class="calibre" id="calibre_link-3751"></a>dedicated Unix user accounts to separate the Hadoop
      processes from each other, and from other services running on the same
      machine. The <a class="calibre" id="calibre_link-1932"></a><a class="calibre" id="calibre_link-2447"></a><a class="calibre" id="calibre_link-3832"></a>HDFS, MapReduce, and YARN services are usually run as
      separate users, named <code class="literal">hdfs</code>, <code class="literal">mapred</code>, and <code class="literal">yarn</code>, respectively. They all belong to the
      same <code class="literal">hadoop</code> group.</p></div><div class="book" title="Installing Hadoop"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4322">Installing Hadoop</h3></div></div></div><p class="calibre2">Download Hadoop from <a class="calibre" id="calibre_link-1145"></a>the <a class="ulink" href="http://hadoop.apache.org/core/releases.html" target="_top">Apache Hadoop releases
      page</a>, and unpack the contents of the distribution in a sensible
      location, such as <em class="calibre10">/usr/local</em>
      (<em class="calibre10">/opt</em> is another standard choice;
      note that Hadoop should not be installed in a user’s home directory, as
      that may be an NFS-mounted directory):</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">cd /usr/local</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">sudo tar xzf hadoop-<em class="replaceable1"><code class="calibre46">x.y.z</code></em>.tar.gz</code></strong></pre><p class="calibre2">You also need to change the owner of the Hadoop files to be the
      <code class="literal">hadoop</code> user and group:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">sudo chown -R hadoop:hadoop hadoop-<em class="replaceable1"><code class="calibre46">x.y.z</code></em></code></strong></pre><p class="calibre2">It’s convenient to put the Hadoop binaries on the shell path
      too:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">export HADOOP_HOME=/usr/local/hadoop-<em class="replaceable1"><code class="calibre46">x.y.z</code></em></code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</code></strong></pre></div><div class="book" title="Configuring SSH"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4323">Configuring SSH</h3></div></div></div><p class="calibre2">The Hadoop control <a class="calibre" id="calibre_link-1140"></a><a class="calibre" id="calibre_link-3512"></a>scripts (but not the daemons) rely on SSH to perform
      cluster-wide operations. For example, there is a script for stopping and
      starting all the daemons in the cluster. Note that the control scripts
      are optional—cluster-wide operations can be performed by other
      mechanisms, too, such as a distributed shell or dedicated Hadoop
      management applications.</p><p class="calibre2">To work seamlessly, SSH needs to be set up to allow passwordless
      login for the <code class="literal">hdfs</code> and <code class="literal">yarn</code> users from machines in the
      cluster.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-27" id="calibre_link-59">69</a>]</sup> The simplest way to achieve this is to generate a
      public/private key pair and place it in an NFS location that is shared
      across the cluster.</p><p class="calibre2">First, generate an RSA key pair by typing the following. You need
      to do this twice, once as the <code class="literal">hdfs</code>
      user and once as the <code class="literal">yarn</code>
      user:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">ssh-keygen -t rsa -f ~/.ssh/id_rsa</code></strong></pre><p class="calibre2">Even though we want passwordless logins, keys without passphrases
      are not considered good practice (it’s OK to have an empty passphrase
      when running a local pseudo-distributed cluster, as described in <a class="ulink" href="#calibre_link-28" title="Appendix&nbsp;A.&nbsp;Installing Apache Hadoop">Appendix&nbsp;A</a>), so we specify a passphrase when prompted for one. We
      use <em class="calibre10">ssh-agent</em> to avoid the need to
      enter a password for each connection.</p><p class="calibre2">The private key is in the file specified by the <code class="literal">-f</code> option, <em class="calibre10">~/.ssh/id_rsa</em>, and the public key is stored
      in a file with the same name but with <em class="calibre10">.pub</em> appended, <em class="calibre10">~/.ssh/id_rsa.pub</em>.</p><p class="calibre2">Next, we need to make sure that the public key is in the <em class="calibre10">~/.ssh/authorized_keys</em> file on all the
      machines in the cluster that we want to connect to. If the users’ home
      directories are stored on an NFS filesystem, the keys can be shared
      across the cluster by typing the following (first as <code class="literal">hdfs</code> and then as <code class="literal">yarn</code>):</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</code></strong></pre><p class="calibre2">If the home directory is not shared using NFS, the public keys
      will need to be shared by some other means (such as <em class="calibre10">ssh-copy-id</em>).</p><p class="calibre2">Test that you can SSH from the master to a worker machine by
      making sure <em class="calibre10">ssh-agent</em> is
      running,<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-29" id="calibre_link-60">70</a>]</sup> and then run <em class="calibre10">ssh-add</em> to
      store your passphrase. You should be able to SSH to a worker without
      entering the passphrase again.</p></div><div class="book" title="Configuring Hadoop"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4324">Configuring Hadoop</h3></div></div></div><p class="calibre2">Hadoop must have its <a class="calibre" id="calibre_link-1139"></a>configuration set appropriately to run in distributed mode
      on a cluster. The important configuration settings to achieve this are
      discussed in <a class="ulink" href="#calibre_link-30" title="Hadoop Configuration">Hadoop Configuration</a>.</p></div><div class="book" title="Formatting the HDFS Filesystem"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4325">Formatting the HDFS Filesystem</h3></div></div></div><p class="calibre2">Before it can be used, a <a class="calibre" id="calibre_link-1143"></a><a class="calibre" id="calibre_link-1943"></a><a class="calibre" id="calibre_link-1675"></a>brand-new HDFS installation needs to be formatted. The
      formatting process creates an empty filesystem by creating the storage
      directories and the initial versions of the namenode’s persistent data
      structures. <a class="calibre" id="calibre_link-1378"></a><a class="calibre" id="calibre_link-2744"></a>Datanodes are not involved in the initial formatting
      process, since the namenode manages all of the filesystem’s metadata,
      and datanodes can join or leave the cluster dynamically. For the same
      reason, you don’t need to say how large a filesystem to create, since
      this is determined by the number of datanodes in the cluster, which can
      be increased as needed, long after the filesystem is formatted.</p><p class="calibre2">Formatting HDFS is a fast operation. Run the following command as
      the <code class="literal">hdfs</code> user:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hdfs namenode -format</code></strong></pre></div><div class="book" title="Starting and Stopping the Daemons"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4326">Starting and Stopping the Daemons</h3></div></div></div><p class="calibre2">Hadoop comes with scripts for <a class="calibre" id="calibre_link-1147"></a><a class="calibre" id="calibre_link-1329"></a>running commands and starting and stopping daemons across
      the whole cluster. To use these scripts (which can be found in the
      <em class="calibre10">sbin</em> directory), you need to tell
      Hadoop which machines are in the cluster. There is a file for this
      purpose, called <em class="calibre10">slaves</em>, which
      <a class="calibre" id="calibre_link-3415"></a>contains a list of the machine hostnames or IP addresses,
      one per line. The <em class="calibre10">slaves</em> file lists
      the machines that the datanodes and node managers should run on. It
      resides in Hadoop’s configuration directory, although it may be placed
      elsewhere (and given another name) by changing the <code class="literal">HADOOP_SLAVES</code> setting in <em class="calibre10">hadoop-env.sh</em>. Also, this file does not need
      to be distributed to worker nodes, since they are used only by the
      control scripts running on the namenode or resource manager.</p><p class="calibre2">The HDFS daemons <a class="calibre" id="calibre_link-1960"></a>are started by running the following command as the
      <code class="literal">hdfs</code> user:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">start-dfs.sh</code></strong></pre><p class="calibre2">The machine (or machines) that the namenode and secondary namenode
      run on is determined by interrogating the Hadoop configuration for their
      hostnames. For example, the script finds the namenode’s hostname by
      executing the following:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hdfs getconf -namenodes</code></strong></pre><p class="calibre2">By default, this finds the namenode’s hostname <a class="calibre" id="calibre_link-1735"></a>from <code class="literal">fs.defaultFS</code>. In
      slightly more detail, the <em class="calibre10">start-dfs.sh</em> script does the
      following:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">Starts a <a class="calibre" id="calibre_link-2767"></a>namenode on each machine returned by executing
          <code class="literal">hdfs getconf</code> <code class="literal">-namenodes</code><sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-31" id="calibre_link-61">71</a>]</sup></p></li><li class="listitem"><p class="calibre2">Starts a datanode on <a class="calibre" id="calibre_link-1391"></a>each machine listed in the <em class="calibre10">slaves</em> file</p></li><li class="listitem"><p class="calibre2">Starts a secondary namenode <a class="calibre" id="calibre_link-3303"></a><a class="calibre" id="calibre_link-2764"></a>on each machine returned by executing <code class="literal">hdfs getconf -secondarynamenodes</code></p></li></ul></div><p class="calibre2">The YARN daemons <a class="calibre" id="calibre_link-3846"></a>are started in a similar way, by running the following
      command as the <code class="literal">yarn</code> user on the
      machine hosting the resource manager:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">start-yarn.sh</code></strong></pre><p class="calibre2">In this case, the <a class="calibre" id="calibre_link-3232"></a>resource manager is always run on the machine from which
      the <em class="calibre10">start-yarn.sh</em> script was run.
      More specifically, the script:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">Starts a resource manager on the local machine</p></li><li class="listitem"><p class="calibre2">Starts a <a class="calibre" id="calibre_link-2803"></a>node manager on each machine listed in the
          <em class="calibre10">slaves</em> file</p></li></ul></div><p class="calibre2">Also provided are <em class="calibre10">stop-dfs.sh</em>
      and <em class="calibre10">stop-yarn.sh</em> scripts to stop
      the daemons started by the corresponding start scripts.</p><p class="calibre2">These scripts start and stop Hadoop daemons using the <em class="calibre10">hadoop-daemon.sh</em> script (or the <em class="calibre10">yarn-daemon.sh</em> script, in the case of YARN).
      If you use the aforementioned scripts, you shouldn’t call <em class="calibre10">hadoop-daemon.sh</em> directly. But if you need to
      control Hadoop daemons from another system or from your own scripts, the
      <em class="calibre10">hadoop-daemon.sh</em> script is a good
      integration point. Likewise, <em class="calibre10">hadoop-</em><em class="calibre10">daemons.sh</em> (with an “s”) is handy
      for starting the same daemon on a set of hosts.</p><p class="calibre2">Finally, there is only one <a class="calibre" id="calibre_link-2509"></a>MapReduce daemon—the job history server, which is started
      as follows, <a class="calibre" id="calibre_link-1330"></a><a class="calibre" id="calibre_link-1148"></a>as the <code class="literal">mapred</code>
      user:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">mr-jobhistory-daemon.sh start historyserver</code></strong></pre></div><div class="book" title="Creating User Directories"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4327">Creating User Directories</h3></div></div></div><p class="calibre2">Once you have a Hadoop <a class="calibre" id="calibre_link-1142"></a><a class="calibre" id="calibre_link-1501"></a>cluster up and running, you need to give users access to
      it. This involves creating a home directory for each user and setting
      ownership permissions on it:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -mkdir /user/<em class="replaceable1"><code class="calibre46">username</code></em></code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -chown <em class="replaceable1"><code class="calibre46">username</code></em>:<em class="replaceable1"><code class="calibre46">username</code></em> /user/<em class="replaceable1"><code class="calibre46">username</code></em></code></strong></pre><p class="calibre2">This is a good time to set space limits on the directory. The
      following sets a 1 TB limit on the given user directory:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hdfs dfsadmin -setSpaceQuota 1t /user/<em class="replaceable1"><code class="calibre46">username</code></em></code></strong></pre></div></div><div class="book" title="Hadoop Configuration"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-30">Hadoop Configuration</h2></div></div></div><p class="calibre2">There are a handful of files <a class="calibre" id="calibre_link-1116"></a><a class="calibre" id="calibre_link-1235"></a>for controlling the configuration of a Hadoop installation;
    the most important ones are listed in <a class="ulink" href="#calibre_link-32" title="Table&nbsp;10-1.&nbsp;Hadoop configuration files">Table&nbsp;10-1</a>.</p><div class="table"><a id="calibre_link-32" class="calibre"></a><div class="table-title">Table&nbsp;10-1.&nbsp;Hadoop configuration files</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Filename</td><td class="calibre20">Format</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><em class="calibre10">hadoop-env.sh</em></td><td class="calibre23">Bash script</td><td class="calibre25">Environment <a class="calibre" id="calibre_link-1122"></a><a class="calibre" id="calibre_link-1572"></a><a class="calibre" id="calibre_link-1853"></a>variables that are used in the scripts to run
            Hadoop</td></tr><tr class="calibre26"><td class="calibre23"><em class="calibre10">mapred-env.sh</em></td><td class="calibre23">Bash script</td><td class="calibre25">Environment <a class="calibre" id="calibre_link-2419"></a>variables that are used in the scripts to run
            MapReduce (overrides variables set in <em class="calibre10">hadoop-env.sh</em>)</td></tr><tr class="calibre19"><td class="calibre23"><em class="calibre10">yarn-env.sh</em></td><td class="calibre23">Bash script</td><td class="calibre25">Environment <a class="calibre" id="calibre_link-3850"></a>variables that are used in the scripts to run YARN
            (overrides variables set in <em class="calibre10">hadoop-env.sh</em>)</td></tr><tr class="calibre26"><td class="calibre23"><em class="calibre10">core-site.xml</em></td><td class="calibre23">Hadoop configuration
            XML</td><td class="calibre25">Configuration <a class="calibre" id="calibre_link-1258"></a>settings for Hadoop Core, such as I/O settings that
            are common to HDFS, MapReduce, and YARN</td></tr><tr class="calibre19"><td class="calibre23"><em class="calibre10">hdfs-site.xml</em></td><td class="calibre23">Hadoop configuration
            XML</td><td class="calibre25">Configuration <a class="calibre" id="calibre_link-1975"></a>settings for HDFS daemons: the namenode, the
            secondary namenode, and the datanodes</td></tr><tr class="calibre26"><td class="calibre23"><em class="calibre10">mapred-site.xml</em></td><td class="calibre23">Hadoop configuration
            XML</td><td class="calibre25">Configuration <a class="calibre" id="calibre_link-2420"></a>settings for MapReduce daemons: the job history
            server</td></tr><tr class="calibre19"><td class="calibre23"><em class="calibre10">yarn-site.xml</em></td><td class="calibre23">Hadoop configuration
            XML</td><td class="calibre25">Configuration <a class="calibre" id="calibre_link-3851"></a>settings for YARN daemons: the resource manager, the
            web app proxy server, and the node managers</td></tr><tr class="calibre26"><td class="calibre23"><em class="calibre10">slaves</em></td><td class="calibre23">Plain text</td><td class="calibre25">A list of <a class="calibre" id="calibre_link-3416"></a>machines (one per line) that each run a datanode and
            a node manager</td></tr><tr class="calibre19"><td class="calibre23"> <em class="calibre10">hadoop-metrics2</em>  <em class="calibre10">.properties</em></td><td class="calibre23">Java properties</td><td class="calibre25">Properties <a class="calibre" id="calibre_link-1855"></a>for controlling how metrics are published in Hadoop
            (see <a class="ulink" href="#calibre_link-33" title="Metrics and JMX">Metrics and JMX</a>)</td></tr><tr class="calibre26"><td class="calibre23"><em class="calibre10">log4j.properties</em></td><td class="calibre23">Java properties</td><td class="calibre25">Properties for <a class="calibre" id="calibre_link-2353"></a>system logfiles, the namenode audit log, and the
            task log for the task JVM process (<a class="ulink" href="#calibre_link-34" title="Hadoop Logs">Hadoop Logs</a>)</td></tr><tr class="calibre19"><td class="calibre27"><em class="calibre10">hadoop-policy.xml</em></td><td class="calibre27">Hadoop configuration
            XML</td><td class="calibre28">Configuration <a class="calibre" id="calibre_link-1857"></a>settings for access control lists when running
            Hadoop in secure mode</td></tr></tbody></table></div></div><p class="calibre2">These files are all found in the <em class="calibre10">etc/hadoop</em> directory of the Hadoop
    distribution. The configuration directory can be relocated to another part
    of the filesystem (outside the Hadoop installation, which makes upgrades
    marginally easier) as long as daemons are started with the <code class="literal">--config</code> option (or, equivalently, with the
    <code class="literal">HADOOP_CONF_DIR</code> environment <a class="calibre" id="calibre_link-1868"></a>variable set) specifying the location of this directory on
    the local filesystem.</p><div class="book" title="Configuration Management"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4328">Configuration Management</h3></div></div></div><p class="calibre2">Hadoop does not have a <a class="calibre" id="calibre_link-1117"></a>single, global location for configuration information.
      Instead, each Hadoop node in the cluster has its own set of
      configuration files, and it is up to administrators to ensure that they
      are kept in sync across the system. There are parallel shell tools that
      can help do this, <a class="calibre" id="calibre_link-1548"></a><a class="calibre" id="calibre_link-3077"></a>such as <em class="calibre10">dsh</em> or <em class="calibre10">pdsh</em>. This is an area where Hadoop cluster
      management tools like Cloudera Manager and Apache Ambari really shine,
      since they take care of propagating changes across the cluster.</p><p class="calibre2">Hadoop is designed so that it is possible to have a single set of
      configuration files that are used for all master and worker machines.
      The great advantage of this is simplicity, both conceptually (since
      there is only one configuration to deal with) and operationally (as the
      Hadoop scripts are sufficient to manage a single configuration
      setup).</p><p class="calibre2">For some clusters, the one-size-fits-all configuration model
      breaks down. For example, if you expand the cluster with new machines
      that have a different hardware specification from the existing ones, you
      need a different configuration for the new machines to take advantage of
      their extra resources.</p><p class="calibre2">In these cases, you need to have the concept of a
      <em class="calibre10">class</em> of machine and maintain a separate
      configuration for each class. Hadoop doesn’t provide tools to do this,
      but there are several excellent tools for doing precisely this type of
      configuration management, such as Chef, Puppet, CFEngine, and
      Bcfg2.</p><p class="calibre2">For a cluster of any size, it can be a challenge to keep all of
      the machines in sync. Consider what happens if the machine is
      unavailable when you push out an update. Who ensures it gets the update
      when it becomes available? This is a big problem and can lead to
      divergent installations, so even if you use the Hadoop control scripts
      for managing Hadoop, it may be a good idea to use configuration
      management tools for maintaining the cluster. These tools are also
      excellent for doing regular maintenance, such as patching security holes
      and updating system packages.</p></div><div class="book" title="Environment Settings"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4329">Environment Settings</h3></div></div></div><p class="calibre2">In this section, we consider <a class="calibre" id="calibre_link-1123"></a><a class="calibre" id="calibre_link-1573"></a>how to set the variables in <em class="calibre10">hadoop-env.sh</em>. There are also analogous
      configuration files for MapReduce and YARN (but not for HDFS), called
      <em class="calibre10">mapred-env.sh</em> and <em class="calibre10">yarn-env.sh</em>, where variables pertaining to
      those components can be set. Note that the MapReduce and YARN files
      override the values set in <em class="calibre10">hadoop-env.sh</em>.</p><div class="book" title="Java"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4330">Java</h4></div></div></div><p class="calibre2">The location of the <a class="calibre" id="calibre_link-2162"></a>Java implementation to use is determined by <a class="calibre" id="calibre_link-2199"></a>the <code class="literal">JAVA_HOME</code> setting
        in <em class="calibre10">hadoop-env.sh</em> or the <code class="literal">JAVA_HOME</code> shell environment variable, if not
        set in <em class="calibre10">hadoop-env.sh</em>. It’s a good
        idea to set the value in <em class="calibre10">hadoop-env.sh</em>, so that it is clearly
        defined in one place and to ensure that the whole cluster is using the
        same version of Java.</p></div><div class="book" title="Memory heap size"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4331">Memory heap size</h4></div></div></div><p class="calibre2">By default, Hadoop allocates 1,000 MB (1 GB) of <a class="calibre" id="calibre_link-2665"></a>memory to each daemon it runs. This is controlled by
        <a class="calibre" id="calibre_link-1870"></a>the <code class="literal">HADOOP_HEAPSIZE</code>
        setting in <em class="calibre10">hadoop-env.sh</em>. There
        are also environment <a class="calibre" id="calibre_link-3909"></a>variables to allow you to change the heap size for a
        single daemon. For example, you can set <code class="literal">YARN_RESOURCEMANAGER_HEAPSIZE</code> in <em class="calibre10">yarn-env.sh</em> to override the heap size for
        the resource manager.</p><p class="calibre2">Surprisingly, there are no corresponding environment variables
        for HDFS daemons, despite it being very common to give the namenode
        more heap space. There is another way to set the namenode heap size,
        however; this is discussed in the following sidebar.</p><div class="sidebar"><a id="calibre_link-167" class="calibre"></a><div class="sidebar-title">How Much Memory Does a Namenode Need?</div><p class="calibre2">A namenode can eat up <a class="calibre" id="calibre_link-2759"></a><a class="calibre" id="calibre_link-2667"></a>memory, since a reference to every block of every file
          is maintained in memory. It’s difficult to give a precise formula
          because memory usage depends on the number of blocks per file, the
          filename length, and the number of directories in the filesystem;
          plus, it can change from one Hadoop release to another.</p><p class="calibre2">The default of 1,000 MB of namenode memory is normally enough
          for a few million files, but as a rule of thumb for sizing purposes,
          you can conservatively allow 1,000 MB per million blocks of
          storage.</p><p class="calibre2">For example, a 200-node cluster with 24 TB of disk space per
          node, a block size of 128 MB, and a replication factor of 3 has room
          for about 2 million blocks (or more): 200 × 24,000,000 MB ⁄ (128 MB
          × 3). So in this case, setting the namenode memory to 12,000 MB
          would be a good starting point.</p><p class="calibre2">You can increase the namenode’s memory without changing the
          memory allocated to other Hadoop daemons by setting <code class="literal">HADOOP_NAMENODE_OPTS</code> in <em class="calibre10">hadoop-env.sh</em> to include a <a class="calibre" id="calibre_link-1877"></a>JVM option for setting the memory size. <code class="literal">HADOOP_NAMENODE_OPTS</code> allows you to pass
          extra options to the namenode’s JVM. So, for example, if you were
          using a Sun JVM, <code class="literal">-Xmx2000m</code> would specify that 2,000 MB
          of memory should be allocated to the namenode.</p><p class="calibre2">If you change the namenode’s memory allocation, don’t forget
          to do the same for the secondary namenode (using the <code class="literal">HADOOP_SECONDARYNAMENODE_OPTS</code> variable),
          since its memory requirements are comparable to the primary
          namenode’s.</p></div><p class="calibre2">In addition to the memory requirements of the <a class="calibre" id="calibre_link-1328"></a><a class="calibre" id="calibre_link-2664"></a>daemons, the node manager allocates containers to
        applications, so we need to factor these into the total memory
        footprint of a worker machine; see <a class="ulink" href="#calibre_link-35" title="Memory settings in YARN and MapReduce">Memory settings in YARN and MapReduce</a>.</p></div><div class="book" title="System logfiles"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-11">System logfiles</h4></div></div></div><p class="calibre2">System logfiles produced by <a class="calibre" id="calibre_link-3585"></a><a class="calibre" id="calibre_link-2364"></a>Hadoop are stored in <code class="literal"><em class="replaceable">$HADOOP_HOME/logs</em></code> by default.
        This can be changed using the <code class="literal">HADOOP_LOG_DIR</code> setting in <em class="calibre10">hadoop-env.sh</em>. It’s a <a class="calibre" id="calibre_link-1876"></a>good idea to change this so that logfiles are kept out
        of the directory that Hadoop is installed in. Changing this keeps
        logfiles in one place, even after the installation directory changes
        due to an upgrade. A common choice is <em class="calibre10">/var/log/hadoop</em>, set by including the
        following line in <em class="calibre10">hadoop-env.sh</em>:</p><pre class="screen1">export HADOOP_LOG_DIR=/var/log/hadoop</pre><p class="calibre2">The log directory will be created if it doesn’t already exist.
        (If it does not exist, confirm that the relevant Unix Hadoop user has
        permission to create it.) Each Hadoop daemon running on a machine
        produces two logfiles. The first is the log output written via log4j.
        This file, whose name ends in <em class="calibre10">.log</em>, should be the first port of call when
        diagnosing problems because most application log messages are written
        here. The standard Hadoop log4j configuration uses a daily rolling
        file appender to rotate logfiles. Old logfiles are never deleted, so
        you should arrange for them to be periodically deleted or archived, so
        as to not run out of disk space on the local node.</p><p class="calibre2">The second logfile is the combined standard output and standard
        error log. This logfile, whose name ends in <em class="calibre10">.out</em>, usually contains little or no output,
        since Hadoop uses log4j for logging. It is rotated only when the
        daemon is restarted, and only the last five logs are retained. Old
        logfiles are suffixed with a number between 1 and 5, with 5 being the
        oldest file.</p><p class="calibre2">Logfile names (of both types) are a combination of the name of
        the user running the <a class="calibre" id="calibre_link-1326"></a>daemon, the daemon name, and the machine hostname. For
        example, <em class="calibre10">hadoop-hdfs-datanode-ip-10-45-174-112.log.2014-09-20</em>
        is the name of a logfile after it has been rotated. This naming
        structure makes it possible to archive logs from all machines in the
        cluster in a single directory, if needed, since the filenames are
        unique.</p><p class="calibre2">The username in the logfile name is actually the default for
        <a class="calibre" id="calibre_link-1874"></a>the <code class="literal">HADOOP_IDENT_STRING</code> setting in <em class="calibre10">hadoop-env.sh</em>. If you wish to give the
        Hadoop instance a different identity for the purposes of naming the
        logfiles, change <code class="literal">HADOOP_IDENT_STRING</code> to be the identifier you
        want.</p></div><div class="book" title="SSH settings"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4332">SSH settings</h4></div></div></div><p class="calibre2">The control scripts <a class="calibre" id="calibre_link-3513"></a>allow you to run commands on (remote) worker nodes from
        the master node using SSH. It can be useful to customize the SSH
        settings, for various reasons. For example, you may want to reduce the
        connection timeout (using the <code class="literal">ConnectTimeout</code> option) so the
        <a class="calibre" id="calibre_link-1246"></a>control scripts don’t hang around waiting to see whether
        a dead node is going to respond. Obviously, this can be taken too far.
        If the timeout is too low, then busy nodes will be skipped, which is
        bad.</p><p class="calibre2">Another useful SSH setting is <code class="literal">StrictHostKeyChecking</code>, which <a class="calibre" id="calibre_link-3555"></a>can be set to <code class="literal">no</code> to
        automatically add new host keys to the known hosts files. The default,
        <code class="literal">ask</code>, prompts the user to confirm
        that the key fingerprint has been verified, which is not a suitable
        setting in a large cluster environment.<sup class="calibre6">[<a class="firstname" href="#calibre_link-36" id="calibre_link-62">72</a>]</sup></p><p class="calibre2">To pass extra options to SSH, define <a class="calibre" id="calibre_link-1880"></a>the <code class="literal">HADOOP_SSH_OPTS</code>
        environment variable in <em class="calibre10">hadoop-env.sh</em>. See the <code class="literal">ssh</code> and <code class="literal">ssh_config</code> manual pages for more SSH
        <a class="calibre" id="calibre_link-1124"></a><a class="calibre" id="calibre_link-1574"></a>settings.</p></div></div><div class="book" title="Important Hadoop Daemon Properties"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-696">Important Hadoop Daemon Properties</h3></div></div></div><p class="calibre2">Hadoop has a bewildering <a class="calibre" id="calibre_link-1120"></a><a class="calibre" id="calibre_link-1323"></a><a class="calibre" id="calibre_link-3066"></a>number of configuration properties. In this section, we
      address the ones that you need to
      define (or at least understand why the default is appropriate) for any
      real-world working cluster. These properties are set in the Hadoop site
      files: <em class="calibre10">core-site.xml</em>, <em class="calibre10">hdfs-site.xml</em>, and <em class="calibre10">yarn-site.xml</em>. Typical instances of these
      files are shown in Examples <a class="ulink" href="#calibre_link-37" title="Example&nbsp;10-1.&nbsp;A typical core-site.xml configuration file">10-1</a>, <a class="ulink" href="#calibre_link-38" title="Example&nbsp;10-2.&nbsp;A typical hdfs-site.xml configuration file">10-2</a>, and <a class="ulink" href="#calibre_link-39" title="Example&nbsp;10-3.&nbsp;A typical yarn-site.xml configuration file">10-3</a>.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-40" id="calibre_link-63">73</a>]</sup> You can learn more about the format of Hadoop’s
      configuration files in <a class="ulink" href="#calibre_link-41" title="The Configuration API">The Configuration API</a>.</p><p class="calibre2">To find the actual configuration of a running daemon, visit the
      <span class="calibre"><em class="calibre10">/conf</em></span> page on its web server. For example,
      <span class="calibre"><em class="calibre10">http://<em class="replaceable"><code class="replaceable">resource-manager-host</code></em>:8088/conf</em></span>
      shows the configuration that the resource manager is running with. This
      page shows the combined site and default configuration files that the
      daemon is running with, and also <a class="calibre" id="calibre_link-1259"></a><a class="calibre" id="calibre_link-3852"></a><a class="calibre" id="calibre_link-1976"></a>shows which file each property was picked up from.</p><div class="example"><a id="calibre_link-37" class="calibre"></a><div class="example-title">Example&nbsp;10-1.&nbsp;A typical core-site.xml configuration file</div><div class="book"><pre class="screen"><code class="cp">&lt;?xml version="1.0"?&gt;</code>
<code class="c1">&lt;!-- core-site.xml --&gt;</code>
<code class="nt">&lt;configuration&gt;</code>
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>fs.defaultFS<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>hdfs://namenode/<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
<code class="nt">&lt;/configuration&gt;</code></pre></div></div><div class="example"><a id="calibre_link-38" class="calibre"></a><div class="example-title">Example&nbsp;10-2.&nbsp;A typical hdfs-site.xml configuration file</div><div class="book"><pre class="screen"><code class="cp">&lt;?xml version="1.0"?&gt;</code>
<code class="c1">&lt;!-- hdfs-site.xml --&gt;</code>
<code class="nt">&lt;configuration&gt;</code>
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>dfs.namenode.name.dir<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>/disk1/hdfs/name,/remote/hdfs/name<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>

  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>dfs.datanode.data.dir<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>/disk1/hdfs/data,/disk2/hdfs/data<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
  
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>dfs.namenode.checkpoint.dir<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>/disk1/hdfs/namesecondary,/disk2/hdfs/namesecondary<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
<code class="nt">&lt;/configuration&gt;</code></pre></div></div><div class="example"><a id="calibre_link-39" class="calibre"></a><div class="example-title">Example&nbsp;10-3.&nbsp;A typical yarn-site.xml configuration file</div><div class="book"><pre class="screen"><code class="cp">&lt;?xml version="1.0"?&gt;</code>
<code class="c1">&lt;!-- yarn-site.xml --&gt;</code>
<code class="nt">&lt;configuration&gt;</code>
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>yarn.resourcemanager.hostname<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>resourcemanager<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>

  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>yarn.nodemanager.local-dirs<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>/disk1/nm-local-dir,/disk2/nm-local-dir<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>

  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>yarn.nodemanager.aux-services<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>mapreduce.shuffle<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>

  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>yarn.nodemanager.resource.memory-mb<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>16384<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>

  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>yarn.nodemanager.resource.cpu-vcores<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>16<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
<code class="nt">&lt;/configuration&gt;</code></pre></div></div><div class="book" title="HDFS"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4333">HDFS</h4></div></div></div><p class="calibre2">To run HDFS, you need to <a class="calibre" id="calibre_link-1938"></a>designate one machine as a namenode. In this case, the
        property <code class="literal">fs.defaultFS</code> is an HDFS
        filesystem URI whose host is the namenode’s hostname or IP address and
        whose port is the port that the namenode will listen on for RPCs. If
        no port is specified, the default of 8020 is used.</p><p class="calibre2">The <code class="literal">fs.defaultFS</code> property
        <a class="calibre" id="calibre_link-1741"></a>also doubles as specifying the default filesystem. The
        default filesystem is used to resolve relative paths, which are handy
        to use because they save typing (and avoid hardcoding knowledge of a
        particular namenode’s address). For example, with the default
        filesystem defined in <a class="ulink" href="#calibre_link-37" title="Example&nbsp;10-1.&nbsp;A typical core-site.xml configuration file">Example&nbsp;10-1</a>, the relative URI
        <em class="calibre10">/a/b</em> is resolved to <em class="calibre10">hdfs://namenode/a/b</em>.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">If you are running HDFS, the fact that <code class="literal">fs.defaultFS</code> is used to specify both the
          HDFS namenode <span class="calibre">and</span> the default
          filesystem means HDFS has to be the default filesystem in the server
          configuration. Bear in mind, however, that it is possible to specify
          a different filesystem as the default in the client configuration,
          for convenience.</p><p class="calibre2">For example, if you use both HDFS and S3 filesystems, then you
          have a choice of specifying either as the default in the client
          configuration, which allows you to refer to the default with a
          relative URI and the other with an absolute URI.</p></div><p class="calibre2">There are a few other configuration properties you should set
        for HDFS: those that set the storage directories for the namenode and
        for datanodes. The <a class="calibre" id="calibre_link-1474"></a>property <code class="literal">dfs.namenode.name.dir</code> specifies a list of
        directories where the namenode stores persistent filesystem metadata (the edit log and
        the filesystem image). A copy of each metadata file is stored in each
        directory for redundancy. It’s common to configure <code class="literal">dfs.namenode.name.dir</code> so that the namenode
        metadata is written to one or two local disks, as well as a remote
        disk, such as an NFS-mounted directory. Such a setup guards against
        failure of a local disk and failure of the entire namenode, since in
        both cases the files can be recovered and used to start a new
        namenode. (The secondary namenode takes only periodic checkpoints of
        the namenode, so it does not provide an up-to-date backup of the
        namenode.)</p><p class="calibre2">You should also set <a class="calibre" id="calibre_link-1456"></a>the <code class="literal">dfs.datanode.data.dir</code> property, which
        specifies a list of directories for a datanode to store its blocks in.
        Unlike the namenode, which uses multiple directories for redundancy, a
        datanode round-robins writes between its storage directories,
        so for performance you should specify a storage directory for each
        local disk. Read performance also benefits from having multiple disks
        for storage, because blocks will be spread across them and concurrent
        reads for distinct blocks will be correspondingly spread across
        disks.</p><div class="note" title="Tip"><h3 class="title3">Tip</h3><p class="calibre2">For maximum performance, you should mount storage disks with
          the <code class="literal">noatime</code> option. This setting
          means that last accessed time information is not written on file
          reads, which gives significant performance gains.</p></div><p class="calibre2">Finally, you should configure where the secondary namenode
        stores its checkpoints of the filesystem. The <code class="literal">dfs.namenode.checkpoint.dir</code> property
        <a class="calibre" id="calibre_link-1468"></a>specifies a list of directories where the checkpoints
        are kept. Like the storage directories for the namenode, which keep
        redundant copies of the namenode
        metadata, the checkpointed filesystem image is stored in each
        checkpoint directory for redundancy.</p><p class="calibre2"><a class="ulink" href="#calibre_link-42" title="Table&nbsp;10-2.&nbsp;Important HDFS daemon properties">Table&nbsp;10-2</a> summarizes the
        important configuration properties for HDFS.</p><div class="table"><a id="calibre_link-42" class="calibre"></a><div class="table-title">Table&nbsp;10-2.&nbsp;Important HDFS daemon properties</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Type</td><td class="calibre20">Default value</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">fs.defaultFS</code></td><td class="calibre23">URI</td><td class="calibre23"><code class="uri">file:///</code></td><td class="calibre25">The default filesystem. The URI defines the hostname
                and port that the namenode’s RPC server runs on. The default
                port is 8020. This property is set in <em class="calibre10">core-site.xml</em>.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">dfs.namenode.name.dir</code></td><td class="calibre23">Comma-separated
                directory names</td><td class="calibre23"><code class="uri">file://${hadoop.tmp.dir}/dfs/name</code></td><td class="calibre25">The list of directories where the namenode stores its
                persistent metadata. The
                namenode stores a copy of the metadata in each directory in
                the list.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">dfs.datanode.data.dir</code></td><td class="calibre23">Comma-separated directory names</td><td class="calibre23"><code class="uri">file://${hadoop.tmp.dir}/dfs/data</code></td><td class="calibre25">A list of directories where the datanode stores blocks.
                Each block is stored in only one of these directories.</td></tr><tr class="calibre26"><td class="calibre27"> <code class="uri">dfs.namenode.checkpoint.dir</code>
                </td><td class="calibre27">Comma-separated directory names</td><td class="calibre27"><code class="uri">file://${hadoop.tmp.dir}/dfs/namesecondary</code></td><td class="calibre28">A list of directories where the secondary namenode stores
                checkpoints. It stores a copy of the checkpoint in each
                directory in the <a class="calibre" id="calibre_link-1939"></a>list.</td></tr></tbody></table></div></div><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">Note that the storage directories for HDFS are under Hadoop’s
          temporary directory by default (this is configured via the <code class="literal">hadoop.tmp.dir</code> property, whose default is
          <code class="literal">/tmp/hadoop-${user.name}</code>).
          Therefore, it is critical that these properties are set so that data
          is not lost by the system when it clears out temporary
          directories.</p></div></div><div class="book" title="YARN"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-320">YARN</h4></div></div></div><p class="calibre2">To run YARN, you need <a class="calibre" id="calibre_link-3834"></a>to designate one machine as a resource manager. The
        simplest way to do this is to set the <a class="calibre" id="calibre_link-3885"></a>property <code class="literal">yarn.resourcemanager.hostname</code> to the
        hostname or IP address of the machine running the resource manager.
        Many of the resource manager’s server addresses are derived from this
        property. For <a class="calibre" id="calibre_link-3877"></a>example, <span class="calibre"><code class="literal">yarn.resourcemanager</code></span><code class="literal">.address</code> takes the form of a host-port pair,
        and the host defaults to <code class="literal">yarn</code><code class="literal">.resourcemanager.hostname</code>. In a
        MapReduce client configuration, this property is used to connect to
        the resource manager over RPC.</p><p class="calibre2">During a MapReduce job, intermediate data and working files are
        written to temporary local files. Because this data includes the
        potentially very large output of map tasks, you need to ensure
        <a class="calibre" id="calibre_link-3867"></a>that the <code class="literal">yarn.nodemanager.local-dirs</code> property, which
        controls the location of local temporary storage for YARN containers,
        is configured to use disk partitions that are large enough. The
        property takes a comma-separated list of directory names, and you
        should use all available local disks to spread disk I/O (the
        directories are used in round-robin fashion). Typically, you will use
        the same disks and partitions (but different directories) for YARN
        local storage as you use for datanode block storage, as governed by
        the <code class="literal">dfs.datanode.data.dir</code> property,
        <a class="calibre" id="calibre_link-1457"></a>which was discussed earlier.</p><p class="calibre2">Unlike MapReduce 1, YARN doesn’t have tasktrackers to serve map
        outputs to reduce tasks, so for this function it relies on shuffle
        handlers, which are long-running auxiliary services running in node
        managers. Because YARN is a general-purpose service, the MapReduce
        shuffle handlers need to be enabled explicitly in <em class="calibre10">yarn-site.xml</em> by setting <a class="calibre" id="calibre_link-3858"></a>the <code class="literal">yarn.nodemanager.aux-services</code> property to
        <code class="literal">mapreduce_shuffle</code>.</p><p class="calibre2"><a class="ulink" href="#calibre_link-43" title="Table&nbsp;10-3.&nbsp;Important YARN daemon properties">Table&nbsp;10-3</a> summarizes the
        important configuration properties for YARN. The resource-related
        settings are covered in more detail in the next sections.</p><div class="table"><a id="calibre_link-43" class="calibre"></a><div class="table-title">Table&nbsp;10-3.&nbsp;Important YARN daemon properties</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Type</td><td class="calibre20">Default value</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">yarn.resourcemanager.hostname</code></td><td class="calibre23">Hostname</td><td class="calibre23"><code class="uri">0.0.0.0</code></td><td class="calibre25">The hostname of the machine the resource manager runs
                on. Abbreviated <code class="uri">${y.rm.hostname}</code> below.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">yarn.resourcemanager.address</code></td><td class="calibre23">Hostname and port</td><td class="calibre23"><code class="uri">${y.rm.hostname}:8032</code></td><td class="calibre25">The hostname and port that the resource manager’s RPC
                server runs on.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">yarn.nodemanager.local-dirs</code></td><td class="calibre23">Comma-separated
                directory names</td><td class="calibre23"><code class="uri">${hadoop.tmp.dir}/nm-local-dir</code></td><td class="calibre25">A list of directories where node managers allow
                containers to store intermediate data. The data is cleared out
                when the application ends.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">yarn.nodemanager.aux-services</code></td><td class="calibre23">Comma-separated
                service names</td><td class="calibre23">&nbsp;</td><td class="calibre25">A list of auxiliary services run by the node manager. A
                service is implemented by the class defined by the property
                <code class="uri">yarn.nodemanager.aux-services.<em class="replaceable"><code class="calibre44">service-name</code></em>.class</code>.
                By default, no auxiliary services are specified.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">yarn.nodemanager.resource.memory-mb</code></td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">8192</td><td class="calibre25">The amount o<a class="calibre" id="calibre_link-3873"></a>f physical memory (in MB) that may be allocated
                to containers being run by the node manager.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">yarn.nodemanager.vmem-pmem-ratio</code></td><td class="calibre23"><code class="uri">float</code></td><td class="calibre23">2.1</td><td class="calibre25">The ratio of <a class="calibre" id="calibre_link-3874"></a>virtual to physical memory for containers.
                Virtual memory usage may exceed the allocation by this
                amount.</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">yarn.nodemanager.resource.cpu-vcores</code></td><td class="calibre27"><code class="uri">int</code></td><td class="calibre27">8</td><td class="calibre28">The <a class="calibre" id="calibre_link-3870"></a>number of CPU cores that may be allocated to
                containers being run by the node manager.</td></tr></tbody></table></div></div></div><div class="book" title="Memory settings in YARN and MapReduce"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-35">Memory settings in YARN and MapReduce</h4></div></div></div><p class="calibre2">YARN treats memory in a more <a class="calibre" id="calibre_link-2453"></a>fine-grained manner than the slot-based model used in
        MapReduce 1. Rather than specifying a fixed maximum number of map and
        reduce slots that may run on a node at once, YARN allows applications
        to request an arbitrary amount of memory (within limits) for a task.
        In the YARN model, node managers allocate memory from a pool, so the
        number of tasks that are running on a particular node depends on the
        sum of their memory requirements, and not simply on a fixed number of
        slots.</p><p class="calibre2">The calculation for how much memory to dedicate to a node
        manager for running containers depends on the amount of physical
        memory on the machine. Each Hadoop daemon uses 1,000 MB, so for a
        datanode and a node manager, the total is 2,000 MB. Set aside enough
        for other processes that are running on the machine, and the remainder
        can be dedicated to the node manager’s containers by setting the
        configuration property <code class="literal">yarn.nodemanager.resource.memory-mb</code> to the
        total allocation in MB. (The default is 8,192 MB, which is normally
        too low for most setups.)</p><p class="calibre2">The next step is to determine how to set memory options for
        individual jobs. There are two main controls: one for the size of the
        container allocated by YARN, and another for the heap size of the Java
        process run in the container.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">The memory controls for MapReduce are all set by the client in
          the job configuration. The YARN settings are cluster settings and
          cannot be modified by the client.</p></div><p class="calibre2">Container sizes are determined by <code class="literal">mapreduce.map.memory.mb</code> and <code class="literal">mapreduce.reduce.memory.mb</code>; both default to
        1,024 MB. These settings are used by the application master when
        negotiating for resources in the cluster, and also by the node
        manager, which runs and monitors the task containers. The heap size of
        the Java process is set by <code class="literal">mapred.child.java.opts</code>, and defaults to 200
        MB. You can also set the Java options separately for map and reduce
        tasks (see <a class="ulink" href="#calibre_link-44" title="Table&nbsp;10-4.&nbsp;MapReduce job memory properties (set by the client)">Table&nbsp;10-4</a>).</p><div class="table"><a id="calibre_link-44" class="calibre"></a><div class="table-title">Table&nbsp;10-4.&nbsp;MapReduce job memory properties (set by the client)</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Type</td><td class="calibre20">Default value</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.map.memory.mb</code></td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">1024</td><td class="calibre25">The <a class="calibre" id="calibre_link-2583"></a>amount of memory for map containers.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.reduce.memory.mb</code></td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">1024</td><td class="calibre25">The <a class="calibre" id="calibre_link-2610"></a>amount of memory for reduce containers.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapred.child.java.opts</code></td><td class="calibre23"><code class="uri">String</code></td><td class="calibre23"><code class="uri">-Xmx200m</code></td><td class="calibre25">The JVM <a class="calibre" id="calibre_link-2423"></a>options used to launch the container process
                that runs map and reduce tasks. In addition to memory
                settings, this property can include JVM properties for
                debugging, for example.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.map.java.opts</code></td><td class="calibre23"><code class="uri">String</code></td><td class="calibre23"><code class="uri">-Xmx200m</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-2579"></a>JVM options used for the child process that runs
                map tasks.</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">mapreduce.reduce.java.opts</code></td><td class="calibre27"><code class="uri">String</code></td><td class="calibre27"><code class="uri">-Xmx200m</code></td><td class="calibre28">The JVM <a class="calibre" id="calibre_link-2606"></a>options used for the child process that runs
                reduce tasks.</td></tr></tbody></table></div></div><p class="calibre2">For example, suppose <code class="literal">mapred.child.java.opts</code> is set to <code class="literal">-Xmx800m</code> and <code class="literal">mapreduce.map.memory.mb</code> is left at its
        default value of 1,024 MB. When a map task is run, the node manager
        will allocate a 1,024 MB container (decreasing the size of its pool by
        that amount for the duration of the task) and will launch the task JVM
        configured with an 800 MB maximum heap size. Note that the JVM process
        will have a larger memory footprint than the heap size, and the
        overhead will depend on such things as the native libraries that are
        in use, the size of the permanent generation space, and so on. The
        important thing is that the physical memory used by the JVM process,
        including any processes that it spawns, such as Streaming processes,
        does not exceed its allocation (1,024 MB). If a container uses more
        memory than it has been allocated, then it may be terminated by the
        node manager and marked as failed.</p><p class="calibre2">YARN schedulers impose a minimum or maximum on memory
        allocations. The default minimum is 1,024 MB (set by <code class="literal">yarn.scheduler.minimum-allocation-mb</code>),
        <a class="calibre" id="calibre_link-3906"></a><a class="calibre" id="calibre_link-3905"></a>and the default maximum is 8,192 MB (set by <code class="literal">yarn.scheduler.maximum-allocation-mb</code>).</p><p class="calibre2">There are also virtual memory constraints that a container must
        meet. If a container’s virtual <a class="calibre" id="calibre_link-1250"></a><a class="calibre" id="calibre_link-2663"></a>memory usage exceeds a given multiple of the allocated
        physical memory, the node manager may terminate the process. The
        multiple is expressed by <a class="calibre" id="calibre_link-3875"></a>the <code class="literal">yarn.nodemanager.vmem-pmem-ratio</code> property,
        which defaults to 2.1. In the example used earlier, the virtual memory
        threshold above which the task may be terminated is 2,150 MB, which is
        2.1 × 1,024 MB.</p><p class="calibre2">When configuring memory parameters it’s very useful to be able
        to monitor a task’s actual memory usage during a job run, and this is
        possible via MapReduce task counters. The <a class="calibre" id="calibre_link-2989"></a><a class="calibre" id="calibre_link-3767"></a><a class="calibre" id="calibre_link-1200"></a>counters <code class="literal">PHYSICAL_MEMORY_BYTES</code>, <code class="literal">VIRTUAL_MEMORY_BYTES</code>, and <code class="literal">COMMITTED_</code><code class="literal">HEAP_BYTES</code> (described in <a class="ulink" href="#calibre_link-45" title="Table&nbsp;9-2.&nbsp;Built-in MapReduce task counters">Table&nbsp;9-2</a>) provide snapshot values of
        memory usage and are therefore suitable for observation during the
        course of a task attempt.</p><p class="calibre2">Hadoop also provides settings to control how much memory is used
        for MapReduce operations. These can be set on a per-job basis and are
        covered in <a class="ulink" href="#calibre_link-46" title="Shuffle and Sort">Shuffle and Sort</a>.</p></div><div class="book" title="CPU settings in YARN and MapReduce"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4334">CPU settings in YARN and MapReduce</h4></div></div></div><p class="calibre2">In addition to memory, YARN treats CPU usage as a managed
        resource, and applications can request the number of cores they need.
        The number of cores that a node manager can allocate to containers is
        controlled by the <code class="literal">yarn.nodemanager.resource.cpu-vcores</code>
        property. <a class="calibre" id="calibre_link-3871"></a>It should be set to the total number of cores on the
        machine, minus a core for each daemon process running on the machine
        (datanode, node manager, and any other long-running processes).</p><p class="calibre2">MapReduce jobs can control the number of <a class="calibre" id="calibre_link-2574"></a>cores allocated to map and reduce containers by setting
        <code class="literal">mapreduce.map.cpu.vcores</code> and
        <code class="literal">mapreduce.reduce.cpu.vcores</code>. Both
        <a class="calibre" id="calibre_link-2602"></a>default to 1, an appropriate setting for normal
        single-threaded MapReduce tasks, which can only saturate a single
        <a class="calibre" id="calibre_link-1121"></a><a class="calibre" id="calibre_link-1324"></a><a class="calibre" id="calibre_link-3067"></a><a class="calibre" id="calibre_link-3835"></a><a class="calibre" id="calibre_link-2454"></a>core.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">While the number of cores is tracked during scheduling (so a
          container won’t be allocated on a machine where there are no spare
          cores, for example), the node manager will not, by default, limit
          actual CPU usage of running containers. This means that a container
          can abuse its allocation by using more CPU than it was given,
          possibly starving other containers running on the same host. YARN
          has support for enforcing CPU limits using Linux cgroups. The node
          manager’s container executor <a class="calibre" id="calibre_link-3862"></a>class (<code class="literal">yarn.nodemanager.container-executor.class</code>)
          must be set to <a class="calibre" id="calibre_link-2326"></a>use the <code class="literal">LinuxContainerExecutor</code>
          class, which in turn must be configured to use <a class="calibre" id="calibre_link-3866"></a>cgroups (see the properties under <code class="literal">yarn.nodemanager.linux-container-executor</code>).</p></div></div></div><div class="book" title="Hadoop Daemon Addresses and Ports"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4335">Hadoop Daemon Addresses and Ports</h3></div></div></div><p class="calibre2">Hadoop daemons generally <a class="calibre" id="calibre_link-1118"></a><a class="calibre" id="calibre_link-1320"></a>run both an RPC server for communication between daemons
      (<a class="ulink" href="#calibre_link-47" title="Table&nbsp;10-5.&nbsp;RPC server properties">Table&nbsp;10-5</a>) and an HTTP server to provide
      web pages for human consumption (<a class="ulink" href="#calibre_link-48" title="Table&nbsp;10-6.&nbsp;HTTP server properties">Table&nbsp;10-6</a>). Each server is configured by setting
      the network address and port number to listen on. A port number of 0
      instructs the server to start on a free port, but this is generally
      discouraged because it is incompatible with setting cluster-wide
      firewall policies.</p><p class="calibre2">In general, the properties for setting a server’s RPC and HTTP
      addresses serve double duty: they determine the network interface that
      the server will bind to, and they are used by clients or other machines
      in the cluster to connect to the server. For example, node managers use
      the <code class="literal">yarn.resourcemanager.resource-tracker.address</code>
      property to find the address of their resource manager.</p><p class="calibre2">It is often desirable for servers to bind to multiple network
      interfaces, but setting the network address to <code class="literal">0.0.0.0</code>, which works for the server, breaks
      the second case, since the address is not resolvable by clients or other
      machines in the cluster. One solution is to have separate configurations
      for clients and servers, but a better way is to set the bind host for
      the server. By setting <code class="literal">yarn.resourcemanager.hostname</code> to the
      (externally resolvable) hostname or IP address and <code class="literal">yarn.resourcemanager.bind-host</code> to <code class="literal">0.0.0.0</code>, you ensure that the resource manager
      will bind to all addresses on the machine, while at the same time
      providing a resolvable address for node managers and clients.</p><p class="calibre2">In addition to an RPC server, datanodes run a TCP/IP server for
      block transfers. The server address and port are set by the <code class="literal">dfs.datanode.address</code> property <a class="calibre" id="calibre_link-1454"></a>, which has a default value of <code class="literal">0.0.0.0:50010</code>.</p><div class="table"><a id="calibre_link-47" class="calibre"></a><div class="table-title">Table&nbsp;10-5.&nbsp;RPC server properties</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Default value</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">fs.defaultFS</code></td><td class="calibre23"><code class="uri">file:///</code></td><td class="calibre25">When <a class="calibre" id="calibre_link-1739"></a><a class="calibre" id="calibre_link-3248"></a>set to an HDFS URI, this property determines the
              namenode’s RPC server address and port. The default port is 8020
              if not specified.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">dfs.namenode.rpc-bind-host</code></td><td class="calibre23">&nbsp;</td><td class="calibre25">The <a class="calibre" id="calibre_link-1478"></a>address the namenode’s RPC server will bind to. If
              not set (the default), the bind address is determined by
              <code class="uri">fs.defaultFS</code>. It can be set
              to <code class="uri">0.0.0.0</code> to make the
              namenode listen on all interfaces.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">dfs.datanode.ipc.address</code></td><td class="calibre23"><code class="uri">0.0.0.0:50020</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-1459"></a>datanode’s RPC server address and port.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.jobhistory.address</code></td><td class="calibre23"><code class="uri">0.0.0.0:10020</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-2568"></a>job history server’s RPC server address and port.
              This is used by the client (typically outside the cluster) to
              query job history.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.jobhistory.bind-host</code></td><td class="calibre23">&nbsp;</td><td class="calibre25">The <a class="calibre" id="calibre_link-2569"></a>address the job history server’s RPC and HTTP
              servers will bind to.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">yarn.resourcemanager.hostname</code></td><td class="calibre23"><code class="uri">0.0.0.0</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-3886"></a>hostname of the machine the resource manager runs
              on. Abbreviated <code class="uri">${y.rm.hostname}</code> below.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">yarn.resourcemanager.bind-host</code></td><td class="calibre23">&nbsp;</td><td class="calibre25">The <a class="calibre" id="calibre_link-3884"></a>address the resource manager’s RPC and HTTP
              servers will bind to.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">yarn.resourcemanager.address</code></td><td class="calibre23"><code class="uri">${y.rm.hostname}:8032</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-3878"></a>resource manager’s RPC server address and port.
              This is used by the client (typically outside the cluster) to
              communicate with the resource manager.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">yarn.resourcemanager.admin.address</code></td><td class="calibre23"><code class="uri">${y.rm.hostname}:8033</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-3881"></a>resource manager’s admin RPC server address and
              port. This is used by the admin client (invoked with <code class="uri">yarn rmadmin</code>, typically run outside
              the cluster) to communicate with the resource manager.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">yarn.resourcemanager.scheduler.address</code></td><td class="calibre23"><code class="uri">${y.rm.hostname}:8030</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-3895"></a>resource manager scheduler’s RPC server address
              and port. This is used by (in-cluster) application masters to
              communicate with the resource manager.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">yarn.resourcemanager.resource-tracker.address</code></td><td class="calibre23"><code class="uri">${y.rm.hostname}:8031</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-3894"></a>resource manager resource tracker’s RPC server
              address and port. This is used by (in-cluster) node managers to
              communicate with the resource manager.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">yarn.nodemanager.hostname</code></td><td class="calibre23"><code class="uri">0.0.0.0</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-3865"></a>hostname of the machine the node manager runs on.
              Abbreviated <code class="uri">${y.nm.hostname}</code>
              below.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">yarn.nodemanager.bind-host</code></td><td class="calibre23">&nbsp;</td><td class="calibre25">The <a class="calibre" id="calibre_link-3860"></a>address the node manager’s RPC and HTTP servers
              will bind to.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">yarn.nodemanager.address</code></td><td class="calibre23"><code class="uri">${y.nm.hostname}:0</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-3857"></a>node manager’s RPC server address and port. This
              is used by (in-cluster) application masters to communicate with
              node managers.</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">yarn.nodemanager.localizer.address</code></td><td class="calibre27"><code class="uri">${y.nm.hostname}:8040</code></td><td class="calibre28">The <a class="calibre" id="calibre_link-3868"></a>node manager localizer’s RPC server address and
              port.</td></tr></tbody></table></div></div><div class="table"><a id="calibre_link-48" class="calibre"></a><div class="table-title">Table&nbsp;10-6.&nbsp;HTTP server properties</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Default value</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">dfs.namenode.http-address</code></td><td class="calibre23"><code class="uri">0.0.0.0:50070</code></td><td class="calibre25">The namenode’s <a class="calibre" id="calibre_link-2060"></a><a class="calibre" id="calibre_link-1472"></a>HTTP server address and port.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">dfs.namenode.http-bind-host</code></td><td class="calibre23">&nbsp;</td><td class="calibre25">The <a class="calibre" id="calibre_link-1473"></a>address the namenode’s HTTP server will bind
              to.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">dfs.namenode.secondary.http-address</code></td><td class="calibre23"><code class="uri">0.0.0.0:50090</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-1481"></a>secondary namenode’s HTTP server address and
              port.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">dfs.datanode.http.address</code></td><td class="calibre23"><code class="uri">0.0.0.0:50075</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-1458"></a>datanode’s HTTP server address and port. (Note
              that the property name is inconsistent with the ones for the
              namenode.)</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapreduce.jobhistory.webapp.address</code></td><td class="calibre23"><code class="uri">0.0.0.0:19888</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-2570"></a>MapReduce job history server’s address and port.
              This property is set in <em class="calibre10">mapred-site.xml</em>.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapreduce.shuffle.port</code></td><td class="calibre23"><code class="uri">13562</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-2624"></a>shuffle handler’s HTTP port number. This is used
              for serving map outputs, and is not a user-accessible web UI.
              This property is set in <em class="calibre10">mapred-site.xml</em>.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">yarn.resourcemanager.webapp.address</code></td><td class="calibre23"><code class="uri">${y.rm.hostname}:8088</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-3897"></a>resource manager’s HTTP server address and
              port.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">yarn.nodemanager.webapp.address</code></td><td class="calibre23"><code class="uri">${y.nm.hostname}:8042</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-3876"></a>node manager’s HTTP server address and
              port.</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">yarn.web-proxy.address</code></td><td class="calibre27">&nbsp;</td><td class="calibre28">The <a class="calibre" id="calibre_link-3907"></a>web app proxy server’s HTTP server address and
              port. If not set (the default), then the web app proxy server
              will run in the resource manager process.</td></tr></tbody></table></div></div><p class="calibre2">There is also a setting for controlling which network interfaces
      the datanodes use as their IP addresses (for HTTP and RPC servers). The
      relevant property is <code class="literal">dfs.datanode.dns.interface</code>, which <a class="calibre" id="calibre_link-1732"></a>is set to <code class="literal">default</code> to
      use the default network interface. You can set this explicitly to report
      the address of a particular interface (<code class="literal">eth0</code>, for <a class="calibre" id="calibre_link-1119"></a><a class="calibre" id="calibre_link-1321"></a>example).</p></div><div class="book" title="Other Hadoop Properties"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-510">Other Hadoop Properties</h3></div></div></div><p class="calibre2">This section discusses some <a class="calibre" id="calibre_link-1115"></a>other properties that you might consider setting.</p><div class="book" title="Cluster membership"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4336">Cluster membership</h4></div></div></div><p class="calibre2">To aid in the addition and removal of nodes in the future, you
        can specify a file containing a list of authorized machines that may
        join the cluster as datanodes or node managers. The file is specified
        using <a class="calibre" id="calibre_link-1466"></a>the <code class="literal">dfs.hosts</code> and
        <code class="literal">yarn.resourcemanager.nodes.include-path</code>
        properties <a class="calibre" id="calibre_link-3892"></a>(for datanodes and node managers, respectively), and the
        corresponding <code class="literal">dfs.hosts.exclude</code> and
        <code class="literal">yarn.resourcemanager.nodes.exclude-path</code>
        properties specify the files <a class="calibre" id="calibre_link-3890"></a>used for decommissioning. See <a class="ulink" href="#calibre_link-9" title="Commissioning and Decommissioning Nodes">Commissioning and Decommissioning Nodes</a> for further
        discussion.</p></div><div class="book" title="Buffer size"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4337">Buffer size</h4></div></div></div><p class="calibre2">Hadoop uses a <a class="calibre" id="calibre_link-1027"></a>buffer size of 4 KB (4,096 bytes) for its I/O
        operations. This is a conservative setting, and with modern hardware
        and operating systems, you will likely see performance benefits by
        increasing it; 128 KB (131,072 bytes) is a common choice. Set the
        value in bytes <a class="calibre" id="calibre_link-2147"></a>using the <code class="literal">io.file.buffer.size</code> property in <em class="calibre10">core-site.xml</em>.</p></div><div class="book" title="HDFS block size"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4338">HDFS block size</h4></div></div></div><p class="calibre2">The HDFS block size <a class="calibre" id="calibre_link-1017"></a>is 128 MB by default, but many clusters use more (e.g.,
        256 MB, which is 268,435,456 bytes) to ease memory pressure on the
        namenode and to give mappers more data to work on. Use the <code class="literal">dfs.blocksize</code> property <a class="calibre" id="calibre_link-1451"></a>in <em class="calibre10">hdfs-site.xml</em> to
        specify the size in bytes.</p></div><div class="book" title="Reserved storage space"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4339">Reserved storage space</h4></div></div></div><p class="calibre2">By default, datanodes will <a class="calibre" id="calibre_link-3216"></a><a class="calibre" id="calibre_link-1510"></a>try to use all of the space available in their storage
        directories. If you want to reserve some space on the storage volumes
        for non-HDFS use, you can set <code class="literal">dfs.datanode.du.reserved</code> to <a class="calibre" id="calibre_link-1733"></a>the amount, in bytes, of space to reserve.</p></div><div class="book" title="Trash"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4340">Trash</h4></div></div></div><p class="calibre2">Hadoop filesystems <a class="calibre" id="calibre_link-1743"></a><a class="calibre" id="calibre_link-3718"></a>have a trash facility, in which deleted files are not
        actually deleted but rather are moved to a trash folder, where they
        remain for a minimum period before being permanently deleted by the
        system. The minimum period in minutes that a file will remain in the
        trash is set using the <code class="literal">fs.trash.interval</code> configuration property in
        <em class="calibre10">core-site.xml</em>. By default, the
        trash interval is zero, which disables trash.</p><p class="calibre2">Like in many operating systems, Hadoop’s trash facility is a
        user-level feature, meaning that only files that are deleted using the
        filesystem shell are put in the trash. Files deleted programmatically
        are deleted immediately. It is possible to use the trash
          programmatically, however, by constructing <a class="calibre" id="calibre_link-3717"></a>a <code class="literal">Trash</code> instance,
        then calling its <code class="literal">moveToTrash()</code> method with
        the <code class="literal">Path</code> of the file intended for
        deletion. The method returns a value indicating success; a value of
        <code class="literal">false</code> means either that trash is
        not enabled or that the file is already in the trash.</p><p class="calibre2">When trash is enabled, users each have their own trash
        directories called <em class="calibre10">.Trash</em> in
        their home directories. File recovery is simple: you look for the file
        in a subdirectory of <em class="calibre10">.Trash</em> and
        move it out of the trash subtree.</p><p class="calibre2">HDFS will automatically delete files in trash folders, but other
        filesystems will not, so you have to arrange for this to be done
        periodically. You can <em class="calibre10">expunge</em> the trash, which
        will delete files that have been in the trash longer than their
        minimum period, using the filesystem shell:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -expunge</code></strong></pre><p class="calibre2">The <code class="literal">Trash</code> class exposes an
        <code class="literal">expunge()</code> method that has the same
        effect.</p></div><div class="book" title="Job scheduler"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4341">Job scheduler</h4></div></div></div><p class="calibre2">Particularly in <a class="calibre" id="calibre_link-2258"></a><a class="calibre" id="calibre_link-3280"></a><a class="calibre" id="calibre_link-3843"></a>a multiuser setting, consider updating the job scheduler
        queue configuration to reflect your organizational needs. For example,
        you can set up a queue for each group using the cluster. See <a class="ulink" href="#calibre_link-49" title="Scheduling in YARN">Scheduling in YARN</a>.</p></div><div class="book" title="Reduce slow start"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-494">Reduce slow start</h4></div></div></div><p class="calibre2">By default, schedulers wait until 5% of the map tasks in a job
        have completed before scheduling reduce tasks for the same job. For
        large jobs, this can cause problems with cluster utilization, since
        they take up reduce containers while waiting for the map tasks to
        complete. Setting <code class="literal">mapreduce.job.reduce.slowstart.completedmaps</code>
        to a higher <a class="calibre" id="calibre_link-2561"></a>value, such as 0.80 (80%), can help improve
        throughput.</p></div><div class="book" title="Short-circuit local reads"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-188">Short-circuit local reads</h4></div></div></div><p class="calibre2">When reading a <a class="calibre" id="calibre_link-3169"></a>file from HDFS, the client contacts the datanode and the
        data is sent to the client via a TCP connection. If the block being
        read is on the same node as the client, then it is more efficient for
        the client to bypass the network and read the block data directly from
        the disk. This is termed <a class="calibre" id="calibre_link-3389"></a>a <em class="calibre10">short-circuit local read</em>, and
        can make applications like HBase perform better.</p><p class="calibre2">You can enable short-circuit local reads by <a class="calibre" id="calibre_link-1453"></a>setting <code class="literal">dfs.client.read.shortcircuit</code> to <code class="literal">true</code>. Short-circuit local reads are
        implemented using Unix domain sockets, which use a local path for
        client-datanode communication. The path is set using the <a class="calibre" id="calibre_link-1464"></a>property <code class="literal">dfs.domain.socket.path</code>, and must be a path
        that only the datanode user (typically <code class="literal">hdfs</code>) or root can create, such as <em class="calibre10">/var/run/hadoop-hdfs/dn_socket</em>.</p></div></div></div><div class="book" title="Security"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-169">Security</h2></div></div></div><p class="calibre2">Early versions of Hadoop <a class="calibre" id="calibre_link-1137"></a><a class="calibre" id="calibre_link-3307"></a>assumed that HDFS and MapReduce clusters would be used by a
    group of cooperating users within a secure environment. The measures for
    restricting access were designed to
    prevent accidental data loss, rather than to prevent unauthorized access
    to data. For example, the file permissions system in HDFS prevents one
    user from accidentally wiping out the whole filesystem because of a bug in
    a program, or by mistakenly typing <code class="literal">hadoop fs -rmr
    /</code>, but it doesn’t prevent a malicious user from assuming root’s
    identity to access or delete any data in the cluster.</p><p class="calibre2">In security parlance, what was missing was a secure
    <em class="calibre10">authentication</em> mechanism to assure Hadoop that the
    user seeking to perform an operation on the cluster is who he claims to be
    and therefore can be trusted. HDFS file permissions provide only a
    mechanism for <em class="calibre10">authorization</em>, which controls what a
    particular user can do to a particular file. For example, a file may be readable only by a
    certain group of users, so anyone not in that group is not authorized to
    read it. However, authorization is not enough by itself, because the
    system is still open to abuse via spoofing by a malicious user who can
    gain network access to the cluster.</p><p class="calibre2">It’s common to restrict access to data that contains personally
    identifiable information (such as an end user’s full name or IP address)
    to a small set of users (of the cluster) within the organization who are
    authorized to access such information. Less sensitive (or anonymized) data
    may be made available to a larger set of users. It is convenient to host a
    mix of datasets with different security levels on the same cluster (not
    least because it means the datasets with lower security levels can be
    shared). However, to meet regulatory requirements for data protection,
    secure authentication must be in place for shared clusters.</p><p class="calibre2">This is the situation that Yahoo! faced in 2009, which led a team of
    engineers there to implement secure authentication for Hadoop. In their
    design, Hadoop itself does not manage user credentials; instead, it relies
    on Kerberos, a mature open-source network authentication protocol, to
    authenticate the user. However, Kerberos doesn’t manage permissions.
    Kerberos says that a user is who she says she is; it’s Hadoop’s job to
    determine whether that user has permission to perform a given
    action.</p><p class="calibre2">There’s a lot to security in Hadoop, and this section only covers
    the highlights. For more, readers are referred to <span class="calibre"><a class="ulink" href="http://shop.oreilly.com/product/0636920033332.do" target="_top">Hadoop
    Security</a></span> by Ben Spivey and Joey Echeverria (O’Reilly,
    2014).</p><div class="book" title="Kerberos and Hadoop"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4342">Kerberos and Hadoop</h3></div></div></div><p class="calibre2">At a high level, there are <a class="calibre" id="calibre_link-3311"></a><a class="calibre" id="calibre_link-2301"></a>three steps that a client must take to access a service
      when using Kerberos, each of which involves a message exchange with a
      server:</p><div class="book"><ol class="orderedlist"><li class="listitem"><p class="calibre2"><span class="calibre">Authentication.</span> The client
          <a class="calibre" id="calibre_link-928"></a>authenticates itself to the Authentication Server and
          receives a timestamped
          <a class="calibre" id="calibre_link-3691"></a><a class="calibre" id="calibre_link-3683"></a>Ticket-Granting Ticket (TGT).</p></li><li class="listitem"><p class="calibre2"><span class="calibre">Authorization.</span> The client
          <a class="calibre" id="calibre_link-931"></a>uses the TGT to request a service ticket from the
          Ticket-Granting Server.</p></li><li class="listitem"><p class="calibre2"><span class="calibre">Service request.</span> The client
          <a class="calibre" id="calibre_link-3376"></a>uses the service ticket to authenticate itself to the
          server that is providing the service the client is using. In the
          case of Hadoop, this might be the namenode or the resource
          manager.</p></li></ol></div><p class="calibre2">Together, the Authentication Server and the Ticket Granting Server
      <a class="calibre" id="calibre_link-2304"></a><a class="calibre" id="calibre_link-2298"></a>form the <em class="calibre10">Key Distribution Center</em>
      (KDC). The process is shown graphically in <a class="ulink" href="#calibre_link-50" title="Figure&nbsp;10-2.&nbsp;The three-step Kerberos ticket exchange protocol">Figure&nbsp;10-2</a>.</p><div class="book"><div class="figure"><a id="calibre_link-50" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4343" class="calibre"></a><img alt="The three-step Kerberos ticket exchange protocol" src="images/000049.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;10-2.&nbsp;The three-step Kerberos ticket exchange protocol</div></div></div><p class="calibre2">The authorization and service request steps are not user-level
      actions; the client performs these steps on the user’s behalf. The
      authentication step, however, is normally carried out explicitly by the
      user <a class="calibre" id="calibre_link-2310"></a>using the <code class="literal">kinit</code>
      command, which will prompt for a password. However, this doesn’t mean
      you need to enter your password every time you run a job or access HDFS,
      since TGTs last for 10 hours by default (and can be renewed for up to a
      week). It’s common to automate authentication at operating system login
      time, thereby <a class="calibre" id="calibre_link-3409"></a>providing <em class="calibre10">single sign-on</em> to
      Hadoop.</p><p class="calibre2">In cases where you don’t want to be prompted for a password (for
      running an unattended MapReduce
      job, for example), you can create a Kerberos
      <em class="calibre10">keytab</em> file using <a class="calibre" id="calibre_link-2312"></a>the <code class="literal">ktutil</code> command. A
      keytab is a file that stores passwords and may be supplied to <code class="literal">kinit</code> with the <code class="literal">-t</code> option.</p><div class="book" title="An example"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4344">An example</h4></div></div></div><p class="calibre2">Let’s look at an example of the process in action. The first
        step is to enable Kerberos authentication by setting the <code class="literal">hadoop.security.authentication</code> property
        <a class="calibre" id="calibre_link-1861"></a><a class="calibre" id="calibre_link-1260"></a>in <em class="calibre10">core-site.xml</em> to
        <code class="literal">kerberos</code>.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-51" id="calibre_link-64">74</a>]</sup> The default setting is <code class="literal">simple</code>, which signifies that the old backward-compatible (but insecure)
        behavior of using the operating system username to determine identity
        should be employed.</p><p class="calibre2">We also need to enable service-level <a class="calibre" id="calibre_link-1862"></a>authorization by setting <code class="literal">hadoop.security.authorization</code> to <code class="literal">true</code> in the same file. You may configure
        <a class="calibre" id="calibre_link-862"></a><a class="calibre" id="calibre_link-859"></a>access control lists (ACLs) in <a class="calibre" id="calibre_link-1858"></a>the <em class="calibre10">hadoop-policy.xml</em> configuration file to
        control which users and groups have permission to connect to each
        Hadoop service. Services are defined at the protocol level, so there
        are ones for MapReduce job submission, namenode communication, and so
        on. By default, all ACLs are set to <code class="literal">*</code>, which means that all users have
        permission to access each service; however, on a real cluster you
        should lock the ACLs down to only those users and groups that should
        have access.</p><p class="calibre2">The format for an ACL is a comma-separated list of usernames,
        followed by whitespace, followed by a comma-separated list of group
        names. For example, the ACL <code class="literal">preston,howard directors,inventors</code>
        would authorize access to users named <code class="literal">preston</code> or <code class="literal">howard</code>, or in groups <code class="literal">directors</code> or <code class="literal">inventors</code>.</p><p class="calibre2">With Kerberos authentication turned on, let’s see what happens
        when we try to copy a local file to HDFS:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -put quangle.txt .</code></strong>
10/07/03 15:44:58 WARN ipc.Client: Exception encountered while connecting to the
server: javax.security.sasl.SaslException: GSS initiate failed [Caused by 
GSSException: No valid credentials provided (Mechanism level: Failed to find 
any Kerberos tgt)]
Bad connection to FS. command aborted. exception: Call to localhost/
127.0.0.1:8020 failed on local exception: java.io.IOException: 
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: 
No valid credentials provided
(Mechanism level: Failed to find any Kerberos tgt)]</pre><p class="calibre2">The operation fails because we don’t have a Kerberos ticket. We
        can get one by authenticating to the KDC, using <code class="literal">kinit</code>:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">kinit</code></strong>
Password for hadoop-user@LOCALDOMAIN: <em class="replaceable"><code class="replaceable">password</code></em>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -put quangle.txt .</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -stat %n quangle.txt</code></strong>
quangle.txt</pre><p class="calibre2">And we see that the file is successfully written to HDFS. Notice
        that even though we carried out two filesystem commands, we only
        needed to call <code class="literal">kinit</code> once, since
        the Kerberos ticket is valid for 10 hours (use the <code class="literal">klist</code> command to see the expiry time of your
        tickets and <code class="literal">kdestroy</code> to <a class="calibre" id="calibre_link-2299"></a><a class="calibre" id="calibre_link-2311"></a>invalidate your tickets). After we get a ticket,
        everything works just as it <a class="calibre" id="calibre_link-3312"></a><a class="calibre" id="calibre_link-2302"></a><a class="calibre" id="calibre_link-929"></a>normally would.</p></div></div><div class="book" title="Delegation Tokens"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4345">Delegation Tokens</h3></div></div></div><p class="calibre2">In a distributed system <a class="calibre" id="calibre_link-3310"></a><a class="calibre" id="calibre_link-1422"></a>such as HDFS or MapReduce, there are many client-server
      interactions, each of which must be authenticated. For example, an HDFS
      read operation will involve multiple calls to the namenode and calls to
      one or more datanodes. Instead of using the three-step Kerberos ticket
      exchange protocol to authenticate each call, which would present a high
      load on the KDC on a busy cluster, Hadoop uses <em class="calibre10">delegation
      tokens</em> to allow later <a class="calibre" id="calibre_link-927"></a>authenticated access without having to contact the KDC
      again. Delegation tokens are created and used transparently by Hadoop on
      behalf of users, so there’s no action you need to take as a user beyond
      using <code class="literal">kinit</code> to sign in, but it’s
      useful to have a basic idea of how they are used.</p><p class="calibre2">A delegation token is generated by the server (the namenode, in
      this case) and can be thought of as a shared secret between the client
      and the server. On the first RPC call to the namenode, the client has no
      delegation token, so it uses Kerberos to authenticate. As a part of the
      response, it gets a delegation token from the namenode. In subsequent
      calls it presents the delegation token, which the namenode can verify
      (since it generated it using a secret key), and hence the client is
      authenticated to the server.</p><p class="calibre2">When it wants to perform operations on HDFS blocks, the client
      uses a special kind of delegation token, <a class="calibre" id="calibre_link-1008"></a>called a <em class="calibre10">block access token</em>, that
      the namenode passes to the client in response to a metadata request. The
      client uses the block access token to authenticate itself to datanodes.
      This is possible only because the namenode shares its secret key used to
      generate the block access token with datanodes (sending it in heartbeat
      messages), so that they can verify block access tokens. Thus, an HDFS
      block may be accessed only by a client with a valid block access token
      from a namenode. This closes the security hole in unsecured Hadoop where
      only the block ID was needed to gain access to a block. This property is
      enabled by <a class="calibre" id="calibre_link-1449"></a>setting <code class="literal">dfs.block.access.token.enable</code> to <code class="literal">true</code>.</p><p class="calibre2">In MapReduce, job resources and metadata (such as JAR files, input
      splits, and configuration files) are shared in HDFS for the application
      master to access, and user code runs on the node managers and accesses
      files on HDFS (the process is explained in <a class="ulink" href="#calibre_link-52" title="Anatomy of a MapReduce Job Run">Anatomy of a MapReduce Job Run</a>). Delegation tokens are used by
      these components to access HDFS during the course of the job. When the
      job has finished, the delegation tokens are invalidated.</p><p class="calibre2">Delegation tokens are automatically obtained for the default HDFS
      instance,
      but if your job needs to access other HDFS clusters, you can load the
      delegation tokens for these by setting the <code class="literal">mapreduce.job.hdfs-servers</code> job <a class="calibre" id="calibre_link-2547"></a>property to a comma-separated list of HDFS URIs.</p></div><div class="book" title="Other Security Enhancements"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4346">Other Security Enhancements</h3></div></div></div><p class="calibre2">Security has been <a class="calibre" id="calibre_link-3308"></a>tightened throughout the Hadoop stack to protect against
      unauthorized access to resources. The more notable features are listed
      here:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">Tasks can be run using the operating system account for the
          user who submitted the job, rather than the user running the node
          manager. This means
          that the operating system is used to isolate running tasks, so they
          can’t send signals to each other (to kill another user’s tasks, for
          example) and so local information, such as task data, is kept
          private via local filesystem permissions.</p><p class="calibre2">This feature is enabled by <a class="calibre" id="calibre_link-3863"></a><a class="calibre" id="calibre_link-2327"></a>setting <code class="literal">yarn.nodemanager.container-executor.class</code>
          to <code class="literal">org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</code>.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-53" id="calibre_link-65">75</a>]</sup> In addition, administrators need to ensure that each
          user is given an account on every node in the cluster (typically
          using LDAP).</p></li><li class="listitem"><p class="calibre2">When tasks are run as the user who submitted the job, the
          distributed cache (see <a class="ulink" href="#calibre_link-54" title="Distributed Cache">Distributed Cache</a>) is
          secure. Files that are world-readable are put in a shared cache (the
          insecure default); otherwise, they go in a private cache, readable
          only by the owner.</p></li><li class="listitem"><p class="calibre2">Users can view and modify only their own jobs, not others.
          This is enabled by <a class="calibre" id="calibre_link-2528"></a>setting <code class="literal">mapreduce.cluster.acls.enabled</code> to <code class="literal">true</code>. There are two job configuration
          properties, <code class="literal">mapreduce.job.acl-view-job</code> and <code class="literal">mapreduce.job.acl-modify-job</code>, which may
          <a class="calibre" id="calibre_link-2544"></a><a class="calibre" id="calibre_link-2543"></a>be set to a comma-separated list of users to control
          who may view or modify a particular job.</p></li><li class="listitem"><p class="calibre2">The shuffle is secure, preventing a malicious user from
          requesting another user’s map outputs.</p></li><li class="listitem"><p class="calibre2">When appropriately configured, it’s no longer possible for a
          malicious user to run a rogue secondary namenode, datanode, or node
          manager that can join the cluster and potentially compromise data
          stored in the cluster. This is enforced by requiring daemons to
          authenticate with the master node they are connecting to.</p><p class="calibre2">To enable this feature, you first need to configure Hadoop to
          use a keytab previously generated with the <code class="literal">ktutil</code> command. For a datanode, for
          example, you would set <a class="calibre" id="calibre_link-1461"></a>the <code class="literal">dfs.datanode.keytab.file</code> property to the
          keytab filename <a class="calibre" id="calibre_link-1460"></a>and <code class="literal">dfs.datanode.kerberos.principal</code> to the
          username to use for the datanode. Finally, the ACL <a class="calibre" id="calibre_link-1375"></a>for the <code class="literal">DataNodeProtocol</code> (which
          is used by datanodes to communicate with the namenode) must be set
          in <em class="calibre10">hadoop-policy.xml</em>, by
          <a class="calibre" id="calibre_link-3313"></a>restricting <code class="literal">security.datanode.protocol.acl</code> to the
          datanode’s username.</p></li><li class="listitem"><p class="calibre2">A datanode may be run on a privileged port (one lower than
          1024), so a client may be reasonably sure that it was started
          securely.</p></li><li class="listitem"><p class="calibre2">A task may communicate only with its parent application
          master, thus preventing an attacker from obtaining MapReduce data
          from another user’s job.</p></li><li class="listitem"><p class="calibre2">Various parts of Hadoop can be configured to encrypt network
          data, including <a class="calibre" id="calibre_link-1860"></a>RPC (<code class="literal">hadoop.rpc.protection</code>), HDFS block
          <a class="calibre" id="calibre_link-1465"></a>transfers (<code class="literal">dfs.encrypt.data.transfer</code>), the MapReduce
          shuffle <a class="calibre" id="calibre_link-2625"></a>(<code class="literal">mapreduce.shuffle.ssl.enabled</code>), and the
          web UIs (<code class="literal">hadokop.ssl.enabled</code>).
          Work is <a class="calibre" id="calibre_link-1863"></a>ongoing to encrypt data “at rest,” too, so that HDFS
          blocks can be stored in encrypted <a class="calibre" id="calibre_link-1138"></a><a class="calibre" id="calibre_link-3309"></a>form, for example.</p></li></ul></div></div></div><div class="book" title="Benchmarking a Hadoop Cluster"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4347">Benchmarking a Hadoop Cluster</h2></div></div></div><p class="calibre2">Is the cluster set up correctly? <a class="calibre" id="calibre_link-1113"></a><a class="calibre" id="calibre_link-983"></a>The best way to answer this question is empirically: run
    some jobs and confirm that you get the expected results. Benchmarks make
    good tests because you also get numbers that you can compare with other
    clusters as a sanity check on whether your new cluster is performing
    roughly as expected. And you can tune a cluster using benchmark results to
    squeeze the best performance out of it. This is often done with monitoring
    systems in place (see <a class="ulink" href="#calibre_link-55" title="Monitoring">Monitoring</a>), so you can see how
    resources are being used across the cluster.</p><p class="calibre2">To get the best results, you should run benchmarks on a cluster that
    is not being used by others. In practice, this will be just before it is
    put into service and users start relying on it. Once users have scheduled
    periodic jobs on a cluster, it is generally impossible to find a time when
    the cluster is not being used (unless you arrange downtime with users), so
    you should run benchmarks to your satisfaction before this happens.</p><p class="calibre2">Experience has shown that most hardware failures for new systems are
    hard drive failures. By running I/O-intensive benchmarks—such as the ones
    described next—you can “burn in” the cluster before it goes live.</p><div class="book" title="Hadoop Benchmarks"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4348">Hadoop Benchmarks</h3></div></div></div><p class="calibre2">Hadoop comes with several benchmarks that you can run very easily
      with minimal setup cost. Benchmarks are packaged in the tests JAR file,
      and you can get a list of them, with descriptions, by invoking the JAR
      file with no arguments:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-*-tests.jar</code></strong></pre><p class="calibre2">Most of the benchmarks show usage instructions when invoked with
      no arguments. For example:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-*-tests.jar \
  TestDFSIO</code></strong>
TestDFSIO.1.7
Missing arguments.
Usage: TestDFSIO [genericOptions] -read [-random | -backward |
-skip [-skipSize Size]] | -write | -append | -clean [-compression codecClassName]
[-nrFiles N] [-size Size[B|KB|MB|GB|TB]] [-resFile resultFileName]
[-bufferSize Bytes] [-rootDir]</pre><div class="book" title="Benchmarking MapReduce with TeraSort"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4349">Benchmarking MapReduce with TeraSort</h4></div></div></div><p class="calibre2">Hadoop comes with a <a class="calibre" id="calibre_link-2446"></a><a class="calibre" id="calibre_link-3647"></a>MapReduce program called <span class="calibre"><em class="calibre10">TeraSort</em></span>
        that does a total sort of its input.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-56" id="calibre_link-66">76</a>]</sup> It is very useful for benchmarking <a class="calibre" id="calibre_link-1930"></a>HDFS and MapReduce together, as the full input dataset
        is transferred through the shuffle. The three steps are: generate some
        random data, perform the sort, then validate the results.</p><p class="calibre2">First, we generate some random data <a class="calibre" id="calibre_link-3646"></a>using <code class="literal">teragen</code> (found
        in the examples JAR file, not the tests one). It runs a map-only job
        that generates a specified number of rows of binary data. Each row is
        100 bytes long, so to generate one terabyte of data using 1,000 maps,
        run the following (<code class="literal">10t</code> is short for
        10 trillion):</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar \
  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \
  teragen -Dmapreduce.job.maps=1000 10t random-data</code></strong></pre><p class="calibre2">Next, run <code class="literal">terasort</code>:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar \
  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \
  terasort random-data sorted-data</code></strong></pre><p class="calibre2">The overall execution time of the sort is the metric we are
        interested in, but it’s instructive to watch the job’s progress via
        the web UI
        (<span class="calibre"><em class="calibre10">http://<em class="replaceable"><code class="replaceable">resource-manager-host</code></em>:8088/</em></span>),
        where you can get a feel for how long each phase of the job takes.
        Adjusting the parameters
        mentioned in <a class="ulink" href="#calibre_link-57" title="Tuning a Job">Tuning a Job</a> is a useful exercise,
        too.</p><p class="calibre2">As a final sanity check, we validate that the data in <em class="calibre10">sorted-data</em> is, in fact, correctly
        sorted:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar \
  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \
  teravalidate sorted-data report</code></strong></pre><p class="calibre2">This command runs a short MapReduce job that performs a series
        of checks on the sorted data to check whether the sort is accurate.
        Any errors can be found in the <em class="calibre10">report/part-r-00000</em> output file.</p></div><div class="book" title="Other benchmarks"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4350">Other benchmarks</h4></div></div></div><p class="calibre2">There are many more Hadoop benchmarks, but the following are
        widely used:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2"><span class="calibre"><em class="calibre10">TestDFSIO</em></span> tests <a class="calibre" id="calibre_link-3648"></a>the I/O performance of HDFS. It does this by using a
            MapReduce job as a convenient way to read or write files in
            parallel.</p></li><li class="listitem"><p class="calibre2"><span class="calibre"><em class="calibre10">MRBench</em></span> (invoked <a class="calibre" id="calibre_link-2708"></a>with <code class="literal">mrbench</code>)
            runs a small job a number of times. It acts as a good counterpoint
            to TeraSort, as it checks whether small job runs are
            responsive.</p></li><li class="listitem"><p class="calibre2"><span class="calibre"><em class="calibre10">NNBench</em></span> (invoked <a class="calibre" id="calibre_link-2792"></a>with <code class="literal">nnbench</code>) is
            useful for load-testing namenode hardware.</p></li><li class="listitem"><p class="calibre2"><em class="calibre10">Gridmix</em> is a <a class="calibre" id="calibre_link-1811"></a>suite of benchmarks designed to model a realistic
            cluster workload by mimicking a variety of <u style="
    text-decoration: underline 0.1em;
">data-access pattern</u>s
            seen in practice. <br>See the documentation in the distribution for
            how to run Gridmix.</p></li><li class="listitem"><p class="calibre2"><em class="calibre10">SWIM</em>, <a class="calibre" id="calibre_link-3567"></a>or the <a class="ulink" href="https://github.com/SWIMProjectUCB/SWIM/wiki" target="_top">Statistical
            Workload Injector for MapReduce</a>, is a repository of
            real-life MapReduce workloads that you can use to generate
            representative test workloads for your system.</p></li><li class="listitem"><p class="calibre2"><a class="ulink" href="http://www.tpc.org/tpcx-hs/" target="_top"><em class="calibre10">TPCx-HS</em></a>
            is a<a class="calibre" id="calibre_link-3712"></a> standardized benchmark based on TeraSort from the
            Transaction Processing Performance Council.</p></li></ul></div></div></div><div class="book" title="User Jobs"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4351">User Jobs</h3></div></div></div><p class="calibre2">For tuning, <a class="calibre" id="calibre_link-2254"></a>it is best to include a few jobs that are representative
      of the jobs that your users run, so your cluster is tuned for these and
      not just for the standard benchmarks. If this is your first Hadoop
      cluster and you don’t have any user jobs yet, then either Gridmix or
      SWIM is a good substitute.</p><p class="calibre2">When running your own jobs as benchmarks, you should select a
      dataset for your user jobs and use it each time you run the benchmarks
      to allow comparisons between runs. When you set up a new cluster or
      upgrade a cluster, you will be able to use the same dataset to compare
      the performance with previous <a class="calibre" id="calibre_link-1114"></a><a class="calibre" id="calibre_link-984"></a>runs.</p></div></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" type="footnote" id="calibre_link-23"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-58">68</a>] </sup>ECC memory is strongly recommended, as several Hadoop
              users have reported seeing many checksum errors when using
              non-ECC memory on Hadoop clusters.</p></div><div class="footnote" type="footnote" id="calibre_link-27"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-59">69</a>] </sup>The <code class="literal">mapred</code> user doesn’t use
          SSH, as in Hadoop 2 and later, the only MapReduce daemon is the job
          history server.</p></div><div class="footnote" type="footnote" id="calibre_link-29"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-60">70</a>] </sup>See its man page for instructions on how to start <em class="calibre10">ssh-agent</em>.</p></div><div class="footnote" type="footnote" id="calibre_link-31"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-61">71</a>] </sup>There can be more than one namenode when running HDFS
              HA.</p></div><div class="footnote" id="calibre_link-36"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-62">72</a>] </sup>For more discussion on the security implications of SSH host
            keys, consult the article <a class="ulink" href="http://www.securityfocus.com/infocus/1806" target="_top">“SSH Host Key
            Protection”</a> by Brian Hatch.</p></div><div class="footnote" type="footnote" id="calibre_link-40"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-63">73</a>] </sup>Notice that there is no site file for MapReduce shown here.
          This is because the only MapReduce daemon is the job history server,
          and the defaults are sufficient.</p></div><div class="footnote" type="footnote" id="calibre_link-51"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-64">74</a>] </sup>To use Kerberos authentication with Hadoop, you need to
            install, configure, and run a KDC (Hadoop does not come with one).
            Your organization may already have a KDC you can use (an Active
            Directory installation, for example); if not, you can set up an
            MIT Kerberos 5 KDC.</p></div><div class="footnote" type="footnote" id="calibre_link-53"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-65">75</a>] </sup><code class="literal">LinuxTaskController</code> uses a
              <a class="calibre" id="calibre_link-2328"></a>setuid executable called <em class="calibre10">container-executor</em>, found in the
              <em class="calibre10">bin</em> directory. You should
              ensure that this binary is owned by root and has the setuid bit
              set (with <code class="literal">chmod +s</code>).</p></div><div class="footnote" type="footnote" id="calibre_link-56"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-66">76</a>] </sup>In 2008, TeraSort was used to break the world record for
            sorting 1 TB of data; see <a class="ulink" href="#calibre_link-67" title="A Brief History of Apache Hadoop">A Brief History of Apache Hadoop</a>.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-0"><section type="chapter" id="calibre_link-4352" title="Chapter&nbsp;11.&nbsp;Administering Hadoop"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;11.&nbsp;Administering Hadoop</h2></div></div></div><p class="calibre2">The previous chapter was devoted to setting up a Hadoop cluster. In
  this chapter, we look at the procedures to keep a cluster running
  smoothly.</p><div class="book" title="HDFS"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4353">HDFS</h2></div></div></div><div class="book" title="Persistent Data Structures"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4354">Persistent Data Structures</h3></div></div></div><p class="calibre2">As an administrator, <a class="calibre" id="calibre_link-3575"></a><a class="calibre" id="calibre_link-1952"></a><a class="calibre" id="calibre_link-2980"></a><a class="calibre" id="calibre_link-1361"></a><a class="calibre" id="calibre_link-4355"></a><a class="calibre" id="calibre_link-1131"></a>it is invaluable to have a basic understanding of how the
      components of HDFS—the namenode, the secondary namenode, and the
      datanodes—organize their
      persistent data on disk. Knowing which files are which can help you
      diagnose problems or spot that something is awry.</p><div class="book" title="Namenode directory structure"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4356">Namenode directory structure</h4></div></div></div><p class="calibre2">A running namenode <a class="calibre" id="calibre_link-2753"></a><a class="calibre" id="calibre_link-1506"></a>has a directory structure like this:</p><pre class="screen1">${dfs.namenode.name.dir}/
├── current
│&nbsp;&nbsp; ├── VERSION
│&nbsp;&nbsp; ├── edits_0000000000000000001-0000000000000000019
│&nbsp;&nbsp; ├── edits_inprogress_0000000000000000020
│&nbsp;&nbsp; ├── fsimage_0000000000000000000
│&nbsp;&nbsp; ├── fsimage_0000000000000000000.md5
│&nbsp;&nbsp; ├── fsimage_0000000000000000019
│&nbsp;&nbsp; ├── fsimage_0000000000000000019.md5
│&nbsp;&nbsp; └── seen_txid
└── in_use.lock</pre><p class="calibre2">Recall from <a class="ulink" href="#calibre_link-1" title="Chapter&nbsp;10.&nbsp;Setting Up a Hadoop Cluster">Chapter&nbsp;10</a> that <a class="calibre" id="calibre_link-1475"></a>the <code class="literal">dfs.namenode.name.dir</code> property is a list of
        directories, with the same contents mirrored in each directory. This
        mechanism provides resilience, particularly if one of the directories
        is an NFS mount, as is recommended.</p><p class="calibre2">The <em class="calibre10">VERSION</em> file is a
        <a class="calibre" id="calibre_link-3760"></a>Java properties file that contains information about the
        version of HDFS that is running. Here are the contents of a typical
        file:</p><pre class="screen1">#Mon Sep 29 09:54:36 BST 2014
namespaceID=1342387246
clusterID=CID-01b5c398-959c-4ea8-aae6-1e0d9bd8b142
cTime=0
storageType=NAME_NODE
blockpoolID=BP-526805057-127.0.0.1-1411980876842
layoutVersion=-57</pre><p class="calibre2">The <code class="literal">layoutVersion</code> is a
        negative integer that defines the version of HDFS’s persistent data
        structures. This version number has no relation to the release number
        of the Hadoop distribution. Whenever the layout changes, the version
        number is decremented (for example, the version after −57 is −58).
        When this happens, HDFS needs to be upgraded, since a newer namenode
        (or datanode) will not operate if its storage layout is an older
        version. Upgrading HDFS is covered in <a class="ulink" href="#calibre_link-2" title="Upgrades">Upgrades</a>.</p><p class="calibre2">The <code class="literal">namespaceID</code> is a
        <a class="calibre" id="calibre_link-2769"></a>unique identifier for the filesystem namespace, which is
        created when the namenode is first formatted. The <code class="literal">clusterID</code> is a unique <a class="calibre" id="calibre_link-1107"></a>identifier for the HDFS cluster as a whole; this is
        important for HDFS federation (see <a class="ulink" href="#calibre_link-3" title="HDFS Federation">HDFS Federation</a>),
        where a cluster is made up of multiple namespaces and each namespace
        is managed by one namenode. The <code class="literal">blockpoolID</code> is a <a class="calibre" id="calibre_link-1009"></a>unique identifier for the block pool containing all the
        files in the namespace managed by this namenode.</p><p class="calibre2">The <code class="literal">cTime</code> property <a class="calibre" id="calibre_link-1314"></a>marks the creation time of the namenode’s storage. For
        newly formatted storage, the value is always zero, but it is updated
        to a timestamp whenever the filesystem is upgraded.</p><p class="calibre2">The <code class="literal">storageType</code> indicates
        that this storage directory contains data structures for a
        namenode.</p><p class="calibre2">The <em class="calibre10">in_use.lock</em> file is a
        lock file that the namenode uses to lock the storage directory. This
        prevents another namenode instance from running at the same time with
        (and possibly corrupting) the same storage directory.</p><p class="calibre2">The other files in the namenode’s storage directory are
        <a class="calibre" id="calibre_link-1561"></a><a class="calibre" id="calibre_link-1755"></a>the <em class="calibre10">edits</em> and
        <em class="calibre10">fsimage</em> files, and <em class="calibre10">seen_txid</em>. To understand what these files
        are for, we need to dig into the workings of the namenode a <a class="calibre" id="calibre_link-2754"></a><a class="calibre" id="calibre_link-1507"></a>little more.</p></div><div class="book" title="The filesystem image and edit log"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-24">The filesystem image and edit log</h4></div></div></div><p class="calibre2">When a filesystem <a class="calibre" id="calibre_link-1682"></a><a class="calibre" id="calibre_link-2675"></a><a class="calibre" id="calibre_link-1559"></a><a class="calibre" id="calibre_link-2355"></a>client performs a write operation (such as creating or
        moving a file), the transaction is first recorded in the edit log. The
        namenode also has an in-memory representation of the filesystem
        metadata, which it updates after the edit log has been modified. The
        in-memory metadata is used to serve read requests.</p><p class="calibre2">Conceptually the edit log is a single entity, but it is
        represented as a number of files on disk. Each file is called a
        <em class="calibre10">segment</em>, and has the prefix <em class="calibre10">edits</em> and a suffix that indicates the
        transaction IDs contained in it. Only one file is open for writes at
        any one time (<em class="calibre10">edits_inprogress_0000000000000000020</em>
        in the preceding example), and it is flushed and synced after
        every transaction before a success code is returned to the client. For
        namenodes that write to multiple directories, the write must be
        flushed and synced to every copy before returning successfully. This
        ensures that no transaction is lost due to machine failure.</p><p class="calibre2">Each <em class="calibre10">fsimage</em> file is a
        complete persistent checkpoint of the filesystem metadata. (The suffix
        indicates the last transaction in the image.) However, it is <span class="calibre">not</span> updated for every filesystem write
        operation, because writing out the <em class="calibre10">fsimage</em> file, which can grow to be
        gigabytes in size, would be very slow. This does not compromise
        resilience because if the namenode fails, then the latest state of its
        metadata can be reconstructed by loading the latest <em class="calibre10">fsimage</em> from disk into memory, and then
        applying each of the transactions from the relevant point onward in
        the edit log. In fact, this is precisely what the namenode does when
        it starts up (see <a class="ulink" href="#calibre_link-4" title="Safe Mode">Safe Mode</a>).</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Each <em class="calibre10">fsimage</em> file
          contains a serialized form of all the directory and file inodes in
          the filesystem. Each inode is an internal representation of a file
          or directory’s metadata and contains such information as the file’s
          replication level, modification and access times, access
          permissions, block size, and the blocks the file is made up of. For
          directories, the modification time, permissions, and quota metadata
          are stored.</p><p class="calibre2">An <em class="calibre10">fsimage</em> file does
          <span class="calibre">not</span> record the datanodes on which
          the blocks are stored. Instead, the namenode keeps this mapping in
          memory, which it constructs by asking the datanodes for their block
          lists when they join the cluster and periodically afterward to
          ensure the namenode’s block mapping is up to date.</p></div><p class="calibre2">As described, the edit log would grow without bound (even if it
        was spread across several physical <em class="calibre10">edits</em> files). Though this state of affairs
        would have no impact on the system while the namenode is running, if
        the namenode were restarted, it would take a long time to apply each
        of the transactions in its (very long) edit log. During this time, the
        filesystem would be offline, which is generally undesirable.</p><p class="calibre2">The solution is to run the secondary namenode, whose purpose is
        to produce <a class="calibre" id="calibre_link-1093"></a><a class="calibre" id="calibre_link-2743"></a>checkpoints of the primary’s in-memory filesystem
        metadata.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-5" id="calibre_link-20">77</a>]</sup> The checkpointing process proceeds as follows (and is
        shown schematically in <a class="ulink" href="#calibre_link-6" title="Figure&nbsp;11-1.&nbsp;The checkpointing process">Figure&nbsp;11-1</a> for
        the edit log and image files shown earlier):</p><div class="book"><ol class="orderedlist"><li class="listitem"><p class="calibre2">The secondary asks the primary to roll its in-progress
            <em class="calibre10">edits</em> file, so new edits go
            to a new file. The primary also updates the <em class="calibre10">seen_txid</em> file in all its storage
            directories.</p></li><li class="listitem"><p class="calibre2">The secondary retrieves the latest <em class="calibre10">fsimage</em> and <em class="calibre10">edits</em> files from the primary (using
            HTTP GET).</p></li><li class="listitem"><p class="calibre2">The secondary loads <em class="calibre10">fsimage</em> into memory, applies each
            transaction from <em class="calibre10">edits</em>, then
            creates a new merged <em class="calibre10">fsimage</em>
            file.</p></li><li class="listitem"><p class="calibre2">The secondary sends the new <em class="calibre10">fsimage</em> back to the primary (using HTTP
            PUT), and the primary saves it as a temporary <em class="calibre10">.ckpt</em> file.</p></li><li class="listitem"><p class="calibre2">The primary renames the temporary <em class="calibre10">fsimage</em> file to make it available.</p></li></ol></div><p class="calibre2">At the end of the process, the primary has an up-to-date
        <em class="calibre10">fsimage</em> file and a short
        in-progress <em class="calibre10">edits</em> file (it is not
        necessarily empty, as it may have received some edits while the
        checkpoint was being taken). It is possible for an administrator to
        run this process manually while the namenode is in safe mode, using
        the <code class="literal">hdfs dfsadmin </code><code class="literal">-saveNamespace</code> command.</p><p class="calibre2">This procedure makes it clear why the secondary has similar
        memory requirements to the primary (since it loads the <em class="calibre10">fsimage</em> into memory), which is the reason
        that the secondary needs a dedicated machine on large clusters.</p><p class="calibre2">The schedule for checkpointing is controlled by two
        configuration parameters. The <a class="calibre" id="calibre_link-3301"></a>secondary namenode checkpoints every <a class="calibre" id="calibre_link-1470"></a>hour (<code class="literal">dfs.namenode.checkpoint.period</code> in seconds),
        or sooner if the edit log has reached one million transactions since
        the last <a class="calibre" id="calibre_link-1471"></a>checkpoint (<code class="literal">dfs.namenode.checkpoint.txns</code>), which it
        checks every minute (<code class="literal">dfs.namenode.checkpoint.check.period</code> in
        <a class="calibre" id="calibre_link-1560"></a><a class="calibre" id="calibre_link-1683"></a><a class="calibre" id="calibre_link-2356"></a><a class="calibre" id="calibre_link-2676"></a><a class="calibre" id="calibre_link-1562"></a><a class="calibre" id="calibre_link-1756"></a>seconds).</p><div class="figure"><a id="calibre_link-6" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4357" class="calibre"></a><img alt="The checkpointing process" src="images/000058.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;11-1.&nbsp;The checkpointing process</div></div></div><div class="book" title="Secondary namenode directory structure"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4358">Secondary namenode directory structure</h4></div></div></div><p class="calibre2">The layout of the <a class="calibre" id="calibre_link-3302"></a><a class="calibre" id="calibre_link-1511"></a><a class="calibre" id="calibre_link-2765"></a>secondary’s checkpoint <a class="calibre" id="calibre_link-1469"></a>directory (<code class="literal">dfs.namenode.checkpoint.dir</code>) is identical to
        the namenode’s. This is by design, since in the event of total
        namenode failure (when there are no recoverable backups, even from
        NFS), it allows recovery from a secondary namenode. This can be
        achieved either by copying the relevant storage directory to a new
        namenode or, if the secondary is taking over as the new primary
        namenode, by using the <code class="literal">-importCheckpoint</code> option when starting the
        namenode daemon. The <code class="literal">-importCheckpoint</code> option will load the
        namenode metadata from the latest checkpoint in the directory defined
        by the <code class="literal">dfs.namenode.checkpoint.dir</code>
        property, but only if there is no metadata in the <code class="literal">dfs.namenode.name.dir</code> directory, <a class="calibre" id="calibre_link-1476"></a>to ensure that there is no risk of overwriting precious
        metadata.</p></div><div class="book" title="Datanode directory structure"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4359">Datanode directory structure</h4></div></div></div><p class="calibre2">Unlike namenodes, <a class="calibre" id="calibre_link-1386"></a><a class="calibre" id="calibre_link-1503"></a>datanodes do not need to be explicitly formatted,
        because they create their storage directories automatically on
        startup. Here are the key files and directories:</p><pre class="screen1">${dfs.datanode.data.dir}/
├── current
│&nbsp;&nbsp; ├── BP-526805057-127.0.0.1-1411980876842
│&nbsp;&nbsp; │&nbsp;&nbsp; └── current
│&nbsp;&nbsp; │&nbsp;&nbsp;  &nbsp;&nbsp; ├── VERSION
│&nbsp;&nbsp; │&nbsp;&nbsp;  &nbsp;&nbsp; ├── finalized
│&nbsp;&nbsp; │&nbsp;&nbsp;  &nbsp;&nbsp; │&nbsp;&nbsp; ├── blk_1073741825
│&nbsp;&nbsp; │&nbsp;&nbsp;  &nbsp;&nbsp; │&nbsp;&nbsp; ├── blk_1073741825_1001.meta
│&nbsp;&nbsp; │&nbsp;&nbsp;  &nbsp;&nbsp; │&nbsp;&nbsp; ├── blk_1073741826
│&nbsp;&nbsp; │&nbsp;&nbsp;  &nbsp;&nbsp; │&nbsp;&nbsp; └── blk_1073741826_1002.meta
│&nbsp;&nbsp; │&nbsp;&nbsp;  &nbsp;&nbsp; └── rbw
│&nbsp;&nbsp; └── VERSION
└── in_use.lock</pre><p class="calibre2">HDFS blocks are stored in files with a <em class="calibre10">blk_</em> prefix; they consist of the raw bytes
        of a portion of the file being stored. Each block has an associated
        metadata file with a <em class="calibre10">.meta</em>
        suffix. It is made up of a header with version and type information,
        followed by a series of checksums for sections of the block.</p><p class="calibre2">Each block belongs to a block pool, and each block pool has its
        own storage directory that is formed from its ID (it’s the same block
        pool ID from the namenode’s <span class="calibre"><em class="calibre10">VERSION</em></span>
        file).</p><p class="calibre2">When the number of blocks in a directory grows to a certain
        size, the datanode creates a new subdirectory in which to place new
        blocks and their accompanying metadata. It creates a new subdirectory
        every time the number of blocks in a directory reaches 64 (set by
        <a class="calibre" id="calibre_link-1462"></a>the <code class="literal">dfs.datanode.numblocks</code> configuration
        property). The effect is to have a tree with high fan-out, so even for
        systems with a very large number of blocks, the directories will be
        only a few levels deep. By taking this measure, the datanode ensures
        that there is a manageable number of files per directory, which avoids
        the problems that most operating systems encounter when there are a
        large number of files (tens or hundreds of thousands) in a single
        directory.</p><p class="calibre2">If the configuration property <code class="literal">dfs.datanode.data.dir</code> specifies multiple
        directories on different drives, blocks are written in a round-robin
        fashion. Note that blocks are <span class="calibre">not</span>
        replicated on each drive on a single datanode; instead, block
        replication is across <a class="calibre" id="calibre_link-1953"></a><a class="calibre" id="calibre_link-1362"></a><a class="calibre" id="calibre_link-1132"></a>distinct datanodes.</p></div></div><div class="book" title="Safe Mode"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4">Safe Mode</h3></div></div></div><p class="calibre2">When the namenode <a class="calibre" id="calibre_link-1956"></a><a class="calibre" id="calibre_link-3258"></a><a class="calibre" id="calibre_link-1135"></a>starts, the first thing it does is load its <a class="calibre" id="calibre_link-1757"></a>image file (<em class="calibre10">fsimage</em>)
      into memory and apply the edits from the edit log. Once it has
      reconstructed a consistent in-memory image of the filesystem metadata,
      it creates a new <em class="calibre10">fsimage</em> file
      (effectively doing the checkpoint
      itself, without recourse to the secondary namenode) and an empty edit
      log. During this process, the <a class="calibre" id="calibre_link-2761"></a>namenode is running in <em class="calibre10">safe mode</em>,
      which means that it offers only a read-only view of the filesystem to
      clients.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">Strictly speaking, in safe mode, only filesystem operations that
        access the filesystem metadata (such as producing a directory listing)
        are guaranteed to work. Reading a file will work only when the blocks
        are available on the current set of datanodes in the cluster, and file
        modifications (writes, deletes, or renames) will always fail.</p></div><p class="calibre2">Recall that the locations of blocks in the system are not
      persisted by the namenode; this information resides with the datanodes,
      in the form of a list of the blocks each one is storing. During normal
      operation of the system, the namenode has a map of block locations
      stored in memory. Safe mode is needed to give the datanodes time to
      check in to the namenode with their block lists, so the namenode can be
      informed of enough block locations to run the filesystem effectively. If
      the namenode didn’t wait for enough datanodes to check in, it would start the
      process of replicating blocks to new datanodes, which would be
      unnecessary in most cases (because it only needed to wait for the extra
      datanodes to check in) and would put a great strain on the cluster’s
      resources. Indeed, while in safe mode, the namenode does not issue any
      block-replication or deletion instructions to datanodes.</p><p class="calibre2">Safe mode is exited when <a class="calibre" id="calibre_link-2700"></a>the <em class="calibre10">minimal replication condition</em>
      is reached, plus an extension time of 30 seconds. The minimal
      replication condition is when 99.9% of the blocks in the whole
      filesystem meet their minimum replication level (which defaults to 1 and
      is set <a class="calibre" id="calibre_link-1477"></a>by <code class="literal">dfs.namenode.replication.min</code>; see <a class="ulink" href="#calibre_link-7" title="Table&nbsp;11-1.&nbsp;Safe mode properties">Table&nbsp;11-1</a>).</p><p class="calibre2">When you are starting a newly formatted HDFS cluster, the namenode
      does not go into safe mode, since there are no blocks in the
      system.</p><div class="table"><a id="calibre_link-7" class="calibre"></a><div class="table-title">Table&nbsp;11-1.&nbsp;Safe mode properties</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre36"><col class="calibre39"><col class="calibre39"><col class="c3"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Type</td><td class="calibre20">Default
              value</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">dfs.namenode.replication.min</code></td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">1</td><td class="calibre25">The minimum number of replicas that have to be written
              for a write to be successful.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">dfs.namenode.safemode.threshold-pct</code></td><td class="calibre23"><code class="uri">float</code></td><td class="calibre23">0.999</td><td class="calibre25">The <a class="calibre" id="calibre_link-1480"></a>proportion of blocks in the system that must
              meet the minimum
              replication level defined by
              <code class="uri">dfs.namenode.replication.min</code> before the
              namenode will exit safe mode. Setting this value to 0 or less
              forces the namenode not to start in safe mode. Setting this
              value to more than 1 means the namenode never exits safe
              mode.</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">dfs.namenode.safemode.extension</code></td><td class="calibre27"><code class="uri">int</code></td><td class="calibre27">30000</td><td class="calibre28">The <a class="calibre" id="calibre_link-1479"></a>time, in milliseconds, to extend safe mode after
              the minimum replication condition defined by <code class="uri">dfs.namenode.safemode.threshold-pct</code>
              has been satisfied. For small clusters (tens of nodes), it can
              be set to 0.</td></tr></tbody></table></div></div><div class="book" title="Entering and leaving safe mode"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4360">Entering and leaving safe mode</h4></div></div></div><p class="calibre2">To see whether the namenode is in safe mode, you can use
        <a class="calibre" id="calibre_link-1486"></a>the <code class="literal">dfsadmin</code>
        command:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hdfs dfsadmin -safemode get</code></strong>
Safe mode is ON</pre><p class="calibre2">The front page of the HDFS web UI provides another indication of
        whether the namenode is in safe mode.</p><p class="calibre2">Sometimes you want to wait for the namenode to exit safe mode
        before carrying out a command, particularly in scripts. The <code class="literal">wait</code> option achieves this:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hdfs dfsadmin -safemode wait</code></strong>
# command to read or write a file</pre><p class="calibre2">An administrator has the ability to make the namenode enter or
        leave safe mode at any time. It is sometimes necessary to do this when
        carrying out maintenance on the cluster or after upgrading a cluster,
        to confirm that data is still readable. To enter safe mode, use the
        following command:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hdfs dfsadmin -safemode enter</code></strong>
Safe mode is ON</pre><p class="calibre2">You can use this command when the namenode is still in safe mode
        while starting up to ensure that it never leaves safe mode. Another
        way of making sure that the namenode <span class="calibre">stays in safe mode indefinitely is to set
        the property <code class="literal">dfs.namenode</code></span><code class="literal">.safemode</code><code class="literal">.threshold-pct</code>
        to a value over 1.</p><p class="calibre2">You can make the namenode leave safe mode by using the
        <a class="calibre" id="calibre_link-1957"></a><a class="calibre" id="calibre_link-3259"></a><a class="calibre" id="calibre_link-1136"></a><a class="calibre" id="calibre_link-2762"></a>following:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hdfs dfsadmin -safemode leave</code></strong>
Safe mode is OFF</pre></div></div><div class="book" title="Audit Logging"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-692">Audit Logging</h3></div></div></div><p class="calibre2">HDFS can log all <a class="calibre" id="calibre_link-1925"></a><a class="calibre" id="calibre_link-925"></a><a class="calibre" id="calibre_link-1111"></a>filesystem access requests, a feature that some
      organizations require for auditing purposes. Audit logging is
      implemented using log4j logging at the <code class="literal">INFO</code> level. In the default configuration it is
      disabled, but it’s easy to enable by adding the following line to
      <em class="calibre10">hadoop-env.sh</em>:</p><pre class="screen1">export HDFS_AUDIT_LOGGER="INFO,RFAAUDIT"</pre><p class="calibre2">A log line is written to the audit log (<em class="calibre10">hdfs-audit.log</em>) for every HDFS event. Here’s
      an example for a list status request on <em class="calibre10">/user/tom</em>:</p><pre class="screen1">2014-09-30 21:35:30,484 INFO FSNamesystem.audit: allowed=true   ugi=tom
 (auth:SIMPLE)   ip=/127.0.0.1   cmd=listStatus  src=/user/tom   dst=null
 perm=null       proto=rpc</pre></div><div class="book" title="Tools"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4361">Tools</h3></div></div></div><div class="book" title="dfsadmin"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4362">dfsadmin</h4></div></div></div><p class="calibre2">The <em class="calibre10">dfsadmin</em> tool is a
        <a class="calibre" id="calibre_link-1961"></a><a class="calibre" id="calibre_link-1109"></a>multipurpose tool for finding information about the
        state of HDFS, as well as for performing administration operations on
        HDFS. It is invoked as <code class="literal">hdfs dfsadmin</code> and requires
        superuser privileges.</p><p class="calibre2">Some of the available commands to <em class="calibre10">dfsadmin</em> are described in <a class="ulink" href="#calibre_link-8" title="Table&nbsp;11-2.&nbsp;dfsadmin commands">Table&nbsp;11-2</a>. Use the <code class="literal">-help</code>
        command to get more information.</p><div class="table"><a id="calibre_link-8" class="calibre"></a><div class="table-title">Table&nbsp;11-2.&nbsp;dfsadmin commands</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Command</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">-help</code></td><td class="calibre25">Shows help for a given command, or all commands if no
                command is specified.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">-report</code></td><td class="calibre25">Shows filesystem statistics (similar to those shown in
                the web UI) and information on connected datanodes.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">-metasave</code></td><td class="calibre25">Dumps information to a file in Hadoop’s log directory
                about blocks that are being replicated or deleted, as well as
                a list of connected datanodes.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">-safemode</code></td><td class="calibre25">Changes or queries the state of safe mode. See <a class="ulink" href="#calibre_link-4" title="Safe Mode">Safe Mode</a>.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">-saveNamespace</code></td><td class="calibre25">Saves the current in-memory filesystem image to a new
                <em class="calibre10">fsimage</em> file and resets
                the <em class="calibre10">edits</em> file. This
                operation may be performed only in safe mode.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">-fetchImage</code></td><td class="calibre25">Retrieves the latest <span class="calibre"><em class="calibre10">fsimage</em></span> from
                the namenode and saves it in a local file.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">-refreshNodes</code></td><td class="calibre25">Updates the set of datanodes that are permitted to
                connect to the namenode. See <a class="ulink" href="#calibre_link-9" title="Commissioning and Decommissioning Nodes">Commissioning and Decommissioning Nodes</a>.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">-upgradeProgress</code></td><td class="calibre25">Gets information on the progress of an HDFS upgrade or
                forces an upgrade to proceed. See <a class="ulink" href="#calibre_link-2" title="Upgrades">Upgrades</a>.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">-finalizeUpgrade</code></td><td class="calibre25">Removes the previous version of the namenode and
                datanode storage directories. Used after an upgrade has been
                applied and the cluster is running successfully on the new
                version. See <a class="ulink" href="#calibre_link-2" title="Upgrades">Upgrades</a>.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">-setQuota</code></td><td class="calibre25">Sets directory quotas. Directory quotas set a limit on
                the number of names (files or directories) in the directory
                tree. Directory quotas are useful for preventing users from
                creating large numbers of small files, a measure that helps
                preserve the namenode’s memory (recall that accounting
                information for every file, directory, and block in the
                filesystem is stored in memory).</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">-clrQuota</code></td><td class="calibre25">Clears specified directory quotas.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">-setSpaceQuota</code></td><td class="calibre25">Sets space quotas on directories. Space quotas set a
                limit on the size of files that may be stored in a directory
                tree. They are useful for giving users a limited amount of
                storage.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">-clrSpaceQuota</code></td><td class="calibre25">Clears specified space quotas.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">-refreshServiceAcl</code></td><td class="calibre25">Refreshes the namenode’s service-level authorization
                policy file.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">-allowSnapshot</code></td><td class="calibre25">Allows snapshot creation for the specified
                directory.</td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">-disallowSnapshot</code></td><td class="calibre28">Disallows snapshot creation for the specified
                <a class="calibre" id="calibre_link-1487"></a>directory.</td></tr></tbody></table></div></div></div><div class="book" title="Filesystem check (fsck)"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-16">Filesystem check (fsck)</h4></div></div></div><p class="calibre2">Hadoop provides an <em class="calibre10">fsck</em>
        utility for checking the <a class="calibre" id="calibre_link-1746"></a><a class="calibre" id="calibre_link-1013"></a><a class="calibre" id="calibre_link-1670"></a>health of files in HDFS. The tool looks for blocks that
        are missing from all datanodes, as well as under- or over-replicated
        blocks. Here is an example of checking the whole filesystem for a
        small cluster:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hdfs fsck /</code></strong>
......................Status: HEALTHY
 Total size:	511799225 B
 Total dirs:	10
 Total files:	22
 Total blocks (validated):	22 (avg. block size 23263601 B)
 Minimally replicated blocks:	22 (100.0 %)
 Over-replicated blocks:	0 (0.0 %)
 Under-replicated blocks:	0 (0.0 %)
 Mis-replicated blocks:		0 (0.0 %)
 Default replication factor:	3
 Average block replication:	3.0
 Corrupt blocks:		0
 Missing replicas:		0 (0.0 %)
 Number of data-nodes:		4
 Number of racks:		1


The filesystem under path '/' is HEALTHY</pre><p class="calibre2"><em class="calibre10">fsck</em> recursively walks the
        filesystem namespace, starting at the given path (here the filesystem
        root), and checks the files it finds. It prints a dot for every file
        it checks. To check a file, <em class="calibre10">fsck</em>
        retrieves the metadata for the file’s blocks and looks for problems or
        inconsistencies. Note that <em class="calibre10">fsck</em>
        retrieves all of its information from the namenode; it does not
        communicate with any datanodes to actually retrieve any block
        data.</p><p class="calibre2">Most of the output from <em class="calibre10">fsck</em>
        is self-explanatory, but here are some of the conditions it looks
        for:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Over-replicated blocks</span></dt><dd class="calibre8"><p class="calibre2">These are blocks that exceed their target replication for
              the file they belong to. Normally, over-replication is not a
              problem, and HDFS will automatically delete excess
              replicas.</p></dd><dt class="calibre7"><span class="term">Under-replicated blocks</span></dt><dd class="calibre8"><p class="calibre2">These are blocks that do not meet their target replication
              for the file they belong to. HDFS will automatically create new
              replicas of under-replicated blocks until they meet the target
              replication. You can get information about the blocks being
              replicated (or waiting to be replicated) using <code class="literal">hdfs dfsadmin -metasave</code>.</p></dd><dt class="calibre7"><span class="term">Misreplicated blocks</span></dt><dd class="calibre8"><p class="calibre2">These are blocks that do not satisfy the block replica
              placement policy (see <a class="ulink" href="#calibre_link-10" title="Replica Placement">Replica Placement</a>). For
              example, for a replication level of three in a multirack
              cluster, if all three replicas of a block are on the same rack,
              then the block is misreplicated because the replicas should be
              spread across at least two racks for resilience. HDFS will
              automatically re-replicate misreplicated blocks so that they
              satisfy the rack placement policy.</p></dd><dt class="calibre7"><span class="term">Corrupt blocks</span></dt><dd class="calibre8"><p class="calibre2">These are blocks whose replicas are all corrupt. Blocks
              with at least one noncorrupt replica are not reported as
              corrupt; the namenode will replicate the noncorrupt replica
              until the target replication is met.</p></dd><dt class="calibre7"><span class="term">Missing replicas</span></dt><dd class="calibre8"><p class="calibre2">These are blocks with no replicas anywhere in the
              cluster.</p></dd></dl></div><p class="calibre2">Corrupt or missing blocks are the biggest cause for concern, as
        they mean data has been lost. By default, <em class="calibre10">fsck</em> leaves files with corrupt or missing
        blocks, but you can tell it to perform one of the following actions on
        them:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2"><span class="calibre">Move</span> the affected files
            to the <em class="calibre10">/lost+found</em> directory
            in HDFS, using the <code class="literal">-move</code>
            option. Files are broken into chains of contiguous blocks to aid
            any salvaging efforts you may attempt.</p></li><li class="listitem"><p class="calibre2"><span class="calibre">Delete</span> the affected
            files, using the <code class="literal">-delete</code>
            option. Files cannot be recovered after being deleted.</p></li></ul></div><div class="book" title="Finding the blocks for a file"><div class="titlepage1"><div class="book"><div class="book"><h5 class="title8" id="calibre_link-4363">Finding the blocks for a file</h5></div></div></div><p class="calibre2">The <em class="calibre10">fsck</em> tool provides an
          easy way to find out which blocks are in any particular file. For
          example:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hdfs fsck /user/tom/part-00007 -files -blocks -racks</code></strong>
/user/tom/part-00007 25582428 bytes, 1 block(s):  OK
0. blk_-3724870485760122836_1035 len=25582428 repl=3 [/default-rack/10.251.43.2:
50010,/default-rack/10.251.27.178:50010, /default-rack/10.251.123.163:50010]</pre><p class="calibre2">This says that the file <em class="calibre10">/user/tom/part-00007</em> is made up of one
          block and shows the datanodes where the block is located. The
          <em class="calibre10">fsck</em> options used are as
          follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">The <code class="literal">-files</code> option shows
              the line with the filename, size, number of blocks, and its
              health (whether there are any missing blocks).</p></li><li class="listitem"><p class="calibre2">The <code class="literal">-blocks</code> option
              shows information about each block in the file, one line per
              block.</p></li><li class="listitem"><p class="calibre2">The <code class="literal">-racks</code> option
              displays the rack location and the datanode addresses for each
              block.</p></li></ul></div><p class="calibre2">Running <code class="literal">hdfs fsck</code> without
          any arguments displays full usage <a class="calibre" id="calibre_link-1747"></a><a class="calibre" id="calibre_link-1671"></a><a class="calibre" id="calibre_link-1014"></a>instructions.</p></div></div><div class="book" title="Datanode block scanner"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-227">Datanode block scanner</h4></div></div></div><p class="calibre2">Every datanode runs a <a class="calibre" id="calibre_link-1377"></a><span class="calibre"><em class="calibre10">block scanner</em></span>, which periodically
        verifies all the blocks stored on the datanode. This allows bad blocks
        to be detected and fixed before they are read by clients. The scanner
        maintains a list of blocks to verify and scans them one by one for
        checksum errors. It employs a throttling mechanism to preserve disk
        bandwidth on the datanode.</p><p class="calibre2">Blocks are verified every three weeks to guard against disk
        errors over time (this period is controlled by the <code class="literal">dfs.datanode.scan.period.hours</code> property,
        <a class="calibre" id="calibre_link-1463"></a>which defaults to 504 hours). Corrupt blocks are
        reported to the namenode to be fixed.</p><p class="calibre2">You can get a block verification report for a datanode by
        visiting the datanode’s web interface at
        <span class="calibre"><em class="calibre10">http://<em class="replaceable"><code class="replaceable">datanode</code></em>:50075/blockScannerReport</em></span>.
        Here’s an example of a report, which should be
        self-explanatory:</p><pre class="screen1">Total Blocks                 :  21131
Verified in last hour        :     70
Verified in last day         :   1767
Verified in last week        :   7360
Verified in last four weeks  :  20057
Verified in SCAN_PERIOD      :  20057
Not yet verified             :   1074
Verified since restart       :  35912
Scans since restart          :   6541
Scan errors since restart    :      0
Transient scan errors        :      0
Current scan rate limit KBps :   1024
Progress this period         :    109%
Time left in cur period      :  53.08%</pre><p class="calibre2">If you specify the <code class="literal">listblocks</code>
        parameter,
        <span class="calibre"><em class="calibre10">http://<em class="replaceable"><code class="replaceable">datanode</code></em>:50075/blockScannerReport?listblocks</em></span>,
        the report is preceded by a list of all the blocks on the datanode
        along with their latest verification status. Here is a snippet of the
        block list (lines are split to fit the page):</p><pre class="screen1">blk_6035596358209321442    : status : ok     type : none   scan time :
 0                   not yet verified
blk_3065580480714947643    : status : ok     type : remote scan time :
 1215755306400       2008-07-11 05:48:26,400
blk_8729669677359108508    : status : ok     type : local  scan time :
 1215755727345       2008-07-11 05:55:27,345</pre><p class="calibre2">The first column is the block ID, followed by some key-value
        pairs. The status can be one of <code class="literal">failed</code> or <code class="literal">ok</code>, according to whether the last scan of
        the block detected a checksum error. The type of scan is <code class="literal">local</code> if it was performed by the background
        thread, <code class="literal">remote</code> if it was performed
        by a client or a remote datanode, or <code class="literal">none</code> if a scan of this block has yet to be
        made. The last piece of information is the scan time, which is
        displayed as the number of milliseconds since midnight on January 1,
        1970, and also as a more readable value.</p></div><div class="book" title="Balancer"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-17">Balancer</h4></div></div></div><p class="calibre2">Over time, the distribution of blocks across <a class="calibre" id="calibre_link-1376"></a><a class="calibre" id="calibre_link-975"></a><a class="calibre" id="calibre_link-1322"></a>datanodes can become unbalanced. An unbalanced cluster
        can affect locality for MapReduce, and it puts a greater strain on the
        highly utilized datanodes, so it’s best avoided.</p><p class="calibre2">The <em class="calibre10">balancer</em> program is a Hadoop daemon
        that redistributes blocks by moving them from overutilized datanodes
        to underutilized datanodes, while adhering to the block replica
        placement policy that makes data loss unlikely by placing block
        replicas on different racks (see <a class="ulink" href="#calibre_link-10" title="Replica Placement">Replica Placement</a>).
        It moves blocks until the cluster is deemed to be balanced, which
        means that the utilization of every datanode (ratio of used space on
        the node to total capacity of the node) differs from the utilization
        of the cluster (ratio of used space on the cluster to total capacity
        of the cluster) by no more than a given threshold percentage. You can
        start the balancer with:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">start-balancer.sh</code></strong></pre><p class="calibre2">The <code class="literal">-threshold</code> argument
        specifies the threshold percentage that defines what it means for the
        cluster to be balanced. The flag is optional; if omitted, the
        threshold is 10%. At any one time, only one balancer may be running on
        the cluster.</p><p class="calibre2">The balancer runs until the cluster is balanced, it cannot move
        any more blocks, or it loses contact with the namenode. It produces a
        logfile in the standard log directory, where it writes a line for
        every iteration of redistribution that it carries out. Here is the
        output from a short run on a small cluster (slightly reformatted to
        fit the page):</p><pre class="screen1">Time Stamp      Iteration#  Bytes Already Moved  ...Left To Move  ...Being Moved
Mar 18, 2009 5:23:42 PM  0                 0 KB        219.21 MB       150.29 MB
Mar 18, 2009 5:27:14 PM  1            195.24 MB         22.45 MB       150.29 MB
The cluster is balanced. Exiting...
Balancing took 6.072933333333333 minutes</pre><p class="calibre2">The balancer is designed to run in the background without unduly
        taxing the cluster or interfering with other clients using the
        cluster. It limits the bandwidth that it uses to copy a block from one
        node to another. The default is a modest 1 MB/s, but this can be
        changed by <a class="calibre" id="calibre_link-1455"></a>setting the <code class="literal">dfs.datanode.balance.bandwidthPerSec</code>
        property in <em class="calibre10">hdfs-site.xml</em>,
        specified <a class="calibre" id="calibre_link-3576"></a><a class="calibre" id="calibre_link-1962"></a><a class="calibre" id="calibre_link-1110"></a>in bytes.</p></div></div></div><div class="book" title="Monitoring"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-55">Monitoring</h2></div></div></div><p class="calibre2">Monitoring <a class="calibre" id="calibre_link-3577"></a><a class="calibre" id="calibre_link-2704"></a><a class="calibre" id="calibre_link-1127"></a>is an important part of system administration. In this
    section, we look at the monitoring facilities in Hadoop and how they can
    hook into external monitoring systems.</p><p class="calibre2">The purpose of monitoring is to detect when the cluster is not
    providing the expected level of service. The master daemons are the most
    important to monitor: the namenodes (primary and secondary) and the
    resource manager. Failure of datanodes and node managers is to be
    expected, particularly on larger clusters, so you should provide extra
    capacity so that the cluster can tolerate having a small percentage of
    dead nodes at any time.</p><p class="calibre2">In addition to the facilities described next, some administrators
    run test jobs on a periodic basis as a test of the cluster’s
    health.</p><div class="book" title="Logging"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-691">Logging</h3></div></div></div><p class="calibre2">All Hadoop <a class="calibre" id="calibre_link-2705"></a><a class="calibre" id="calibre_link-2361"></a><a class="calibre" id="calibre_link-1327"></a>daemons produce logfiles that can be very useful for
      finding out what is happening in the system. <a class="ulink" href="#calibre_link-11" title="System logfiles">System logfiles</a>
      explains how to configure these files.</p><div class="book" title="Setting log levels"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4364">Setting log levels</h4></div></div></div><p class="calibre2">When debugging a <a class="calibre" id="calibre_link-1408"></a>problem, it is very convenient to be able to change the
        log level temporarily for a particular component in the system.</p><p class="calibre2">Hadoop daemons have a web page for changing the log level for
        any log4j log name, which can be found at
        <span class="calibre"><em class="calibre10">/logLevel</em></span> in the daemon’s web UI. By convention,
        log names in Hadoop correspond to the names of the classes doing the
        logging, although there are exceptions to this rule, so you should
        consult the source code to find log names.</p><p class="calibre2">It’s also possible to enable logging for all packages that start
        with a given prefix. For example, to enable debug logging for all
        classes related to the resource manager, we would visit the its web UI
        at
        <span class="calibre"><em class="calibre10">http://<em class="replaceable"><code class="replaceable">resource-manager-host</code></em>:8088/logLevel</em></span>
        and set the log name <code class="literal">org.apache.hadoop.yarn.server.resourcemanager</code>
        to level <code class="literal">DEBUG</code>.</p><p class="calibre2">The same thing can be achieved from the command line as
        follows:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop daemonlog -setlevel <em class="replaceable1"><code class="calibre46">resource-manager-host</code></em>:8088 \</code></strong>
<strong class="userinput"><code class="calibre9">  org.apache.hadoop.yarn.server.resourcemanager DEBUG</code></strong></pre><p class="calibre2">Log levels changed in this way are reset when the daemon
        restarts, which is usually what you want. However, to make a
        persistent change to a log level, you can simply change <a class="calibre" id="calibre_link-2354"></a>the <em class="calibre10">log4j.properties</em> file in the configuration
        directory. In this case, the line to add is:</p><pre class="screen1">log4j.logger.org.apache.hadoop.yarn.server.resourcemanager=DEBUG</pre></div><div class="book" title="Getting stack traces"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4365">Getting stack traces</h4></div></div></div><p class="calibre2">Hadoop daemons expose a web page (<span class="calibre"><em class="calibre10">/stacks</em></span>
        in the web UI) that produces a thread dump for all running threads in
        the daemon’s JVM. For example, you can get a <a class="calibre" id="calibre_link-3684"></a><a class="calibre" id="calibre_link-3515"></a>thread dump for a <a class="calibre" id="calibre_link-3235"></a>resource manager from
        <span class="calibre"><em class="calibre10">http://<em class="replaceable"><code class="replaceable">resource-manager-host</code></em>:8088/stacks</em></span>.</p></div></div><div class="book" title="Metrics and JMX"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-33">Metrics and JMX</h3></div></div></div><p class="calibre2">The Hadoop <a class="calibre" id="calibre_link-2706"></a><a class="calibre" id="calibre_link-2694"></a>daemons collect information about events and measurements
      that are collectively known as <em class="calibre10">metrics</em>. For
      example, datanodes collect the following metrics (and many more): the
      number of bytes written, the number of blocks replicated, and the number
      of read requests from clients (both local and remote).</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">The metrics system in Hadoop 2 and later is sometimes referred
        to as <span class="calibre"><em class="calibre10">metrics2</em></span> to distinguish it from the older
        (now deprecated) metrics system in earlier versions of Hadoop.</p></div><p class="calibre2">Metrics belong to a <em class="calibre10">context</em>; “dfs,”
      “mapred,” “yarn,” and “rpc” are examples of different contexts. Hadoop
      daemons usually collect metrics under several contexts. For example,
      datanodes collect metrics for the “dfs” and “rpc” contexts.</p><div class="sidebar"><a id="calibre_link-4366" class="calibre"></a><div class="sidebar-title">How Do Metrics Differ from Counters?</div><p class="calibre2">The main difference <a class="calibre" id="calibre_link-1268"></a><a class="calibre" id="calibre_link-2692"></a>is their scope: metrics are collected by Hadoop daemons,
        whereas counters (see <a class="ulink" href="#calibre_link-12" title="Counters">Counters</a>) are collected for
        MapReduce tasks and aggregated for the whole job. They have different
        audiences, too: broadly speaking, metrics are for administrators, and
        counters are for MapReduce users.</p><p class="calibre2">The way they are collected and aggregated is also different.
        Counters are a MapReduce feature, and the MapReduce system ensures
        that counter values are propagated from the task JVMs where they are
        produced back to the application master, and finally back to the
        client running the MapReduce job. (Counters are propagated via RPC
        heartbeats; see <a class="ulink" href="#calibre_link-13" title="Progress and Status Updates">Progress and Status Updates</a>.) Both the
        task process and the application master perform aggregation.</p><p class="calibre2">The collection mechanism for metrics is decoupled from the
        component that receives the updates, and there are various pluggable
        outputs, including local files, Ganglia, and JMX. The daemon
        collecting the metrics performs aggregation on them before they are
        sent to the output.</p></div><p class="calibre2">All Hadoop metrics are published <a class="calibre" id="calibre_link-2206"></a><a class="calibre" id="calibre_link-2186"></a>to JMX (Java Management Extensions), so you can use
      standard JMX tools like JConsole (which comes with the JDK) to view
      them. For remote monitoring, you must set the JMX system <a class="calibre" id="calibre_link-1177"></a>property <code class="literal">com.sun.management</code><code class="literal">.jmxremote.port</code> (and others for security) to
      allow access. To do this for the namenode, say, you would set the
      following <a class="calibre" id="calibre_link-1878"></a><a class="calibre" id="calibre_link-1854"></a>in <em class="calibre10">hadoop-env.sh</em>:</p><pre class="screen1">HADOOP_NAMENODE_OPTS="-Dcom.sun.management.jmxremote.port=8004"</pre><p class="calibre2">You can also view JMX metrics (in JSON format) gathered by a
      particular Hadoop daemon by connecting to its <span class="calibre"><em class="calibre10">/jmx</em></span>
      web page. This is handy for debugging. For example, you can view
      namenode metrics at
      <span class="calibre"><em class="calibre10">http://<em class="replaceable"><code class="replaceable">namenode-host</code></em>:50070/jmx</em></span>.</p><p class="calibre2">Hadoop comes with a number of metrics sinks for publishing metrics
      to external systems, such as local files or the Ganglia monitoring
      system. Sinks are configured in <a class="calibre" id="calibre_link-1856"></a>the <em class="calibre10">hadoop-metrics2.properties</em> file; see that
      file for sample <a class="calibre" id="calibre_link-3578"></a><a class="calibre" id="calibre_link-1128"></a>configuration settings.</p></div></div><div class="book" title="Maintenance"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4367">Maintenance</h2></div></div></div><div class="book" title="Routine Administration Procedures"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-19">Routine Administration Procedures</h3></div></div></div><div class="book" title="Metadata backups"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4368">Metadata backups</h4></div></div></div><p class="calibre2">If the namenode’s persistent <a class="calibre" id="calibre_link-3579"></a><a class="calibre" id="calibre_link-4369"></a><a class="calibre" id="calibre_link-1125"></a><a class="calibre" id="calibre_link-2673"></a><a class="calibre" id="calibre_link-973"></a>metadata is lost or damaged, the entire filesystem is
        rendered unusable, so it is critical that backups are made of these
        files. You should keep multiple copies of different ages (one hour,
        one day, one week, and one month, say) to protect against corruption,
        either in the copies themselves or in the live files running on the
        namenode.</p><p class="calibre2">A straightforward way to make backups is to use <a class="calibre" id="calibre_link-1488"></a>the <code class="literal">dfsadmin</code> command to download a
        copy of the namenode’s most <a class="calibre" id="calibre_link-1758"></a>recent <span class="calibre"><em class="calibre10">fsimage</em></span>:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hdfs dfsadmin -fetchImage fsimage.backup</code></strong></pre><p class="calibre2">You can write a script to run this command from an offsite
        location to store archive copies of the <span class="calibre"><em class="calibre10">fsimage</em></span>.
        The script should additionally test the integrity of the copy. This
        can be done by starting a local namenode daemon and verifying that it
        has successfully read the <em class="calibre10">fsimage</em>
        and <em class="calibre10">edits</em> files into <a class="calibre" id="calibre_link-1563"></a>memory (by scanning the namenode log for the appropriate
        success message, for example).<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-14" id="calibre_link-22">78</a>]</sup></p></div><div class="book" title="Data backups"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4370">Data backups</h4></div></div></div><p class="calibre2">Although HDFS is designed to store <a class="calibre" id="calibre_link-972"></a>data reliably, data loss can occur, just like in any
        storage system; thus, a backup strategy is essential.
        With the large data volumes that Hadoop can store, deciding what data
        to back up and where to store it is a challenge. The key here is to
        prioritize your data. The highest priority is the data that cannot be
        regenerated and that is critical to the business; however, data that
        is either straightforward to regenerate or essentially disposable
        because it is of limited business value is the lowest priority, and
        you may choose not to make backups of this low-priority data.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">Do not make the mistake of thinking that HDFS replication is a
          substitute for making backups. Bugs in HDFS can cause replicas to be
          lost, and so can hardware failures. Although Hadoop is expressly
          designed so that hardware failure is very unlikely to result in data
          loss, the possibility can never be completely ruled out,
          particularly when combined with software bugs or human error.</p><p class="calibre2">When it comes to backups, think of HDFS in the same way as you
          would RAID. Although the data will survive the loss of an individual
          RAID disk, it may not survive if the RAID controller fails or is
          buggy (perhaps overwriting some data), or the entire array is
          damaged.</p></div><p class="calibre2">It’s common to have a policy for user directories in HDFS. For
        example, they may have space quotas and be backed up nightly. Whatever
        the policy, make sure your users know what it is, so they know what to
        expect.</p><p class="calibre2">The <em class="calibre10">distcp</em> tool is ideal for
        making backups to other HDFS clusters (preferably running on a
        different version of the software, to guard against loss due to bugs
        in HDFS) or other Hadoop filesystems (such as S3) because it can copy
        files in parallel. Alternatively, you can employ an entirely different
        storage system for backups, using one of the methods for exporting
        data from HDFS described in <a class="ulink" href="#calibre_link-15" title="Hadoop Filesystems">Hadoop Filesystems</a>.</p><p class="calibre2">HDFS allows administrators and users to take
        <em class="calibre10">snapshots</em> of the filesystem. A snapshot is a
        read-only copy of a filesystem subtree at a given point in time.
        Snapshots are very efficient since they do not copy data; they simply
        record each file’s metadata and block list, which is sufficient to
        reconstruct the filesystem contents at the time the snapshot was
        taken.</p><p class="calibre2">Snapshots are not a replacement for data backups, but they are a
        useful tool for point-in-time data recovery for files that were
        mistakenly deleted by users. You might have a policy of taking
        periodic snapshots and keeping them for a specific period of time
        according to age. For example, you might keep hourly snapshots for the
        previous day and daily snapshots for the previous month.</p></div><div class="book" title="Filesystem check (fsck)"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4371">Filesystem check (fsck)</h4></div></div></div><p class="calibre2">It is advisable to <a class="calibre" id="calibre_link-1748"></a>run HDFS’s <em class="calibre10">fsck</em> tool
        regularly (i.e., daily) on the whole filesystem to proactively look
        for missing or corrupt blocks. See <a class="ulink" href="#calibre_link-16" title="Filesystem check (fsck)">Filesystem check (fsck)</a>.</p></div><div class="book" title="Filesystem balancer"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4372">Filesystem balancer</h4></div></div></div><p class="calibre2">Run the balancer <a class="calibre" id="calibre_link-976"></a>tool (see <a class="ulink" href="#calibre_link-17" title="Balancer">Balancer</a>) regularly to
        keep the filesystem datanodes
        evenly <a class="calibre" id="calibre_link-3580"></a>balanced.</p></div></div><div class="book" title="Commissioning and Decommissioning Nodes"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-9">Commissioning and Decommissioning Nodes</h3></div></div></div><p class="calibre2">As an administrator of a <a class="calibre" id="calibre_link-3571"></a><a class="calibre" id="calibre_link-1197"></a>Hadoop cluster, you will need to add or remove nodes from
      time to time. For example, to grow the storage available to a cluster,
      you commission new nodes. Conversely, sometimes you may wish to shrink a
      cluster, and to do so, you decommission nodes. Sometimes it is necessary
      to decommission a node if it is misbehaving, perhaps because it is
      failing more often than it should or its performance is noticeably
      slow.</p><p class="calibre2">Nodes normally run both a datanode and a node manager, and both
      are typically commissioned or
      decommissioned in tandem.</p><div class="book" title="Commissioning new nodes"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4373">Commissioning new nodes</h4></div></div></div><p class="calibre2">Although commissioning a new node <a class="calibre" id="calibre_link-1379"></a><a class="calibre" id="calibre_link-2795"></a><a class="calibre" id="calibre_link-2746"></a><a class="calibre" id="calibre_link-3221"></a>can be as simple as configuring <a class="calibre" id="calibre_link-1977"></a>the <em class="calibre10">hdfs-site.xml</em>
        file to point to the namenode, configuring the <em class="calibre10">yarn-site.xml</em> file to point to the resource
        manager, and starting the datanode and resource manager daemons, it is
        generally best to have a list of authorized nodes.</p><p class="calibre2">It is a potential security risk to allow any machine to connect
        to the namenode and act as a datanode, because the machine may gain
        access to data that it is not authorized to see. Furthermore, because
        such a machine is not a real datanode, it is not under your control
        and may stop at any time, potentially causing data loss. (Imagine what
        would happen if a number of such nodes were connected and a block of
        data was present only on the “alien” nodes.) This scenario is a risk
        even inside a firewall, due to the possibility of misconfiguration, so datanodes (and node
        managers) should be explicitly managed on all production
        clusters.</p><p class="calibre2">Datanodes that are permitted to connect to the namenode are
        specified in a file whose name is specified by the <code class="literal">dfs.hosts</code> property. <a class="calibre" id="calibre_link-1467"></a>The file resides on the namenode’s local filesystem, and
        it contains a line for each datanode, specified by network address (as
        reported by the datanode; you can see what this is by looking at the
        namenode’s web UI). If you need to specify multiple network addresses
        for a datanode, put them on one line, separated by whitespace.</p><p class="calibre2">Similarly, node managers that may connect to the resource
        manager are specified in a file whose name is specified by the
        <code class="literal">yarn.resourcemanager.nodes.include-path</code>
        property. <a class="calibre" id="calibre_link-3893"></a>In most cases, there is one shared file, referred to as
        the <em class="calibre10">include file</em>, that both <code class="literal">dfs.hosts</code> and <code class="literal">yarn.resourcemanager.nodes.include-path</code>
        refer to, since nodes in the cluster run both datanode and node
        manager daemons.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">The file (or files) specified by the <code class="literal">dfs.hosts</code> and <code class="literal">yarn.resourcemanager.nodes.include-path</code>
          properties is different from<a class="calibre" id="calibre_link-3417"></a> the <em class="calibre10">slaves</em> file.
          The former is used by the namenode and resource manager to determine
          which worker nodes may connect. The <em class="calibre10">slaves</em> file is used by the Hadoop control
          scripts to perform cluster-wide operations, such as cluster
          restarts. It is never used by the Hadoop daemons.</p></div><p class="calibre2">To add new nodes to the cluster:</p><div class="book"><ol class="orderedlist"><li class="listitem"><p class="calibre2">Add the network addresses of the new nodes to the include
            file.</p></li><li class="listitem"><p class="calibre2">Update the namenode with the new set of permitted datanodes
            using this command:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hdfs dfsadmin -refreshNodes</code></strong></pre></li><li class="listitem"><p class="calibre2">Update the resource manager with the new set of permitted
            node managers using:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">yarn rmadmin -refreshNodes</code></strong></pre></li><li class="listitem"><p class="calibre2">Update the <em class="calibre10">slaves</em> file
            with the new nodes, so that they are included in future operations
            performed by the Hadoop control scripts.</p></li><li class="listitem"><p class="calibre2">Start the new datanodes and node managers.</p></li><li class="listitem"><p class="calibre2">Check that the new datanodes and node managers appear in the
            web UI.</p></li></ol></div><p class="calibre2">HDFS will not move blocks from old datanodes to new datanodes to
        balance the cluster. To do this, you should run the balancer
        <a class="calibre" id="calibre_link-3572"></a><a class="calibre" id="calibre_link-1198"></a><a class="calibre" id="calibre_link-1380"></a><a class="calibre" id="calibre_link-2747"></a><a class="calibre" id="calibre_link-2796"></a><a class="calibre" id="calibre_link-3222"></a>described in <a class="ulink" href="#calibre_link-17" title="Balancer">Balancer</a>.</p></div><div class="book" title="Decommissioning old nodes"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4374">Decommissioning old nodes</h4></div></div></div><p class="calibre2">Although HDFS <a class="calibre" id="calibre_link-3573"></a><a class="calibre" id="calibre_link-1409"></a>is designed to tolerate <a class="calibre" id="calibre_link-1383"></a><a class="calibre" id="calibre_link-2797"></a><a class="calibre" id="calibre_link-2750"></a><a class="calibre" id="calibre_link-3223"></a>datanode failures, this does not mean you can just
        terminate datanodes en masse with no ill effect. With a replication
        level of three, for example, the chances are very high that you will
        lose data by simultaneously shutting down three datanodes if they are
        on different racks. The way to decommission datanodes is to inform the namenode of
        the nodes that you wish to take out of circulation, so that it can
        replicate the blocks to other datanodes before the datanodes are shut
        down.</p><p class="calibre2">With node managers, Hadoop is more forgiving. If you shut down a
        node manager that is running MapReduce tasks, the application master
        <a class="calibre" id="calibre_link-906"></a>will notice the failure and reschedule the tasks on
        other nodes.</p><p class="calibre2">The decommissioning process is controlled by an
        <em class="calibre10">exclude file</em>, which is set for HDFS iby the
        <code class="literal">dfs.hosts.exclude</code> property and for YARN by
        <a class="calibre" id="calibre_link-3891"></a>the <code class="literal">yarn.resourcemanager.nodes.exclude-path</code>
        property. It is often the case that these properties refer to the same
        file. The exclude file lists the nodes that are not permitted to
        connect to the cluster.</p><p class="calibre2">The rules for whether a node manager may connect to the resource
        manager are simple: a node manager may connect only if it appears in
        the include file and does <span class="calibre">not</span>
        appear in the exclude file. An unspecified or empty include file is
        taken to mean that all nodes are in the include file.</p><p class="calibre2">For HDFS, the rules are slightly different. If a datanode
        appears in both the include and the exclude file, then it may connect,
        but only to be decommissioned. <a class="ulink" href="#calibre_link-18" title="Table&nbsp;11-3.&nbsp;HDFS include and exclude file precedence">Table&nbsp;11-3</a> summarizes the different
        combinations for datanodes. As for node managers, an unspecified or
        empty include file means all nodes are included.</p><div class="table"><a id="calibre_link-18" class="calibre"></a><div class="table-title">Table&nbsp;11-3.&nbsp;HDFS include and exclude file precedence</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Node appears in include file</td><td class="calibre20">Node appears in exclude file</td><td class="calibre21">Interpretation</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">No</td><td class="calibre23">No</td><td class="calibre25">Node may not connect.</td></tr><tr class="calibre26"><td class="calibre23">No</td><td class="calibre23">Yes</td><td class="calibre25">Node may not connect.</td></tr><tr class="calibre19"><td class="calibre23">Yes</td><td class="calibre23">No</td><td class="calibre25">Node may connect.</td></tr><tr class="calibre26"><td class="calibre27">Yes</td><td class="calibre27">Yes</td><td class="calibre28">Node may connect and will be decommissioned.</td></tr></tbody></table></div></div><p class="calibre2">To remove nodes from the cluster:</p><div class="book"><ol class="orderedlist"><li class="listitem"><p class="calibre2">Add the network addresses of the nodes to be decommissioned
            to the exclude file. Do not update the include file at this
            point.</p></li><li class="listitem"><p class="calibre2">Update the namenode with the new set of permitted datanodes,
            using this command:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hdfs dfsadmin -refreshNodes</code></strong></pre></li><li class="listitem"><p class="calibre2">Update the resource manager with the new set of permitted
            node managers using:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">yarn rmadmin -refreshNodes</code></strong></pre></li><li class="listitem"><p class="calibre2">Go to the web UI and check whether the admin state has
            changed to “Decommission In Progress” for the datanodes being
            decommissioned. They will start copying their blocks to other
            datanodes in the cluster.</p></li><li class="listitem"><p class="calibre2">When all the datanodes report their state as
            “Decommissioned,” all the blocks have been replicated. Shut down
            the decommissioned nodes.</p></li><li class="listitem"><p class="calibre2">Remove the nodes from the include file, and run:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hdfs dfsadmin -refreshNodes</code></strong></pre><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">yarn rmadmin -refreshNodes</code></strong></pre></li><li class="listitem"><p class="calibre2">Remove the nodes <a class="calibre" id="calibre_link-3574"></a><a class="calibre" id="calibre_link-1410"></a><a class="calibre" id="calibre_link-2751"></a><a class="calibre" id="calibre_link-2798"></a><a class="calibre" id="calibre_link-3224"></a><a class="calibre" id="calibre_link-1384"></a>from the <em class="calibre10">slaves</em>
            file.</p></li></ol></div></div></div><div class="book" title="Upgrades"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-2">Upgrades</h3></div></div></div><p class="calibre2">Upgrading a Hadoop <a class="calibre" id="calibre_link-3581"></a><a class="calibre" id="calibre_link-3748"></a><a class="calibre" id="calibre_link-1154"></a><a class="calibre" id="calibre_link-1688"></a>cluster requires careful planning. The most important
      consideration is the HDFS upgrade. If the layout version of the
      filesystem has changed, then the upgrade will automatically migrate the
      filesystem data and metadata to a format that is compatible with the new
      version. As with any procedure that involves data migration, there is a
      risk of data loss, so you should be sure that both your data and the
      metadata are backed up (see <a class="ulink" href="#calibre_link-19" title="Routine Administration Procedures">Routine Administration Procedures</a>).</p><p class="calibre2">Part of the planning process should include a trial run on a small
      test cluster with a copy of data that you can afford to lose. A trial
      run will allow you to familiarize yourself with the process, customize
      it to your particular cluster configuration and toolset, and iron out
      any snags before running the upgrade procedure on a production cluster.
      A test cluster also has the benefit of being available to test client
      upgrades on. You can read about general compatibility concerns for
      clients in the following sidebar.</p><div class="sidebar"><a id="calibre_link-4375" class="calibre"></a><div class="sidebar-title">Compatibility</div><p class="calibre2">When moving from one <a class="calibre" id="calibre_link-1203"></a>release to another, you need to think about the upgrade
        steps that are needed. There are several aspects to consider: API
        compatibility, data compatibility, and wire compatibility.</p><p class="calibre2">API compatibility concerns the contract between user code and
        the published Hadoop APIs, such as the Java MapReduce APIs. Major
        releases (e.g., from 1.x.y to 2.0.0) are allowed to break API
        compatibility, so user programs may need to be modified and
        recompiled. Minor releases (e.g., from 1.0.x to 1.1.0) and point
        releases (e.g., from 1.0.1 to 1.0.2) should not break
        compatibility.</p><div class="note2" title="Note"><h3 class="title9">Note</h3><p class="calibre2">Hadoop uses a classification scheme for API elements to denote
          their stability. The preceding rules for API compatibility cover
          those elements that are marked <code class="literal">InterfaceStability.Stable</code>. Some elements
          of the public Hadoop APIs, however, are marked with the <code class="literal">InterfaceStability.Evolving</code> or
          <span class="calibre"><span class="calibre"><code class="literal">InterfaceStability</code></span></span><span class="calibre"><code class="literal">.Unstable</code></span> annotations (all these
          annotations are <a class="calibre" id="calibre_link-2858"></a>in the <code class="literal">org.apache.hadoop.classification</code> package),
          which mean they are allowed to break compatibility on minor and
          point releases, respectively.</p></div><p class="calibre2">Data compatibility concerns persistent data and metadata
        formats, such as the format in which the HDFS namenode stores its
        persistent data. The formats can change across minor or major
        releases, but the change is transparent to users because the upgrade
        will automatically migrate the data. There may be some restrictions
        about upgrade paths, and these are covered in the release notes. For
        example, it may be necessary to upgrade via an intermediate release
        rather than upgrading directly to the later final release in one
        step.</p><p class="calibre2">Wire compatibility concerns the interoperability between clients
        and servers via wire protocols such as RPC and HTTP. The rule for wire
        compatibility is that the client must have the same major release
        number as the server, but may differ in its minor or point release
        number (e.g., client version 2.0.2 will work with server 2.0.1 or
        2.1.0, but not necessarily with server 3.0.0).</p><div class="note2" title="Note"><h3 class="title9">Note</h3><p class="calibre2">This rule for wire compatibility differs from earlier versions
          of Hadoop, where internal clients (like datanodes) had to be
          upgraded in lockstep with servers. The fact that internal client and
          server versions can be mixed allows Hadoop 2 to support rolling
          upgrades.</p></div><p class="calibre2">The full set of compatibility rules that Hadoop adheres to are
        documented at the <a class="ulink" href="http://bit.ly/hadoop_compatibility" target="_top">Apache
        Software Foundation’s website</a>.</p></div><p class="calibre2">Upgrading a cluster when the filesystem layout has not changed is
      fairly straightforward: install
      the new version of Hadoop on the cluster (and on clients at the same
      time), shut down the old daemons, update the configuration files, and
      then start up the new daemons and switch clients to use the new
      libraries. This process is reversible, so rolling back an upgrade is
      also straightforward.</p><p class="calibre2">After every successful upgrade, you should perform a couple of
      final cleanup steps:</p><div class="book"><ol class="orderedlist"><li class="listitem"><p class="calibre2">Remove the old installation and configuration files from the
          cluster.</p></li><li class="listitem"><p class="calibre2">Fix any deprecation warnings in your code and
          configuration.</p></li></ol></div><p class="calibre2">Upgrades are where Hadoop cluster management tools like Cloudera
      Manager and Apache Ambari come into their own. They simplify the upgrade
      process and also make it easy to do rolling upgrades, where nodes are
      upgraded in batches (or one at a time for master nodes), so that clients
      don’t experience service interruptions.</p><div class="book" title="HDFS data and metadata upgrades"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4376">HDFS data and metadata upgrades</h4></div></div></div><p class="calibre2">If you use the <a class="calibre" id="calibre_link-1963"></a><a class="calibre" id="calibre_link-2685"></a>procedure just described to upgrade to a new version of
        HDFS and it expects a different layout version, then the namenode will
        refuse to run. A message like the following will appear in its
        log:</p><pre class="screen1">File system image contains an old layout version -16.
An upgrade to version -18 is required.
Please restart NameNode with -upgrade option.</pre><p class="calibre2">The most reliable way of finding out whether you need to upgrade
        the filesystem is by performing a trial on a test cluster.</p><p class="calibre2">An upgrade of HDFS makes a copy of the previous version’s
        metadata and data. Doing an upgrade does not double the storage
        requirements of the cluster, as the datanodes use hard links to keep
        two references (for the current and previous version) to the same
        block of data. This design makes it straightforward to roll back to
        the previous version of the filesystem, if you need to. You should
        understand that any changes made to the data on the upgraded system
        will be lost after the rollback completes, however.</p><p class="calibre2">You can keep only the previous version of the filesystem, which
        means you can’t roll back several versions. Therefore, to carry out
        another upgrade to HDFS data and metadata, you will need to delete the
        previous version, a process called <em class="calibre10">finalizing the
        upgrade</em>. Once an upgrade is finalized, there is no
        procedure for rolling back to a previous version.</p><p class="calibre2">In general, you can skip releases when upgrading, but in some
        cases, you may have to go through intermediate releases. The release
        notes make it clear when this is required.</p><p class="calibre2">You should only attempt to upgrade a healthy filesystem. Before
        running the upgrade, do a full <em class="calibre10">fsck</em>
        (see <a class="ulink" href="#calibre_link-16" title="Filesystem check (fsck)">Filesystem check (fsck)</a>). As an extra precaution, you can keep a
        copy of the <em class="calibre10">fsck</em> output that lists
        all the files and blocks in the system, so you can compare it with the
        output of running <em class="calibre10">fsck</em> after the
        upgrade.</p><p class="calibre2">It’s also worth clearing out temporary files before doing the
        upgrade—both local temporary files and those in the MapReduce system
        directory on HDFS.</p><p class="calibre2">With these preliminaries out of the way, here is the high-level
        procedure for upgrading a cluster when the filesystem layout needs to
        be migrated:</p><div class="book"><ol class="orderedlist"><li class="listitem"><p class="calibre2">Ensure that any previous upgrade is finalized before
              proceeding with another upgrade.</p></li><li class="listitem"><p class="calibre2">Shut down the YARN and MapReduce daemons.</p></li><li class="listitem"><p class="calibre2">Shut down HDFS, and back up the namenode directories.</p></li><li class="listitem"><p class="calibre2">Install the new version of Hadoop on the cluster and on
            clients.</p></li><li class="listitem"><p class="calibre2">Start HDFS with the <code class="literal">-upgrade</code> option.</p></li><li class="listitem"><p class="calibre2">Wait until the upgrade is complete.</p></li><li class="listitem"><p class="calibre2">Perform some sanity checks on HDFS.</p></li><li class="listitem"><p class="calibre2">Start the YARN and MapReduce daemons.</p></li><li class="listitem"><p class="calibre2">Roll back or finalize the upgrade (optional).</p></li></ol></div><p class="calibre2">While running the upgrade procedure, it is a good idea to remove
        the Hadoop scripts from <a class="calibre" id="calibre_link-2956"></a>your <code class="literal">PATH</code> environment
        variable. This forces you to be explicit about which version of the
        scripts you are running. It can be convenient to define two
        environment variables for the new installation directories; in the
        following instructions, we have defined <code class="literal">OLD_HADOOP_HOME</code> and <code class="literal">NEW_HADOOP_HOME</code>.</p><div class="book" title="Start the upgrade"><div class="titlepage1"><div class="book"><div class="book"><h5 class="title8" id="calibre_link-4377">Start the upgrade</h5></div></div></div><p class="calibre2">To perform the upgrade, run the following command (this is
          step 5 in the high-level upgrade procedure):</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">$NEW_HADOOP_HOME/bin/start-dfs.sh -upgrade</code></strong></pre><p class="calibre2">This causes the namenode to upgrade its metadata, placing the
          previous version in a new directory called <em class="calibre10">previous</em> under <code class="literal">dfs.namenode.name.dir</code>. Similarly,
          datanodes upgrade their storage directories, preserving the old copy
          in a directory called <em class="calibre10">previous</em>.</p></div><div class="book" title="Wait until the upgrade is complete"><div class="titlepage1"><div class="book"><div class="book"><h5 class="title8" id="calibre_link-4378">Wait until the upgrade is complete</h5></div></div></div><p class="calibre2">The upgrade process is not instantaneous, but you can check
          the progress of an upgrade using <em class="calibre10">dfsadmin</em> (step 6; upgrade events also
          appear in the daemons’ logfiles):</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">$NEW_HADOOP_HOME/bin/hdfs dfsadmin -upgradeProgress status</code></strong>
Upgrade for version -18 has been completed.
Upgrade is not finalized.</pre></div><div class="book" title="Check the upgrade"><div class="titlepage1"><div class="book"><div class="book"><h5 class="title8" id="calibre_link-4379">Check the upgrade</h5></div></div></div><p class="calibre2">This shows that the upgrade is complete. At this stage, you
          should run some sanity checks (step 7) on the filesystem (e.g.,
          check files and blocks using <em class="calibre10">fsck</em>, test basic file operations). You
          might choose to put HDFS into safe mode while you are running some
          of these checks (the ones that are read-only) to prevent others from
          making changes; see <a class="ulink" href="#calibre_link-4" title="Safe Mode">Safe Mode</a>.</p></div><div class="book" title="Roll back the upgrade (optional)"><div class="titlepage1"><div class="book"><div class="book"><h5 class="title8" id="calibre_link-4380">Roll back the upgrade (optional)</h5></div></div></div><p class="calibre2">If you find that the new version is not working correctly, you
          may choose to roll back to the previous version (step 9). This is
          possible only if you have not finalized the upgrade.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">A rollback reverts the filesystem state to before the
            upgrade was performed, so any changes made in the meantime will be
            lost. In other words, it rolls back to the previous state of the
            filesystem, rather than downgrading the current state of the
            filesystem to a former version.</p></div><p class="calibre2">First, shut down the new daemons:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">$NEW_HADOOP_HOME/bin/stop-dfs.sh</code></strong></pre><p class="calibre2">Then start up the old version of HDFS with the <code class="literal">-rollback</code> option:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">$OLD_HADOOP_HOME/bin/start-dfs.sh -rollback</code></strong></pre><p class="calibre2">This command gets the namenode and datanodes to replace their
          current storage directories
          with their previous copies. The filesystem will be returned to its
          previous state.</p></div><div class="book" title="Finalize the upgrade (optional)"><div class="titlepage1"><div class="book"><div class="book"><h5 class="title8" id="calibre_link-4381">Finalize the upgrade (optional)</h5></div></div></div><p class="calibre2">When you are happy with the new version of HDFS, you can
          finalize the upgrade (step 9) to remove the previous storage
          directories.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">After an upgrade has been finalized, there is no way to roll
            back to the previous version.</p></div><p class="calibre2">This step is required before performing another
          upgrade:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">$NEW_HADOOP_HOME/bin/hdfs dfsadmin -finalizeUpgrade</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">$NEW_HADOOP_HOME/bin/hdfs dfsadmin -upgradeProgress status</code></strong>
There are no upgrades in progress.</pre><p class="calibre2">HDFS is now fully upgraded to the new <a class="calibre" id="calibre_link-3582"></a><a class="calibre" id="calibre_link-1155"></a><a class="calibre" id="calibre_link-3749"></a><a class="calibre" id="calibre_link-1126"></a><a class="calibre" id="calibre_link-1964"></a><a class="calibre" id="calibre_link-2686"></a><a class="calibre" id="calibre_link-1689"></a>version.</p></div></div></div></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" type="footnote" id="calibre_link-5"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-20">77</a>] </sup>It is actually possible to start a <a class="calibre" id="calibre_link-2768"></a>namenode with the <code class="literal">-checkpoint</code> option so that it runs the
            checkpointing process against another (primary) namenode. This is
            functionally equivalent to running a secondary namenode, but at
            the time of this writing offers no advantages over the secondary
            namenode (and indeed, the secondary namenode is the most tried and
            tested option). When running in a high-availability environment
            (see <a class="ulink" href="#calibre_link-21" title="HDFS High Availability">HDFS High Availability</a>), the standby node performs
            checkpointing.</p></div><div class="footnote" type="footnote" id="calibre_link-14"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-22">78</a>] </sup>Hadoop comes with an Offline Image Viewer and an Offline
            Edits Viewer, which can be used to check the integrity of the
            <em class="calibre10">fsimage</em> and <em class="calibre10">edits</em> files. Note that both viewers
            support older formats of these files, so you can use them to
            diagnose problems in these files generated by previous releases of
            Hadoop. Type <code class="literal">hdfs oiv</code> and <code class="literal">hdfs
            oev</code> to invoke these tools.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-88"><div class="book" type="part" id="calibre_link-4382" title="Part&nbsp;IV.&nbsp;Related Projects"><div class="book"><div class="book"><div class="book"><h1 class="title6">Part&nbsp;IV.&nbsp;Related Projects</h1></div></div></div></div></div>

<div class="calibre1" id="calibre_link-133"><section type="chapter" id="calibre_link-4383" title="Chapter&nbsp;12.&nbsp;Avro"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;12.&nbsp;Avro</h2></div></div></div><p class="calibre2"><a class="ulink" href="http://avro.apache.org/" target="_top">Apache Avro</a><sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-134" id="calibre_link-153">79</a>]</sup> is a language-neutral <a class="calibre" id="calibre_link-934"></a>data serialization system. The project was created by
  <a class="calibre" id="calibre_link-1318"></a>Doug Cutting (the creator of Hadoop) to address the major
  downside of Hadoop Writables: lack of language portability. Having a data
  format that can be processed by many languages (currently C, C++, C#, Java,
  JavaScript, Perl, PHP, Python, and Ruby) makes it easier to share datasets
  with a wider audience than one tied to a single language. It is also more
  future-proof, allowing data to potentially outlive the language used to read
  and write it.</p><p class="calibre2">But why a new data serialization system? Avro has a set of features
  that, taken together, differentiate it from
  other systems such as Apache Thrift or Google’s Protocol
  Buffers.<sup class="calibre6">[<a class="firstname" href="#calibre_link-135" id="calibre_link-154">80</a>]</sup> Like in these systems and others, Avro data is described using
  a language-independent <em class="calibre10">schema</em>. However, unlike in
  some other systems, code generation is optional in Avro, which means you can
  read and write data that conforms to a given schema even if your code has
  not seen that particular schema before. To achieve this, Avro assumes that
  the schema is always present—at both read and write time—which makes for a
  very compact encoding, since encoded values do not need to be tagged with a
  field identifier.</p><p class="calibre2">Avro schemas are usually written in JSON, and data is usually encoded
  using a binary format, but there are other options, too. There is a
  higher-level language called <span class="calibre"><em class="calibre10">Avro IDL</em></span> for writing
  schemas in a C-like language that is more familiar to developers. There is
  also a JSON-based data encoder, which, being human readable, is useful for
  prototyping and debugging Avro data.</p><p class="calibre2">The <em class="calibre10"><a class="ulink" href="http://avro.apache.org/docs/current/spec.html" target="_top">Avro
  specification</a></em> precisely defines the binary format that
  all implementations must support. It also specifies many of the other
  features of Avro that implementations should support. One area that the
  specification does not rule on,
  however, is APIs: implementations have complete latitude in the APIs they
  expose for working with Avro data, since each one is necessarily language
  specific. The fact that there is only one binary format is significant,
  because it means the barrier for implementing a new language binding is
  lower and avoids the problem of a
  combinatorial explosion of languages and formats, which would harm interoperability.</p><p class="calibre2">Avro has rich <em class="calibre10">schema resolution</em> capabilities.
  Within certain carefully defined constraints, the schema used to read data
  need not be identical to the schema that was used to write the data. This is
  the mechanism by which Avro supports schema evolution. For example, a new,
  optional field may be added to a record by declaring it in the schema used
  to read the old data. New and old clients alike will be able to read the old
  data, while new clients can write new data that uses the new field.
  Conversely, if an old client sees newly encoded data, it will gracefully
  ignore the new field and carry on processing as it would have done with old
  data.</p><p class="calibre2">Avro specifies an <em class="calibre10">object container format</em> for
  sequences of objects, similar to Hadoop’s sequence file. An <em class="calibre10">Avro
  datafile</em> has a metadata section where the schema is stored,
  which makes the file self-describing. Avro datafiles support compression and
  are splittable, which is crucial for a MapReduce data input format. In fact,
  support goes beyond MapReduce: all of the data processing frameworks in this
  book (Pig, Hive, Crunch, Spark) can read and write Avro datafiles.</p><p class="calibre2">Avro can be used for RPC, too, although this isn’t covered here. More
  information is in the <a class="calibre" id="calibre_link-935"></a>specification.</p><div class="book" title="Avro Data Types and Schemas"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-263">Avro Data Types and Schemas</h2></div></div></div><p class="calibre2">Avro defines a small <a class="calibre" id="calibre_link-938"></a><a class="calibre" id="calibre_link-3285"></a>number of primitive data types, which can be used to build
    application-specific data structures
    by writing schemas. For interoperability, implementations must support all
    Avro types.</p><p class="calibre2">Avro’s primitive types are listed in <a class="ulink" href="#calibre_link-136" title="Table&nbsp;12-1.&nbsp;Avro primitive types">Table&nbsp;12-1</a>. Each primitive type may also be specified
    using a more verbose form by using the <code class="literal">type</code> attribute, such as:</p><a id="calibre_link-4384" class="calibre"></a><pre class="screen1"><code class="p">{</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"null"</code> <code class="p">}</code></pre><div class="table"><a id="calibre_link-136" class="calibre"></a><div class="table-title">Table&nbsp;12-1.&nbsp;Avro primitive types</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Type</td><td class="calibre20">Description</td><td class="calibre21">Schema</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">null</code></td><td class="calibre23">The absence of a value</td><td class="calibre25"><code class="uri">"null"</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">boolean</code></td><td class="calibre23">A binary value</td><td class="calibre25"><code class="uri">"boolean"</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">32-bit signed integer</td><td class="calibre25"><code class="uri">"int"</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">long</code></td><td class="calibre23">64-bit signed integer</td><td class="calibre25"><code class="uri">"long"</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">float</code></td><td class="calibre23">Single-precision (32-bit) IEEE 754 floating-point
            number</td><td class="calibre25"><code class="uri">"float"</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">double</code></td><td class="calibre23">Double-precision (64-bit) IEEE 754 floating-point
            number</td><td class="calibre25"><code class="uri">"double"</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">bytes</code></td><td class="calibre23">Sequence of 8-bit unsigned bytes</td><td class="calibre25"><code class="uri">"bytes"</code></td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">string</code></td><td class="calibre27">Sequence of Unicode characters</td><td class="calibre28"><code class="uri">"string"</code></td></tr></tbody></table></div></div><p class="calibre2">Avro also defines the complex types listed in <a class="ulink" href="#calibre_link-137" title="Table&nbsp;12-2.&nbsp;Avro complex types">Table&nbsp;12-2</a>, along with a representative example of a
    schema of each type.</p><div class="table"><a id="calibre_link-137" class="calibre"></a><div class="table-title">Table&nbsp;12-2.&nbsp;Avro complex types</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Type</td><td class="calibre20">Description</td><td class="calibre21">Schema example</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">array</code></td><td class="calibre23">An ordered collection of objects. All objects in a
            particular array must have the same schema.</td><td class="calibre25"><a id="calibre_link-4385" class="calibre"></a><pre class="programlisting1"><code class="p">{</code>
  <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"array"</code><code class="p">,</code>
  <code class="nt">"items"</code><code class="p">:</code> <code class="sb">"long"</code>
<code class="p">}</code></pre></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">map</code></td><td class="calibre23">An unordered collection of key-value pairs. Keys must be
            strings and values may be any type, although within a particular
            map, all values must have the same schema.</td><td class="calibre25"><a id="calibre_link-4386" class="calibre"></a><pre class="programlisting1"><code class="p">{</code>
  <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"map"</code><code class="p">,</code>
  <code class="nt">"values"</code><code class="p">:</code> <code class="sb">"string"</code>
<code class="p">}</code></pre></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">record</code></td><td class="calibre23">A collection of named fields of any type.</td><td class="calibre25"><a id="calibre_link-4387" class="calibre"></a><pre class="programlisting1"><code class="p">{</code>
  <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"record"</code><code class="p">,</code>
  <code class="nt">"name"</code><code class="p">:</code> <code class="sb">"WeatherRecord"</code><code class="p">,</code>
  <code class="nt">"doc"</code><code class="p">:</code> <code class="sb">"A weather reading."</code><code class="p">,</code>
  <code class="nt">"fields"</code><code class="p">:</code> <code class="p">[</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"year"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"int"</code><code class="p">},</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"temperature"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"int"</code><code class="p">},</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"stationId"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">}</code>
  <code class="p">]</code>
<code class="p">}</code></pre></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">enum</code></td><td class="calibre23">A set of named values.</td><td class="calibre25"><a id="calibre_link-4388" class="calibre"></a><pre class="programlisting1"><code class="p">{</code>
  <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"enum"</code><code class="p">,</code>
  <code class="nt">"name"</code><code class="p">:</code> <code class="sb">"Cutlery"</code><code class="p">,</code>
  <code class="nt">"doc"</code><code class="p">:</code> <code class="sb">"An eating utensil."</code><code class="p">,</code>
  <code class="nt">"symbols"</code><code class="p">:</code> <code class="p">[</code><code class="sb">"KNIFE"</code><code class="p">,</code> <code class="sb">"FORK"</code><code class="p">,</code> <code class="sb">"SPOON"</code><code class="p">]</code>
<code class="p">}</code></pre></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">fixed</code></td><td class="calibre23">A fixed number of 8-bit unsigned bytes.</td><td class="calibre25"><a id="calibre_link-4389" class="calibre"></a><pre class="programlisting1"><code class="p">{</code>
  <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"fixed"</code><code class="p">,</code>
  <code class="nt">"name"</code><code class="p">:</code> <code class="sb">"Md5Hash"</code><code class="p">,</code>
  <code class="nt">"size"</code><code class="p">:</code> <code class="mi">16</code>
<code class="p">}</code></pre></td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">union</code></td><td class="calibre27">A union of schemas. A union is represented by a JSON array,
            where each element in the array is a schema. Data represented by a
            union must match one of the schemas in the union.</td><td class="calibre28"><a id="calibre_link-4390" class="calibre"></a><pre class="programlisting1"><code class="p">[</code>
  <code class="sb">"null"</code><code class="p">,</code>
  <code class="sb">"string"</code><code class="p">,</code>
  <code class="p">{</code><code class="nt">"type"</code><code class="p">:</code> <code class="sb">"map"</code><code class="p">,</code> <code class="nt">"values"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">}</code>
<code class="p">]</code></pre></td></tr></tbody></table></div></div><p class="calibre2">Each Avro language API has a representation for each Avro type that
    is specific to the language. For example, Avro’s <code class="literal">double</code> type is represented in C, C++, and Java
    by a <code class="literal">double</code>, in Python by a <code class="literal">float</code>, and in Ruby by a <code class="literal">Float</code>.</p><p class="calibre2">What’s more, there may be more than one representation, or mapping,
    for a language. All languages support a dynamic mapping, which can be used
    even when the schema is not known ahead of runtime. Java calls this the
    <em class="calibre10">Generic</em> mapping.</p><p class="calibre2">In addition, the Java and C++ implementations can generate code to
    represent the data for an Avro schema. Code generation, which is called
    the <em class="calibre10">Specific</em> mapping in Java, is an optimization
    that is useful when you have a copy of the schema before you read or write
    data. Generated classes also provide a more domain-oriented API for user
    code than Generic ones.</p><p class="calibre2">Java has a third mapping, the <em class="calibre10">Reflect</em>
    mapping, which maps Avro types onto preexisting Java types using
    reflection. It is slower than the Generic and Specific mappings but can be
    a convenient way of defining a type, since Avro can infer a schema
    automatically.</p><p class="calibre2">Java’s type mappings are shown in <a class="ulink" href="#calibre_link-138" title="Table&nbsp;12-3.&nbsp;Avro Java type mappings">Table&nbsp;12-3</a>. As the table shows, the Specific
    mapping is the same as the Generic one unless otherwise noted (and the
    Reflect one is the same as the Specific one unless noted). The Specific
    mapping differs from the Generic one only for <code class="literal">record</code>, <code class="literal">enum</code>,
    and <code class="literal">fixed</code>, all of which have generated
    classes (the names of which are controlled by the <code class="literal">name</code> and optional <code class="literal">namespace</code> attributes).</p><div class="table"><a id="calibre_link-138" class="calibre"></a><div class="table-title">Table&nbsp;12-3.&nbsp;Avro Java type mappings</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Avro type</td><td class="calibre20">Generic Java mapping</td><td class="calibre20">Specific Java mapping</td><td class="calibre21">Reflect Java mapping</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">null</code></td><td colspan="3" class="calibre25"><code class="uri">null</code>
            type</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">boolean</code></td><td colspan="3" class="calibre25"><code class="uri">boolean</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">int</code></td><td colspan="2" class="calibre23"><code class="uri">int</code></td><td class="calibre25"><code class="uri">byte</code>, <code class="uri">short</code>, <code class="uri">int</code>, or <code class="uri">char</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">long</code></td><td colspan="3" class="calibre25"><code class="uri">long</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">float</code></td><td colspan="3" class="calibre25"><code class="uri">float</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">double</code></td><td colspan="3" class="calibre25"><code class="uri">double</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">bytes</code></td><td colspan="2" class="calibre23"><code class="uri">java.nio.ByteBuffer</code></td><td class="calibre25">Array of <code class="uri">byte</code>s</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">string</code></td><td colspan="2" class="calibre23"><code class="uri">org.apache.avro.util.Utf8</code> or <code class="uri">java.lang.String</code></td><td class="calibre25"><code class="uri">java.lang.String</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">array</code></td><td colspan="2" class="calibre23"><code class="uri">org.apache.avro.generic.GenericArray</code></td><td class="calibre25">Array or <code class="uri">java.util.</code><code class="uri">Collection</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">map</code></td><td colspan="3" class="calibre25"><code class="uri">java.util.Map</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">record</code></td><td class="calibre23"><code class="uri">org.apache.avro.generic.GenericRecord</code></td><td class="calibre23">Generated class implementing <code class="uri">org.apache.avro.specific.SpecificRecord</code></td><td class="calibre25">Arbitrary user class with a zero-argument constructor; all inherited
            nontransient instance fields
            are used</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">enum</code></td><td class="calibre23"><code class="uri">java.lang.String</code></td><td class="calibre23">Generated Java enum</td><td class="calibre25">Arbitrary Java enum</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">fixed</code></td><td class="calibre23"><code class="uri">org.apache.avro.</code>  <code class="uri">generic.GenericFixed</code> </td><td class="calibre23">Generated class <a class="calibre" id="calibre_link-939"></a><a class="calibre" id="calibre_link-3286"></a>implementing <code class="uri">org.apache.avro.specific.SpecificFixed</code></td><td class="calibre25"><code class="uri">org.apache.avro.generic.GenericFixed</code></td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">union</code></td><td colspan="3" class="calibre28"><code class="uri">java.lang.Object</code></td></tr></tbody></table></div></div><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Avro <code class="literal">string</code> can be represented
      by either <a class="calibre" id="calibre_link-3558"></a>Java <code class="literal">String</code> or the Avro
      <code class="literal">Utf8</code> Java type. The reason to use
      <code class="literal">Utf8</code> is efficiency: because it is
      mutable, a single <code class="literal">Utf8</code> instance may
      be reused for reading or writing a series of values. Also, Java <code class="literal">String</code> decodes UTF-8 at object construction
      time, whereas Avro <code class="literal">Utf8</code> does it
      lazily, which can increase performance in some cases.</p><p class="calibre2"><code class="literal">Utf8</code> implements Java’s <code class="literal">java.lang.CharSequence</code> interface, <a class="calibre" id="calibre_link-1090"></a>which allows some interoperability with Java libraries. In
      other cases, it may be necessary to convert <code class="literal">Utf8</code> instances to <code class="literal">String</code> objects by calling its <code class="literal">toString()</code> method.</p><p class="calibre2"><code class="literal">Utf8</code> is the default for Generic
      and Specific, but it’s possible to use <code class="literal">String</code> for a particular mapping. There are a
      couple of ways to achieve this. The first is to set
      <a class="calibre" id="calibre_link-961"></a>the <code class="literal">avro.java.string</code>
      property in the schema to <code class="literal">String</code>:</p><a id="calibre_link-4391" class="calibre"></a><pre class="programlisting"><code class="p">{</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">,</code> <code class="nt">"avro.java.string"</code><code class="p">:</code> <code class="sb">"String"</code> <code class="p">}</code></pre><p class="calibre2">Alternatively, for the Specific mapping, you can generate classes
      that have <code class="literal">String</code>-based getters and
      setters. When using the Avro Maven plug-in, this is done by setting the
      configuration property <code class="literal">stringType</code> to
      <code class="literal">String</code> (<a class="ulink" href="#calibre_link-139" title="The Specific API">The Specific API</a> has a demonstration of this).</p><p class="calibre2">Finally, note that the Java Reflect mapping always uses <code class="literal">String</code> objects, since it is designed for Java
      compatibility, not performance.</p></div></div><div class="book" title="In-Memory Serialization and Deserialization"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-218">In-Memory Serialization and Deserialization</h2></div></div></div><p class="calibre2">Avro provides APIs for <a class="calibre" id="calibre_link-954"></a><a class="calibre" id="calibre_link-3355"></a><a class="calibre" id="calibre_link-1437"></a>serialization and deserialization that are useful when you
    want to integrate Avro with an existing system, such as a messaging system
    where the framing format is already defined. In other cases, consider
    using Avro’s datafile format.</p><p class="calibre2">Let’s write a Java program to read and write Avro data from and to
    streams. We’ll start with a simple Avro schema for representing a pair of
    strings as a record:</p><a id="calibre_link-4392" class="calibre"></a><pre class="screen1"><code class="p">{</code>
  <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"record"</code><code class="p">,</code>
  <code class="nt">"name"</code><code class="p">:</code> <code class="sb">"StringPair"</code><code class="p">,</code>
  <code class="nt">"doc"</code><code class="p">:</code> <code class="sb">"A pair of strings."</code><code class="p">,</code>
  <code class="nt">"fields"</code><code class="p">:</code> <code class="p">[</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"left"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">},</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"right"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">}</code>
  <code class="p">]</code>
<code class="p">}</code></pre><p class="calibre2">If this schema is saved in a file on the classpath called <em class="calibre10">StringPair.avsc</em> (<span class="calibre"><em class="calibre10">.avsc</em></span> is
    the conventional extension for an Avro schema), we can load it using the
    following two lines of code:</p><a id="calibre_link-4393" class="calibre"></a><pre class="screen1">    <code class="n">Schema</code><code class="o">.</code><code class="na">Parser</code> <code class="n">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Schema</code><code class="o">.</code><code class="na">Parser</code><code class="o">();</code>
    <code class="n">Schema</code> <code class="n">schema</code> <code class="o">=</code> <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code>
        <code class="n">getClass</code><code class="o">().</code><code class="na">getResourceAsStream</code><code class="o">(</code><code class="sb">"StringPair.avsc"</code><code class="o">));</code></pre><p class="calibre2">We can create an instance of an Avro record using the Generic API as
    follows:</p><a id="calibre_link-4394" class="calibre"></a><pre class="screen1">    <code class="n">GenericRecord</code> <code class="n">datum</code> <code class="o">=</code> <code class="k">new</code> <code class="n">GenericData</code><code class="o">.</code><code class="na">Record</code><code class="o">(</code><code class="n">schema</code><code class="o">);</code>
    <code class="n">datum</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="sb">"left"</code><code class="o">,</code> <code class="sb">"L"</code><code class="o">);</code>
    <code class="n">datum</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="sb">"right"</code><code class="o">,</code> <code class="sb">"R"</code><code class="o">);</code></pre><p class="calibre2">Next, we serialize the record to an output stream:</p><a id="calibre_link-4395" class="calibre"></a><pre class="screen1">    <code class="n">ByteArrayOutputStream</code> <code class="n">out</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ByteArrayOutputStream</code><code class="o">();</code>
    <code class="n">DatumWriter</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;</code> <code class="n">writer</code> <code class="o">=</code>
        <code class="k">new</code> <code class="n">GenericDatumWriter</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;(</code><code class="n">schema</code><code class="o">);</code>
    <code class="n">Encoder</code> <code class="n">encoder</code> <code class="o">=</code> <code class="n">EncoderFactory</code><code class="o">.</code><code class="na">get</code><code class="o">().</code><code class="na">binaryEncoder</code><code class="o">(</code><code class="n">out</code><code class="o">,</code> <code class="k">null</code><code class="o">);</code>
    <code class="n">writer</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">datum</code><code class="o">,</code> <code class="n">encoder</code><code class="o">);</code>
    <code class="n">encoder</code><code class="o">.</code><code class="na">flush</code><code class="o">();</code>
    <code class="n">out</code><code class="o">.</code><code class="na">close</code><code class="o">();</code></pre><p class="calibre2">There are two important objects here: <a class="calibre" id="calibre_link-1397"></a><a class="calibre" id="calibre_link-1567"></a>the <code class="literal">DatumWriter</code> and the
    <code class="literal">Encoder</code>. A <code class="literal">DatumWriter</code> translates data
    objects into the types understood by an <code class="literal">Encoder</code>,
    which the latter writes to the output stream. Here we are using a
    <code class="literal">GenericDatumWriter</code>, which <a class="calibre" id="calibre_link-1786"></a>passes the fields <a class="calibre" id="calibre_link-1790"></a>of <code class="literal">GenericRecord</code> to the
    <code class="literal">Encoder</code>. We pass a <code class="literal">null</code> to the encoder factory because we are not
    reusing a previously constructed encoder here.</p><p class="calibre2">In this example, only one object is written to the stream, but we
    could call <code class="literal">write()</code> with more objects
    before closing the stream if we wanted to.</p><p class="calibre2">The <code class="literal">GenericDatumWriter</code> needs to be passed the
    schema because it follows the schema to determine which values from the
    data objects to write out. After we have called the writer’s
    <code class="literal">write()</code> method, we flush the encoder, then close
    the output stream.</p><p class="calibre2">We can reverse the process and read the object back from the byte
    buffer:</p><a id="calibre_link-4396" class="calibre"></a><pre class="screen1">    <code class="n">DatumReader</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;</code> <code class="n">reader</code> <code class="o">=</code>
        <code class="k">new</code> <code class="n">GenericDatumReader</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;(</code><code class="n">schema</code><code class="o">);</code>
    <code class="n">Decoder</code> <code class="n">decoder</code> <code class="o">=</code> <code class="n">DecoderFactory</code><code class="o">.</code><code class="na">get</code><code class="o">().</code><code class="na">binaryDecoder</code><code class="o">(</code><code class="n">out</code><code class="o">.</code><code class="na">toByteArray</code><code class="o">(),</code>
        <code class="k">null</code><code class="o">);</code>
    <code class="n">GenericRecord</code> <code class="n">result</code> <code class="o">=</code> <code class="n">reader</code><code class="o">.</code><code class="na">read</code><code class="o">(</code><code class="k">null</code><code class="o">,</code> <code class="n">decoder</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">result</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"left"</code><code class="o">).</code><code class="na">toString</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"L"</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">result</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"right"</code><code class="o">).</code><code class="na">toString</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"R"</code><code class="o">));</code></pre><p class="calibre2">We pass <code class="literal">null</code> to the calls to
    <code class="literal">binaryDecoder()</code> and <code class="literal">read()</code>
    because we are not reusing objects here (the decoder or the record,
    respectively).</p><p class="calibre2">The objects returned by <code class="literal">result.get("left")</code> and <code class="literal">result.get("right")</code> are of type <code class="literal">Utf8</code>, so we convert them into Java <code class="literal">String</code> objects by calling their <code class="literal">toString()</code> methods.</p><div class="book" title="The Specific API"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-139">The Specific API</h3></div></div></div><p class="calibre2">Let’s look now at the equivalent code using the Specific API. We
      can generate the <code class="literal">StringPair</code> class from the schema
      file by using Avro’s Maven plug-in for compiling schemas. The following
      is the relevant part of <a class="calibre" id="calibre_link-2657"></a>the Maven Project Object Model (POM):</p><a id="calibre_link-4397" class="calibre"></a><pre class="screen1"><code class="nt">&lt;project&gt;</code>
  ...
  <code class="nt">&lt;build&gt;</code>
    <code class="nt">&lt;plugins&gt;</code>
      <code class="nt">&lt;plugin&gt;</code>
        <code class="nt">&lt;groupId&gt;</code>org.apache.avro<code class="nt">&lt;/groupId&gt;</code>
        <code class="nt">&lt;artifactId&gt;</code>avro-maven-plugin<code class="nt">&lt;/artifactId&gt;</code>
        <code class="nt">&lt;version&gt;</code>${avro.version}<code class="nt">&lt;/version&gt;</code>
        <code class="nt">&lt;executions&gt;</code>
          <code class="nt">&lt;execution&gt;</code>
            <code class="nt">&lt;id&gt;</code>schemas<code class="nt">&lt;/id&gt;</code>
            <code class="nt">&lt;phase&gt;</code>generate-sources<code class="nt">&lt;/phase&gt;</code>
            <code class="nt">&lt;goals&gt;</code>
              <code class="nt">&lt;goal&gt;</code>schema<code class="nt">&lt;/goal&gt;</code>
            <code class="nt">&lt;/goals&gt;</code>
            <code class="nt">&lt;configuration&gt;</code>
              <code class="nt">&lt;includes&gt;</code>
                <code class="nt">&lt;include&gt;</code>StringPair.avsc<code class="nt">&lt;/include&gt;</code>
              <code class="nt">&lt;/includes&gt;</code>
              <code class="nt">&lt;stringType&gt;</code>String<code class="nt">&lt;/stringType&gt;</code>
              <code class="nt">&lt;sourceDirectory&gt;</code>src/main/resources<code class="nt">&lt;/sourceDirectory&gt;</code>
              <code class="nt">&lt;outputDirectory&gt;</code>${project.build.directory}/generated-sources/java
              <code class="nt">&lt;/outputDirectory&gt;</code>
            <code class="nt">&lt;/configuration&gt;</code>
          <code class="nt">&lt;/execution&gt;</code>
        <code class="nt">&lt;/executions&gt;</code>
      <code class="nt">&lt;/plugin&gt;</code>
    <code class="nt">&lt;/plugins&gt;</code>
  <code class="nt">&lt;/build&gt;</code>
  ...
<code class="nt">&lt;/project&gt;</code></pre><p class="calibre2">As an alternative to Maven, you can
      use Avro’s Ant task, <code class="literal">org.apache.avro.specific</code><code class="literal">.SchemaTask</code>, or
      the Avro command-line tools<sup class="calibre6">[<a class="firstname" href="#calibre_link-140" id="calibre_link-155">81</a>]</sup> to generate Java code for a schema.</p><p class="calibre2">In the code for serializing and deserializing, instead of a
      <code class="literal">GenericRecord</code> we construct a
      <code class="literal">StringPair</code> instance, which we write to the stream
      using a <code class="literal">SpecificDatumWriter</code> and read back
      <a class="calibre" id="calibre_link-955"></a><a class="calibre" id="calibre_link-1438"></a><a class="calibre" id="calibre_link-3356"></a>using <a class="calibre" id="calibre_link-3475"></a>a <code class="literal">SpecificDatumReader</code>:</p><pre class="screen1">    <span class="calibre24"><strong class="calibre9"><code class="n1">StringPair</code> <code class="n1">datum</code> <code class="o1">=</code> <code class="kc">new</code> <code class="n1">StringPair</code><code class="o1">();</code>
    <code class="n1">datum</code><code class="o1">.</code><code class="na1">setLeft</code><code class="o1">(</code><code class="s">"L"</code><code class="o1">);</code>
    <code class="n1">datum</code><code class="o1">.</code><code class="na1">setRight</code><code class="o1">(</code><code class="s">"R"</code><code class="o1">);</code></strong></span>

    <code class="n">ByteArrayOutputStream</code> <code class="n">out</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ByteArrayOutputStream</code><code class="o">();</code>
    <span class="calibre24"><strong class="calibre9"><code class="n1">DatumWriter</code><code class="o1">&lt;</code><code class="n1">StringPair</code><code class="o1">&gt;</code> <code class="n1">writer</code> <code class="o1">=</code>
        <code class="kc">new</code> <code class="n1">SpecificDatumWriter</code><code class="o1">&lt;</code><code class="n1">StringPair</code><code class="o1">&gt;(</code><code class="n1">StringPair</code><code class="o1">.</code><code class="na1">class</code><code class="o1">);</code></strong></span>
    <code class="n">Encoder</code> <code class="n">encoder</code> <code class="o">=</code> <code class="n">EncoderFactory</code><code class="o">.</code><code class="na">get</code><code class="o">().</code><code class="na">binaryEncoder</code><code class="o">(</code><code class="n">out</code><code class="o">,</code> <code class="k">null</code><code class="o">);</code>
    <code class="n">writer</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">datum</code><code class="o">,</code> <code class="n">encoder</code><code class="o">);</code>
    <code class="n">encoder</code><code class="o">.</code><code class="na">flush</code><code class="o">();</code>
    <code class="n">out</code><code class="o">.</code><code class="na">close</code><code class="o">();</code>
    
    <span class="calibre24"><strong class="calibre9"><code class="n1">DatumReader</code><code class="o1">&lt;</code><code class="n1">StringPair</code><code class="o1">&gt;</code> <code class="n1">reader</code> <code class="o1">=</code>
        <code class="kc">new</code> <code class="n1">SpecificDatumReader</code><code class="o1">&lt;</code><code class="n1">StringPair</code><code class="o1">&gt;(</code><code class="n1">StringPair</code><code class="o1">.</code><code class="na1">class</code><code class="o1">);</code></strong></span>
    <code class="n">Decoder</code> <code class="n">decoder</code> <code class="o">=</code> <code class="n">DecoderFactory</code><code class="o">.</code><code class="na">get</code><code class="o">().</code><code class="na">binaryDecoder</code><code class="o">(</code><code class="n">out</code><code class="o">.</code><code class="na">toByteArray</code><code class="o">(),</code>
        <code class="k">null</code><code class="o">);</code>
    <code class="n">StringPair</code> <code class="n">result</code> <code class="o">=</code> <code class="n">reader</code><code class="o">.</code><code class="na">read</code><code class="o">(</code><code class="k">null</code><code class="o">,</code> <code class="n">decoder</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">result</code><code class="o">.</code><span class="calibre24"><strong class="calibre9"><code class="n1">getLeft</code><code class="o1">()</code></strong></span><code class="o">,</code> <code class="n">is</code><code class="o">(</code><code class="sb">"L"</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">result</code><code class="o">.</code><span class="calibre24"><strong class="calibre9"><code class="n1">getRight</code><code class="o1">()</code></strong></span><code class="o">,</code> <code class="n">is</code><code class="o">(</code><code class="sb">"R"</code><code class="o">));</code></pre></div></div><div class="book" title="Avro Datafiles"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-275">Avro Datafiles</h2></div></div></div><p class="calibre2">Avro’s object <a class="calibre" id="calibre_link-941"></a>container file format is for storing sequences of Avro
    objects. It is very similar in design to Hadoop’s sequence file format,
    described in <a class="ulink" href="#calibre_link-141" title="SequenceFile">SequenceFile</a>. The main difference is that
    Avro datafiles are designed to be portable across languages, so, for
    example, you can write a file in Python and read it in C (we will do
    exactly this in the next section).</p><p class="calibre2">A datafile has a header containing metadata, including the Avro
    schema and a <em class="calibre10">sync marker</em>, followed by a series of
    (optionally compressed) blocks containing the serialized Avro objects.
    Blocks are separated by a sync marker that is unique to the file (the
    marker for a particular file is found in the header) and that permits
    rapid resynchronization with a block boundary after seeking to an
    arbitrary point in the file, such as an HDFS block boundary. Thus, Avro
    datafiles are splittable, which makes them amenable to efficient MapReduce
    processing.</p><p class="calibre2">Writing Avro objects to a datafile is similar to writing to a
    stream. We use a <code class="literal">DatumWriter</code> as before, but
    instead of using an <code class="literal">Encoder</code>, we create a
    <code class="literal">DataFileWriter</code> instance <a class="calibre" id="calibre_link-1372"></a>with the <code class="literal">DatumWriter</code>. Then we can
    create a new datafile (which, by convention, has a
    <span class="calibre"><em class="calibre10">.avro</em></span> extension) and append objects to it:</p><a id="calibre_link-4398" class="calibre"></a><pre class="screen1">    <code class="n">File</code> <code class="n">file</code> <code class="o">=</code> <code class="k">new</code> <code class="n">File</code><code class="o">(</code><code class="sb">"data.avro"</code><code class="o">);</code>
    <code class="n">DatumWriter</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;</code> <code class="n">writer</code> <code class="o">=</code>
        <code class="k">new</code> <code class="n">GenericDatumWriter</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;(</code><code class="n">schema</code><code class="o">);</code>
    <code class="n">DataFileWriter</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;</code> <code class="n">dataFileWriter</code> <code class="o">=</code>
        <code class="k">new</code> <code class="n">DataFileWriter</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;(</code><code class="n">writer</code><code class="o">);</code>
    <code class="n">dataFileWriter</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">schema</code><code class="o">,</code> <code class="n">file</code><code class="o">);</code>
    <code class="n">dataFileWriter</code><code class="o">.</code><code class="na">append</code><code class="o">(</code><code class="n">datum</code><code class="o">);</code>
    <code class="n">dataFileWriter</code><code class="o">.</code><code class="na">close</code><code class="o">();</code></pre><p class="calibre2">The objects that we write to the datafile must conform to the file’s
    schema; otherwise, an exception will be thrown when we call
    <code class="literal">append()</code>.</p><p class="calibre2">This example demonstrates writing to a local file
    (<code class="literal">java.io.File</code> in the previous snippet), but we can
    write to any <code class="literal">java.io.OutputStream</code> by using the
    overloaded <code class="literal">create()</code> method on
    <code class="literal">DataFileWriter</code>. To write a file to HDFS, for
    example, we get an <code class="literal">OutputStream</code> by calling
    <code class="literal">create()</code> on <code class="literal">FileSystem</code>
    (see <a class="ulink" href="#calibre_link-142" title="Writing Data">Writing Data</a>).</p><p class="calibre2">Reading back objects from a datafile is similar to the earlier case
    of reading objects from an in-memory stream, with one important
    difference: we don’t have to specify a schema, since it is read from the
    file metadata. Indeed, we can get the schema from the
    <code class="literal">DataFileReader</code> instance, <a class="calibre" id="calibre_link-1370"></a>using <code class="literal">getSchema()</code>, and verify that
    it is the same as the one we used to write the original object:</p><a id="calibre_link-4399" class="calibre"></a><pre class="screen1">    <code class="n">DatumReader</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;</code> <code class="n">reader</code> <code class="o">=</code> <code class="k">new</code> <code class="n">GenericDatumReader</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;();</code>
    <code class="n">DataFileReader</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;</code> <code class="n">dataFileReader</code> <code class="o">=</code>
        <code class="k">new</code> <code class="n">DataFileReader</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;(</code><code class="n">file</code><code class="o">,</code> <code class="n">reader</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="sb">"Schema is the same"</code><code class="o">,</code> <code class="n">schema</code><code class="o">,</code> <code class="n">is</code><code class="o">(</code><code class="n">dataFileReader</code><code class="o">.</code><code class="na">getSchema</code><code class="o">()));</code></pre><p class="calibre2"><code class="literal">DataFileReader</code> is a regular Java iterator, so
    we can iterate through its data objects by calling its
    <code class="literal">hasNext()</code> and <code class="literal">next()</code>
    methods. The following snippet checks that there is only one record and
    that it has the expected field values:</p><a id="calibre_link-4400" class="calibre"></a><pre class="screen1">    <code class="n">assertThat</code><code class="o">(</code><code class="n">dataFileReader</code><code class="o">.</code><code class="na">hasNext</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="k">true</code><code class="o">));</code>
    <code class="n">GenericRecord</code> <code class="n">result</code> <code class="o">=</code> <code class="n">dataFileReader</code><code class="o">.</code><code class="na">next</code><code class="o">();</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">result</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"left"</code><code class="o">).</code><code class="na">toString</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"L"</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">result</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"right"</code><code class="o">).</code><code class="na">toString</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"R"</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">dataFileReader</code><code class="o">.</code><code class="na">hasNext</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="k">false</code><code class="o">));</code></pre><p class="calibre2">Rather than using the usual <code class="literal">next()</code> method,
    however, it is preferable to use the overloaded form that takes an
    instance of the object to be returned (in this case,
    <code class="literal">Generic</code><code class="literal">Record</code>), since it
    will reuse the object and save allocation and garbage collection costs for
    files containing many objects. The following is idiomatic:</p><a id="calibre_link-4401" class="calibre"></a><pre class="screen1">    <code class="n">GenericRecord</code> <code class="n">record</code> <code class="o">=</code> <code class="k">null</code><code class="o">;</code>
    <code class="k">while</code> <code class="o">(</code><code class="n">dataFileReader</code><code class="o">.</code><code class="na">hasNext</code><code class="o">())</code> <code class="o">{</code>
      <code class="n">record</code> <code class="o">=</code> <code class="n">dataFileReader</code><code class="o">.</code><code class="na">next</code><code class="o">(</code><code class="n">record</code><code class="o">);</code>
      <code class="c2">// process record</code>
    <code class="o">}</code></pre><p class="calibre2">If object reuse is not important, you can use this shorter
    form:</p><a id="calibre_link-4402" class="calibre"></a><pre class="screen1">    <code class="k">for</code> <code class="o">(</code><code class="n">GenericRecord</code> <code class="n">record</code> <code class="o">:</code> <code class="n">dataFileReader</code><code class="o">)</code> <code class="o">{</code>
      <code class="c2">// process record</code>
    <code class="o">}</code></pre><p class="calibre2">For the general case of reading a file on a Hadoop filesystem, use
    Avro’s <code class="literal">FsInput</code> to <a class="calibre" id="calibre_link-1759"></a>specify the input file using a Hadoop
    <code class="literal">Path</code> object. <code class="literal">DataFileReader</code>
    actually offers random access to Avro datafiles (via its
    <code class="literal">seek()</code> and <code class="literal">sync()</code>
    methods); however, in many cases, sequential streaming access is
    sufficient, for which <code class="literal">DataFileStream</code> should
    <a class="calibre" id="calibre_link-1371"></a>be used. <code class="literal">DataFileStream</code> can read from
    any <a class="calibre" id="calibre_link-942"></a>Java <code class="literal">InputStream</code>.</p></div><div class="book" title="Interoperability"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4403">Interoperability</h2></div></div></div><p class="calibre2">To demonstrate Avro’s language <a class="calibre" id="calibre_link-945"></a>interoperability, let’s write a datafile using one language (Python) and read it back with
    another (Java).</p><div class="book" title="Python API"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4404">Python API</h3></div></div></div><p class="calibre2">The program in <a class="ulink" href="#calibre_link-143" title="Example&nbsp;12-1.&nbsp;A Python program for writing Avro record pairs to a datafile">Example&nbsp;12-1</a> r<a class="calibre" id="calibre_link-3097"></a>eads comma-separated strings from standard input and
      writes them as <code class="literal">StringPair</code> records to
      an Avro datafile. Like in the Java code for writing a datafile, we
      create a <code class="literal">DatumWriter</code> and a
      <code class="literal">DataFileWriter</code> object. Notice that we have
      embedded the Avro schema in the code, although we could equally well
      have read it from a file.</p><p class="calibre2">Python represents Avro records as dictionaries; each line that is
      read from standard in is turned into a <code class="literal">dict</code> object and appended to the
      <code class="literal">DataFileWriter</code>.</p><div class="example"><a id="calibre_link-143" class="calibre"></a><div class="example-title">Example&nbsp;12-1.&nbsp;A Python program for writing Avro record pairs to a
        datafile</div><div class="book"><pre class="screen"><code class="k">import</code> <code class="nn">os</code>
<code class="k">import</code> <code class="nn">string</code>
<code class="k">import</code> <code class="nn">sys</code>

<code class="k">from</code> <code class="nn">avro</code> <code class="k">import</code> <code class="n">schema</code>
<code class="k">from</code> <code class="nn">avro</code> <code class="k">import</code> <code class="n">io</code>
<code class="k">from</code> <code class="nn">avro</code> <code class="k">import</code> <code class="n">datafile</code>

<code class="k">if</code> <code class="n">__name__</code> <code class="o">==</code> <code class="sb">'__main__'</code><code class="p">:</code>
  <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">sys</code><code class="o">.</code><code class="n">argv</code><code class="p">)</code> <code class="o">!=</code> <code class="mi">2</code><code class="p">:</code>
    <code class="n">sys</code><code class="o">.</code><code class="n">exit</code><code class="p">(</code><code class="sb">'Usage: </code><code class="si">%s</code><code class="sb"> &lt;data_file&gt;'</code> <code class="o">%</code> <code class="n">sys</code><code class="o">.</code><code class="n">argv</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
  <code class="n">avro_file</code> <code class="o">=</code> <code class="n">sys</code><code class="o">.</code><code class="n">argv</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>
  <code class="n">writer</code> <code class="o">=</code> <code class="nb">open</code><code class="p">(</code><code class="n">avro_file</code><code class="p">,</code> <code class="sb">'wb'</code><code class="p">)</code>
  <code class="n">datum_writer</code> <code class="o">=</code> <code class="n">io</code><code class="o">.</code><code class="n">DatumWriter</code><code class="p">()</code>
  <code class="n">schema_object</code> <code class="o">=</code> <code class="n">schema</code><code class="o">.</code><code class="n">parse</code><code class="p">(</code><code class="sb">"</code><code class="se">\</code>
<code class="sb">{ "</code><code class="nb">type</code><code class="sb">": "</code><code class="n">record</code><code class="sb">",</code>
  <code class="sb">"name"</code><code class="p">:</code> <code class="sb">"StringPair"</code><code class="p">,</code>
  <code class="sb">"doc"</code><code class="p">:</code> <code class="sb">"A pair of strings."</code><code class="p">,</code>
  <code class="sb">"fields"</code><code class="p">:</code> <code class="p">[</code>
    <code class="p">{</code><code class="sb">"name"</code><code class="p">:</code> <code class="sb">"left"</code><code class="p">,</code> <code class="sb">"type"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">},</code>
    <code class="p">{</code><code class="sb">"name"</code><code class="p">:</code> <code class="sb">"right"</code><code class="p">,</code> <code class="sb">"type"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">}</code>
  <code class="p">]</code>
<code class="p">}</code><code class="sb">")</code>
  <code class="n">dfw</code> <code class="o">=</code> <code class="n">datafile</code><code class="o">.</code><code class="n">DataFileWriter</code><code class="p">(</code><code class="n">writer</code><code class="p">,</code> <code class="n">datum_writer</code><code class="p">,</code> <code class="n">schema_object</code><code class="p">)</code>
  <code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">sys</code><code class="o">.</code><code class="n">stdin</code><code class="o">.</code><code class="n">readlines</code><code class="p">():</code>
    <code class="p">(</code><code class="n">left</code><code class="p">,</code> <code class="n">right</code><code class="p">)</code> <code class="o">=</code> <code class="n">string</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">line</code><code class="o">.</code><code class="n">strip</code><code class="p">(),</code> <code class="sb">','</code><code class="p">)</code>
    <code class="n">dfw</code><code class="o">.</code><code class="n">append</code><code class="p">({</code><code class="sb">'left'</code><code class="p">:</code><code class="n">left</code><code class="p">,</code> <code class="sb">'right'</code><code class="p">:</code><code class="n">right</code><code class="p">});</code>
  <code class="n">dfw</code><code class="o">.</code><code class="n">close</code><code class="p">()</code></pre></div></div><p class="calibre2">Before we can run the program, we need to install Avro for
      Python:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">easy_install avro</code></strong></pre><p class="calibre2">To run the program, we specify the name of the file to write
      output to (<em class="calibre10">pairs.avro</em>) and send
      input pairs over standard in, marking the end of file by typing
      Ctrl-D:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">python ch12-avro/src/main/py/write_pairs.py pairs.avro</code></strong>
<strong class="userinput"><code class="calibre9">a,1</code></strong>
<strong class="userinput"><code class="calibre9">c,2</code></strong>
<strong class="userinput"><code class="calibre9">b,3</code></strong>
<strong class="userinput"><code class="calibre9">b,2</code></strong>
<strong class="userinput"><code class="calibre9">^D</code></strong></pre></div><div class="book" title="Avro Tools"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4405">Avro Tools</h3></div></div></div><p class="calibre2">Next, we’ll use the <a class="calibre" id="calibre_link-960"></a>Avro tools (written in Java) to display the contents of
      <em class="calibre10">pairs.avro</em>. The tools JAR is
      available from the Avro website; here we assume it’s been placed in a
      local directory called <span class="calibre"><em class="calibre10">$AVRO_HOME</em></span>. The
      <code class="literal">tojson</code> command <a class="calibre" id="calibre_link-3695"></a>converts an Avro datafile to JSON and prints it to the
      console:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">java -jar $AVRO_HOME/avro-tools-*.jar tojson pairs.avro</code></strong>
{"left":"a","right":"1"}
{"left":"c","right":"2"}
{"left":"b","right":"3"}
{"left":"b","right":"2"}</pre><p class="calibre2">We have successfully exchanged complex data between two Avro
      implementations <a class="calibre" id="calibre_link-946"></a>(Python and Java).</p></div></div><div class="book" title="Schema Resolution"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-220">Schema Resolution</h2></div></div></div><p class="calibre2">We can choose to use a different <a class="calibre" id="calibre_link-952"></a>schema for reading the data back (the <em class="calibre10">reader’s
    schema</em>) from the one we used to write it (the
    <em class="calibre10">writer’s schema</em>). This is a powerful tool because
    it enables schema evolution. To illustrate, consider a new schema for
    string pairs with an added <code class="literal">description</code>
    field:</p><a id="calibre_link-4406" class="calibre"></a><pre class="screen1"><code class="p">{</code>
  <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"record"</code><code class="p">,</code>
  <code class="nt">"name"</code><code class="p">:</code> <code class="sb">"StringPair"</code><code class="p">,</code>
  <code class="nt">"doc"</code><code class="p">:</code> <code class="sb">"A pair of strings with an added field."</code><code class="p">,</code>
  <code class="nt">"fields"</code><code class="p">:</code> <code class="p">[</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"left"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">},</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"right"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">},</code>
    <span class="calibre24"><strong class="calibre9"><code class="p1">{</code><code class="na1">"name"</code><code class="p1">:</code> <code class="s">"description"</code><code class="p1">,</code> <code class="na1">"type"</code><code class="p1">:</code> <code class="s">"string"</code><code class="p1">,</code> <code class="na1">"default"</code><code class="p1">:</code> <code class="s">""</code><code class="p1">}</code></strong></span>
  <code class="si">]</code>
<code class="si">}</code></pre><p class="calibre2">We can use this schema to read the data we serialized earlier
    because, crucially, we have given the <code class="literal">description</code> field a default value (the empty
    string),<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-144" id="calibre_link-156">82</a>]</sup> which Avro will use when there is no such field defined in
    the records it is reading. Had we omitted the <code class="literal">default</code>
    attribute, we would get an error
    when trying to read the old data.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">To make the default value <code class="literal">null</code>
      rather than the empty string, we would instead define the
      <code class="literal">description</code> field using a union with the <code class="literal">null</code> Avro type:</p><a id="calibre_link-4407" class="calibre"></a><pre class="programlisting"><code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"description"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="p">[</code><code class="sb">"null"</code><code class="p">,</code> <code class="sb">"string"</code><code class="p">],</code> <code class="nt">"default"</code><code class="p">:</code> <code class="k">null</code><code class="p">}</code></pre></div><p class="calibre2">When the reader’s schema is different from the writer’s, we use the
    constructor for <code class="literal">GenericDatumReader</code> that takes two
    schema objects, the writer’s and the reader’s, in that order:</p><a id="calibre_link-4408" class="calibre"></a><pre class="screen1">    <code class="n">DatumReader</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;</code> <code class="n">reader</code> <code class="o">=</code>
        <span class="calibre24"><strong class="calibre9"><code class="kc">new</code> <code class="n1">GenericDatumReader</code><code class="o1">&lt;</code><code class="n1">GenericRecord</code><code class="o1">&gt;(</code><code class="n1">schema</code><code class="o1">,</code> <code class="n1">newSchema</code><code class="o1">);</code></strong></span>
    <code class="n">Decoder</code> <code class="n">decoder</code> <code class="o">=</code> <code class="n">DecoderFactory</code><code class="o">.</code><code class="na">get</code><code class="o">().</code><code class="na">binaryDecoder</code><code class="o">(</code><code class="n">out</code><code class="o">.</code><code class="na">toByteArray</code><code class="o">(),</code>
        <code class="k">null</code><code class="o">);</code>
    <code class="n">GenericRecord</code> <code class="n">result</code> <code class="o">=</code> <code class="n">reader</code><code class="o">.</code><code class="na">read</code><code class="o">(</code><code class="k">null</code><code class="o">,</code> <code class="n">decoder</code><code class="o">);</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">result</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"left"</code><code class="o">).</code><code class="na">toString</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"L"</code><code class="o">));</code>
    <code class="n">assertThat</code><code class="o">(</code><code class="n">result</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"right"</code><code class="o">).</code><code class="na">toString</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"R"</code><code class="o">));</code>
    <span class="calibre24"><strong class="calibre9"><code class="n1">assertThat</code><code class="o1">(</code><code class="n1">result</code><code class="o1">.</code><code class="na1">get</code><code class="o1">(</code><code class="s">"description"</code><code class="o1">).</code><code class="na1">toString</code><code class="o1">(),</code> <code class="n1">is</code><code class="o1">(</code><code class="s">""</code><code class="o1">));</code></strong></span></pre><p class="calibre2">For datafiles, which have the writer’s schema stored in the
    metadata, we only need to specify the reader’s schema explicitly, which we
    can do by passing <code class="literal">null</code> for the writer’s
    schema:</p><a id="calibre_link-4409" class="calibre"></a><pre class="screen1">    <code class="n">DatumReader</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;</code> <code class="n">reader</code> <code class="o">=</code>
        <code class="k">new</code> <code class="n">GenericDatumReader</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;(</code><span class="calibre24"><strong class="calibre9"><code class="kc">null</code></strong></span><code class="o">,</code> <code class="n">newSchema</code><code class="o">);</code></pre><p class="calibre2">Another common use of a different reader’s schema is to drop fields
    in a record, an operation called <em class="calibre10">projection</em>. This
    is useful when you have records with a large number of fields and you want
    to read only some of them. For example, this schema can be used to get
    only the <code class="literal">right</code> field of a <code class="literal">StringPair</code>:</p><a id="calibre_link-4410" class="calibre"></a><pre class="screen1"><code class="p">{</code>
  <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"record"</code><code class="p">,</code>
  <code class="nt">"name"</code><code class="p">:</code> <code class="sb">"StringPair"</code><code class="p">,</code>
  <code class="nt">"doc"</code><code class="p">:</code> <code class="sb">"The right field of a pair of strings."</code><code class="p">,</code>
  <code class="nt">"fields"</code><code class="p">:</code> <code class="p">[</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"right"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">}</code>
  <code class="p">]</code>
<code class="p">}</code></pre><p class="calibre2">The rules for schema resolution have a direct bearing on how schemas
    may evolve from one version to the next, and are spelled out in the Avro
    specification for all Avro types. A summary of the rules for record
    evolution from the point of view of readers and writers (or servers and
    clients) is presented in <a class="ulink" href="#calibre_link-145" title="Table&nbsp;12-4.&nbsp;Schema resolution of records">Table&nbsp;12-4</a>.</p><div class="table"><a id="calibre_link-145" class="calibre"></a><div class="table-title">Table&nbsp;12-4.&nbsp;Schema resolution of records</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">New schema</td><td class="calibre20">Writer</td><td class="calibre20">Reader</td><td class="calibre21">Action</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td rowspan="2" class="calibre23">Added field</td><td class="calibre23">Old</td><td class="calibre23">New</td><td class="calibre25">The reader uses the default value of the new field, since
            it is not written by the writer.</td></tr><tr class="calibre26"><td class="calibre23">New</td><td class="calibre23">Old</td><td class="calibre25">The reader does not know about the new field written by the
            writer, so it is ignored (projection).</td></tr><tr class="calibre19"><td rowspan="2" class="calibre27">Removed field</td><td class="calibre23">Old</td><td class="calibre23">New</td><td class="calibre25">The reader ignores the removed field (projection).</td></tr><tr class="calibre26"><td class="calibre27">New</td><td class="calibre27">Old</td><td class="calibre28">The removed field is not written by the writer. If the old
            schema had a default defined for the field, the reader uses this;
            otherwise, it gets an error. In this case, it is best to update
            the reader’s schema, either at the same time as or before the
            writer’s.</td></tr></tbody></table></div></div><p class="calibre2">Another useful technique for evolving Avro schemas is the use of
    name <em class="calibre10">aliases</em>. Aliases allow you to use different
    names in the schema used to read the Avro data than in the schema
    originally used to write the data. For example, the following reader’s
    schema can be used to read <code class="literal">StringPair</code>
    data with the new field names <code class="literal">first</code> and
    <code class="literal">second</code> instead of <code class="literal">left</code> and <code class="literal">right</code> (which are what it was written
    with):</p><a id="calibre_link-4411" class="calibre"></a><pre class="screen1"><code class="p">{</code>
  <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"record"</code><code class="p">,</code>
  <code class="nt">"name"</code><code class="p">:</code> <code class="sb">"StringPair"</code><code class="p">,</code>
  <code class="nt">"doc"</code><code class="p">:</code> <code class="sb">"A pair of strings with aliased field names."</code><code class="p">,</code>
  <code class="nt">"fields"</code><code class="p">:</code> <code class="p">[</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <span class="calibre24"><strong class="calibre9"><code class="s">"first"</code></strong></span><code class="si">,</code> <code class="sb">"type"</code><code class="si">:</code> <code class="sb">"string"</code><code class="si">,</code> <span class="calibre24"><strong class="calibre9"><code class="s">"aliases"</code><code class="err">:</code> <code class="p1">[</code><code class="s">"left"</code><code class="p1">]</code></strong></span><code class="si">},</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <span class="calibre24"><strong class="calibre9"><code class="s">"second"</code></strong></span><code class="si">,</code> <code class="sb">"type"</code><code class="si">:</code> <code class="sb">"string"</code><code class="si">,</code> <span class="calibre24"><strong class="calibre9"><code class="s">"aliases"</code><code class="err">:</code> <code class="p1">[</code><code class="s">"right"</code><code class="p1">]</code></strong></span><code class="si">}</code>
  <code class="si">]</code>
<code class="si">}</code></pre><p class="calibre2">Note that the aliases are used to translate (at read time) the
    writer’s schema into the reader’s, but the alias names are not available
    to the reader. In this example, the reader cannot use the field names
    <code class="literal">left</code> and <code class="literal">right</code>, because they have already been <a class="calibre" id="calibre_link-953"></a>translated to <code class="literal">first</code> and
    <code class="literal">second</code>.</p></div><div class="book" title="Sort Order"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4412">Sort Order</h2></div></div></div><p class="calibre2">Avro defines a sort <a class="calibre" id="calibre_link-956"></a><a class="calibre" id="calibre_link-3427"></a>order for objects. For most Avro types, the order is the
    natural one you would expect—for example, numeric types are ordered by
    ascending numeric value. Others are a little more subtle. For instance,
    enums are compared by the order in which the symbols are defined and not
    by the values of the symbol strings.</p><p class="calibre2">All types except <code class="literal">record</code> have
    preordained rules for their sort order, as described in the Avro
    specification, that cannot be overridden by the user. For records,
    however, you can control the sort order by specifying the <code class="literal">order</code> attribute for a field. It takes one of
    three values: <code class="literal">ascending</code> (the default),
    <code class="literal">descending</code> (to reverse the order), or
    <code class="literal">ignore</code> (so the field is skipped for
    comparison purposes).</p><p class="calibre2">For example, the following schema (<em class="calibre10">SortedStringPair.avsc</em>) defines an ordering of
    <code class="literal">StringPair</code> records by the <code class="literal">right</code> field in descending order. The <code class="literal">left</code> field is ignored for the purposes of
    ordering, but it is still present in the projection:</p><a id="calibre_link-4413" class="calibre"></a><pre class="screen1"><code class="p">{</code>
  <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"record"</code><code class="p">,</code>
  <code class="nt">"name"</code><code class="p">:</code> <code class="sb">"StringPair"</code><code class="p">,</code>
  <code class="nt">"doc"</code><code class="p">:</code> <code class="sb">"A pair of strings, sorted by right field descending."</code><code class="p">,</code>
  <code class="nt">"fields"</code><code class="p">:</code> <code class="p">[</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"left"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">,</code> <code class="nt">"order"</code><code class="p">:</code> <code class="sb">"ignore"</code><code class="p">},</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"right"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">,</code> <code class="nt">"order"</code><code class="p">:</code> <code class="sb">"descending"</code><code class="p">}</code>
  <code class="p">]</code>
<code class="p">}</code></pre><p class="calibre2">The record’s fields are compared pairwise in the document order of
    the reader’s schema. Thus, by specifying an appropriate reader’s schema,
    you can impose an arbitrary ordering
    on data records. This schema (<em class="calibre10">SwitchedStringPair.avsc</em>) defines a sort order
    by the <code class="literal">right</code> field, then the <code class="literal">left</code>:</p><a id="calibre_link-4414" class="calibre"></a><pre class="screen1"><code class="p">{</code>
  <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"record"</code><code class="p">,</code>
  <code class="nt">"name"</code><code class="p">:</code> <code class="sb">"StringPair"</code><code class="p">,</code>
  <code class="nt">"doc"</code><code class="p">:</code> <code class="sb">"A pair of strings, sorted by right then left."</code><code class="p">,</code>
  <code class="nt">"fields"</code><code class="p">:</code> <code class="p">[</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"right"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">},</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"left"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">}</code>
  <code class="p">]</code>
<code class="p">}</code></pre><p class="calibre2">Avro implements efficient binary comparisons. That is to say, Avro
    does not have to deserialize binary data into objects to perform the
    comparison, because it can instead work directly on the byte
    streams.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-146" id="calibre_link-157">83</a>]</sup> In the case of the original <code class="literal">StringPair</code> schema (with no <code class="literal">order</code> attributes), for example, Avro implements
    the binary comparison as follows.</p><p class="calibre2">The first field, <code class="literal">left</code>, is a
    UTF-8-encoded string, for which Avro can compare the bytes
    lexicographically. If they differ, the order is determined, and Avro can
    stop the comparison there. Otherwise, if the two byte sequences are the
    same, it compares the second two (<code class="literal">right</code>) fields, again lexicographically at the
    byte level because the field is another UTF-8 string.</p><p class="calibre2">Notice that this description of a comparison function has exactly
    the same logic as the binary comparator we wrote for Writables in <a class="ulink" href="#calibre_link-147" title="Implementing a RawComparator for speed">Implementing a RawComparator for speed</a>. The great thing is that Avro
    provides the comparator for us, so we don’t have to write and maintain
    this code. It’s also easy to change the sort order just by changing the
    reader’s schema. For the <em class="calibre10">SortedStringPair.avsc</em> and <em class="calibre10">SwitchedStringPair.avsc</em> schemas, the comparison
    function Avro uses is essentially the same as the one just described. The
    differences are which fields are considered, the order in which they are
    considered, and whether the sort order is ascending or descending.</p><p class="calibre2">Later in the chapter, we’ll use Avro’s sorting logic in conjunction
    with MapReduce to sort Avro datafiles in parallel.</p></div><div class="book" title="Avro MapReduce"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-401">Avro MapReduce</h2></div></div></div><p class="calibre2">Avro provides a <a class="calibre" id="calibre_link-948"></a><a class="calibre" id="calibre_link-2443"></a>number of classes for making it easy to run MapReduce
    programs on Avro data. We’ll use the new MapReduce API classes from
    <a class="calibre" id="calibre_link-2854"></a>the <code class="literal">org.apache.avro.mapreduce</code> package, but you can
    find (old-style) MapReduce classes in the <code class="literal">org.apache</code><code class="literal">.avro.mapred</code> package.</p><p class="calibre2">Let’s rework the MapReduce program for finding the maximum
    temperature for each year in the weather dataset, this time using the Avro
    MapReduce API. We will represent weather records using the following
    schema:</p><a id="calibre_link-4415" class="calibre"></a><pre class="screen1"><code class="p">{</code>
  <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"record"</code><code class="p">,</code>
  <code class="nt">"name"</code><code class="p">:</code> <code class="sb">"WeatherRecord"</code><code class="p">,</code>
  <code class="nt">"doc"</code><code class="p">:</code> <code class="sb">"A weather reading."</code><code class="p">,</code>
  <code class="nt">"fields"</code><code class="p">:</code> <code class="p">[</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"year"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"int"</code><code class="p">},</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"temperature"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"int"</code><code class="p">},</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"stationId"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">}</code>
  <code class="p">]</code>
<code class="p">}</code></pre><p class="calibre2">The program in <a class="ulink" href="#calibre_link-148" title="Example&nbsp;12-2.&nbsp;MapReduce program to find the maximum temperature, creating Avro output">Example&nbsp;12-2</a> reads
    text input (in the format we saw in earlier chapters) and writes Avro
    datafiles containing weather records as output.</p><div class="example"><a id="calibre_link-148" class="calibre"></a><div class="example-title">Example&nbsp;12-2.&nbsp;MapReduce program to find the maximum temperature, creating Avro
      output</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">AvroGenericMaxTemperature</code> <code class="k">extends</code> <code class="n">Configured</code> <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>
  
  <code class="k">private</code> <code class="k">static</code> <code class="k">final</code> <code class="n">Schema</code> <code class="n">SCHEMA</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Schema</code><code class="o">.</code><code class="na">Parser</code><code class="o">().</code><code class="na">parse</code><code class="o">(</code>
      <code class="sb">"{"</code> <code class="o">+</code>
      <code class="sb">"  \"type\": \"record\","</code> <code class="o">+</code>
      <code class="sb">"  \"name\": \"WeatherRecord\","</code> <code class="o">+</code>
      <code class="sb">"  \"doc\": \"A weather reading.\","</code> <code class="o">+</code>
      <code class="sb">"  \"fields\": ["</code> <code class="o">+</code>
      <code class="sb">"    {\"name\": \"year\", \"type\": \"int\"},"</code> <code class="o">+</code>
      <code class="sb">"    {\"name\": \"temperature\", \"type\": \"int\"},"</code> <code class="o">+</code>
      <code class="sb">"    {\"name\": \"stationId\", \"type\": \"string\"}"</code> <code class="o">+</code>
      <code class="sb">"  ]"</code> <code class="o">+</code>
      <code class="sb">"}"</code>
  <code class="o">);</code>

  <code class="k">public</code> <code class="k">static</code> <code class="k">class</code> <code class="nc">MaxTemperatureMapper</code>
      <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">AvroKey</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">&gt;,</code>
            <code class="n">AvroValue</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;&gt;</code> <code class="o">{</code>
    <code class="k">private</code> <code class="n">NcdcRecordParser</code> <code class="n">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="n">NcdcRecordParser</code><code class="o">();</code>
    <code class="k">private</code> <code class="n">GenericRecord</code> <code class="n">record</code> <code class="o">=</code> <code class="k">new</code> <code class="n">GenericData</code><code class="o">.</code><code class="na">Record</code><code class="o">(</code><code class="n">SCHEMA</code><code class="o">);</code>

    <code class="nd">@Override</code>
    <code class="k">protected</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">LongWritable</code> <code class="n">key</code><code class="o">,</code> <code class="n">Text</code> <code class="n">value</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
        <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
      <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">value</code><code class="o">.</code><code class="na">toString</code><code class="o">());</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">isValidTemperature</code><code class="o">())</code> <code class="o">{</code>
        <code class="n">record</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="sb">"year"</code><code class="o">,</code> <code class="n">parser</code><code class="o">.</code><code class="na">getYearInt</code><code class="o">());</code>
        <code class="n">record</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="sb">"temperature"</code><code class="o">,</code> <code class="n">parser</code><code class="o">.</code><code class="na">getAirTemperature</code><code class="o">());</code>
        <code class="n">record</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="sb">"stationId"</code><code class="o">,</code> <code class="n">parser</code><code class="o">.</code><code class="na">getStationId</code><code class="o">());</code>
        <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="k">new</code> <code class="n">AvroKey</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">&gt;(</code><code class="n">parser</code><code class="o">.</code><code class="na">getYearInt</code><code class="o">()),</code>
            <code class="k">new</code> <code class="n">AvroValue</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;(</code><code class="n">record</code><code class="o">));</code>
      <code class="o">}</code>
    <code class="o">}</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="k">class</code> <code class="nc">MaxTemperatureReducer</code>
      <code class="k">extends</code> <code class="n">Reducer</code><code class="o">&lt;</code><code class="n">AvroKey</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">&gt;,</code> <code class="n">AvroValue</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;,</code>
            <code class="n">AvroKey</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;,</code> <code class="n">NullWritable</code><code class="o">&gt;</code> <code class="o">{</code>

    <code class="nd">@Override</code>
    <code class="k">protected</code> <code class="kt">void</code> <code class="nf">reduce</code><code class="o">(</code><code class="n">AvroKey</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">&gt;</code> <code class="n">key</code><code class="o">,</code> <code class="n">Iterable</code><code class="o">&lt;</code><code class="n">AvroValue</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;&gt;</code>
        <code class="n">values</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
      <code class="n">GenericRecord</code> <code class="n">max</code> <code class="o">=</code> <code class="k">null</code><code class="o">;</code>
      <code class="k">for</code> <code class="o">(</code><code class="n">AvroValue</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;</code> <code class="n">value</code> <code class="o">:</code> <code class="n">values</code><code class="o">)</code> <code class="o">{</code>
        <code class="n">GenericRecord</code> <code class="n">record</code> <code class="o">=</code> <code class="n">value</code><code class="o">.</code><code class="na">datum</code><code class="o">();</code>
        <code class="k">if</code> <code class="o">(</code><code class="n">max</code> <code class="o">==</code> <code class="k">null</code> <code class="o">||</code> 
            <code class="o">(</code><code class="n">Integer</code><code class="o">)</code> <code class="n">record</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"temperature"</code><code class="o">)</code> <code class="o">&gt;</code> <code class="o">(</code><code class="n">Integer</code><code class="o">)</code> <code class="n">max</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"temperature"</code><code class="o">))</code> <code class="o">{</code>
          <code class="n">max</code> <code class="o">=</code> <code class="n">newWeatherRecord</code><code class="o">(</code><code class="n">record</code><code class="o">);</code>
        <code class="o">}</code>
      <code class="o">}</code>
      <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="k">new</code> <code class="n">AvroKey</code><code class="o">(</code><code class="n">max</code><code class="o">),</code> <code class="n">NullWritable</code><code class="o">.</code><code class="na">get</code><code class="o">());</code>
    <code class="o">}</code>
    <code class="k">private</code> <code class="n">GenericRecord</code> <code class="nf">newWeatherRecord</code><code class="o">(</code><code class="n">GenericRecord</code> <code class="n">value</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">GenericRecord</code> <code class="n">record</code> <code class="o">=</code> <code class="k">new</code> <code class="n">GenericData</code><code class="o">.</code><code class="na">Record</code><code class="o">(</code><code class="n">SCHEMA</code><code class="o">);</code>
      <code class="n">record</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="sb">"year"</code><code class="o">,</code> <code class="n">value</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"year"</code><code class="o">));</code>
      <code class="n">record</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="sb">"temperature"</code><code class="o">,</code> <code class="n">value</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"temperature"</code><code class="o">));</code>
      <code class="n">record</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="sb">"stationId"</code><code class="o">,</code> <code class="n">value</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"stationId"</code><code class="o">));</code>
      <code class="k">return</code> <code class="n">record</code><code class="o">;</code>
    <code class="o">}</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="na">length</code> <code class="o">!=</code> <code class="mi">2</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"Usage: %s [generic options] &lt;input&gt; &lt;output&gt;\n"</code><code class="o">,</code>
          <code class="n">getClass</code><code class="o">().</code><code class="na">getSimpleName</code><code class="o">());</code>
      <code class="n">ToolRunner</code><code class="o">.</code><code class="na">printGenericCommandUsage</code><code class="o">(</code><code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">);</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>

    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Job</code><code class="o">(</code><code class="n">getConf</code><code class="o">(),</code> <code class="sb">"Max temperature"</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setJarByClass</code><code class="o">(</code><code class="n">getClass</code><code class="o">());</code>

    <code class="n">job</code><code class="o">.</code><code class="na">getConfiguration</code><code class="o">().</code><code class="na">setBoolean</code><code class="o">(</code>
        <code class="n">Job</code><code class="o">.</code><code class="na">MAPREDUCE_JOB_USER_CLASSPATH_FIRST</code><code class="o">,</code> <code class="k">true</code><code class="o">);</code>

    <code class="n">FileInputFormat</code><code class="o">.</code><code class="na">addInputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]));</code>
    <code class="n">FileOutputFormat</code><code class="o">.</code><code class="na">setOutputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">]));</code>

    <code class="n">AvroJob</code><code class="o">.</code><code class="na">setMapOutputKeySchema</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">Schema</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">Schema</code><code class="o">.</code><code class="na">Type</code><code class="o">.</code><code class="na">INT</code><code class="o">));</code>
    <code class="n">AvroJob</code><code class="o">.</code><code class="na">setMapOutputValueSchema</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">SCHEMA</code><code class="o">);</code>
    <code class="n">AvroJob</code><code class="o">.</code><code class="na">setOutputKeySchema</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">SCHEMA</code><code class="o">);</code>

    <code class="n">job</code><code class="o">.</code><code class="na">setInputFormatClass</code><code class="o">(</code><code class="n">TextInputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputFormatClass</code><code class="o">(</code><code class="n">AvroKeyOutputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="n">job</code><code class="o">.</code><code class="na">setMapperClass</code><code class="o">(</code><code class="n">MaxTemperatureMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setReducerClass</code><code class="o">(</code><code class="n">MaxTemperatureReducer</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="k">return</code> <code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">;</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="k">new</code> <code class="n">AvroGenericMaxTemperature</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">This program uses the Generic Avro mapping. This frees us from
    generating code to represent records, at the expense of type safety (field
    names are referred to by string value, such as <code class="literal">"temperature"</code>).<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-149" id="calibre_link-158">84</a>]</sup> The schema for weather records is inlined in the code for
    convenience (and read into the <code class="literal">SCHEMA</code>
    constant), although in practice it might be more maintainable to read the
    schema from a local file in the driver code and pass it to the mapper and
    reducer via the Hadoop job configuration. (Techniques for achieving this
    are discussed in <a class="ulink" href="#calibre_link-150" title="Side Data Distribution">Side Data Distribution</a>.)</p><p class="calibre2">There are a couple of differences from the regular Hadoop MapReduce
    API. The first is the use of wrappers around Avro Java types. For this
    MapReduce program, the key is the year (an integer), and the value is the
    weather record, which is represented by Avro’s
    <code class="literal">GenericRecord</code>. This translates to
    <code class="literal">AvroKey&lt;Integer&gt;</code> for the key type and
    <code class="literal">AvroValue</code><code class="literal">&lt;GenericRecord&gt;</code>
    for the value type in the map output (and reduce input).</p><p class="calibre2">The <code class="literal">MaxTemperatureReducer</code>
    iterates through the records for each key (year) and finds the one with
    the maximum temperature. It is necessary to make a copy of the record with
    the highest temperature found so far, since the iterator reuses the
    instance for reasons of efficiency (and only the fields are
    updated).</p><p class="calibre2">The second major difference from regular MapReduce is the use of
    <code class="literal">AvroJob</code> for configuring the job.
    <code class="literal">AvroJob</code> is a convenience class for
    specifying the Avro schemas for the input, map output, and final output
    data. In this program, no input schema is set, because we are reading from
    a text file. The map output key schema is an Avro <code class="literal">int</code> and the value schema is the weather record
    schema. The final output key schema is the weather record schema, and the
    output format is <code class="literal">AvroKeyOutputFormat</code>,
    which writes keys to Avro datafiles and ignores the values (which are
    <code class="literal">NullWritable</code>).</p><p class="calibre2">The following commands show how to run the program on a small sample
    dataset:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">export HADOOP_CLASSPATH=avro-examples.jar</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">export HADOOP_USER_CLASSPATH_FIRST=true # override version of Avro in Hadoop</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar avro-examples.jar AvroGenericMaxTemperature \
  input/ncdc/sample.txt output</code></strong></pre><p class="calibre2">On completion we can look at the output using the Avro tools JAR to
    render the Avro datafile as JSON, one record per line:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">java -jar $AVRO_HOME/avro-tools-*.jar tojson output/part-r-00000.avro</code></strong>
{"year":1949,"temperature":111,"stationId":"012650-99999"}
{"year":1950,"temperature":22,"stationId":"011990-99999"}</pre><p class="calibre2">In this example we read a text file and created an Avro datafile,
    but other combinations are possible, which is useful for converting
    between Avro formats and other formats (such as
    <code class="literal">SequenceFiles</code>). See the documentation for the Avro
    MapReduce package for details.</p></div><div class="book" title="Sorting Using Avro MapReduce"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-714">Sorting Using Avro MapReduce</h2></div></div></div><p class="calibre2">In this section, we use <a class="calibre" id="calibre_link-3428"></a><a class="calibre" id="calibre_link-957"></a><a class="calibre" id="calibre_link-2503"></a><a class="calibre" id="calibre_link-3434"></a>Avro’s sort capabilities and combine them with MapReduce to
    write a program to sort an Avro datafile (<a class="ulink" href="#calibre_link-151" title="Example&nbsp;12-3.&nbsp;A MapReduce program to sort an Avro datafile">Example&nbsp;12-3</a>).</p><div class="example"><a id="calibre_link-151" class="calibre"></a><div class="example-title">Example&nbsp;12-3.&nbsp;A MapReduce program to sort an Avro datafile</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">AvroSort</code> <code class="k">extends</code> <code class="n">Configured</code> <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>

  <code class="k">static</code> <code class="k">class</code> <code class="nc">SortMapper</code><code class="o">&lt;</code><code class="n">K</code><code class="o">&gt;</code> <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">AvroKey</code><code class="o">&lt;</code><code class="n">K</code><code class="o">&gt;,</code> <code class="n">NullWritable</code><code class="o">,</code>
      <code class="n">AvroKey</code><code class="o">&lt;</code><code class="n">K</code><code class="o">&gt;,</code> <code class="n">AvroValue</code><code class="o">&lt;</code><code class="n">K</code><code class="o">&gt;&gt;</code> <code class="o">{</code>
    <code class="nd">@Override</code>
    <code class="k">protected</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">AvroKey</code><code class="o">&lt;</code><code class="n">K</code><code class="o">&gt;</code> <code class="n">key</code><code class="o">,</code> <code class="n">NullWritable</code> <code class="n">value</code><code class="o">,</code>
        <code class="n">Context</code> <code class="n">context</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
      <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">key</code><code class="o">,</code> <code class="k">new</code> <code class="n">AvroValue</code><code class="o">&lt;</code><code class="n">K</code><code class="o">&gt;(</code><code class="n">key</code><code class="o">.</code><code class="na">datum</code><code class="o">()));</code>
    <code class="o">}</code>
  <code class="o">}</code>

  <code class="k">static</code> <code class="k">class</code> <code class="nc">SortReducer</code><code class="o">&lt;</code><code class="n">K</code><code class="o">&gt;</code> <code class="k">extends</code> <code class="n">Reducer</code><code class="o">&lt;</code><code class="n">AvroKey</code><code class="o">&lt;</code><code class="n">K</code><code class="o">&gt;,</code> <code class="n">AvroValue</code><code class="o">&lt;</code><code class="n">K</code><code class="o">&gt;,</code>
      <code class="n">AvroKey</code><code class="o">&lt;</code><code class="n">K</code><code class="o">&gt;,</code> <code class="n">NullWritable</code><code class="o">&gt;</code> <code class="o">{</code>
    <code class="nd">@Override</code>
    <code class="k">protected</code> <code class="kt">void</code> <code class="nf">reduce</code><code class="o">(</code><code class="n">AvroKey</code><code class="o">&lt;</code><code class="n">K</code><code class="o">&gt;</code> <code class="n">key</code><code class="o">,</code> <code class="n">Iterable</code><code class="o">&lt;</code><code class="n">AvroValue</code><code class="o">&lt;</code><code class="n">K</code><code class="o">&gt;&gt;</code> <code class="n">values</code><code class="o">,</code>
        <code class="n">Context</code> <code class="n">context</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
      <code class="k">for</code> <code class="o">(</code><code class="n">AvroValue</code><code class="o">&lt;</code><code class="n">K</code><code class="o">&gt;</code> <code class="n">value</code> <code class="o">:</code> <code class="n">values</code><code class="o">)</code> <code class="o">{</code>
        <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="k">new</code> <code class="n">AvroKey</code><code class="o">(</code><code class="n">value</code><code class="o">.</code><code class="na">datum</code><code class="o">()),</code> <code class="n">NullWritable</code><code class="o">.</code><code class="na">get</code><code class="o">());</code>
      <code class="o">}</code>
    <code class="o">}</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    
    <code class="k">if</code> <code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="na">length</code> <code class="o">!=</code> <code class="mi">3</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code>
          <code class="sb">"Usage: %s [generic options] &lt;input&gt; &lt;output&gt; &lt;schema-file&gt;\n"</code><code class="o">,</code>
          <code class="n">getClass</code><code class="o">().</code><code class="na">getSimpleName</code><code class="o">());</code>
      <code class="n">ToolRunner</code><code class="o">.</code><code class="na">printGenericCommandUsage</code><code class="o">(</code><code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">);</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>
    
    <code class="n">String</code> <code class="n">input</code> <code class="o">=</code> <code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">];</code>
    <code class="n">String</code> <code class="n">output</code> <code class="o">=</code> <code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">];</code>
    <code class="n">String</code> <code class="n">schemaFile</code> <code class="o">=</code> <code class="n">args</code><code class="o">[</code><code class="mi">2</code><code class="o">];</code>

    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Job</code><code class="o">(</code><code class="n">getConf</code><code class="o">(),</code> <code class="sb">"Avro sort"</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setJarByClass</code><code class="o">(</code><code class="n">getClass</code><code class="o">());</code>

    <code class="n">job</code><code class="o">.</code><code class="na">getConfiguration</code><code class="o">().</code><code class="na">setBoolean</code><code class="o">(</code>
        <code class="n">Job</code><code class="o">.</code><code class="na">MAPREDUCE_JOB_USER_CLASSPATH_FIRST</code><code class="o">,</code> <code class="k">true</code><code class="o">);</code>

    <code class="n">FileInputFormat</code><code class="o">.</code><code class="na">addInputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">input</code><code class="o">));</code>
    <code class="n">FileOutputFormat</code><code class="o">.</code><code class="na">setOutputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">output</code><code class="o">));</code>

    <code class="n">AvroJob</code><code class="o">.</code><code class="na">setDataModelClass</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">GenericData</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="n">Schema</code> <code class="n">schema</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Schema</code><code class="o">.</code><code class="na">Parser</code><code class="o">().</code><code class="na">parse</code><code class="o">(</code><code class="k">new</code> <code class="n">File</code><code class="o">(</code><code class="n">schemaFile</code><code class="o">));</code>
    <code class="n">AvroJob</code><code class="o">.</code><code class="na">setInputKeySchema</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">schema</code><code class="o">);</code>
    <code class="n">AvroJob</code><code class="o">.</code><code class="na">setMapOutputKeySchema</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">schema</code><code class="o">);</code>
    <code class="n">AvroJob</code><code class="o">.</code><code class="na">setMapOutputValueSchema</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">schema</code><code class="o">);</code>
    <code class="n">AvroJob</code><code class="o">.</code><code class="na">setOutputKeySchema</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">schema</code><code class="o">);</code>

    <code class="n">job</code><code class="o">.</code><code class="na">setInputFormatClass</code><code class="o">(</code><code class="n">AvroKeyInputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputFormatClass</code><code class="o">(</code><code class="n">AvroKeyOutputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="n">job</code><code class="o">.</code><code class="na">setOutputKeyClass</code><code class="o">(</code><code class="n">AvroKey</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputValueClass</code><code class="o">(</code><code class="n">NullWritable</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="n">job</code><code class="o">.</code><code class="na">setMapperClass</code><code class="o">(</code><code class="n">SortMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setReducerClass</code><code class="o">(</code><code class="n">SortReducer</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="k">return</code> <code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">;</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="k">new</code> <code class="n">AvroSort</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">This program (which uses the Generic Avro mapping and hence does not
    require any code generation) can sort Avro records of any type,
    represented in Java by the generic type parameter <code class="literal">K</code>. We choose a value that is the same as the
    key, so that when the values are grouped by key we can emit all of the
    values in the case that more than one of them share the same key
    (according to the sorting function). This means we don’t lose any
    records.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-152" id="calibre_link-159">85</a>]</sup> The mapper simply emits the input key wrapped in an
    <code class="literal">AvroKey</code> and an <code class="literal">AvroValue</code>.
    The reducer acts as an identity, passing the values through as output
    keys, which will get written to an Avro datafile.</p><p class="calibre2">The sorting happens in the MapReduce shuffle, and the sort function
    is determined by the Avro schema that is passed to the program. Let’s use
    the program to sort the <em class="calibre10">pairs.avro</em>
    file created earlier, using the <em class="calibre10">SortedStringPair.avsc</em> schema to sort by the
    <code class="literal">right</code> field in descending order. First, we inspect the
    input using the Avro tools JAR:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">java -jar $AVRO_HOME/avro-tools-*.jar tojson input/avro/pairs.avro</code></strong>
{"left":"a","right":"1"}
{"left":"c","right":"2"}
{"left":"b","right":"3"}
{"left":"b","right":"2"}</pre><p class="calibre2">Then we run the sort:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar avro-examples.jar AvroSort input/avro/pairs.avro output \
  ch12-avro/src/main/resources/SortedStringPair.avsc</code></strong></pre><p class="calibre2">Finally, we inspect the output and see that it is sorted <a class="calibre" id="calibre_link-949"></a><a class="calibre" id="calibre_link-2444"></a><a class="calibre" id="calibre_link-958"></a><a class="calibre" id="calibre_link-2504"></a><a class="calibre" id="calibre_link-3429"></a><a class="calibre" id="calibre_link-3435"></a>correctly:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">java -jar $AVRO_HOME/avro-tools-*.jar tojson output/part-r-00000.avro</code></strong>
{"left":"b","right":"3"}
{"left":"b","right":"2"}
{"left":"c","right":"2"}
{"left":"a","right":"1"}</pre></div><div class="book" title="Avro in Other Languages"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4416">Avro in Other Languages</h2></div></div></div><p class="calibre2">For languages and <a class="calibre" id="calibre_link-947"></a>frameworks other than Java, there are a few choices for
    working with Avro data.</p><p class="calibre2"><code class="literal">AvroAsTextInputFormat</code> is designed
    <a class="calibre" id="calibre_link-962"></a>to allow Hadoop Streaming programs to read Avro datafiles.
    Each datum in the file is converted to a string, which is the JSON
    representation of the datum, or just to the raw bytes if the type is Avro
    <code class="literal">bytes</code>. Going the other way, you can
    specify <code class="literal">AvroTextOutputFormat</code> as the
    output format of a Streaming job to create Avro datafiles with a <code class="literal">bytes</code> schema, where each datum is the
    tab-delimited key-value pair written from the Streaming output. Both of
    these classes can be found in the <code class="literal">org.apache.avro.mapred</code> package.</p><p class="calibre2">It’s also worth considering other frameworks like Pig, Hive, Crunch,
    and Spark for doing Avro processing, since they can all read and write
    Avro datafiles by specifying the appropriate storage formats. See the
    relevant chapters in this book for details.</p></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" type="footnote" id="calibre_link-134"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-153">79</a>] </sup>Named after the British aircraft manufacturer from the 20th
      century.</p></div><div class="footnote" id="calibre_link-135"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-154">80</a>] </sup>Avro also performs favorably compared to other serialization
      libraries, as the <a class="ulink" href="http://code.google.com/p/thrift-protobuf-compare/" target="_top">benchmarks</a>
      demonstrate.</p></div><div class="footnote" id="calibre_link-140"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-155">81</a>] </sup>Avro can be downloaded in both <a class="ulink" href="http://avro.apache.org/releases.html" target="_top">source and binary
          forms</a>. Get usage instructions for the Avro tools by typing
          <code class="literal">java -jar
          avro-tools-*.jar</code>.</p></div><div class="footnote" type="footnote" id="calibre_link-144"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-156">82</a>] </sup>Default values for fields are encoded using JSON. See the Avro
        specification for a description of this encoding for each data
        type.</p></div><div class="footnote" type="footnote" id="calibre_link-146"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-157">83</a>] </sup>A useful consequence of this property is that you can compute an
        Avro datum’s hash code from either the object or the binary
        representation (the latter by using the static
        <code class="literal">hashCode()</code> method on
        <code class="literal">BinaryData</code>) and get the same result in both
        cases.</p></div><div class="footnote" type="footnote" id="calibre_link-149"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-158">84</a>] </sup>For an example that uses the Specific mapping with generated
        classes, see the <code class="literal">AvroSpecificMaxTemperature</code> class in the
        example code.</p></div><div class="footnote" type="footnote" id="calibre_link-152"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-159">85</a>] </sup>If we had used the identity mapper and reducer here, the program
        would sort and remove duplicate keys at the same time. We encounter
        this idea of duplicating information from the key in the value object
        again in <a class="ulink" href="#calibre_link-160" title="Secondary Sort">Secondary Sort</a>.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-208"><section type="chapter" id="calibre_link-4417" title="Chapter&nbsp;13.&nbsp;Parquet"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;13.&nbsp;Parquet</h2></div></div></div><p class="calibre2"><a class="ulink" href="http://parquet.incubator.apache.org/" target="_top">Apache
  Parquet</a> is a <a class="calibre" id="calibre_link-2909"></a><a class="calibre" id="calibre_link-4418"></a><a class="calibre" id="calibre_link-1175"></a>columnar storage format that can efficiently store nested
  data.</p><p class="calibre2">Columnar formats are attractive since they enable greater efficiency,
  in terms of both <span class="calibre">file size</span> and <span class="calibre">query performance</span>. File sizes are usually smaller
  than row-oriented equivalents since in a columnar format the values from one
  column are stored next to each other, which usually allows a very efficient
  encoding. A column storing a timestamp, for example, can be encoded by
  storing the first value and the differences between subsequent values (which
  tend to be small due to temporal locality: records from around the same time
  are stored next to each other). Query performance is improved too since a
  query engine can skip over columns that are not needed to answer a query.
  (This idea is illustrated in <a class="ulink" href="#calibre_link-209" title="Figure&nbsp;5-4.&nbsp;Row-oriented versus column-oriented storage">Figure&nbsp;5-4</a>.) This
  chapter looks at Parquet in more depth, but there are other columnar formats
  that work with Hadoop—notably <a class="calibre" id="calibre_link-2848"></a><a class="calibre" id="calibre_link-2846"></a><a class="calibre" id="calibre_link-2012"></a>ORCFile (Optimized Record Columnar File), which is a part of
  the Hive project.</p><p class="calibre2">A key strength of Parquet is its ability to store data that has a
  deeply <span class="calibre"><em class="calibre10">nested</em></span> structure in true columnar fashion. This
  is important since schemas with several levels of nesting are common in
  real-world systems. Parquet uses a novel technique for storing nested
  structures in a flat columnar format with little overhead, which was
  introduced by Google engineers in the Dremel paper.<sup class="calibre6">[<a class="firstname" href="#calibre_link-210" id="calibre_link-222">86</a>]</sup> The result is that even nested fields can be read
  independently of other fields, resulting in significant performance
  improvements.</p><p class="calibre2">Another feature of Parquet is the large number of <a class="calibre" id="calibre_link-2927"></a>tools that support it as a format. The engineers at Twitter
  and Cloudera who created Parquet wanted it to be easy to try new tools to
  process existing data, so to facilitate this they divided the project into a
  specification (<em class="calibre10">parquet-format</em>), which defines the
  file format in a language-neutral way, and implementations of the
  specification for different languages (Java and C++) that made it easy for
  tools to read or write Parquet files. In fact, most of the data processing
  components covered in this book understand the Parquet format (MapReduce,
  Pig, Hive, Cascading, Crunch, and Spark). This flexibility also extends to
  the in-memory representation: the Java implementation is not tied to a
  single representation, so you can use in-memory data models for Avro,
  Thrift, or Protocol Buffers to read your data from and write it to Parquet
  files.</p><div class="book" title="Data Model"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4419">Data Model</h2></div></div></div><p class="calibre2">Parquet defines a <a class="calibre" id="calibre_link-2914"></a>small number of primitive types, listed in <a class="ulink" href="#calibre_link-211" title="Table&nbsp;13-1.&nbsp;Parquet primitive types">Table&nbsp;13-1</a>.</p><div class="table"><a id="calibre_link-211" class="calibre"></a><div class="table-title">Table&nbsp;13-1.&nbsp;Parquet primitive types</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Type</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">boolean</code></td><td class="calibre25">Binary value</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">int32</code></td><td class="calibre25">32-bit signed integer</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">int64</code></td><td class="calibre25">64-bit signed integer</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">int96</code></td><td class="calibre25">96-bit signed integer</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">float</code></td><td class="calibre25">Single-precision (32-bit) IEEE 754 floating-point
            number</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">double</code></td><td class="calibre25">Double-precision (64-bit) IEEE 754 floating-point
            number</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">binary</code></td><td class="calibre25">Sequence of 8-bit unsigned bytes</td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">fixed_len_byte_array</code></td><td class="calibre28">Fixed number of 8-bit unsigned bytes</td></tr></tbody></table></div></div><p class="calibre2">The data stored in a Parquet file is described by a schema, which
    has at its root a <code class="literal">message</code> containing a
    group of fields. Each field has a repetition (<code class="literal">required</code>, <code class="literal">optional</code>, or <code class="literal">repeated</code>), a type, and a name. Here is
    a simple Parquet schema for a weather record:</p><pre class="screen1">message WeatherRecord {
  required int32 year;
  required int32 temperature;
  required binary stationId (UTF8);
}</pre><p class="calibre2">Notice that there is no primitive string type. Instead, Parquet
    defines logical types that specify how primitive types should be
    interpreted, so there is a separation between the serialized
    representation (the primitive type) and the semantics that are specific to
    the application (the logical type). Strings are represented as <code class="literal">binary</code> primitives with a <code class="literal">UTF8</code> annotation. Some of the logical types
    defined by Parquet are listed in <a class="ulink" href="#calibre_link-212" title="Table&nbsp;13-2.&nbsp;Parquet logical types">Table&nbsp;13-2</a>,
    along with a representative example schema of each. Among those not listed
    in the table are signed integers, unsigned integers, more date/time types,
    and JSON and BSON document types. See the Parquet specification for
    details.</p><div class="table"><a id="calibre_link-212" class="calibre"></a><div class="table-title">Table&nbsp;13-2.&nbsp;Parquet logical types</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Logical type annotation</td><td class="calibre20">Description</td><td class="calibre21">Schema example</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">UTF8</code></td><td class="calibre23">A UTF-8 character string. Annotates <code class="uri">binary</code>.</td><td class="calibre25"><pre class="programlisting1">message m {
  required binary a (UTF8);
}</pre></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">ENUM</code></td><td class="calibre23">A set of named values. Annotates <code class="uri">binary</code>.</td><td class="calibre25"><pre class="programlisting1">message m {
  required binary a (ENUM);
}</pre></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">DECIMAL(<em class="replaceable"><code class="calibre44">precision</code></em>,<em class="replaceable"><code class="calibre44">scale</code></em>)</code></td><td class="calibre23">An arbitrary-precision signed decimal number. Annotates
            <code class="uri">int32</code>, <code class="uri">int64</code>, <code class="uri">binary</code>, or <code class="uri">fixed_len_byte_array</code>.</td><td class="calibre25"><pre class="programlisting1">message m {
  required int32 a (DECIMAL(5,2));
}</pre></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">DATE</code></td><td class="calibre23">A date with no time value. Annotates <code class="uri">int32</code>. Represented by the number of days
            since the Unix epoch (January 1, 1970).</td><td class="calibre25"><pre class="programlisting1">message m {
  required int32 a (DATE);
}</pre></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">LIST</code></td><td class="calibre23">An ordered collection of values. Annotates <code class="uri">group</code>.</td><td class="calibre25"><pre class="programlisting1">message m {
  required group a (LIST) {
    repeated group list {
      required int32 element;
    }
  }
}</pre></td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">MAP</code></td><td class="calibre27">An unordered collection of key-value pairs. Annotates
            <code class="uri">group</code>.</td><td class="calibre28"><pre class="programlisting1">message m {
  required group a (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional int32 value;
    }
  }
}</pre></td></tr></tbody></table></div></div><p class="calibre2">Complex types in Parquet are created using the <code class="literal">group</code> type, which adds a layer of
    nesting.<sup class="calibre6">[<a class="firstname" href="#calibre_link-213" id="calibre_link-223">87</a>]</sup> A group with no annotation is simply a nested record.</p><p class="calibre2">Lists and maps are built from groups with a particular two-level
    group structure, as shown in <a class="ulink" href="#calibre_link-212" title="Table&nbsp;13-2.&nbsp;Parquet logical types">Table&nbsp;13-2</a>. A
    list is represented as a <code class="literal">LIST</code> group
    with a nested repeating group (called <code class="literal">list</code>) that contains an element field. In this
    example, a list of 32-bit integers has a required <code class="literal">int32</code> element field. For maps, the outer group
    <code class="literal">a</code> (annotated <code class="literal">MAP</code>) contains an inner repeating group <code class="literal">key_value</code> that contains the key and value
    fields. In this example, the values have been marked <code class="literal">optional</code> so that it’s possible to have
    <code class="literal">null</code> values in the map.</p><div class="book" title="Nested Encoding"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4420">Nested Encoding</h3></div></div></div><p class="calibre2">In a column-oriented <a class="calibre" id="calibre_link-2921"></a><a class="calibre" id="calibre_link-2782"></a><a class="calibre" id="calibre_link-1568"></a>store, a column’s values are stored together. For a flat
      table where there is no nesting and no repetition—such as the weather
      record schema—this is simple enough since each column has the same
      number of values, making it straightforward to determine which row each
      value belongs to.</p><p class="calibre2">In the general case where there is nesting or repetition—such as
      the map schema—it is more challenging, since the structure of the
      nesting needs to be encoded too. Some columnar formats avoid the problem
      by flattening the structure so that only the top-level columns are
      stored in column-major fashion (this is the approach that Hive’s RCFile
      takes, for example). A map with nested columns would be stored in such a
      way that the keys and values are interleaved, so it would not be
      possible to read only the keys, say, without also reading the values
      into memory.</p><p class="calibre2">Parquet uses the encoding from Dremel, where every primitive type
      field in the schema is stored in a separate column, and for each value
      written, the structure is encoded by means of two integers: the
      definition level and the repetition level. The details are
      intricate,<sup class="calibre6">[<a class="firstname" href="#calibre_link-214" id="calibre_link-224">88</a>]</sup> but you can think of storing definition and repetition
      levels like this as a generalization of using a bit field to encode
      <code class="literal">null</code>s for a flat record, where the
      non-<code class="literal">null</code> values are written one after another.</p><p class="calibre2">The upshot of this encoding is that any column (even nested ones)
      can be read independently of the others. In the case of a Parquet map,
      for example, the keys can be read without accessing any of the values,
      which can result in significant performance improvements, especially if
      the values are large (such as nested records with many <a class="calibre" id="calibre_link-2915"></a>fields).</p></div></div><div class="book" title="Parquet File Format"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4421">Parquet File Format</h2></div></div></div><p class="calibre2">A Parquet file <a class="calibre" id="calibre_link-2916"></a>consists of a header followed by one or more blocks,
    terminated by a footer. The header contains only a 4-byte magic number,
    <code class="literal">PAR1</code>, that identifies the file as being
    in Parquet format, and all the file metadata is stored in the footer. The
    footer’s metadata includes the format version, the schema, any extra
    key-value pairs, and metadata for
    every block in the file. The final two fields in the footer are a 4-byte
    field encoding the length of the footer metadata, and the magic number
    again (<code class="literal">PAR1</code>).</p><p class="calibre2">The consequence of storing the <a class="calibre" id="calibre_link-2682"></a>metadata in the footer is that reading a Parquet file
    requires an initial seek to the end of the file (minus 8 bytes) to read
    the footer metadata length, then a second seek backward by that length to
    read the footer metadata. Unlike sequence files and Avro datafiles, where
    the metadata is stored in the header and sync markers are used to separate
    blocks, Parquet files don’t need sync markers since the block boundaries
    are stored in the footer metadata. (This is possible because the metadata is written after all the blocks
    have been written, so the writer can retain the block boundary positions
    in memory until the file is closed.) Therefore, Parquet files are
    splittable, since the blocks can be located after reading the footer and
    can then be processed in parallel (by MapReduce, for example).</p><p class="calibre2">Each block in a Parquet file stores a <em class="calibre10">row
    group</em>, which is made up of <em class="calibre10">column
    chunks</em> containing the column data for those rows. The data for
    each column chunk is written in <em class="calibre10">pages</em>; this is
    illustrated in <a class="ulink" href="#calibre_link-215" title="Figure&nbsp;13-1.&nbsp;The internal structure of a Parquet file">Figure&nbsp;13-1</a>.</p><div class="figure"><a id="calibre_link-215" class="calibre"></a><div class="book"><div class="book"><img alt="The internal structure of a Parquet file" src="images/000066.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;13-1.&nbsp;The internal structure of a Parquet file</div></div><p class="calibre2">Each page contains values from the same column, making a page a very
    good candidate for compression since the values are likely to be similar.
    The first level of compression is achieved through how the values are
    encoded. The simplest encoding is plain encoding, where values are written
    in full (e.g., an <code class="literal">int32</code> is written
    using a 4-byte little-endian representation), but this doesn’t afford any
    compression in itself.</p><p class="calibre2">Parquet also uses more compact encodings, including delta encoding
    (the difference between values is stored), run-length encoding (sequences
    of identical values are encoded as a single value and the count), and
    dictionary encoding (a dictionary of values is built and itself encoded,
    then values are encoded as integers representing the indexes in the
    dictionary). In most cases, it also applies techniques such as bit packing
    to save space by storing several small values in a single byte.</p><p class="calibre2">When writing files, Parquet will choose an appropriate encoding
    automatically, based on the column type. For
    example, Boolean values will be written using a combination of run-length
    encoding and bit packing. Most types are encoded using dictionary encoding
    by default; however, a plain encoding will be used as a fallback if the
    dictionary becomes too large. The threshold size at which this happens is
    referred to as the <span class="calibre"><em class="calibre10">dictionary page size</em></span> and is the
    same as the page size by default (so the dictionary has to fit into one
    page if it is to be used). Note that the encoding that is actually used is
    stored in the file metadata to ensure that readers use the correct
    encoding.</p><p class="calibre2">In addition to the encoding, a second level of compression can be
    applied using a standard compression algorithm on the encoded page bytes.
     By default, no <a class="calibre" id="calibre_link-1213"></a>compression is applied, but Snappy, gzip, and LZO
    compressors are all supported.</p><p class="calibre2">For nested data, each page will also store the definition and
    repetition levels for all the values in the page. Since levels are small
    integers (the maximum is determined by the amount of nesting specified in
    the schema), they can be very efficiently encoded using a bit-packed
    run-length <a class="calibre" id="calibre_link-2917"></a>encoding.</p></div><div class="book" title="Parquet Configuration"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4422">Parquet Configuration</h2></div></div></div><p class="calibre2">Parquet file <a class="calibre" id="calibre_link-2913"></a>properties are set at write time. The properties listed in
    <a class="ulink" href="#calibre_link-216" title="Table&nbsp;13-3.&nbsp;ParquetOutputFormat properties">Table&nbsp;13-3</a> are appropriate if you
    are creating Parquet files from MapReduce (using the formats discussed in
    <a class="ulink" href="#calibre_link-217" title="Parquet MapReduce">Parquet MapReduce</a>), Crunch, Pig, or Hive.</p><div class="table"><a id="calibre_link-216" class="calibre"></a><div class="table-title">Table&nbsp;13-3.&nbsp;ParquetOutputFormat properties</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Type</td><td class="calibre20">Default value</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">parquet.block.size</code></td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">134217728 (128 MB)</td><td class="calibre25">The <a class="calibre" id="calibre_link-2930"></a>size in bytes of a block (row group).</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">parquet.page.size</code></td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">1048576 (1 MB)</td><td class="calibre25">The <a class="calibre" id="calibre_link-2937"></a>size in bytes of a page.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">parquet.dictionary.page.size</code></td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">1048576 (1 MB)</td><td class="calibre25">The <a class="calibre" id="calibre_link-2933"></a>maximum allowed size in bytes of a dictionary before
            falling back to plain encoding for a page.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">parquet.enable.dictionary</code></td><td class="calibre23"><code class="uri">boolean</code></td><td class="calibre23"><code class="uri">true</code></td><td class="calibre25">Whether to use <a class="calibre" id="calibre_link-2934"></a>dictionary encoding.</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">parquet.compression</code></td><td class="calibre27"><code class="uri">String</code></td><td class="calibre27"><code class="uri">UNCOMPRESSED</code></td><td class="calibre28">The <a class="calibre" id="calibre_link-2932"></a>type of compression to use for Parquet files:
            <code class="uri">UNCOMPRESSED</code>, <code class="uri">SNAPPY</code>, <code class="uri">GZIP</code>, or <code class="uri">LZO</code>. Used <a class="calibre" id="calibre_link-2597"></a>instead of <code class="uri">mapreduce.output.fileoutputformat.compress</code>.</td></tr></tbody></table></div></div><p class="calibre2">Setting the block size is a trade-off between scanning efficiency
    and memory usage. Larger blocks are more efficient to scan through since
    they contain more rows, which improves sequential I/O (as there’s less
    overhead in setting up each column chunk). However, each block is buffered
    in memory for both reading and writing, which limits how large blocks can
    be. The default block size is 128 MB.</p><p class="calibre2">The Parquet file <a class="calibre" id="calibre_link-1016"></a>block size should be no larger than the HDFS block size for
    the file so that each Parquet block can be read from a single HDFS block
    (and therefore from a single datanode). It is common to set them to be the
    same, and indeed both defaults are for 128 MB block sizes.</p><p class="calibre2">A page is the smallest unit of storage in a Parquet file, so
    retrieving an arbitrary row (with a single column, for the sake of
    illustration) requires that the page containing the row be decompressed
    and decoded. Thus, for single-row lookups, it is more efficient to have
    smaller pages, so there are fewer values to read through before reaching
    the target value. However, smaller pages incur a higher storage and
    processing overhead, due to the extra metadata (offsets, dictionaries)
    resulting from more pages. The default page size is 1 MB.</p></div><div class="book" title="Writing and Reading Parquet Files"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4423">Writing and Reading Parquet Files</h2></div></div></div><p class="calibre2">Most of the time Parquet <a class="calibre" id="calibre_link-2928"></a><a class="calibre" id="calibre_link-1631"></a><a class="calibre" id="calibre_link-3165"></a><a class="calibre" id="calibre_link-3817"></a>files are processed using higher-level tools like Pig, Hive,
    or Impala, but sometimes low-level sequential access may be required,
    which we cover in this section.</p><p class="calibre2">Parquet has a pluggable in-memory data model to facilitate
    integration of the Parquet file format with a wide range of tools and
    components. <code class="literal">ReadSupport</code> and
    <code class="literal">WriteSupport</code> are <a class="calibre" id="calibre_link-3170"></a><a class="calibre" id="calibre_link-3810"></a>the integration points in Java, and implementations of these
    classes do the conversion between the objects used by the tool or
    component and the objects used to represent each Parquet type in the
    schema.</p><p class="calibre2">To demonstrate, we’ll use a simple in-memory model that comes
    bundled with <a class="calibre" id="calibre_link-2935"></a><a class="calibre" id="calibre_link-2936"></a>Parquet in the <code class="literal">parquet.example.data</code> and <code class="literal">parquet.example.data.simple</code> packages. Then, in
    the next section, we’ll use an Avro representation to do the same
    thing.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">As the names suggest, the example classes that come with Parquet
      are an object model for demonstrating how to work with Parquet files;
      for production, one of the supported frameworks should be used (Avro,
      Protocol Buffers, or Thrift).</p></div><p class="calibre2">To write a Parquet file, we need to define a <a class="calibre" id="calibre_link-3290"></a>Parquet schema, represented by an instance of
    <code class="literal">parquet.schema.MessageType</code>:</p><pre class="screen1"><code class="n">MessageType</code> <code class="n">schema</code> <code class="o">=</code> <code class="n">MessageTypeParser</code><code class="o">.</code><code class="na">parseMessageType</code><code class="o">(</code>
    <code class="sb">"message Pair {\n"</code> <code class="o">+</code>
    <code class="sb">"  required binary left (UTF8);\n"</code> <code class="o">+</code>
    <code class="sb">"  required binary right (UTF8);\n"</code> <code class="o">+</code>
    <code class="sb">"}"</code><code class="o">);</code></pre><p class="calibre2">Next, we need to create an instance of a Parquet message for each
    record to be written to the file. For the <code class="literal">parquet.example.data</code> package, a message is
    represented by an instance of <code class="literal">Group</code>, constructed
    using a <code class="literal">GroupFactory</code>:</p><pre class="screen1"><code class="n">GroupFactory</code> <code class="n">groupFactory</code> <code class="o">=</code> <code class="k">new</code> <code class="n">SimpleGroupFactory</code><code class="o">(</code><code class="n">schema</code><code class="o">);</code>
<code class="n">Group</code> <code class="n">group</code> <code class="o">=</code> <code class="n">groupFactory</code><code class="o">.</code><code class="na">newGroup</code><code class="o">()</code>
    <code class="o">.</code><code class="na">append</code><code class="o">(</code><code class="sb">"left"</code><code class="o">,</code> <code class="sb">"L"</code><code class="o">)</code>
    <code class="o">.</code><code class="na">append</code><code class="o">(</code><code class="sb">"right"</code><code class="o">,</code> <code class="sb">"R"</code><code class="o">);</code></pre><p class="calibre2">Notice that the values in the message are <code class="literal">UTF8</code> logical types, and
    <code class="literal">Group</code> provides a natural conversion from a Java
    <code class="literal">String</code> for us.</p><p class="calibre2">The following snippet of code shows how to create a Parquet file and
    write a message to it. The <code class="literal">write()</code> method would
    normally be called in a loop to write multiple messages to the file, but
    this only writes one here:</p><pre class="screen1"><code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Configuration</code><code class="o">();</code>
<code class="n">Path</code> <code class="n">path</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="sb">"data.parquet"</code><code class="o">);</code>
<code class="n">GroupWriteSupport</code> <code class="n">writeSupport</code> <code class="o">=</code> <code class="k">new</code> <code class="n">GroupWriteSupport</code><code class="o">();</code>
<code class="n">GroupWriteSupport</code><code class="o">.</code><code class="na">setSchema</code><code class="o">(</code><code class="n">schema</code><code class="o">,</code> <code class="n">conf</code><code class="o">);</code>
<code class="n">ParquetWriter</code><code class="o">&lt;</code><code class="n">Group</code><code class="o">&gt;</code> <code class="n">writer</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ParquetWriter</code><code class="o">&lt;</code><code class="n">Group</code><code class="o">&gt;(</code><code class="n">path</code><code class="o">,</code> <code class="n">writeSupport</code><code class="o">,</code>
    <code class="n">ParquetWriter</code><code class="o">.</code><code class="na">DEFAULT_COMPRESSION_CODEC_NAME</code><code class="o">,</code>
    <code class="n">ParquetWriter</code><code class="o">.</code><code class="na">DEFAULT_BLOCK_SIZE</code><code class="o">,</code>
    <code class="n">ParquetWriter</code><code class="o">.</code><code class="na">DEFAULT_PAGE_SIZE</code><code class="o">,</code>
    <code class="n">ParquetWriter</code><code class="o">.</code><code class="na">DEFAULT_PAGE_SIZE</code><code class="o">,</code> <code class="c2">/* dictionary page size */</code>
    <code class="n">ParquetWriter</code><code class="o">.</code><code class="na">DEFAULT_IS_DICTIONARY_ENABLED</code><code class="o">,</code>
    <code class="n">ParquetWriter</code><code class="o">.</code><code class="na">DEFAULT_IS_VALIDATING_ENABLED</code><code class="o">,</code>
    <code class="n">ParquetProperties</code><code class="o">.</code><code class="na">WriterVersion</code><code class="o">.</code><code class="na">PARQUET_1_0</code><code class="o">,</code> <code class="n">conf</code><code class="o">);</code>
<code class="n">writer</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">group</code><code class="o">);</code>
<code class="n">writer</code><code class="o">.</code><code class="na">close</code><code class="o">();</code></pre><p class="calibre2">The <code class="literal">ParquetWriter</code> constructor <a class="calibre" id="calibre_link-2941"></a>needs to be provided with a
    <code class="literal">WriteSupport</code> instance, which defines how the
    message type is translated to Parquet’s types. In this case, we are using
    the <code class="literal">Group</code> message type, so
    <code class="literal">GroupWriteSupport</code> is used. Notice that the Parquet
    schema is set on the <code class="literal">Configuration</code> object by calling
    the <code class="literal">setSchema()</code> static method on
    <code class="literal">GroupWriteSupport</code>, and then the
    <code class="literal">Configuration</code> object is passed to
    <code class="literal">ParquetWriter</code>. This example also illustrates the
    Parquet file properties that may be set, corresponding to the ones listed
    in <a class="ulink" href="#calibre_link-216" title="Table&nbsp;13-3.&nbsp;ParquetOutputFormat properties">Table&nbsp;13-3</a>.</p><p class="calibre2">Reading a Parquet file is simpler than writing one, since the schema
    does not need to be specified as it is stored in the Parquet file. (It is,
    however, possible to set a <em class="calibre10">read schema</em> to return a
    subset of the columns in the file, via projection.) Also, there are no
    file properties to be set since they are set at write time:</p><pre class="screen1"><code class="n">GroupReadSupport</code> <code class="n">readSupport</code> <code class="o">=</code> <code class="k">new</code> <code class="n">GroupReadSupport</code><code class="o">();</code>
<code class="n">ParquetReader</code><code class="o">&lt;</code><code class="n">Group</code><code class="o">&gt;</code> <code class="n">reader</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ParquetReader</code><code class="o">&lt;</code><code class="n">Group</code><code class="o">&gt;(</code><code class="n">path</code><code class="o">,</code> <code class="n">readSupport</code><code class="o">);</code></pre><p class="calibre2"><code class="literal">ParquetReader</code> has a
    <code class="literal">read()</code> method <a class="calibre" id="calibre_link-2939"></a>to read the next message. It returns <code class="literal">null</code> when the end of the file is reached:</p><pre class="screen1"><code class="n">Group</code> <code class="n">result</code> <code class="o">=</code> <code class="n">reader</code><code class="o">.</code><code class="na">read</code><code class="o">();</code>
<code class="n">assertNotNull</code><code class="o">(</code><code class="n">result</code><code class="o">);</code>
<code class="n">assertThat</code><code class="o">(</code><code class="n">result</code><code class="o">.</code><code class="na">getString</code><code class="o">(</code><code class="sb">"left"</code><code class="o">,</code> <code class="mi">0</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"L"</code><code class="o">));</code>
<code class="n">assertThat</code><code class="o">(</code><code class="n">result</code><code class="o">.</code><code class="na">getString</code><code class="o">(</code><code class="sb">"right"</code><code class="o">,</code> <code class="mi">0</code><code class="o">),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"R"</code><code class="o">));</code>
<code class="n">assertNull</code><code class="o">(</code><code class="n">reader</code><code class="o">.</code><code class="na">read</code><code class="o">());</code></pre><p class="calibre2">Note that the <code class="literal">0</code> parameter passed
    to the <code class="literal">getString()</code> method specifies the index of
    the field to retrieve, since fields may have repeated values.</p><div class="book" title="Avro, Protocol Buffers, and Thrift"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4424">Avro, Protocol Buffers, and Thrift</h3></div></div></div><p class="calibre2">Most applications will prefer to define models using a framework
      like <a class="calibre" id="calibre_link-950"></a><a class="calibre" id="calibre_link-2910"></a><a class="calibre" id="calibre_link-3074"></a><a class="calibre" id="calibre_link-3687"></a><a class="calibre" id="calibre_link-2925"></a><a class="calibre" id="calibre_link-2922"></a>Avro, Protocol Buffers, or Thrift, and Parquet caters to
      all of these cases. Instead of <code class="literal">ParquetWriter</code> and
      <code class="literal">ParquetReader</code>, use
      <code class="literal">AvroParquetWriter</code>,
      <code class="literal">ProtoParquetWriter</code>, or
      <code class="literal">ThriftParquetWriter</code>, and the respective
      <a class="calibre" id="calibre_link-3076"></a><a class="calibre" id="calibre_link-3689"></a>reader classes. These classes take care of translating
      between Avro, Protocol Buffers, or Thrift schemas and Parquet schemas
      (as well as performing the equivalent mapping between the framework
      types and Parquet types), which means you don’t need to deal with
      Parquet schemas directly.</p><p class="calibre2">Let’s repeat the previous example but using the Avro Generic API,
      just like we did in <a class="ulink" href="#calibre_link-218" title="In-Memory Serialization and Deserialization">In-Memory Serialization and Deserialization</a>. The Avro
      <a class="calibre" id="calibre_link-3287"></a>schema is:</p><pre class="screen1"><code class="p">{</code>
  <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"record"</code><code class="p">,</code>
  <code class="nt">"name"</code><code class="p">:</code> <code class="sb">"StringPair"</code><code class="p">,</code>
  <code class="nt">"doc"</code><code class="p">:</code> <code class="sb">"A pair of strings."</code><code class="p">,</code>
  <code class="nt">"fields"</code><code class="p">:</code> <code class="p">[</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"left"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">},</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"right"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">}</code>
  <code class="p">]</code>
<code class="p">}</code></pre><p class="calibre2">We create a schema instance and a generic record with:</p><pre class="screen1"><code class="n">Schema</code><code class="o">.</code><code class="na">Parser</code> <code class="n">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Schema</code><code class="o">.</code><code class="na">Parser</code><code class="o">();</code>
<code class="n">Schema</code> <code class="n">schema</code> <code class="o">=</code> <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">getClass</code><code class="o">().</code><code class="na">getResourceAsStream</code><code class="o">(</code><code class="sb">"StringPair.avsc"</code><code class="o">));</code>

<code class="n">GenericRecord</code> <code class="n">datum</code> <code class="o">=</code> <code class="k">new</code> <code class="n">GenericData</code><code class="o">.</code><code class="na">Record</code><code class="o">(</code><code class="n">schema</code><code class="o">);</code>
<code class="n">datum</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="sb">"left"</code><code class="o">,</code> <code class="sb">"L"</code><code class="o">);</code>
<code class="n">datum</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="sb">"right"</code><code class="o">,</code> <code class="sb">"R"</code><code class="o">);</code></pre><p class="calibre2">Then we can write a Parquet file:</p><pre class="screen1"><code class="n">Path</code> <code class="n">path</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="sb">"data.parquet"</code><code class="o">);</code>
<code class="n">AvroParquetWriter</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;</code> <code class="n">writer</code> <code class="o">=</code>
    <code class="k">new</code> <code class="n">AvroParquetWriter</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;(</code><code class="n">path</code><code class="o">,</code> <code class="n">schema</code><code class="o">);</code>
<code class="n">writer</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">datum</code><code class="o">);</code>
<code class="n">writer</code><code class="o">.</code><code class="na">close</code><code class="o">();</code></pre><p class="calibre2"><code class="literal">AvroParquetWriter</code> <a class="calibre" id="calibre_link-965"></a>converts the Avro schema into a Parquet schema, and also
      translates each Avro <code class="literal">GenericRecord</code> instance into
      the corresponding Parquet types to write to the Parquet file. The file
      is a regular Parquet file—it is identical to the one written in the
      previous section using <code class="literal">ParquetWriter</code> with
      <code class="literal">GroupWriteSupport</code>, except for an extra piece of
      metadata to store the Avro schema. We can see this by inspecting the
      file’s metadata using Parquet’s command-line tools:<sup class="calibre6">[<a class="firstname" href="#calibre_link-219" id="calibre_link-225">89</a>]</sup></p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">parquet-tools meta data.parquet</code></strong>
...
extra:       avro.schema = {"type":"record","name":"StringPair", ...
...</pre><p class="calibre2">Similarly, to see the Parquet schema that was generated from the
      Avro schema, we can use the following:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">parquet-tools schema data.parquet</code></strong>
message StringPair {
  required binary left (UTF8);
  required binary right (UTF8);
}</pre><p class="calibre2">To read the Parquet file back, we <a class="calibre" id="calibre_link-964"></a>use an <code class="literal">AvroParquetReader</code> and get
      back Avro <code class="literal">GenericRecord</code> objects:</p><pre class="screen1"><code class="n">AvroParquetReader</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;</code> <code class="n">reader</code> <code class="o">=</code>
    <code class="k">new</code> <code class="n">AvroParquetReader</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;(</code><code class="n">path</code><code class="o">);</code>
<code class="n">GenericRecord</code> <code class="n">result</code> <code class="o">=</code> <code class="n">reader</code><code class="o">.</code><code class="na">read</code><code class="o">();</code>
<code class="n">assertNotNull</code><code class="o">(</code><code class="n">result</code><code class="o">);</code>
<code class="n">assertThat</code><code class="o">(</code><code class="n">result</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"left"</code><code class="o">).</code><code class="na">toString</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"L"</code><code class="o">));</code>
<code class="n">assertThat</code><code class="o">(</code><code class="n">result</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"right"</code><code class="o">).</code><code class="na">toString</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"R"</code><code class="o">));</code>
<code class="n">assertNull</code><code class="o">(</code><code class="n">reader</code><code class="o">.</code><code class="na">read</code><code class="o">());</code></pre><div class="book" title="Projection and read schemas"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4425">Projection and read schemas</h4></div></div></div><p class="calibre2">It’s often the case that you only need to read a few columns in
        the file, and indeed this is the <span class="calibre">raison
        d’être</span> of a columnar format like Parquet: to save time and
        I/O. You can use a projection schema to select the columns to read.
        For example, the following schema will read only the <code class="literal">right</code> field of a <code class="literal">StringPair</code>:</p><pre class="screen1"><code class="p">{</code>
  <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"record"</code><code class="p">,</code>
  <code class="nt">"name"</code><code class="p">:</code> <code class="sb">"StringPair"</code><code class="p">,</code>
  <code class="nt">"doc"</code><code class="p">:</code> <code class="sb">"The right field of a pair of strings."</code><code class="p">,</code>
  <code class="nt">"fields"</code><code class="p">:</code> <code class="p">[</code>
    <code class="p">{</code><code class="nt">"name"</code><code class="p">:</code> <code class="sb">"right"</code><code class="p">,</code> <code class="nt">"type"</code><code class="p">:</code> <code class="sb">"string"</code><code class="p">}</code>
  <code class="p">]</code>
<code class="p">}</code></pre><p class="calibre2">In order to use a projection schema, set it on the configuration
        using the <code class="literal">setRequestedProjection()</code> static
        convenience method <a class="calibre" id="calibre_link-966"></a>on <code class="literal">AvroReadSupport</code>:</p><pre class="screen1"><code class="n">Schema</code> <code class="n">projectionSchema</code> <code class="o">=</code> <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code>
    <code class="n">getClass</code><code class="o">().</code><code class="na">getResourceAsStream</code><code class="o">(</code><code class="sb">"ProjectedStringPair.avsc"</code><code class="o">));</code>
<code class="n">Configuration</code> <code class="n">conf</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Configuration</code><code class="o">();</code>
<code class="n">AvroReadSupport</code><code class="o">.</code><code class="na">setRequestedProjection</code><code class="o">(</code><code class="n">conf</code><code class="o">,</code> <code class="n">projectionSchema</code><code class="o">);</code></pre><p class="calibre2">Then pass the configuration into the constructor for
        <code class="literal">AvroParquetReader</code>:</p><pre class="screen1"><code class="n">AvroParquetReader</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;</code> <code class="n">reader</code> <code class="o">=</code>
    <code class="k">new</code> <code class="n">AvroParquetReader</code><code class="o">&lt;</code><code class="n">GenericRecord</code><code class="o">&gt;(</code><span class="calibre24"><strong class="calibre9"><code class="n1">conf</code></strong></span><code class="o">,</code> <code class="n">path</code><code class="o">);</code>
<code class="n">GenericRecord</code> <code class="n">result</code> <code class="o">=</code> <code class="n">reader</code><code class="o">.</code><code class="na">read</code><code class="o">();</code>
<span class="calibre24"><strong class="calibre9"><code class="n1">assertNull</code><code class="o1">(</code><code class="n1">result</code><code class="o1">.</code><code class="na1">get</code><code class="o1">(</code><code class="s">"left"</code><code class="o1">));</code></strong></span>
<code class="n">assertThat</code><code class="o">(</code><code class="n">result</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="sb">"right"</code><code class="o">).</code><code class="na">toString</code><code class="o">(),</code> <code class="n">is</code><code class="o">(</code><code class="sb">"R"</code><code class="o">));</code></pre><p class="calibre2">Both the Protocol Buffers and Thrift implementations support
        projection in a similar manner. In addition, the Avro implementation
        allows you to specify a reader’s schema by calling
        <code class="literal">setReadSchema()</code> on
        <code class="literal">AvroReadSupport</code>. This schema is used to resolve
        Avro records according to the rules listed in <a class="ulink" href="#calibre_link-145" title="Table&nbsp;12-4.&nbsp;Schema resolution of records">Table&nbsp;12-4</a>.</p><p class="calibre2">The reason that Avro has both a projection schema and a reader’s
        schema is that the projection must be a subset of the schema used to
        write the Parquet file, so it cannot be used to evolve a schema by
        adding new fields.</p><p class="calibre2">The two schemas serve different purposes, and you can use both
        together. The projection schema is used to filter the columns to read
        from the Parquet file. Although it is expressed as an Avro schema, it
        can be viewed simply as a list of Parquet columns to read back. The
        reader’s schema, on the other hand, is used only to resolve Avro
        records. It is never translated to a Parquet schema, since it has no
        bearing on which columns are read from the Parquet file. For example,
        if we added a <code class="literal">description</code> field to
        our Avro schema (like in <a class="ulink" href="#calibre_link-220" title="Schema Resolution">Schema Resolution</a>) and
        used it as the Avro reader’s schema, then the records would contain
        the default value of the field, even though the Parquet file has no
        such <a class="calibre" id="calibre_link-1632"></a><a class="calibre" id="calibre_link-2929"></a><a class="calibre" id="calibre_link-3166"></a><a class="calibre" id="calibre_link-3818"></a><a class="calibre" id="calibre_link-951"></a><a class="calibre" id="calibre_link-2911"></a><a class="calibre" id="calibre_link-2923"></a><a class="calibre" id="calibre_link-2926"></a><a class="calibre" id="calibre_link-3688"></a><a class="calibre" id="calibre_link-3075"></a>field.</p></div></div></div><div class="book" title="Parquet MapReduce"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-217">Parquet MapReduce</h2></div></div></div><p class="calibre2">Parquet comes with a <a class="calibre" id="calibre_link-2491"></a><a class="calibre" id="calibre_link-2919"></a>selection of MapReduce input and output formats for reading
    and writing Parquet files from MapReduce jobs, including ones for working
    with Avro, Protocol Buffers, and Thrift schemas and data.</p><p class="calibre2">The program in <a class="ulink" href="#calibre_link-221" title="Example&nbsp;13-1.&nbsp;MapReduce program to convert text files to Parquet files using AvroParquetOutputFormat">Example&nbsp;13-1</a> is a
    map-only job that reads text files and writes Parquet files where each
    record is the line’s offset in the file (represented by an <code class="literal">int64</code>—converted from a <code class="literal">long</code> in Avro) and the line itself (a string). It
    uses the Avro Generic API for its in-memory data model.</p><div class="example"><a id="calibre_link-221" class="calibre"></a><div class="example-title">Example&nbsp;13-1.&nbsp;MapReduce program to convert text files to Parquet files using
        AvroParquetOutputFormat</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">TextToParquetWithAvro</code> <code class="k">extends</code> <code class="n">Configured</code> <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>

  <code class="k">private</code> <code class="k">static</code> <code class="k">final</code> <code class="n">Schema</code> <code class="n">SCHEMA</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Schema</code><code class="o">.</code><code class="na">Parser</code><code class="o">().</code><code class="na">parse</code><code class="o">(</code>
      <code class="sb">"{\n"</code> <code class="o">+</code>
      <code class="sb">"  \"type\": \"record\",\n"</code> <code class="o">+</code>
      <code class="sb">"  \"name\": \"Line\",\n"</code> <code class="o">+</code>
      <code class="sb">"  \"fields\": [\n"</code> <code class="o">+</code>
      <code class="sb">"    {\"name\": \"offset\", \"type\": \"long\"},\n"</code> <code class="o">+</code>
      <code class="sb">"    {\"name\": \"line\", \"type\": \"string\"}\n"</code> <code class="o">+</code>
      <code class="sb">"  ]\n"</code> <code class="o">+</code>
      <code class="sb">"}"</code><code class="o">);</code>

  <code class="k">public</code> <code class="k">static</code> <code class="k">class</code> <code class="nc">TextToParquetMapper</code>
      <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">Void</code><code class="o">,</code> <code class="n">GenericRecord</code><code class="o">&gt;</code> <code class="o">{</code>

    <code class="k">private</code> <code class="n">GenericRecord</code> <code class="n">record</code> <code class="o">=</code> <code class="k">new</code> <code class="n">GenericData</code><code class="o">.</code><code class="na">Record</code><code class="o">(</code><code class="n">SCHEMA</code><code class="o">);</code>

    <code class="nd">@Override</code>
    <code class="k">protected</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">LongWritable</code> <code class="n">key</code><code class="o">,</code> <code class="n">Text</code> <code class="n">value</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code>
        <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
      <code class="n">record</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="sb">"offset"</code><code class="o">,</code> <code class="n">key</code><code class="o">.</code><code class="na">get</code><code class="o">());</code>
      <code class="n">record</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="sb">"line"</code><code class="o">,</code> <code class="n">value</code><code class="o">.</code><code class="na">toString</code><code class="o">());</code>
      <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="k">null</code><code class="o">,</code> <code class="n">record</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="na">length</code> <code class="o">!=</code> <code class="mi">2</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"Usage: %s [generic options] &lt;input&gt; &lt;output&gt;\n"</code><code class="o">,</code>
          <code class="n">getClass</code><code class="o">().</code><code class="na">getSimpleName</code><code class="o">());</code>
      <code class="n">ToolRunner</code><code class="o">.</code><code class="na">printGenericCommandUsage</code><code class="o">(</code><code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">);</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>

    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Job</code><code class="o">(</code><code class="n">getConf</code><code class="o">(),</code> <code class="sb">"Text to Parquet"</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setJarByClass</code><code class="o">(</code><code class="n">getClass</code><code class="o">());</code>

    <code class="n">FileInputFormat</code><code class="o">.</code><code class="na">addInputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]));</code>
    <code class="n">FileOutputFormat</code><code class="o">.</code><code class="na">setOutputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">]));</code>

    <code class="n">job</code><code class="o">.</code><code class="na">setMapperClass</code><code class="o">(</code><code class="n">TextToParquetMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setNumReduceTasks</code><code class="o">(</code><code class="mi">0</code><code class="o">);</code>

    <code class="n">job</code><code class="o">.</code><code class="na">setOutputFormatClass</code><code class="o">(</code><code class="n">AvroParquetOutputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">AvroParquetOutputFormat</code><code class="o">.</code><code class="na">setSchema</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">SCHEMA</code><code class="o">);</code>

    <code class="n">job</code><code class="o">.</code><code class="na">setOutputKeyClass</code><code class="o">(</code><code class="n">Void</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputValueClass</code><code class="o">(</code><code class="n">Group</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

    <code class="k">return</code> <code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">;</code>
  <code class="o">}</code>

  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="k">new</code> <code class="n">TextToParquetWithAvro</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The job’s output format is set to
    <code class="literal">AvroParquetOutputFormat</code>, and the output key and
    value types are set to <code class="literal">Void</code> and
    <code class="literal">GenericRecord</code> to match, since we are using Avro’s
    Generic API. <code class="literal">Void</code> simply means that the key is
    always set to <code class="literal">null</code>.</p><p class="calibre2">Like <code class="literal">AvroParquetWriter</code> from the previous
    section, <code class="literal">AvroParquetOutputFormat</code> converts
    <a class="calibre" id="calibre_link-963"></a>the Avro schema to a Parquet schema automatically. The Avro
    schema is set on the <code class="literal">Job</code> instance so that the
    MapReduce tasks can find the schema when writing the files.</p><p class="calibre2">The mapper is straightforward; it takes the file offset (key) and
    line (value) and builds an Avro <code class="literal">GenericRecord</code>
    object with them, which it writes out to the MapReduce context object as
    the value (the key is always <code class="literal">null</code>).
    <code class="literal">AvroParquetOutputFormat</code> takes care of the
    conversion of the Avro <code class="literal">GenericRecord</code> to the Parquet
    file format encoding.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">Parquet is a columnar format, so it buffers rows in memory. Even
      though the mapper in this example just passes values through, it must
      have sufficient memory for the Parquet writer to buffer each block (row
      group), which is by default 128 MB. If you get job failures due to out
      of memory errors, you can adjust the Parquet file block size for the
      writer <a class="calibre" id="calibre_link-2931"></a>with <code class="literal">parquet.block.size</code>
      (see <a class="ulink" href="#calibre_link-216" title="Table&nbsp;13-3.&nbsp;ParquetOutputFormat properties">Table&nbsp;13-3</a>). You may also
      need to change the MapReduce task memory allocation (when reading or
      writing) using the settings discussed in <a class="ulink" href="#calibre_link-35" title="Memory settings in YARN and MapReduce">Memory settings in YARN and MapReduce</a>.</p></div><p class="calibre2">The following command runs the program on the four-line text file
    <em class="calibre10">quangle.txt</em>:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar parquet-examples.jar TextToParquetWithAvro \
  input/docs/quangle.txt output</code></strong></pre><p class="calibre2">We can use the Parquet command-line tools to dump the output Parquet
    file for inspection:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">parquet-tools dump output/part-m-00000.parquet</code></strong>
INT64 offset 
--------------------------------------------------------------------------------
*** row group 1 of 1, values 1 to 4 *** 
value 1: R:0 D:0 V:0
value 2: R:0 D:0 V:33
value 3: R:0 D:0 V:57
value 4: R:0 D:0 V:89

BINARY line 
--------------------------------------------------------------------------------
*** row group 1 of 1, values 1 to 4 *** 
value 1: R:0 D:0 V:On the top of the Crumpetty Tree
value 2: R:0 D:0 V:The Quangle Wangle sat,
value 3: R:0 D:0 V:But his face you could not see,
value 4: R:0 D:0 V:On account of his Beaver Hat.</pre><p class="calibre2">Notice how the values within a row group are shown together.
    <code class="literal">V</code> indicates the value, <code class="literal">R</code> the repetition level, and <code class="literal">D</code> the definition level. For this schema, the
    latter two are zero since there is no <a class="calibre" id="calibre_link-2920"></a><a class="calibre" id="calibre_link-2492"></a><a class="calibre" id="calibre_link-1176"></a>nesting.</p></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" id="calibre_link-210"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-222">86</a>] </sup>Sergey Melnik et al., <a class="ulink" href="http://research.google.com/pubs/pub36632.html" target="_top"><span class="calibre"><em class="calibre10">Dremel:
      Interactive Analysis of Web-Scale Datasets</em></span>,</a>
      Proceedings of the 36th International Conference on Very Large Data
      Bases, 2010.</p></div><div class="footnote" id="calibre_link-213"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-223">87</a>] </sup>This is based on the model used in <a class="ulink" href="https://developers.google.com/protocol-buffers/" target="_top">Protocol
        Buffers</a>, where groups are used to define complex types like
        lists and maps.</p></div><div class="footnote" id="calibre_link-214"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-224">88</a>] </sup><a class="ulink" href="http://bit.ly/dremel_parquet" target="_top">Julien
          Le Dem’s exposition</a> is excellent.</p></div><div class="footnote" id="calibre_link-219"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-225">89</a>] </sup>The Parquet tools can be downloaded as a binary tarball from
          the Parquet Maven repository. Search for “parquet-tools” on
          <a class="ulink" href="http://search.maven.org" target="_top">http://search.maven.org</a>.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-276"><section type="chapter" id="calibre_link-4426" title="Chapter&nbsp;14.&nbsp;Flume"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;14.&nbsp;Flume</h2></div></div></div><p class="calibre2">Hadoop is built for processing very large datasets. Often it is
  assumed that the data is already in HDFS, or can be copied there in bulk.
  However, there are many systems that don’t meet this assumption. They
  produce streams of data that we would like to aggregate, store, and analyze
  using Hadoop—and these are <a class="calibre" id="calibre_link-1701"></a><a class="calibre" id="calibre_link-4427"></a>the systems that <a class="ulink" href="http://flume.apache.org/" target="_top">Apache
  Flume</a> is an ideal fit for.</p><p class="calibre2">Flume is designed for high-volume ingestion into Hadoop of event-based
  data. The canonical example is using Flume to collect logfiles from a bank
  of web servers, then moving the log events from those files into new
  aggregated files in HDFS for processing. The usual destination (or
  <em class="calibre10">sink</em> in Flume parlance) is HDFS. However, Flume is
  flexible enough to write to other systems, like HBase or Solr.</p><p class="calibre2">To use Flume, we need to run a Flume <em class="calibre10">agent</em>,
  <a class="calibre" id="calibre_link-870"></a>which is a long-lived Java process that runs
  <em class="calibre10">sources</em> and <em class="calibre10">sinks</em>, connected
  by <em class="calibre10">channels</em>. A source in Flume produces
  <em class="calibre10">events</em> and delivers them to the channel, which stores
  the events until they are forwarded to the sink. You can think of the
  source-channel-sink combination as a basic Flume building block.</p><p class="calibre2">A Flume installation is made up of a collection of connected agents
  running in a distributed topology. Agents on the edge of the system
  (co-located on web server machines, for example) collect data and forward it
  to agents that are responsible for aggregating and then storing the data in
  its final destination. Agents are configured to run a collection of
  particular sources and sinks, so using Flume is mainly a configuration
  exercise in wiring the pieces together. In this chapter, we’ll see how to
  build Flume topologies for data ingestion that you can use as a part of your
  own Hadoop pipeline.</p><div class="book" title="Installing Flume"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4428">Installing Flume</h2></div></div></div><p class="calibre2">Download a stable <a class="calibre" id="calibre_link-1716"></a>release of the Flume binary distribution from the <a class="ulink" href="http://flume.apache.org/download.html" target="_top">download page</a>, and
    unpack the tarball in a suitable location:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">tar xzf apache-flume-<em class="replaceable1"><code class="calibre46">x.y.z</code></em>-bin.tar.gz</code></strong></pre><p class="calibre2">It’s useful to put the Flume binary on your path:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">export FLUME_HOME=~/sw/apache-flume-<em class="replaceable1"><code class="calibre46">x.y.z</code></em>-bin</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">export PATH=$PATH:$FLUME_HOME/bin</code></strong></pre><p class="calibre2">A Flume agent can then be started <a class="calibre" id="calibre_link-1722"></a>with the <code class="literal">flume-ng</code> command, as we’ll see
    next.</p></div><div class="book" title="An Example"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4429">An Example</h2></div></div></div><p class="calibre2">To show <a class="calibre" id="calibre_link-1710"></a>how Flume works, let’s start with a setup that:</p><div class="book"><ol class="orderedlist"><li class="listitem"><p class="calibre2">Watches a local directory for new text files</p></li><li class="listitem"><p class="calibre2">Sends each line of each file to the console as files are
        added</p></li></ol></div><p class="calibre2">We’ll add the files by hand, but it’s easy to imagine a process like
    a web server creating new files that we want to continuously ingest with
    Flume. Also, in a real system, rather than just logging the file contents
    we would write the contents to HDFS for subsequent processing—we’ll see
    how to do that later in the chapter.</p><p class="calibre2">In this example, the Flume <a class="calibre" id="calibre_link-873"></a>agent runs a single source-channel-sink, configured using a
    Java properties file. The configuration controls the types of sources,
    sinks, and channels that are used, as well as how they are connected
    together. For this example, we’ll use the configuration in <a class="ulink" href="#calibre_link-291" title="Example&nbsp;14-1.&nbsp;Flume configuration using a spooling directory source and a logger sink">Example&nbsp;14-1</a>.</p><div class="example"><a id="calibre_link-291" class="calibre"></a><div class="example-title">Example&nbsp;14-1.&nbsp;Flume configuration using a spooling directory source and a
      logger sink</div><div class="book"><pre class="screen"><code class="na">agent1.sources</code> <code class="o">=</code> <code class="sb">source1</code>
<code class="na">agent1.sinks</code> <code class="o">=</code> <code class="sb">sink1</code>
<code class="na">agent1.channels</code> <code class="o">=</code> <code class="sb">channel1</code>

<code class="na">agent1.sources.source1.channels</code> <code class="o">=</code> <code class="sb">channel1</code>
<code class="na">agent1.sinks.sink1.channel</code> <code class="o">=</code> <code class="sb">channel1</code>

<code class="na">agent1.sources.source1.type</code> <code class="o">=</code> <code class="sb">spooldir</code>
<code class="na">agent1.sources.source1.spoolDir</code> <code class="o">=</code> <code class="sb">/tmp/spooldir</code>

<code class="na">agent1.sinks.sink1.type</code> <code class="o">=</code> <code class="sb">logger</code>

<code class="na">agent1.channels.channel1.type</code> <code class="o">=</code> <code class="sb">file</code></pre></div></div><p class="calibre2">Property names form a hierarchy with the agent name at the top
    level. In this example, we have a single agent, called <code class="literal">agent1</code>. The names for the different components
    in an agent are defined at the next level, so for example <code class="literal">agent1.sources</code> lists the names of the sources
    that should be run in <code class="literal">agent1</code> (here it
    is a single source, <code class="literal">source1</code>).
    Similarly, <code class="literal">agent1</code> has a sink (<code class="literal">sink1</code>) and a channel (<code class="literal">channel1</code>).</p><p class="calibre2">The properties for each component are defined at the next level of
    the hierarchy. The configuration properties that are available for a
    component depend on the type of the component. In this case, <code class="literal">agent1.sources.source1.type</code> is set to <code class="literal">spooldir</code>, which is a spooling directory source
    that monitors a spooling directory for new files. The spooling
    <span class="calibre">directory source defines a <code class="literal">spoolDir</code> property, so for <code class="literal">source1</code> the full key is <code class="literal">agent1</code></span><span class="calibre"><code class="literal">.sources</code></span><code class="literal">.source1.spoolDir</code>. The source’s channels are set
    with <code class="literal">agent1</code><code class="literal">.sources.source1.channels</code>.</p><p class="calibre2">The sink is a <code class="literal">logger</code> sink for
    logging events to the console. It too must be connected to the channel
    (with the <code class="literal">agent1.sinks.sink1.channel</code>
    property).<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-292" id="calibre_link-312">90</a>]</sup> The channel is a <code class="literal">file</code>
    channel, which means that events in the channel are persisted to disk for
    durability. The system is illustrated in <a class="ulink" href="#calibre_link-293" title="Figure&nbsp;14-1.&nbsp;Flume agent with a spooling directory source and a logger sink connected by a file channel">Figure&nbsp;14-1</a>.</p><div class="figure"><a id="calibre_link-293" class="calibre"></a><div class="book"><div class="book"><img alt="Flume agent with a spooling directory source and a logger sink connected by a file channel" src="images/000074.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;14-1.&nbsp;Flume agent with a spooling directory source and a logger sink
      connected by a file channel</div></div><p class="calibre2">Before running the example, we need to create the spooling directory
    on the local filesystem:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">mkdir /tmp/spooldir</code></strong></pre><p class="calibre2">Then we can start the Flume agent <a class="calibre" id="calibre_link-1723"></a>using the <code class="literal">flume-ng</code> command:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">flume-ng agent \
  --conf-file spool-to-logger.properties \
  --name agent1 \
  --conf $FLUME_HOME/conf \
  -Dflume.root.logger=INFO,console</code></strong></pre><p class="calibre2">The Flume properties file from <a class="ulink" href="#calibre_link-291" title="Example&nbsp;14-1.&nbsp;Flume configuration using a spooling directory source and a logger sink">Example&nbsp;14-1</a> is
    specified with the <code class="literal">--conf-file</code> flag.
    The agent name must also be passed in with <code class="literal">--name</code> (since a Flume properties file can define
    several agents, we have to say which one to run). The <code class="literal">--conf</code> flag tells Flume where to find its
    general configuration, such as environment settings.</p><p class="calibre2">In a new terminal, create a file in the spooling directory. The
    spooling directory source expects files to be immutable. To prevent
    partially written files from being read by the source, we write the full
    contents to a hidden file. Then, we do an atomic rename so the source can
    read it:<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-294" id="calibre_link-315">91</a>]</sup></p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">echo "Hello Flume" &gt; /tmp/spooldir/.file1.txt</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">mv /tmp/spooldir/.file1.txt /tmp/spooldir/file1.txt</code></strong></pre><p class="calibre2">Back in the agent’s terminal, we see that Flume has detected and
    processed the file:</p><pre class="screen1">Preparing to move file /tmp/spooldir/file1.txt to
 /tmp/spooldir/file1.txt.COMPLETED
Event: { headers:{} body: 48 65 6C 6C 6F 20 46 6C 75 6D 65         Hello Flume }</pre><p class="calibre2">The spooling directory source ingests the file by splitting it into
    lines and creating a Flume event for each line. Events have optional
    headers and a binary body, which is the UTF-8 representation of the line
    of text. The body is logged by the logger sink in both hexadecimal and
    string form. The file we placed in the spooling directory was only one
    line long, so only one event was logged in this case. We also see that the
    file was renamed to <em class="calibre10">file1.txt.COMPLETED</em> by the source, which
    indicates that Flume has completed processing it and won’t process it
    <a class="calibre" id="calibre_link-1711"></a><a class="calibre" id="calibre_link-874"></a>again.</p></div><div class="book" title="Transactions and Reliability"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4430">Transactions and Reliability</h2></div></div></div><p class="calibre2">Flume uses <a class="calibre" id="calibre_link-1720"></a>separate transactions to guarantee delivery from the source
    to the channel and from the channel to the sink. In the example in the
    previous section, the spooling directory source creates an event for each
    line in the file. The source will only mark the file as completed once the
    transactions encapsulating the delivery of the events to the channel have
    been successfully committed.</p><p class="calibre2">Similarly, a transaction is used for the delivery of the events from
    the channel to the sink. If for some unlikely reason the events could not
    be logged, the transaction would be rolled back and the events would
    remain in the channel for later redelivery.</p><p class="calibre2">The channel we are using is a <span class="calibre"><em class="calibre10">file channel</em></span>,
    which has the property of being durable: once an event has been written to
    the channel, it will not be lost, even if the agent restarts. (Flume also
    provides a <span class="calibre"><em class="calibre10">memory channel</em></span> that does not have this
    property, since events are stored in memory. With this channel, events are
    lost if the agent restarts. Depending on the application, this might be
    acceptable. The trade-off is that the memory channel has higher throughput
    than the file channel.)</p><p class="calibre2">The overall effect is that every event produced by the source will
    reach the sink. The major caveat here is that every event will reach the
    sink <span class="calibre">at least once</span>—that is, duplicates
    are possible. Duplicates can be produced in sources or sinks: for example,
    after an agent restart, the spooling directory source will redeliver
    events for an uncompleted file, even if some or all of them had been
    committed to the channel before the restart. After a restart, the logger
    sink will re-log any event that was logged but not committed (which could
    happen if the agent was shut down between these two operations).</p><p class="calibre2">At-least-once semantics might seem like a limitation, but in
    practice it is an acceptable performance trade-off. The stronger semantics
    of <em class="calibre10">exactly once</em> require a two-phase commit
    protocol, which is expensive. This choice is what differentiates Flume
    (at-least-once semantics) as a high-volume parallel event ingest system
    from more traditional enterprise messaging systems (exactly-once
    semantics). With at-least-once semantics, duplicate events can be removed
    further down the processing pipeline. Usually this takes the form of an
    application-specific deduplication job written in MapReduce or
    Hive.</p><div class="book" title="Batching"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4431">Batching</h3></div></div></div><p class="calibre2">For efficiency, Flume tries to <a class="calibre" id="calibre_link-1707"></a><a class="calibre" id="calibre_link-979"></a>process events in batches for each transaction, where
      possible, rather than one by one. Batching helps file channel
      performance in particular, since every transaction results in a local
      disk write and <code class="literal">fsync</code> call.</p><p class="calibre2">The batch size used is determined by the component in question,
      and is configurable in many cases. For example, the spooling directory
      source will read files in batches of 100 lines. (This can be changed by
      setting <a class="calibre" id="calibre_link-981"></a>the <code class="literal">batchSize</code>
      property.) Similarly, the Avro sink (discussed in <a class="ulink" href="#calibre_link-295" title="Distribution: Agent Tiers">Distribution: Agent Tiers</a>) will try to read 100 events from the channel before sending them over
      RPC, although it won’t block if fewer are <a class="calibre" id="calibre_link-1721"></a>available.</p></div></div><div class="book" title="The HDFS Sink"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4432">The HDFS Sink</h2></div></div></div><p class="calibre2">The point of Flume is <a class="calibre" id="calibre_link-1714"></a><a class="calibre" id="calibre_link-1968"></a>to deliver large amounts of data into a Hadoop data store,
    so let’s look at how to configure a <a class="calibre" id="calibre_link-875"></a>Flume agent to deliver events to an HDFS sink. The
    configuration in <a class="ulink" href="#calibre_link-296" title="Example&nbsp;14-2.&nbsp;Flume configuration using a spooling directory source and an HDFS sink">Example&nbsp;14-2</a> updates the previous
    example to use an HDFS sink. The only two settings that are required are
    the sink’s type (<code class="literal">hdfs</code>) and <code class="literal">hdfs.path</code>, which <a class="calibre" id="calibre_link-1980"></a>specifies the directory where files will be placed (if, like
    here, the filesystem is not specified in the path, it’s determined in the
    usual way from Hadoop’s <code class="literal">fs.defaultFS</code>
    property). We’ve also specified a meaningful file prefix and suffix, and
    instructed Flume to write events to the files in text format.</p><div class="example"><a id="calibre_link-296" class="calibre"></a><div class="example-title">Example&nbsp;14-2.&nbsp;Flume configuration using a spooling directory source and an HDFS
      sink</div><div class="book"><pre class="screen"><code class="na">agent1.sources</code> <code class="o">=</code> <code class="sb">source1</code>
<code class="na">agent1.sinks</code> <code class="o">=</code> <code class="sb">sink1</code>
<code class="na">agent1.channels</code> <code class="o">=</code> <code class="sb">channel1</code>

<code class="na">agent1.sources.source1.channels</code> <code class="o">=</code> <code class="sb">channel1</code>
<code class="na">agent1.sinks.sink1.channel</code> <code class="o">=</code> <code class="sb">channel1</code>

<code class="na">agent1.sources.source1.type</code> <code class="o">=</code> <code class="sb">spooldir</code>
<code class="na">agent1.sources.source1.spoolDir</code> <code class="o">=</code> <code class="sb">/tmp/spooldir</code>

<span class="calibre24"><strong class="calibre9"><code class="na1">agent1.sinks.sink1.type</code> <code class="o1">=</code> <code class="s">hdfs</code>
<code class="na1">agent1.sinks.sink1.hdfs.path</code> <code class="o1">=</code> <code class="s">/tmp/flume</code>
<code class="na1">agent1.sinks.sink1.hdfs.filePrefix</code> <code class="o1">=</code> <code class="s">events</code>
<code class="na1">agent1.sinks.sink1.hdfs.fileSuffix</code> <code class="o1">=</code> <code class="s">.log</code>
<code class="na1">agent1.sinks.sink1.hdfs.inUsePrefix</code> <code class="o1">=</code> <code class="s">_</code>
<code class="na1">agent1.sinks.sink1.hdfs.fileType</code> <code class="o1">=</code> <code class="s">DataStream</code></strong></span>

<code class="na">agent1.channels.channel1.type</code> <code class="o">=</code> <code class="sb">file</code></pre></div></div><p class="calibre2">Restart the agent to use the <em class="calibre10">spool-to-hdfs.properties</em> configuration, and
    create a new file in the spooling directory:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">echo -e "Hello\nAgain" &gt; /tmp/spooldir/.file2.txt</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">mv /tmp/spooldir/.file2.txt /tmp/spooldir/file2.txt</code></strong></pre><p class="calibre2">Events will now be delivered to the HDFS sink and written to a file.
    Files in the process of being written to have a <em class="calibre10">.tmp</em> in-use suffix added to their name to
    indicate that they are not yet complete. In this example, we have
    <a class="calibre" id="calibre_link-1979"></a>also set <code class="literal">hdfs.inUsePrefix</code>
    to be <code class="literal">_</code> (underscore; by default it is
    empty), which causes files in the process of being written to have that
    prefix added to their names. This is useful since MapReduce will ignore
    files that have a <code class="literal">_</code> prefix. So, a
    typical temporary filename would be <em class="calibre10">_events.1399295780136.log.tmp</em>; the number is a
    timestamp generated by the HDFS sink.</p><p class="calibre2">A file is kept open by the HDFS sink until it has either been open
    for a given time (default 30 seconds, controlled by the <code class="literal">hdfs.rollInterval</code> property<a class="calibre" id="calibre_link-1983"></a>), has reached a given size (default 1,024 bytes, <a class="calibre" id="calibre_link-1984"></a>set by <code class="literal">hdfs.rollSize</code>), or
    has had a given number of events written to it (default 10, set by
    <code class="literal">hdfs.rollCount</code>). If any of these
    criteria are met, the file is closed and its in-use prefix and suffix are
    removed. New events are written to a new file (which will have an in-use
    prefix and suffix until it is rolled).</p><p class="calibre2">After 30 seconds, we can be sure that the file has been rolled and
    we can take a look at its contents:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -cat /tmp/flume/events.1399295780136.log</code></strong>
Hello
Again</pre><p class="calibre2">The HDFS sink writes files as the user who is running the Flume
    agent, <a class="calibre" id="calibre_link-1982"></a>unless the <code class="literal">hdfs.proxyUser</code>
    property is set, in which case files will be written as that <a class="calibre" id="calibre_link-1969"></a>user.</p><div class="book" title="Partitioning and Interceptors"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4433">Partitioning and Interceptors</h3></div></div></div><p class="calibre2">Large datasets are <a class="calibre" id="calibre_link-1974"></a><a class="calibre" id="calibre_link-2947"></a>often organized into partitions, so that processing can be
      restricted to particular partitions if only a subset of the data is
      being queried. For Flume event data, it’s very common to partition by
      time. A process can be run periodically that transforms completed
      partitions (to remove duplicate events, for example).</p><p class="calibre2">It’s easy to change the example to store data in partitions by
      <a class="calibre" id="calibre_link-1981"></a>setting <code class="literal">hdfs.path</code> to
      include subdirectories that use time format escape sequences:</p><pre class="screen1"><code class="na">agent1.sinks.sink1.hdfs.path</code> <code class="o">=</code> <code class="sb">/tmp/flume/year=%Y/month=%m/day=%d</code></pre><p class="calibre2">Here we have chosen to have day-sized partitions, but other levels
      of granularity are possible, as are other directory layout schemes. (If
      you are using Hive, see <a class="ulink" href="#calibre_link-297" title="Partitions and Buckets">Partitions and Buckets</a> for how
      Hive lays out partitions on disk.) The full list of format escape
      sequences is provided in the documentation for the HDFS sink in the
      <a class="ulink" href="http://flume.apache.org/FlumeUserGuide.html" target="_top">Flume User
      Guide</a>.</p><p class="calibre2">The partition that a Flume event is written to is determined by
      the <code class="literal">timestamp</code> header on the event.
      Events don’t have this header by default, but it can be added using a
      Flume <em class="calibre10">interceptor</em>. Interceptors are components
      that can modify or drop events in the flow; they are attached to
      sources, and are run on events before the events have been placed in a
      channel.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-298" id="calibre_link-316">92</a>]</sup> The following extra configuration lines add a timestamp
      interceptor to <code class="literal">source1</code>, which adds a
      <code class="literal">timestamp</code> header to every event
      produced by the source:</p><pre class="screen1"><code class="na">agent1.sources.source1.interceptors</code> <code class="o">=</code> <code class="sb">interceptor1</code>
<code class="na">agent1.sources.source1.interceptors.interceptor1.type</code> <code class="o">=</code> <code class="sb">timestamp</code></pre><p class="calibre2">Using the timestamp interceptor ensures that the timestamps
      closely reflect the times at which the events were created. For some
      applications, using a timestamp for when the event was written to HDFS
      might be sufficient—although, be aware that when there are multiple
      tiers of Flume agents there can be a significant difference between
      creation time and write time, especially in the event of agent downtime
      (see <a class="ulink" href="#calibre_link-295" title="Distribution: Agent Tiers">Distribution: Agent Tiers</a>). For these cases, the HDFS
      sink has a <a class="calibre" id="calibre_link-1985"></a>setting, <code class="literal">hdfs.useLocalTime</code><code class="literal">Stamp</code>,
      that will use a timestamp generated by the Flume agent running the HDFS
      sink.</p></div><div class="book" title="File Formats"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-304">File Formats</h3></div></div></div><p class="calibre2">It’s normally a good idea <a class="calibre" id="calibre_link-1972"></a>to use a binary format for storing your data in, since the
      resulting files are smaller than they would be if you used text. For the
      HDFS sink, the file format used is <a class="calibre" id="calibre_link-1978"></a>controlled using <code class="literal">hdfs.fileType</code> and a combination of a few other
      properties.</p><p class="calibre2">If unspecified, <code class="literal">hdfs.fileType</code>
      defaults to <code class="literal">SequenceFile</code>, which will
      write events to a sequence file with <code class="literal">LongWritable</code>
      keys that contain the event timestamp (or the current time if the
      <code class="literal">timestamp</code> header is not present) and
      <code class="literal">BytesWritable</code> values that contain the event body.
      It’s possible to use <code class="literal">Text</code> Writable values in the
      sequence file <a class="calibre" id="calibre_link-1986"></a>instead of <code class="literal">BytesWritable</code> by setting
      <code class="literal">hdfs.writeFormat</code> to <code class="literal">Text</code>.</p><p class="calibre2">The configuration is a little different for <a class="calibre" id="calibre_link-943"></a><a class="calibre" id="calibre_link-1706"></a>Avro files. The <code class="literal">hdfs.fileType</code> property is set to <code class="literal">DataStream</code>, just like for plain text.
      Additionally, <code class="literal">serializer</code> (note the
      lack of an <code class="literal">hdfs.</code> prefix) must be set
      to <code class="literal">avro_event</code>. To enable compression,
      set the <code class="literal">serializer</code><code class="literal">.compressionCodec</code> property. Here is an example
      of an HDFS sink configured to write Snappy-compressed Avro files:</p><pre class="screen1"><code class="na">agent1.sinks.sink1.type</code> <code class="o">=</code> <code class="sb">hdfs</code>
<code class="na">agent1.sinks.sink1.hdfs.path</code> <code class="o">=</code> <code class="sb">/tmp/flume</code>
<code class="na">agent1.sinks.sink1.hdfs.filePrefix</code> <code class="o">=</code> <code class="sb">events</code>
<code class="na">agent1.sinks.sink1.hdfs.fileSuffix</code> <code class="o">=</code> <span class="calibre24"><strong class="calibre9"><code class="err">.avro</code></strong></span>
<code class="na">agent1.sinks.sink1.hdfs.fileType</code> <code class="o">=</code> <code class="sb">DataStream</code><span class="calibre24"><strong class="calibre9">
<code class="na1">agent1.sinks.sink1.serializer</code> <code class="o1">=</code> <code class="s">avro_event</code>
<code class="na1">agent1.sinks.sink1.serializer.compressionCodec</code> <code class="o1">=</code> <code class="s">snappy</code></strong></span></pre><p class="calibre2">An event is represented as an Avro record with two fields:
      <code class="literal">headers</code>, an Avro map with string
      values, and <code class="literal">body</code>, an Avro bytes
      field.</p><p class="calibre2">If you want to use a custom Avro schema, there are a couple of
      options. If you have Avro in-memory objects that you want to send to
      Flume, then the <code class="literal">Log4jAppender</code> is appropriate. It
      allows you to log an Avro Generic, Specific, or Reflect object using a
      log4j <code class="literal">Logger</code> and send it to an Avro
      source running in a Flume agent (see <a class="ulink" href="#calibre_link-295" title="Distribution: Agent Tiers">Distribution: Agent Tiers</a>). In this case, <a class="calibre" id="calibre_link-3373"></a>the <code class="literal">serializer</code> property
      for the HDFS sink should be set to
      <code class="literal">org.apache.flume.sink.hdfs.AvroEventSerializer$Builder</code>,
      and the Avro schema set in the header (see the class
      documentation).</p><p class="calibre2">Alternatively, if the events are not originally derived from Avro
      objects, you can write a custom serializer to convert a Flume event into
      an Avro object with a custom schema. The helper <a class="calibre" id="calibre_link-858"></a>class <code class="literal">AbstractAvroEventSerializer</code>
      in the <code class="literal">org.apache.flume.</code><code class="literal">serialization</code>
      package is <a class="calibre" id="calibre_link-2857"></a>a good starting point.</p></div></div><div class="book" title="Fan Out"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-313">Fan Out</h2></div></div></div><p class="calibre2"><em class="calibre10">Fan out</em> is the term <a class="calibre" id="calibre_link-1712"></a><a class="calibre" id="calibre_link-1970"></a><a class="calibre" id="calibre_link-1613"></a>for delivering events from one source to multiple channels,
    so they reach multiple sinks. For example, the configuration in <a class="ulink" href="#calibre_link-299" title="Example&nbsp;14-3.&nbsp;Flume configuration using a spooling directory source, fanning out to an HDFS sink and a logger sink">Example&nbsp;14-3</a> delivers events to both an HDFS sink
    (<code class="literal">sink1a</code> via <code class="literal">channel1a</code>) and a logger sink (<code class="literal">sink1b</code> via <code class="literal">channel1b</code>).</p><div class="example"><a id="calibre_link-299" class="calibre"></a><div class="example-title">Example&nbsp;14-3.&nbsp;Flume configuration using a spooling directory source, fanning
      out to an HDFS sink and a logger sink</div><div class="book"><pre class="screen"><code class="na">agent1.sources</code> <code class="o">=</code> <code class="sb">source1</code>
<span class="calibre24"><strong class="calibre9"><code class="na1">agent1.sinks</code> <code class="o1">=</code> <code class="s">sink1a sink1b</code>
<code class="na1">agent1.channels</code> <code class="o1">=</code> <code class="s">channel1a channel1b</code></strong></span>

<span class="calibre24"><strong class="calibre9"><code class="na1">agent1.sources.source1.channels</code> <code class="o1">=</code> <code class="s">channel1a channel1b</code>
<code class="na1">agent1.sinks.sink1a.channel</code> <code class="o1">=</code> <code class="s">channel1a</code>
<code class="na1">agent1.sinks.sink1b.channel</code> <code class="o1">=</code> <code class="s">channel1b</code></strong></span>

<code class="na">agent1.sources.source1.type</code> <code class="o">=</code> <code class="sb">spooldir</code>
<code class="na">agent1.sources.source1.spoolDir</code> <code class="o">=</code> <code class="sb">/tmp/spooldir</code>

<code class="na">agent1.sinks.sink1a.type</code> <code class="o">=</code> <code class="sb">hdfs</code>
<code class="na">agent1.sinks.sink1a.hdfs.path</code> <code class="o">=</code> <code class="sb">/tmp/flume</code>
<code class="na">agent1.sinks.sink1a.hdfs.filePrefix</code> <code class="o">=</code> <code class="sb">events</code>
<code class="na">agent1.sinks.sink1a.hdfs.fileSuffix</code> <code class="o">=</code> <code class="sb">.log</code>
<code class="na">agent1.sinks.sink1a.hdfs.fileType</code> <code class="o">=</code> <code class="sb">DataStream</code>

<code class="na">agent1.sinks.sink1b.type</code> <code class="o">=</code> <code class="sb">logger</code>

<code class="na">agent1.channels.channel1a.type</code> <code class="o">=</code> <code class="sb">file</code>
<code class="na">agent1.channels.channel1b.type</code> <code class="o">=</code> <code class="sb">memory</code></pre></div></div><p class="calibre2">The key change here is that the source is configured to deliver to
    multiple channels by setting <code class="literal">agent1.sources.source1.channels</code> to a
    space-separated list of channel names, <code class="literal">channel1a</code> and <code class="literal">channel1b</code>. This time, the channel feeding the
    logger sink (<code class="literal">channel1b</code>) is a memory
    channel, since we are logging events for debugging purposes and don’t mind
    losing events on agent restart. Also, each channel is configured to feed
    one sink, just like in the previous examples. The flow is illustrated in
    <a class="ulink" href="#calibre_link-300" title="Figure&nbsp;14-2.&nbsp;Flume agent with a spooling directory source and fanning out to an HDFS sink and a logger sink">Figure&nbsp;14-2</a>.</p><div class="figure"><a id="calibre_link-300" class="calibre"></a><div class="book"><div class="book"><img alt="Flume agent with a spooling directory source and fanning out to an HDFS sink and a logger sink" src="images/000084.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;14-2.&nbsp;Flume agent with a spooling directory source and fanning out to
      an HDFS sink and a logger sink</div></div><div class="book" title="Delivery Guarantees"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4434">Delivery Guarantees</h3></div></div></div><p class="calibre2">Flume uses a separate <a class="calibre" id="calibre_link-1614"></a>transaction to deliver each batch of events from the
      spooling directory source to each channel. In this example, one
      transaction will be used to deliver to the channel feeding the HDFS
      sink, and then another transaction will be used to deliver the same
      batch of events to the channel for the logger sink. If either of these
      transactions fails (if a channel is full, for example), then the events
      will not be removed from the source, and will be retried later.</p><p class="calibre2">In this case, since we don’t mind if some events are not delivered
      to the logger sink, we can designate its channel as an
      <em class="calibre10">optional</em> channel, so that if the transaction
      associated with it fails, this will not cause events to be left in the
      source and tried again later. (Note that if the agent fails before
      <span class="calibre"><em class="calibre10">both</em></span> channel transactions have committed, then the
      affected events will be redelivered after the agent restarts—this is
      true even if the uncommitted channels are marked as optional.) To do
      this, we set the <code class="literal">selector.optional</code> property on the
      source, passing it a space-separated list of <a class="calibre" id="calibre_link-1715"></a><a class="calibre" id="calibre_link-1713"></a><a class="calibre" id="calibre_link-1971"></a>channels:</p><pre class="screen1"><code class="na">agent1.sources.source1.selector.optional</code> <code class="o">=</code> <code class="sb">channel1b</code></pre><div class="sidebar"><div class="sidebar-title">Near-Real-Time Indexing</div><p class="calibre2">Indexing events for search is a good example of where fan out is
        used in practice. A single source of events is sent to both an
        <a class="calibre" id="calibre_link-1973"></a>HDFS sink (this is the main repository of events, so a
        required channel is used) and a Solr (or Elasticsearch) sink, to build
        a search index (using an optional channel).</p><p class="calibre2">The <code class="literal">MorphlineSolrSink</code> extracts <a class="calibre" id="calibre_link-2707"></a>fields from Flume events and transforms them into a Solr
        document (using a Morphline configuration file), which is then loaded
        into a live Solr search server. The process is called <em class="calibre10">near
        real time</em> since ingested data appears in search results in
        a matter of seconds.</p></div></div><div class="book" title="Replicating and Multiplexing Selectors"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4435">Replicating and Multiplexing Selectors</h3></div></div></div><p class="calibre2">In normal fan-out flow, <a class="calibre" id="calibre_link-1615"></a><a class="calibre" id="calibre_link-3322"></a>events are replicated to all channels—but sometimes more
      selective behavior might be desirable, so that some events are sent to
      one channel and others to another. This can be achieved by setting
      <a class="calibre" id="calibre_link-2726"></a>a <em class="calibre10">multiplexing</em> selector on the
      source, and defining routing rules that map particular event header
      values to channels. See the <a class="ulink" href="http://flume.apache.org/FlumeUserGuide.html" target="_top">Flume User
      Guide</a> for configuration details.</p></div></div><div class="book" title="Distribution: Agent Tiers"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-295">Distribution: Agent Tiers</h2></div></div></div><p class="calibre2">How do we scale a set of <a class="calibre" id="calibre_link-1704"></a><a class="calibre" id="calibre_link-871"></a>Flume agents? If there is one agent running on every node
    producing raw data, then with the setup described so far, at any
    particular time each file being written to HDFS will consist entirely of
    the events from one node. It would be better if we could aggregate the
    events from a group of nodes in a single file, since this would result in
    fewer, larger files (with the concomitant reduction in pressure on HDFS,
    and more efficient processing in MapReduce; see <a class="ulink" href="#calibre_link-301" title="Small files and CombineFileInputFormat">Small files and CombineFileInputFormat</a>). Also, if needed, files can be rolled more often
    since they are being fed by a larger number of nodes, leading to a
    reduction between the time when an event is created and when it’s
    available for analysis.</p><p class="calibre2">Aggregating Flume events is achieved by having
    <span class="calibre"><em class="calibre10">tiers</em></span> of Flume agents. The first tier collects events
    from the original sources (such as web servers) and sends them to a
    smaller set of agents in the second tier, which aggregate events from the
    first tier before writing them to HDFS (see <a class="ulink" href="#calibre_link-302" title="Figure&nbsp;14-3.&nbsp;Using a second agent tier to aggregate Flume events from the first tier">Figure&nbsp;14-3</a>). Further tiers may be warranted for very
    large numbers of source nodes.</p><div class="figure"><a id="calibre_link-302" class="calibre"></a><div class="book"><div class="book"><img alt="Using a second agent tier to aggregate Flume events from the first tier" src="images/000002.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;14-3.&nbsp;Using a second agent tier to aggregate Flume events from the
      first tier</div></div><p class="calibre2">Tiers are constructed by using a special sink that sends events over
    the network, and a corresponding source that receives events. The Avro
    sink sends events over Avro RPC to an Avro source running in another Flume
    agent. There is also a Thrift sink that does the same thing using Thrift
    RPC, and is paired with a Thrift source.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-303" id="calibre_link-317">93</a>]</sup></p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">Don’t be confused by the naming: Avro sinks and sources do not
      provide the ability to write (or read) Avro files. They are used only to
      distribute events between agent tiers, and to do so they use Avro RPC to
      communicate (hence the name). If you need to write events to Avro files,
      use the HDFS sink, described in <a class="ulink" href="#calibre_link-304" title="File Formats">File Formats</a>.</p></div><p class="calibre2"><a class="ulink" href="#calibre_link-305" title="Example&nbsp;14-4.&nbsp;A two-tier Flume configuration using a spooling directory source and an HDFS sink">Example&nbsp;14-4</a> shows a two-tier Flume
    configuration. Two agents are defined in the file, named <code class="literal">agent1</code> and <code class="literal">agent2</code>. An agent of type <code class="literal">agent1</code> runs in the first tier, and has a
    <code class="literal">spooldir</code> source and an Avro sink
    connected by a file channel. The <code class="literal">agent2</code>
    agent runs in the second tier, and has an Avro source that listens on the
    port that <code class="literal">agent1</code>’s Avro sink sends
    events to. The sink for <code class="literal">agent2</code> uses the
    same HDFS sink configuration from <a class="ulink" href="#calibre_link-296" title="Example&nbsp;14-2.&nbsp;Flume configuration using a spooling directory source and an HDFS sink">Example&nbsp;14-2</a>.</p><p class="calibre2">Notice that since there are two file channels running on the same
    machine, they are configured to point to different data and checkpoint
    directories (they are in the user’s home directory by default). This way,
    they don’t try to write their files on top of one another.</p><div class="example"><a id="calibre_link-305" class="calibre"></a><div class="example-title">Example&nbsp;14-4.&nbsp;A two-tier Flume configuration using a spooling directory source
      and an HDFS sink</div><div class="book"><pre class="screen"><code class="c1"># First-tier agent</code>

<code class="na">agent1.sources</code> <code class="o">=</code> <code class="sb">source1</code>
<code class="na">agent1.sinks</code> <code class="o">=</code> <code class="sb">sink1</code>
<code class="na">agent1.channels</code> <code class="o">=</code> <code class="sb">channel1</code>

<code class="na">agent1.sources.source1.channels</code> <code class="o">=</code> <code class="sb">channel1</code>
<code class="na">agent1.sinks.sink1.channel</code> <code class="o">=</code> <code class="sb">channel1</code>

<code class="na">agent1.sources.source1.type</code> <code class="o">=</code> <code class="sb">spooldir</code>
<code class="na">agent1.sources.source1.spoolDir</code> <code class="o">=</code> <code class="sb">/tmp/spooldir</code>

<span class="calibre24"><strong class="calibre9"><code class="na1">agent1.sinks.sink1.type</code> <code class="o1">=</code> <code class="s">avro</code>
<code class="na1">agent1.sinks.sink1.hostname</code> <code class="o1">=</code> <code class="s">localhost</code>
<code class="na1">agent1.sinks.sink1.port</code> <code class="o1">=</code> <code class="s">10000</code></strong></span>

<code class="na">agent1.channels.channel1.type</code> <code class="o">=</code> <code class="sb">file</code>
<code class="na">agent1.channels.channel1.checkpointDir</code><code class="o">=</code><code class="sb">/tmp/agent1/file-channel/checkpoint</code>
<code class="na">agent1.channels.channel1.dataDirs</code><code class="o">=</code><code class="sb">/tmp/agent1/file-channel/data</code>

<code class="c1"># Second-tier agent</code>

<code class="na">agent2.sources</code> <code class="o">=</code> <code class="sb">source2</code>
<code class="na">agent2.sinks</code> <code class="o">=</code> <code class="sb">sink2</code>
<code class="na">agent2.channels</code> <code class="o">=</code> <code class="sb">channel2</code>

<code class="na">agent2.sources.source2.channels</code> <code class="o">=</code> <code class="sb">channel2</code>
<code class="na">agent2.sinks.sink2.channel</code> <code class="o">=</code> <code class="sb">channel2</code>

<span class="calibre24"><strong class="calibre9"><code class="na1">agent2.sources.source2.type</code> <code class="o1">=</code> <code class="s">avro</code>
<code class="na1">agent2.sources.source2.bind</code> <code class="o1">=</code> <code class="s">localhost</code>
<code class="na1">agent2.sources.source2.port</code> <code class="o1">=</code> <code class="s">10000</code></strong></span>

<code class="na">agent2.sinks.sink2.type</code> <code class="o">=</code> <code class="sb">hdfs</code>
<code class="na">agent2.sinks.sink2.hdfs.path</code> <code class="o">=</code> <code class="sb">/tmp/flume</code>
<code class="na">agent2.sinks.sink2.hdfs.filePrefix</code> <code class="o">=</code> <code class="sb">events</code>
<code class="na">agent2.sinks.sink2.hdfs.fileSuffix</code> <code class="o">=</code> <code class="sb">.log</code>
<code class="na">agent2.sinks.sink2.hdfs.fileType</code> <code class="o">=</code> <code class="sb">DataStream</code>

<code class="na">agent2.channels.channel2.type</code> <code class="o">=</code> <code class="sb">file</code>
<code class="na">agent2.channels.channel2.checkpointDir</code><code class="o">=</code><code class="sb">/tmp/agent2/file-channel/checkpoint</code>
<code class="na">agent2.channels.channel2.dataDirs</code><code class="o">=</code><code class="sb">/tmp/agent2/file-channel/data</code></pre></div></div><p class="calibre2">The system is illustrated in <a class="ulink" href="#calibre_link-306" title="Figure&nbsp;14-4.&nbsp;Two Flume agents connected by an Avro sink-source pair">Figure&nbsp;14-4</a>.</p><div class="figure"><a id="calibre_link-306" class="calibre"></a><div class="book"><div class="book"><img alt="Two Flume agents connected by an Avro sink-source pair" src="images/000009.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;14-4.&nbsp;Two Flume agents connected by an Avro sink-source pair</div></div><p class="calibre2">Each agent is run independently, using the same <code class="literal">--conf-file</code> parameter but different agent
    <code class="literal">--name</code> parameters:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">flume-ng agent --conf-file spool-to-hdfs-tiered.properties --name agent1 ...</code></strong></pre><p class="calibre2">and:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">flume-ng agent --conf-file spool-to-hdfs-tiered.properties --name agent2 ...</code></strong></pre><div class="book" title="Delivery Guarantees"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4436">Delivery Guarantees</h3></div></div></div><p class="calibre2">Flume uses transactions to ensure that each batch of events is
      reliably delivered from a source to a channel, and from a channel to a
      sink. In the context of the Avro sink-source connection, transactions
      ensure that events are reliably delivered from one agent to the
      next.</p><p class="calibre2">The operation to read a batch of events from the file channel in
      <code class="literal">agent1</code> by the Avro sink will be
      wrapped in a transaction. The transaction will only be committed once
      the Avro sink has received the (synchronous) confirmation that the write
      to the Avro source’s RPC endpoint was successful. This confirmation will
      only be sent once <code class="literal">agent2</code>’s
      transaction wrapping the operation to write the batch of events to its
      file channel has been successfully committed. Thus, the Avro sink-source
      pair guarantees that an event is delivered from one Flume agent’s
      channel to another Flume agent’s channel (at least once).</p><p class="calibre2">If either agent is not running, then clearly events cannot be
      delivered to HDFS. For example, if <code class="literal">agent1</code> stops running, then files will
      accumulate in the spooling directory, to be processed once <code class="literal">agent1</code> starts up again. Also, any events in an
      agent’s own file channel at the point the agent stopped running will be
      available on restart, due to the durability guarantee that file channel
      provides.</p><p class="calibre2">If <code class="literal">agent2</code> stops running, then
      events will be stored in <code class="literal">agent1</code>’s
      file channel until <code class="literal">agent2</code> starts
      again. Note, however, that channels necessarily have a limited capacity;
      if <code class="literal">agent1</code>’s channel fills up while
      <code class="literal">agent2</code> is not running, then any new
      events will be lost. By default, a file channel will not recover more
      than one million events (this can be overridden by its <code class="literal">capacity</code> property), and it will stop accepting
      events if the free disk space for its checkpoint directory falls below
      500 MB (controlled by the <code class="literal">minimum</code><code class="literal">RequiredSpace</code> property).</p><p class="calibre2">Both these scenarios assume that the agent will eventually
      recover, but that is not always the case (if the hardware it is running
      on fails, for example). If <code class="literal">agent1</code>
      doesn’t recover, then the loss is limited to the events in its file
      channel that had not been delivered to <code class="literal">agent2</code> before <code class="literal">agent1</code> shut down. In the architecture
      described here, there are multiple first-tier agents like <code class="literal">agent1</code>, so other nodes in the tier can take
      over the function of the failed node. For example, if the nodes are
      running load-balanced web servers, then other nodes will absorb the
      failed web server’s traffic, and they will generate new Flume events
      that are delivered to <code class="literal">agent2</code>. Thus,
      no new events are lost.</p><p class="calibre2">An unrecoverable <code class="literal">agent2</code> failure
      is more serious, however. Any events in the channels of upstream
      first-tier agents (<code class="literal">agent1</code> instances)
      will be lost, and all new events generated by these agents will not be
      delivered either. The solution to this problem is for <code class="literal">agent1</code> to have multiple redundant Avro sinks,
      arranged in a <em class="calibre10">sink group</em>, so that if the
      destination <code class="literal">agent2</code> Avro endpoint is
      unavailable, it can try another sink from the group. We’ll see how to do
      this in the next <a class="calibre" id="calibre_link-1705"></a><a class="calibre" id="calibre_link-872"></a>section.</p></div></div><div class="book" title="Sink Groups"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-314">Sink Groups</h2></div></div></div><p class="calibre2">A sink group allows <a class="calibre" id="calibre_link-1718"></a><a class="calibre" id="calibre_link-3410"></a>multiple sinks to be treated as one, for <a class="calibre" id="calibre_link-1603"></a><a class="calibre" id="calibre_link-2332"></a>failover or load-balancing purposes (see <a class="ulink" href="#calibre_link-307" title="Figure&nbsp;14-5.&nbsp;Using multiple sinks for load balancing or failover">Figure&nbsp;14-5</a>). If a second-tier agent is
    unavailable, then events will be delivered to another second-tier agent
    and on to HDFS without disruption.</p><div class="figure"><a id="calibre_link-307" class="calibre"></a><div class="book"><div class="book"><img alt="Using multiple sinks for load balancing or failover" src="images/000018.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;14-5.&nbsp;Using multiple sinks for load balancing or failover</div></div><p class="calibre2">To configure a sink group, the agent’s <code class="literal">sinkgroups</code> property <a class="calibre" id="calibre_link-3412"></a>is set to define the sink group’s name; then the sink group
    lists the sinks in the group, and also the type of the sink processor,
    which sets the policy for choosing a sink. <a class="ulink" href="#calibre_link-308" title="Example&nbsp;14-5.&nbsp;A Flume configuration for load balancing between two Avro endpoints using a sink group">Example&nbsp;14-5</a> shows the configuration for
    load balancing between two Avro endpoints.</p><div class="example"><a id="calibre_link-308" class="calibre"></a><div class="example-title">Example&nbsp;14-5.&nbsp;A Flume configuration for load balancing between two Avro
      endpoints using a sink group</div><div class="book"><pre class="screen"><code class="na">agent1.sources</code> <code class="o">=</code> <code class="sb">source1</code>
<code class="na">agent1.sinks</code> <code class="o">=</code> <span class="calibre24"><strong class="calibre9"><code class="err">sink1a</code> <code class="err">sink1b</code></strong></span><span class="calibre24"><strong class="calibre9">
<code class="na1">agent1.sinkgroups</code> <code class="o1">=</code> <code class="s">sinkgroup1</code></strong></span>
<code class="na">agent1.channels</code> <code class="o">=</code> <code class="sb">channel1</code>

<code class="na">agent1.sources.source1.channels</code> <code class="o">=</code> <code class="sb">channel1</code><span class="calibre24"><strong class="calibre9">
<code class="na1">agent1.sinks.sink1a.channel</code> <code class="o1">=</code> <code class="s">channel1</code>
<code class="na1">agent1.sinks.sink1b.channel</code> <code class="o1">=</code> <code class="s">channel1</code></strong></span>

<span class="calibre24"><strong class="calibre9"><code class="na1">agent1.sinkgroups.sinkgroup1.sinks</code> <code class="o1">=</code> <code class="s">sink1a sink1b</code>
<code class="na1">agent1.sinkgroups.sinkgroup1.processor.type</code> <code class="o1">=</code> <code class="s">load_balance</code>
<code class="na1">agent1.sinkgroups.sinkgroup1.processor.backoff</code> <code class="o1">=</code> <code class="s">true</code></strong></span>

<code class="na">agent1.sources.source1.type</code> <code class="o">=</code> <code class="sb">spooldir</code>
<code class="na">agent1.sources.source1.spoolDir</code> <code class="o">=</code> <code class="sb">/tmp/spooldir</code>

<span class="calibre24"><strong class="calibre9"><code class="na1">agent1.sinks.sink1a.type</code> <code class="o1">=</code> <code class="s">avro</code>
<code class="na1">agent1.sinks.sink1a.hostname</code> <code class="o1">=</code> <code class="s">localhost</code>
<code class="na1">agent1.sinks.sink1a.port</code> <code class="o1">=</code> <code class="s">10000</code>

<code class="na1">agent1.sinks.sink1b.type</code> <code class="o1">=</code> <code class="s">avro</code>
<code class="na1">agent1.sinks.sink1b.hostname</code> <code class="o1">=</code> <code class="s">localhost</code>
<code class="na1">agent1.sinks.sink1b.port</code> <code class="o1">=</code> <code class="s">10001</code></strong></span>

<code class="na">agent1.channels.channel1.type</code> <code class="o">=</code> <code class="sb">file</code></pre></div></div><p class="calibre2">There are two Avro sinks defined, <code class="literal">sink1a</code> and <code class="literal">sink1b</code>, which differ only in the Avro endpoint
    they are connected to (since we are running all the examples on localhost,
    it is the port that is different; for a distributed install, the hosts
    would differ and the ports would be the same). We also define <code class="literal">sinkgroup1</code>, and set its sinks to <code class="literal">sink1a</code> and <code class="literal">sink1b</code>.</p><p class="calibre2">The processor type is set to <code class="literal">load_balance</code>, which attempts to distribute the
    event flow over both sinks in the group, using a round-robin selection
    mechanism (you can change this using the <code class="literal">processor.selector</code> property). If a sink is
    unavailable, then the next sink is tried; if they are all unavailable, the
    event is not removed from the channel, just like in the single sink case.
    By default, sink unavailability is not remembered by the sink processor,
    so failing sinks are retried for every batch of events being delivered.
    This can be inefficient, so we have set the <code class="literal">processor.backoff</code> property to change the
    behavior so that failing sinks are blacklisted for an exponentially
    increasing timeout period (up to a maximum period of 30 seconds,
    controlled by <code class="literal">processor.selector.maxTimeOut</code>).</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">There is another type of processor, <code class="literal">failover</code>, that instead of load balancing
      events across sinks uses a preferred sink if it is available, and fails
      over to another sink in the case that the preferred sink is down. The
      failover sink processor maintains a priority order for sinks in the
      group, and attempts delivery in order of priority. If the sink with the
      highest priority is unavailable the one with the next highest priority
      is tried, and so on. Failed sinks are blacklisted for an increasing
      timeout period (up to a maximum period of 30 seconds, controlled by
      <code class="literal">processor.maxpenalty</code>).</p></div><p class="calibre2">The configuration for one of the second-tier agents, <code class="literal">agent2a</code>, is shown in <a class="ulink" href="#calibre_link-309" title="Example&nbsp;14-6.&nbsp;Flume configuration for second-tier agent in a load balancing scenario">Example&nbsp;14-6</a>.</p><div class="example"><a id="calibre_link-309" class="calibre"></a><div class="example-title">Example&nbsp;14-6.&nbsp;Flume configuration for second-tier agent in a load balancing
      scenario</div><div class="book"><pre class="screen"><code class="na">agent2a.sources</code> <code class="o">=</code> <code class="sb">source2a</code>
<code class="na">agent2a.sinks</code> <code class="o">=</code> <code class="sb">sink2a</code>
<code class="na">agent2a.channels</code> <code class="o">=</code> <code class="sb">channel2a</code>

<code class="na">agent2a.sources.source2a.channels</code> <code class="o">=</code> <code class="sb">channel2a</code>
<code class="na">agent2a.sinks.sink2a.channel</code> <code class="o">=</code> <code class="sb">channel2a</code>

<code class="na">agent2a.sources.source2a.type</code> <code class="o">=</code> <code class="sb">avro</code>
<code class="na">agent2a.sources.source2a.bind</code> <code class="o">=</code> <code class="sb">localhost</code>
<code class="na">agent2a.sources.source2a.port</code> <code class="o">=</code> <span class="calibre24"><strong class="calibre9"><code class="err">10000</code></strong></span>

<code class="na">agent2a.sinks.sink2a.type</code> <code class="o">=</code> <code class="sb">hdfs</code>
<code class="na">agent2a.sinks.sink2a.hdfs.path</code> <code class="o">=</code> <code class="sb">/tmp/flume</code>
<code class="na">agent2a.sinks.sink2a.hdfs.filePrefix</code> <code class="o">=</code> <span class="calibre24"><strong class="calibre9"><code class="err">events-a</code></strong></span>
<code class="na">agent2a.sinks.sink2a.hdfs.fileSuffix</code> <code class="o">=</code> <code class="sb">.log</code>
<code class="na">agent2a.sinks.sink2a.hdfs.fileType</code> <code class="o">=</code> <code class="sb">DataStream</code>

<code class="na">agent2a.channels.channel2a.type</code> <code class="o">=</code> <code class="sb">file</code></pre></div></div><p class="calibre2">The configuration for <code class="literal">agent2b</code> is
    the same, except for the Avro source port (since we are running the
    examples on localhost) and the file prefix for the files created by the
    HDFS sink. The file prefix is used to ensure that HDFS files created by
    second-tier agents at the same time don’t collide.</p><p class="calibre2">In the more usual case of agents running on different machines, the
    hostname can be used to make the filename unique by configuring a host
    interceptor (see <a class="ulink" href="#calibre_link-310" title="Table&nbsp;14-1.&nbsp;Flume components">Table&nbsp;14-1</a>) and including the
    <code class="literal">%{host}</code> escape sequence in the file
    path, or prefix:</p><pre class="screen1"><code class="na">agent2.sinks.sink2.hdfs.filePrefix</code> <code class="o">=</code> <span class="calibre24"><strong class="calibre9"><code class="err">events-%{host}</code></strong></span></pre><p class="calibre2">A diagram of the whole system is <a class="calibre" id="calibre_link-3411"></a><a class="calibre" id="calibre_link-1719"></a>shown in <a class="ulink" href="#calibre_link-311" title="Figure&nbsp;14-6.&nbsp;Load balancing between two agents">Figure&nbsp;14-6</a>.</p><div class="figure"><a id="calibre_link-311" class="calibre"></a><div class="book"><div class="book"><img alt="Load balancing between two agents" src="images/000026.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;14-6.&nbsp;Load balancing between two agents</div></div></div><div class="book" title="Integrating Flume with Applications"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4437">Integrating Flume with Applications</h2></div></div></div><p class="calibre2">An Avro source is an RPC <a class="calibre" id="calibre_link-1717"></a>endpoint that accepts Flume events, making it possible to
    write an RPC client to send events to the endpoint, which can be embedded
    in any application that wants to introduce events into Flume.</p><p class="calibre2">The <em class="calibre10">Flume SDK</em> is a module that provides a
    Java <code class="literal">RpcClient</code> class <a class="calibre" id="calibre_link-3249"></a>for sending <code class="literal">Event</code> objects to an Avro
    endpoint (an Avro source running in a Flume agent, usually in another
    tier). Clients can be configured to fail over or load balance between
    endpoints, and Thrift endpoints (Thrift sources) are supported too.</p><p class="calibre2">The Flume <em class="calibre10">embedded agent</em> offers similar
    functionality: it is a cut-down Flume agent that runs in a Java
    application. It has a single special source that your application sends
    Flume <code class="literal">Event</code> objects to by calling a method on
    <a class="calibre" id="calibre_link-1566"></a>the <code class="literal">EmbeddedAgent</code> object; the only
    sinks that are supported are Avro sinks, but it can be configured with
    multiple sinks for failover or load balancing.</p><p class="calibre2">Both the SDK and the embedded agent are described in more <a class="calibre" id="calibre_link-1604"></a><a class="calibre" id="calibre_link-2333"></a>detail in the <a class="ulink" href="http://flume.apache.org/FlumeDeveloperGuide.html" target="_top">Flume Developer
    Guide</a>.</p></div><div class="book" title="Component Catalog"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4438">Component Catalog</h2></div></div></div><p class="calibre2">We’ve only used <a class="calibre" id="calibre_link-1708"></a>a handful of Flume components in this chapter. Flume comes
    with many more, which are briefly described in <a class="ulink" href="#calibre_link-310" title="Table&nbsp;14-1.&nbsp;Flume components">Table&nbsp;14-1</a>. Refer to the <a class="ulink" href="http://flume.apache.org/FlumeUserGuide.html" target="_top">Flume User Guide</a>
    for further information on how to configure and use them.</p><div class="table"><a id="calibre_link-310" class="calibre"></a><div class="table-title">Table&nbsp;14-1.&nbsp;Flume components</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Category</td><td class="calibre20">Component</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td rowspan="10" class="calibre23">Source</td><td class="calibre23">Avro</td><td class="calibre25">Listens on a port for events sent over Avro RPC by an Avro
            sink or the Flume SDK.</td></tr><tr class="calibre26"><td class="calibre23">Exec</td><td class="calibre25">Runs a Unix command (e.g., <code class="uri">tail
            -F</code><em class="replaceable"><code class="calibre47">/path/to/file</code></em>) and converts
            lines read from standard output into events. Note that this source
            cannot guarantee delivery of events to the channel; see the
            spooling directory source or the Flume SDK for better
            alternatives.</td></tr><tr class="calibre19"><td class="calibre23">HTTP</td><td class="calibre25">Listens on a port and converts HTTP requests into events
            using a pluggable handler (e.g., a JSON handler or binary blob
            handler).</td></tr><tr class="calibre26"><td class="calibre23">JMS</td><td class="calibre25">Reads messages from a JMS queue or topic and converts them
            into events.</td></tr><tr class="calibre19"><td class="calibre23">Netcat</td><td class="calibre25">Listens on a port and converts each line of text into an
            event.</td></tr><tr class="calibre26"><td class="calibre23">Sequence generator</td><td class="calibre25">Generates events from an incrementing counter. Useful for
            testing.</td></tr><tr class="calibre19"><td class="calibre23">Spooling directory</td><td class="calibre25">Reads lines from files placed in a spooling directory and
            converts them into events.</td></tr><tr class="calibre26"><td class="calibre23">Syslog</td><td class="calibre25">Reads lines from syslog and converts them into
            events.</td></tr><tr class="calibre19"><td class="calibre23">Thrift</td><td class="calibre25">Listens on a port for events sent over Thrift RPC by a
            Thrift sink or the Flume SDK.</td></tr><tr class="calibre26"><td class="calibre23">Twitter</td><td class="calibre25">Connects to Twitter’s streaming API (1% of the firehose)
            and converts tweets into events.</td></tr><tr class="calibre19"><td rowspan="10" class="calibre23">Sink</td><td class="calibre23">Avro</td><td class="calibre25">Sends events over Avro RPC to an Avro source.</td></tr><tr class="calibre26"><td class="calibre23">Elasticsearch</td><td class="calibre25">Writes events to an Elasticsearch cluster using the
            Logstash format.</td></tr><tr class="calibre19"><td class="calibre23">File roll</td><td class="calibre25">Writes events to the local filesystem.</td></tr><tr class="calibre26"><td class="calibre23">HBase</td><td class="calibre25">Writes events to HBase using a choice of
            serializer.</td></tr><tr class="calibre19"><td class="calibre23">HDFS</td><td class="calibre25">Writes events to HDFS in text, sequence file, Avro, or a
            custom format.</td></tr><tr class="calibre26"><td class="calibre23">IRC</td><td class="calibre25">Sends events to an IRC channel.</td></tr><tr class="calibre19"><td class="calibre23">Logger</td><td class="calibre25">Logs events at <code class="uri">INFO</code>
            level using SLF4J. Useful for testing.</td></tr><tr class="calibre26"><td class="calibre23">Morphline (Solr)</td><td class="calibre25">Runs events through an in-process chain of Morphline
            commands. Typically used to load data into Solr.</td></tr><tr class="calibre19"><td class="calibre23">Null</td><td class="calibre25">Discards all events.</td></tr><tr class="calibre26"><td class="calibre23">Thrift</td><td class="calibre25">Sends events over Thrift RPC to a Thrift source.</td></tr><tr class="calibre19"><td rowspan="3" class="calibre23">Channel</td><td class="calibre23">File</td><td class="calibre25">Stores events in a transaction log stored on the local
            filesystem.</td></tr><tr class="calibre26"><td class="calibre23">JDBC</td><td class="calibre25">Stores events in a database (embedded Derby).</td></tr><tr class="calibre19"><td class="calibre23">Memory</td><td class="calibre25">Stores events in an in-memory queue.</td></tr><tr class="calibre26"><td rowspan="7" class="calibre27">Interceptor</td><td class="calibre23">Host</td><td class="calibre25">Sets a <code class="uri">host</code> header
            containing the agent’s hostname or IP address on all
            events.</td></tr><tr class="calibre19"><td class="calibre23">Morphline</td><td class="calibre25">Filters events through a Morphline configuration file.
            Useful for conditionally dropping events or adding headers based
            on pattern matching or content extraction.</td></tr><tr class="calibre26"><td class="calibre23">Regex extractor</td><td class="calibre25">Sets headers extracted from the event body as text using a
            specified regular expression.</td></tr><tr class="calibre19"><td class="calibre23">Regex filtering</td><td class="calibre25">Includes or excludes events by matching the event body as
            text against a specified regular expression.</td></tr><tr class="calibre26"><td class="calibre23">Static</td><td class="calibre25">Sets a fixed header and value on all events.</td></tr><tr class="calibre19"><td class="calibre23">Timestamp</td><td class="calibre25">Sets a <code class="uri">timestamp</code> header
            containing the time in milliseconds at which the agent processes
            the event.</td></tr><tr class="calibre26"><td class="calibre27">UUID</td><td class="calibre28">Sets an <code class="uri">id</code> header
            containing a universally unique identifier on all events.
            <a class="calibre" id="calibre_link-1709"></a>Useful for later deduplication.</td></tr></tbody></table></div></div></div><div class="book" title="Further Reading"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4439">Further Reading</h2></div></div></div><p class="calibre2">This chapter has given a <a class="calibre" id="calibre_link-1703"></a>short overview of Flume. For more detail, see <span class="calibre"><a class="ulink" href="http://shop.oreilly.com/product/0636920030348.do" target="_top">Using
    Flume</a></span> by Hari Shreedharan (O’Reilly, 2014). There is
    also a lot of practical information about designing ingest pipelines (and
    building Hadoop applications in general) in <span class="calibre"><a class="ulink" href="http://shop.oreilly.com/product/0636920033196.do" target="_top">Hadoop Application
    Architectures</a></span> by Mark Grover, Ted Malaska, Jonathan
    Seidman, and Gwen Shapira (O’Reilly, 2014).</p></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" type="footnote" id="calibre_link-292"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-312">90</a>] </sup>Note that a source has a <code class="literal">channels</code> property (plural) <a class="calibre" id="calibre_link-1089"></a><a class="calibre" id="calibre_link-1087"></a>but a sink has a <code class="literal">channel</code> property (singular). This is because
        a source can feed more than one channel (see <a class="ulink" href="#calibre_link-313" title="Fan Out">Fan Out</a>), but a sink can only be fed by one channel.
        It’s also possible for a channel to feed multiple sinks. This is
        covered in <a class="ulink" href="#calibre_link-314" title="Sink Groups">Sink Groups</a>.</p></div><div class="footnote" type="footnote" id="calibre_link-294"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-315">91</a>] </sup>For a logfile that is continually <a class="calibre" id="calibre_link-2359"></a>appended to, you would periodically roll the logfile and
        move the old file to the spooling directory for Flume to read
        it.</p></div><div class="footnote" type="footnote" id="calibre_link-298"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-316">92</a>] </sup><a class="ulink" href="#calibre_link-310" title="Table&nbsp;14-1.&nbsp;Flume components">Table&nbsp;14-1</a> describes the interceptors
          that Flume provides.</p></div><div class="footnote" type="footnote" id="calibre_link-303"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-317">93</a>] </sup>The Avro sink-source pair is older than the Thrift equivalent,
        and (at the time of writing) has some features that the Thrift one
        doesn’t provide, such as encryption.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-391"><section type="chapter" id="calibre_link-4440" title="Chapter&nbsp;15.&nbsp;Sqoop"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;15.&nbsp;Sqoop</h2></div><div class="book"><div class="author2"><h3 class="author1"><span class="firstname">Aaron</span> <span class="firstname">Kimball</span></h3></div></div></div></div><p class="calibre2">A great strength <a class="calibre" id="calibre_link-2308"></a><a class="calibre" id="calibre_link-3483"></a>of the Hadoop platform is its ability to work with data in
  several different forms. HDFS can reliably store logs and other data from a
  plethora of sources, and MapReduce programs can parse diverse ad hoc data
  formats, extracting relevant information and combining multiple datasets
  into powerful results.</p><p class="calibre2">But to interact with data in storage repositories outside of HDFS, MapReduce programs need
    to use external APIs. Often, valuable data in an organization is stored in structured data
    stores such as relational database management systems (RDBMSs). <a class="ulink" href="http://sqoop.apache.org/" target="_top">Apache Sqoop</a> is an open source
    tool that allows users to extract data from a structured data store into Hadoop for further
    processing. This processing can be done with MapReduce programs or other higher-level tools such
    as Hive. (It’s even possible to use Sqoop to move data from a database into HBase.) When the
    final results of an analytic pipeline are available, Sqoop can export these results back to the
    data store for consumption by other clients.</p><p class="calibre2">In this chapter, we’ll take a look at how Sqoop works and how you can
  use it in your data processing pipeline.</p><div class="book" title="Getting Sqoop"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4441">Getting Sqoop</h2></div></div></div><p class="calibre2">Sqoop is available <a class="calibre" id="calibre_link-3492"></a>in a few places. The primary home of the project is the
    <a class="ulink" href="http://sqoop.apache.org/" target="_top">Apache Software Foundation</a>.
    This repository contains all the Sqoop source code and documentation.
    Official releases are available at this site, as well as the source code
    for the version currently under development. The repository itself
    contains instructions for compiling the project. Alternatively, you can
    get Sqoop from a Hadoop vendor distribution.</p><p class="calibre2">If you download a release from Apache, it will be placed in a
    directory such as <em class="calibre10">/home/<em class="replaceable"><code class="replaceable">yourname</code></em>/sqoop-<em class="replaceable"><code class="replaceable">x.y.z</code></em>/</em>.
    We’ll call this directory <code class="literal">$SQOOP_HOME</code>.
    You can run Sqoop by running the executable script <code class="literal">$SQOOP_HOME/bin/sqoop</code>.</p><p class="calibre2">If you’ve installed a release from a vendor, the package will have
    placed Sqoop’s scripts in a standard location such as <em class="calibre10">/usr/bin/sqoop</em>. You can run Sqoop by simply
    typing <code class="literal">sqoop</code> at the command line.
    (Regardless of how you install Sqoop, we’ll refer to this script as just
    <span class="calibre"><em class="calibre10">sqoop</em></span> from here on.)</p><div class="sidebar"><div class="sidebar-title">Sqoop 2</div><p class="calibre2">Sqoop 2 is a rewrite of Sqoop that addresses the architectural limitations of Sqoop 1.
        For example, Sqoop 1 is a command-line tool and does not provide a Java API, so it’s
        difficult to embed it in other programs. Also, in Sqoop 1 every connector has to know about
        every output format, so it is a lot of work to write new connectors. Sqoop 2 has a server
        component that runs jobs, as well as a range of clients: a command-line interface (CLI), a
        web UI, a REST API, and a Java API. Sqoop 2 also will be able to use alternative execution
        engines, such as Spark. Note that Sqoop 2’s CLI is not compatible with Sqoop 1’s CLI.</p><p class="calibre2">The Sqoop 1 release series is the current stable release series,
      and is what is used in this chapter. Sqoop 2 is under active development
      but does not yet have feature parity with Sqoop 1, so you should check
      that it can support your use case before using it in production.</p></div><p class="calibre2">Running Sqoop with no arguments does not do much of interest:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">sqoop</code></strong>
Try <code class="literal">sqoop help</code> for usage.</pre><p class="calibre2">Sqoop is organized as a set of <a class="calibre" id="calibre_link-3506"></a>tools or commands. If you don’t select a tool, Sqoop does not know what to do.
        <code class="literal">help</code> is the name of one such tool; it can print out the
      list of available tools, like this:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">sqoop help</code></strong>
usage: sqoop COMMAND [ARGS]

Available commands:
  codegen            Generate code to interact with database records
  create-hive-table  Import a table definition into Hive
  eval               Evaluate a SQL statement and display the results
  export             Export an HDFS directory to a database table
  help               List available commands
  import             Import a table from a database to HDFS
  import-all-tables  Import tables from a database to HDFS
  job                Work with saved jobs
  list-databases     List available databases on a server
  list-tables        List available tables in a database
  merge              Merge results of incremental imports
  metastore          Run a standalone Sqoop metastore
  version            Display version information

See 'sqoop help COMMAND' for information on a specific command.</pre><p class="calibre2">As it explains, the <code class="literal">help</code> tool can
    also provide specific usage instructions on a particular tool when you
    provide that tool’s name as an argument:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">sqoop help import</code></strong>
usage: sqoop import [GENERIC-ARGS] [TOOL-ARGS]

Common arguments:
   --connect &lt;jdbc-uri&gt;     Specify JDBC connect string
   --driver &lt;class-name&gt;    Manually specify JDBC driver class to use
   --hadoop-home &lt;dir&gt;      Override $HADOOP_HOME
   --help                   Print usage instructions
-P                          Read password from console
   --password &lt;password&gt;    Set authentication password
   --username &lt;username&gt;    Set authentication username
   --verbose                Print more information while working
...</pre><p class="calibre2">An alternate way of running a Sqoop tool is to use a tool-specific script. This script
      will be named <span class="calibre">sqoop-</span><em class="replaceable"><code class="replaceable">toolname</code></em>
      (e.g., <span class="calibre">sqoop-help</span>, <span class="calibre">sqoop-import</span>, etc.). Running these scripts from the command line is identical to <a class="calibre" id="calibre_link-3493"></a>running <code class="literal">sqoop help</code> or <code class="literal">sqoop import</code>.</p></div><div class="book" title="Sqoop Connectors"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4442">Sqoop Connectors</h2></div></div></div><p class="calibre2">Sqoop has an extension <a class="calibre" id="calibre_link-3486"></a><a class="calibre" id="calibre_link-1245"></a>framework that makes it possible to import data from—and
    export data to—any external storage system that has bulk data transfer
    capabilities. A Sqoop <em class="calibre10">connector</em> is a modular
    component that uses this framework to enable Sqoop imports and exports.
    Sqoop ships with connectors for working with a range of popular databases,
    including MySQL, PostgreSQL, Oracle, SQL Server, DB2, and Netezza. There
    is also a generic JDBC connector for connecting to any database that
    supports Java’s JDBC protocol. Sqoop provides optimized MySQL, PostgreSQL,
    Oracle, and Netezza connectors that use database-specific APIs to perform
    bulk transfers more efficiently (this is discussed more in <a class="ulink" href="#calibre_link-392" title="Direct-Mode Imports">Direct-Mode Imports</a>).</p><p class="calibre2">As well as the built-in Sqoop connectors, various third-party
    connectors are available for data stores, ranging from enterprise data
    warehouses (such as Teradata) to NoSQL stores (such as Couchbase). These
    connectors must be downloaded separately and can be added to an existing
    Sqoop installation by following the instructions that come with the
    connector.</p></div><div class="book" title="A Sample Import"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4443">A Sample Import</h2></div></div></div><p class="calibre2">After you install <a class="calibre" id="calibre_link-3502"></a>Sqoop, you can use it to import data to Hadoop. For the
    examples in this chapter, we’ll use MySQL, which is easy to use and
    available for a large number of platforms.</p><p class="calibre2">To install and <a class="calibre" id="calibre_link-2737"></a>configure MySQL, follow the <a class="ulink" href="http://dev.mysql.com/doc" target="_top">online
        documentation</a>. Chapter 2 (“Installing and Upgrading MySQL”) in particular should
      help. Users of Debian-based Linux systems (e.g., Ubuntu) can type <code class="literal">sudo apt-get install mysql-client mysql-server</code>. Red Hat users can type <code class="literal">sudo yum install mysql mysql-server</code>.</p><p class="calibre2">Now that MySQL is installed, let’s log in and create a <a class="calibre" id="calibre_link-3289"></a><a class="calibre" id="calibre_link-2734"></a>database (<a class="ulink" href="#calibre_link-393" title="Example&nbsp;15-1.&nbsp;Creating a new MySQL database schema">Example&nbsp;15-1</a>).</p><div class="example"><a id="calibre_link-393" class="calibre"></a><div class="example-title">Example&nbsp;15-1.&nbsp;Creating a new MySQL database schema</div><div class="book"><pre class="screen"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">mysql -u root -p</code></strong>
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 235
Server version: 5.6.21 MySQL Community Server (GPL)

Type 'help;' or '\h' for help. Type '\c' to clear the current input
statement.

<code class="literal">mysql&gt;</code> <strong class="userinput"><code class="calibre9">CREATE DATABASE hadoopguide;</code></strong>
Query OK, 1 row affected (0.00 sec)

<code class="literal">mysql&gt;</code> <strong class="userinput"><code class="calibre9">GRANT ALL PRIVILEGES ON hadoopguide.* TO ''@'localhost';</code></strong>
Query OK, 0 rows affected (0.00 sec)

<code class="literal">mysql&gt;</code> <strong class="userinput"><code class="calibre9">quit;</code></strong>
Bye</pre></div></div><p class="calibre2">The password prompt shown in this example asks for your root user
    password. This is likely the same as the password for the root shell
    login. If you are running Ubuntu or another variant of Linux where root
    cannot log in directly, enter the password you picked at MySQL
    installation time. (If you didn’t set a password, then just press
    Return.)</p><p class="calibre2">In this session, we created a new database schema called <code class="literal">hadoopguide</code>, which we’ll use throughout this
    chapter. We then allowed any local user to view and modify the contents of
    the <code class="literal">hadoopguide</code> schema, and closed our
    session.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-394" id="calibre_link-407">94</a>]</sup></p><p class="calibre2">Now let’s log back into the <a class="calibre" id="calibre_link-2738"></a>database (do this as yourself this time, not as root) and create a table to import
      into HDFS (<a class="ulink" href="#calibre_link-395" title="Example&nbsp;15-2.&nbsp;Populating the database">Example&nbsp;15-2</a>).</p><div class="example"><a id="calibre_link-395" class="calibre"></a><div class="example-title">Example&nbsp;15-2.&nbsp;Populating the database</div><div class="book"><pre class="screen"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">mysql hadoopguide</code></strong>
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 257
Server version: 5.6.21 MySQL Community Server (GPL)

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

<code class="literal">mysql&gt;</code> <strong class="userinput"><code class="calibre9">CREATE TABLE widgets(id INT NOT NULL PRIMARY KEY AUTO_INCREMENT,</code></strong>
<code class="literal">    -&gt;</code> <strong class="userinput"><code class="calibre9">widget_name VARCHAR(64) NOT NULL,</code></strong>
<code class="literal">    -&gt;</code> <strong class="userinput"><code class="calibre9">price DECIMAL(10,2),</code></strong>
<code class="literal">    -&gt;</code> <strong class="userinput"><code class="calibre9">design_date DATE,</code></strong>
<code class="literal">    -&gt;</code> <strong class="userinput"><code class="calibre9">version INT,</code></strong>
<code class="literal">    -&gt;</code> <strong class="userinput"><code class="calibre9">design_comment VARCHAR(100));</code></strong>
Query OK, 0 rows affected (0.00 sec)

<code class="literal">mysql&gt;</code> <strong class="userinput"><code class="calibre9">INSERT INTO widgets VALUES (NULL, 'sprocket', 0.25, '2010-02-10',</code></strong>
<code class="literal">    -&gt;</code> <strong class="userinput"><code class="calibre9">1, 'Connects two gizmos');</code></strong>
Query OK, 1 row affected (0.00 sec)

<code class="literal">mysql&gt;</code> <strong class="userinput"><code class="calibre9">INSERT INTO widgets VALUES (NULL, 'gizmo', 4.00, '2009-11-30', 4,</code></strong>
<code class="literal">    -&gt;</code> <strong class="userinput"><code class="calibre9">NULL);</code></strong>
Query OK, 1 row affected (0.00 sec)

<code class="literal">mysql&gt;</code> <strong class="userinput"><code class="calibre9">INSERT INTO widgets VALUES (NULL, 'gadget', 99.99, '1983-08-13',</code></strong>
<code class="literal">    -&gt;</code> <strong class="userinput"><code class="calibre9">13, 'Our flagship product');</code></strong> 
Query OK, 1 row affected (0.00 sec)

<code class="literal">mysql&gt;</code> <strong class="userinput"><code class="calibre9">quit;</code></strong></pre></div></div><p class="calibre2">In this listing, we created a new table called <code class="literal">widgets</code>. We’ll be using this fictional product
    database in further examples in this chapter. The <code class="literal">widgets</code> table contains several fields
    representing a variety of data types.</p><p class="calibre2">Before going any further, you need to download the JDBC driver JAR
    file for MySQL (Connector/J) and add it to Sqoop’s classpath, which is
    simply achieved by placing it in Sqoop’s <em class="calibre10">lib</em> directory.</p><p class="calibre2">Now let’s use Sqoop to import this table into HDFS:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">sqoop import --connect jdbc:mysql://localhost/hadoopguide \</code></strong>
<code class="literal">&gt;</code> <strong class="userinput"><code class="calibre9">--table widgets -m 1</code></strong>
...
14/10/28 21:36:23 INFO tool.CodeGenTool: Beginning code generation
...
14/10/28 21:36:28 INFO mapreduce.Job: Running job: job_1413746845532_0008
14/10/28 21:36:35 INFO mapreduce.Job: Job job_1413746845532_0008 running in
uber mode : false
14/10/28 21:36:35 INFO mapreduce.Job:  map 0% reduce 0%
14/10/28 21:36:41 INFO mapreduce.Job:  map 100% reduce 0%
14/10/28 21:36:41 INFO mapreduce.Job: Job job_1413746845532_0008 completed
successfully
...
14/10/28 21:36:41 INFO mapreduce.ImportJobBase: Retrieved 3 records.</pre><p class="calibre2">Sqoop’s <code class="literal">import</code> tool <a class="calibre" id="calibre_link-2092"></a>will run a <a class="calibre" id="calibre_link-2506"></a><a class="calibre" id="calibre_link-3498"></a>MapReduce job that connects to the MySQL database and reads
    the table. By default, this will use four map tasks in parallel to speed
    up the import process. Each task will write its imported results to a
    different file, but all in a common directory. Because we knew that we had
    only three rows to import in this example, we specified that Sqoop should
    use a single map task (<code class="literal">-m 1</code>) so we get
    a single file in HDFS.</p><p class="calibre2">We can inspect this file’s contents like so:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -cat widgets/part-m-00000</code></strong>
1,sprocket,0.25,2010-02-10,1,Connects two gizmos
2,gizmo,4.00,2009-11-30,4,null
3,gadget,99.99,1983-08-13,13,Our flagship product</pre><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">The connect string (<code class="literal">jdbc:mysql://localhost/hadoopguide</code>)
      shown in the example will read from a database on the local machine. If
      a distributed Hadoop cluster is being used, <code class="literal">localhost</code> should not be specified in the
      connect string, because map tasks not running on the same machine as the
      database will fail to connect. Even if Sqoop is run from the same host
      as the database sever, the full hostname should be specified.</p></div><p class="calibre2">By default, Sqoop will generate comma-delimited text files for our imported data.
      Delimiters can be specified explicitly, as well as field enclosing and escape characters, to
      allow the presence of delimiters in the field contents. The command-line arguments that
      specify delimiter characters, file formats, compression, and more fine-grained control of the
      import process are described in the Sqoop User Guide distributed with Sqoop,<sup class="calibre6">[<a class="firstname" href="#calibre_link-396" id="calibre_link-408">95</a>]</sup> as well as in the online help (<code class="literal">sqoop help
        import</code>, or <code class="literal">man sqoop-import</code> in CDH).</p><div class="book" title="Text and Binary File Formats"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4444">Text and Binary File Formats</h3></div></div></div><p class="calibre2">Sqoop is <a class="calibre" id="calibre_link-3490"></a>capable of importing into a few different file formats. Text files (the default)
        offer a human-readable representation of data, platform independence, and the simplest
        structure. However, they cannot hold binary fields (such as database columns of type
          <code class="literal">VARBINARY</code>), and distinguishing between <code class="literal">null</code> values and <code class="literal">String</code>-based fields containing the value
          <code class="literal">"null"</code> can be problematic (although using the <code class="literal">--null-string</code> import option allows you to control the
        representation of <code class="literal">null</code> values).</p><p class="calibre2">To handle these conditions, Sqoop also <a class="calibre" id="calibre_link-3338"></a><a class="calibre" id="calibre_link-959"></a><a class="calibre" id="calibre_link-2924"></a><a class="calibre" id="calibre_link-3485"></a><a class="calibre" id="calibre_link-3504"></a><a class="calibre" id="calibre_link-3501"></a>supports <code class="literal">SequenceFile</code>s, Avro datafiles, and Parquet
      files. These binary formats provide the most precise representation
      possible of the imported data. They also allow data to be compressed
      while retaining MapReduce’s ability to process different sections of the
      same file in parallel. However, current versions of Sqoop cannot load
      Avro datafiles or <code class="literal">SequenceFile</code>s into Hive (although you can load<a class="calibre" id="calibre_link-2000"></a><a class="calibre" id="calibre_link-944"></a> Avro into Hive manually, and <a class="calibre" id="calibre_link-2918"></a><a class="calibre" id="calibre_link-2013"></a>Parquet can be loaded directly into Hive by Sqoop).
        Another disadvantage of <code class="literal">SequenceFile</code>s is that they are Java specific,
      whereas Avro and Parquet files can be processed by a wide range of
      <a class="calibre" id="calibre_link-3503"></a>languages.</p></div></div><div class="book" title="Generated Code"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4445">Generated Code</h2></div></div></div><p class="calibre2">In addition to <a class="calibre" id="calibre_link-3491"></a>writing the contents of the database table to HDFS, Sqoop
    also provides you with a generated Java source file (<em class="calibre10">widgets.java</em>) written to the current local
    directory. (After running the <code class="literal">sqoop
    import</code> command shown earlier, you can see this file by running
    <code class="literal">ls widgets.java</code>.)</p><p class="calibre2">As you’ll learn in <a class="ulink" href="#calibre_link-397" title="Imports: A Deeper Look">Imports: A Deeper Look</a>, Sqoop can
    use generated code to handle the deserialization of table-specific data
    from the database source before writing it to HDFS.</p><p class="calibre2">The generated class (<code class="literal">widgets</code>) is capable of
    holding a single record retrieved from the imported table. It can
    manipulate such a record in MapReduce or store it in a <code class="literal">SequenceFile</code> in
      HDFS. (<code class="literal">SequenceFile</code>s written by Sqoop during the import process will store
      each imported row in the “value” element of the <code class="literal">SequenceFile</code>’s key-value
    pair format, using the generated class.)</p><p class="calibre2">It is likely that you don’t want to name your generated class
        <code class="literal">widgets</code>, since each instance of the class refers to only a single
      record. We can use a different Sqoop tool to generate source code without performing an
      import; this generated code will still examine the database table to determine the appropriate
      data types for each field:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">sqoop codegen --connect jdbc:mysql://localhost/hadoopguide \</code></strong>
<code class="literal">&gt;</code> <strong class="userinput"><code class="calibre9">--table widgets --class-name Widget</code></strong></pre><p class="calibre2">The <code class="literal">codegen</code> tool <a class="calibre" id="calibre_link-1164"></a><a class="calibre" id="calibre_link-3507"></a>simply generates code; it does not perform the full import.
    We specified that we’d like it to generate a class named
    <code class="literal">Widget</code>; this will be written to <em class="calibre10">Widget.java</em>. We also could have
    specified <code class="literal">--class-name</code> and other
    code-generation arguments during the import process we performed earlier.
    This tool can be used to regenerate code if you accidentally remove the
    source file, or generate code with different settings than were used
    during the import.</p><p class="calibre2">If you’re working with records imported to <code class="literal">SequenceFile</code>s, it is
    inevitable that you’ll need to use the generated classes (to deserialize
    data from the <code class="literal">SequenceFile</code> storage). You can work with text-file-based
    records without using generated code, but as we’ll see in <a class="ulink" href="#calibre_link-398" title="Working with Imported Data">Working with Imported Data</a>, Sqoop’s generated code can handle some
    tedious aspects of data processing for you.</p><div class="book" title="Additional Serialization Systems"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4446">Additional Serialization Systems</h3></div></div></div><p class="calibre2">Recent versions of <a class="calibre" id="calibre_link-3365"></a><a class="calibre" id="calibre_link-3505"></a>Sqoop support Avro-based serialization and schema
      generation as well (see <a class="ulink" href="#calibre_link-133" title="Chapter&nbsp;12.&nbsp;Avro">Chapter&nbsp;12</a>), allowing you to
      use Sqoop in your project without integrating with generated
      code.</p></div></div><div class="book" title="Imports: A Deeper Look"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-397">Imports: A Deeper Look</h2></div></div></div><p class="calibre2">As mentioned earlier, <a class="calibre" id="calibre_link-3494"></a><a class="calibre" id="calibre_link-2090"></a>Sqoop imports a table from a database by running a MapReduce
    job that extracts rows from the table, and writes the records to HDFS. How
    does <a class="calibre" id="calibre_link-2507"></a><a class="calibre" id="calibre_link-3499"></a>MapReduce read the rows? This section explains how Sqoop
    works under the hood.</p><p class="calibre2">At a high level, <a class="ulink" href="#calibre_link-399" title="Figure&nbsp;15-1.&nbsp;Sqoop’s import process">Figure&nbsp;15-1</a> demonstrates how Sqoop interacts
      with both the database source and Hadoop. Like Hadoop itself, Sqoop is written in Java. Java
      provides an API called <a class="calibre" id="calibre_link-2158"></a><a class="calibre" id="calibre_link-2203"></a>Java Database Connectivity, or JDBC, that allows applications to access data
      stored in an RDBMS as well as to inspect the nature of this data. Most database vendors
      provide a JDBC <em class="calibre10">driver</em> that implements the JDBC API and contains the
      necessary code to connect to their database servers.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Based on the URL in the connect string used to access the
      database, Sqoop attempts to predict which driver it should load. You
      still need to download the JDBC driver itself and install it on your
      Sqoop client. For cases where Sqoop does not know which JDBC driver is
      appropriate, users can specify the JDBC driver explicitly with the
        <code class="literal">--driver</code> argument. This capability
      allows Sqoop to work with a wide variety of database platforms.</p></div><p class="calibre2">Before the import can start, Sqoop uses JDBC to examine the table it
    is to import. It retrieves a list of all the columns and their SQL data
    types. These SQL types (<code class="literal">VARCHAR</code>,
    <code class="literal">INTEGER</code>, etc.) can then be mapped to
    Java data types (<code class="literal">String</code>,
    <code class="literal">Integer</code>, etc.), which will hold the field values in
    MapReduce applications. Sqoop’s code generator will use this information
    to create a table-specific class to hold a record extracted from the
    table.</p><div class="book"><div class="figure"><a id="calibre_link-399" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4447" class="calibre"></a><img alt="Sqoop’s import process" src="images/000035.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;15-1.&nbsp;Sqoop’s import process</div></div></div><p class="calibre2">The <code class="literal">Widget</code> class from earlier, for example,
    contains the following methods that retrieve each column from an extracted
    record:</p><a id="calibre_link-4448" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="n">Integer</code> <code class="nf">get_id</code><code class="o">();</code>
<code class="k">public</code> <code class="n">String</code> <code class="nf">get_widget_name</code><code class="o">();</code>
<code class="k">public</code> <code class="n">java</code><code class="o">.</code><code class="na">math</code><code class="o">.</code><code class="na">BigDecimal</code> <code class="nf">get_price</code><code class="o">();</code>
<code class="k">public</code> <code class="n">java</code><code class="o">.</code><code class="na">sql</code><code class="o">.</code><code class="na">Date</code> <code class="nf">get_design_date</code><code class="o">();</code>
<code class="k">public</code> <code class="n">Integer</code> <code class="nf">get_version</code><code class="o">();</code>
<code class="k">public</code> <code class="n">String</code> <code class="nf">get_design_comment</code><code class="o">();</code></pre><p class="calibre2">More critical to the import system’s operation, though, are the
    serialization methods that form <a class="calibre" id="calibre_link-1400"></a>the <code class="literal">DBWritable</code> interface, which allow
    the <code class="literal">Widget</code> class to interact with JDBC:</p><a id="calibre_link-4449" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="kt">void</code> <code class="nf">readFields</code><code class="o">(</code><code class="n">ResultSet</code> <code class="n">__dbResults</code><code class="o">)</code> <code class="k">throws</code> <code class="n">SQLException</code><code class="o">;</code>
<code class="k">public</code> <code class="kt">void</code> <code class="nf">write</code><code class="o">(</code><code class="n">PreparedStatement</code> <code class="n">__dbStmt</code><code class="o">)</code> <code class="k">throws</code> <code class="n">SQLException</code><code class="o">;</code></pre><p class="calibre2">JDBC’s <code class="literal">ResultSet</code> interface <a class="calibre" id="calibre_link-3240"></a>provides a cursor that retrieves records from a query; the
    <code class="literal">readFields()</code> method here will populate the fields
    of the <code class="literal">Widget</code> object with the columns from one row
    of the <code class="literal">ResultSet</code>’s data. The
    <code class="literal">write()</code> method shown here allows Sqoop to insert
    new <code class="literal">Widget</code> rows into a table, a process called
    <em class="calibre10">exporting</em>. Exports are discussed in <a class="ulink" href="#calibre_link-400" title="Performing an Export">Performing an Export</a>.</p><p class="calibre2">The MapReduce job launched by Sqoop uses an
    <code class="literal">InputFormat</code> that <a class="calibre" id="calibre_link-2123"></a>can read sections of a table from a database via JDBC. The
    <code class="literal">DataDrivenDBInputFormat</code> provided <a class="calibre" id="calibre_link-1369"></a>with Hadoop partitions a query’s results over several map
    tasks.</p><p class="calibre2">Reading a table is typically done with a simple query such
    as:</p><a id="calibre_link-4450" class="calibre"></a><pre class="screen1">SELECT <em class="replaceable"><code class="replaceable">col1</code></em>,<em class="replaceable"><code class="replaceable">col2</code></em>,<em class="replaceable"><code class="replaceable">col3</code></em>,... FROM <em class="replaceable"><code class="replaceable">tableName</code></em></pre><p class="calibre2">But often, better import performance can be gained by dividing this
    query across multiple nodes. This is done using a <em class="calibre10">splitting
    column</em>. Using metadata about the table, Sqoop will guess a
    good column to use for splitting the table (typically the primary key for
    the table, if one exists). The minimum and maximum values for the primary
    key column are retrieved, and then these are used in conjunction with a
    target number of tasks to determine the queries that each map task should
    issue.</p><p class="calibre2">For example, suppose the <code class="literal">widgets</code> table had 100,000
      entries, with the <code class="literal">id</code> column containing values 0 through
      99,999. When importing this table, Sqoop would determine that <code class="literal">id</code> is the primary key column for the table. When starting the MapReduce job, the
        <code class="literal">DataDrivenDBInputFormat</code> used to perform the import would issue a
      statement such as <code class="literal">SELECT MIN(id), MAX(id) FROM widgets</code>.
      These values would then be used to interpolate over the entire range of data. Assuming we
      specified that five map tasks should run in parallel (with <strong class="userinput"><code class="calibre9">-m
        5</code></strong>), this would result in each map task executing queries such as <code class="literal">SELECT id, widget_name, ... FROM widgets WHERE id &gt;= 0 AND id &lt;
        20000</code>, <code class="literal">SELECT id, widget_name, ... FROM widgets WHERE id
        &gt;= 20000 AND id &lt; 40000</code>, and so on.</p><p class="calibre2">The choice of splitting column is essential to parallelizing work efficiently. If the
        <code class="literal">id</code> column were not uniformly distributed (perhaps there
      are no widgets with IDs between 50,000 and 75,000), then some map tasks might have little or
      no work to perform, whereas others would have a great deal. Users can specify a particular
      splitting column when running an import job (via the <code class="literal">--split-by</code> argument), to tune the job to the data’s actual distribution. If an
      import job is run as a single (sequential) task with <strong class="userinput"><code class="calibre9">-m
        1</code></strong>, this split process is not performed.</p><p class="calibre2">After generating the deserialization code and configuring the
    <code class="literal">InputFormat</code>, Sqoop sends the job to the MapReduce
    cluster. Map tasks execute the queries and deserialize rows from the
    <code class="literal">ResultSet</code> into instances of the generated class,
      which are either stored directly in <code class="literal">SequenceFile</code>s or transformed into
    delimited text before being written to <a class="calibre" id="calibre_link-2091"></a>HDFS.</p><div class="book" title="Controlling the Import"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4451">Controlling the Import</h3></div></div></div><p class="calibre2">Sqoop does not need to <a class="calibre" id="calibre_link-2083"></a>import an entire table at a time. For example, a subset of
      the table’s columns can be specified for import. Users can also specify
      a <code class="literal">WHERE</code> clause to include in queries
      via the <code class="literal">--where</code> argument, which
      bounds the rows of the table to import. For example, if widgets 0
      through 99,999 were imported last month, but this month our vendor
      catalog included 1,000 new types
      of widget, an import could be configured with the clause <code class="literal">WHERE id &gt;= 100000</code>; this will start an
      import job to retrieve all the new rows added to the source database
      since the previous import run. User-supplied <code class="literal">WHERE</code> clauses are applied before task
      splitting is performed, and are pushed down into the queries executed by
      each task.</p><p class="calibre2">For more control—to perform column transformations, for example—users can specify a
          <code class="literal">--query</code> argument.</p></div><div class="book" title="Imports and Consistency"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4452">Imports and Consistency</h3></div></div></div><p class="calibre2">When importing data to HDFS, it is <a class="calibre" id="calibre_link-2082"></a>important that you ensure access to a consistent snapshot of the source data.
        (Map tasks reading from a database in parallel are running in separate processes. Thus, they
        cannot share a single database transaction.) The best way to do this is to ensure that any
        processes that update existing rows of a table are disabled during the import.</p></div><div class="book" title="Incremental Imports"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4453">Incremental Imports</h3></div></div></div><p class="calibre2">It’s common to run imports on a periodic <a class="calibre" id="calibre_link-2089"></a>basis so that the data in HDFS is kept synchronized with
      the data stored in the database. To do this, there needs to be some way
      of identifying the new data. Sqoop will import rows that have a column
      value (for the column specified with <code class="literal">--check-column</code>) that is greater than some
      specified value (set via <code class="literal">--last-value</code>).</p><p class="calibre2">The value specified as <code class="literal">--last-value</code> can be a row ID that is strictly
      increasing, such as an <code class="literal">AUTO_INCREMENT</code>
      primary key in MySQL. This is suitable for the case where new rows are
      added to the database table, but existing rows are not updated. This
      mode is called <span class="calibre"><em class="calibre10">append</em></span> mode, and is activated via
      <code class="literal">--incremental append</code>. Another option
      is time-based incremental imports (specified by <code class="literal">--incremental lastmodified</code>), which is
      appropriate when existing rows may be updated, and there is a column
      (the check column) that records the last modified time of the
      update.</p><p class="calibre2">At the end of an incremental import, Sqoop will print out the
      value to be specified as <code class="literal">--last-value</code> on the next import. This
      is useful when running incremental imports manually, but for running
      periodic imports it is better to use Sqoop’s saved job facility, which
      automatically stores the last value and uses it on the next job run.
      Type <code class="literal">sqoop job
      --help</code> for usage instructions for saved jobs.</p></div><div class="book" title="Direct-Mode Imports"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-392">Direct-Mode Imports</h3></div></div></div><p class="calibre2">Sqoop’s architecture allows it to <a class="calibre" id="calibre_link-2084"></a><a class="calibre" id="calibre_link-1495"></a>choose from multiple available strategies for performing an import. Most
        databases will use the <code class="literal">DataDrivenDBInputFormat</code>-based approach
        described earlier. Some databases, however, offer specific tools designed to extract data
        quickly. For example, MySQL’s <code class="literal">mysqldump</code> application can
        read from a table with greater throughput than a JDBC channel. The use of these external
        tools is referred to as <em class="calibre10">direct mode</em> in Sqoop’s documentation. Direct
        mode must be specifically enabled by the user (via the <code class="literal">--direct</code> argument), as it is not as general purpose as the JDBC approach. (For
        example, MySQL’s direct mode cannot handle large objects, such as <code class="literal">CLOB</code> or <code class="literal">BLOB</code> columns, and that’s why Sqoop
        needs to use a JDBC-specific API to load these columns into HDFS.)</p><p class="calibre2">For databases that provide such tools, Sqoop can use these to
      great effect. A direct-mode import from MySQL is usually much more
      efficient (in terms of map tasks and time required) than a comparable
      JDBC-based import. Sqoop will still launch multiple map tasks in
      parallel. These tasks will then spawn instances of the <code class="literal">mysqldump</code> program and read its output. Sqoop
      can also perform direct-mode imports from PostgreSQL, Oracle, and
      Netezza.</p><p class="calibre2">Even when direct mode is used to access the contents of a
      database, the metadata is still queried <a class="calibre" id="calibre_link-3495"></a>through JDBC.</p></div></div><div class="book" title="Working with Imported Data"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-398">Working with Imported Data</h2></div></div></div><p class="calibre2">Once data has been <a class="calibre" id="calibre_link-3508"></a><a class="calibre" id="calibre_link-2093"></a>imported to HDFS, it is ready for processing by custom MapReduce programs.
      Text-based imports can easily be used in scripts run with Hadoop Streaming or in MapReduce
      jobs run with the <a class="calibre" id="calibre_link-3678"></a>default <code class="literal">TextInputFormat</code>.</p><p class="calibre2">To use individual fields of an imported record, though, the field delimiters (and any
      escape/enclosing characters) must be parsed and the field values extracted and converted to
      the appropriate data types. For example, the ID of the “sprocket” widget is represented as the
      string <code class="literal">"1"</code> in the text file, but should be parsed into an
        <code class="literal">Integer</code> or <code class="literal">int</code> variable in Java.
      The generated table class provided by Sqoop can automate this process, allowing you to focus
      on the actual MapReduce job to run. Each autogenerated
      class has several overloaded methods named <code class="literal">parse()</code> that operate on
      the data represented as <code class="literal">Text</code>, <code class="literal">CharSequence</code>,
        <code class="literal">char[]</code>, or other common types.</p><p class="calibre2">The MapReduce application called <code class="literal">MaxWidgetId</code>
    (available in the example code) will find the widget with the highest
    ID. The class can be compiled into a JAR file along with <em class="calibre10">Widget.java</em> using the Maven POM that comes with
    the example code. The JAR file is called <em class="calibre10">sqoop-examples.jar</em>, and is executed like
    so:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">HADOOP_CLASSPATH=$SQOOP_HOME/sqoop-<em class="replaceable1"><code class="calibre46">version</code></em>.jar hadoop jar \</code></strong>
<code class="literal">&gt;</code> <strong class="userinput"><code class="calibre9">sqoop-examples.jar MaxWidgetId -libjars $SQOOP_HOME/sqoop-<em class="replaceable1"><code class="calibre46">version</code></em>.jar</code></strong></pre><p class="calibre2">This command line ensures that Sqoop is on the classpath locally
    (via <code class="literal">$HADOOP_CLASSPATH</code>) when running
    the <code class="literal">MaxWidgetId.run()</code> method, as well as when map
    tasks are running on the cluster (via the <code class="literal">-libjars</code> argument).</p><p class="calibre2">When run, the <em class="calibre10">maxwidget</em> path in
    HDFS will contain a file named <em class="calibre10">part-r-00000</em> with the following expected
    result:</p><pre class="screen1">3,gadget,99.99,1983-08-13,13,Our flagship product</pre><p class="calibre2">It is worth noting that in this example MapReduce program, a
    <code class="literal">Widget</code> object was emitted from the mapper to the reducer; the
    autogenerated <code class="literal">Widget</code> class implements the <code class="literal">Writable</code> interface
    provided by Hadoop, which allows the object to be sent via Hadoop’s
    serialization mechanism, as well as written to and read from
      <code class="literal">SequenceFile</code>s.</p><p class="calibre2">The <code class="literal">MaxWidgetId</code> example is built on the new
    MapReduce API. MapReduce applications that rely on Sqoop-generated code
    can be built on the new or old APIs, though some advanced features (such
    as working with large objects) are more convenient to use in the new
    API.</p><p class="calibre2">Avro-based imports can be processed using the APIs described in
    <a class="ulink" href="#calibre_link-401" title="Avro MapReduce">Avro MapReduce</a>. With the Generic Avro mapping, the
    MapReduce program does not need to use schema-specific generated code
    (although this is an option too, by using Avro’s Specific compiler; Sqoop
    does not do the code generation in this case). The example code includes a
    program called <code class="literal">MaxWidgetIdGenericAvro</code>, which finds
    the widget with the highest ID and writes out the result in an Avro
    <a class="calibre" id="calibre_link-2094"></a>datafile.</p><div class="book" title="Imported Data and Hive"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-649">Imported Data and Hive</h3></div></div></div><p class="calibre2">As we’ll see in <a class="ulink" href="#calibre_link-402" title="Chapter&nbsp;17.&nbsp;Hive">Chapter&nbsp;17</a>, for <a class="calibre" id="calibre_link-2021"></a><a class="calibre" id="calibre_link-2085"></a>many types of analysis, using a system such as Hive to
      handle relational operations can dramatically ease the development of
      the analytic pipeline. Especially for data originally from a relational
      data source, using Hive makes a lot of sense. Hive and Sqoop together
      form a powerful toolchain for performing analysis.</p><p class="calibre2">Suppose we had another log of data in our system, coming from a
      web-based widget purchasing system. This might return logfiles containing
      a widget ID, a quantity, a shipping address, and an order date.</p><p class="calibre2">Here is a snippet from an example log of this type:</p><pre class="screen1">1,15,120 Any St.,Los Angeles,CA,90210,2010-08-01
3,4,120 Any St.,Los Angeles,CA,90210,2010-08-01
2,5,400 Some Pl.,Cupertino,CA,95014,2010-07-30
2,7,88 Mile Rd.,Manhattan,NY,10005,2010-07-18</pre><p class="calibre2">By using Hadoop to analyze this purchase log, we can gain insight
      into our sales operation. By combining this data with the data extracted
      from our relational data source (the <code class="literal">widgets</code> table), we can do better. In this
      example session, we will compute which zip code is responsible for the
      most sales dollars, so we can better focus our sales team’s operations.
      Doing this requires data from both the sales log and the <code class="literal">widgets</code> table.</p><p class="calibre2">The table shown in the previous code snippet should be in a local
      file named <em class="calibre10">sales.log</em> for this to
      work.</p><p class="calibre2">First, let’s load the sales data into Hive:</p><pre class="screen1"><code class="literal">hive&gt;</code> <strong class="userinput"><code class="calibre9">CREATE TABLE sales(widget_id INT, qty INT,</code></strong>
<code class="literal">    &gt;</code> <strong class="userinput"><code class="calibre9">street STRING, city STRING, state STRING,</code></strong>
<code class="literal">    &gt;</code> <strong class="userinput"><code class="calibre9">zip INT, sale_date STRING)</code></strong>
<code class="literal">    &gt;</code> <strong class="userinput"><code class="calibre9">ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';</code></strong>
OK
Time taken: 5.248 seconds
<code class="literal">hive&gt;</code> <strong class="userinput"><code class="calibre9">LOAD DATA LOCAL INPATH "ch15-sqoop/sales.log" INTO TABLE sales;</code></strong>
...
Loading data to table default.sales
Table default.sales stats: [numFiles=1, numRows=0, totalSize=189, rawDataSize=0]
OK
Time taken: 0.6 seconds</pre><p class="calibre2">Sqoop can generate a Hive table based on a table from an existing
      relational data source. We’ve already imported the <code class="literal">widgets</code> data to HDFS, so we can generate the
      Hive table definition and then load in the HDFS-resident data:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">sqoop create-hive-table --connect jdbc:mysql://localhost/hadoopguide \</code></strong>
<code class="literal">&gt;</code> <strong class="userinput"><code class="calibre9">--table widgets --fields-terminated-by ','</code></strong>
...
14/10/29 11:54:52 INFO hive.HiveImport: OK
14/10/29 11:54:52 INFO hive.HiveImport: Time taken: 1.098 seconds
14/10/29 11:54:52 INFO hive.HiveImport: Hive import complete.
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">hive</code></strong>
<code class="literal">hive&gt;</code> <strong class="userinput"><code class="calibre9">LOAD DATA INPATH "widgets" INTO TABLE widgets;</code></strong>
Loading data to table widgets
OK
Time taken: 3.265 seconds</pre><p class="calibre2">When creating a Hive table definition with a specific
      already imported dataset in mind, we need to specify the delimiters used
      in that dataset. Otherwise, Sqoop will allow Hive to use its default
      delimiters (which are different from Sqoop’s default delimiters).</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Hive’s type system is less rich than that of most SQL systems.
        Many SQL types do not have direct analogues in Hive. When Sqoop
        generates a Hive table definition for an import, it uses the best Hive
        type available to hold a column’s values. This may result in a
        decrease in precision. When this occurs, Sqoop will provide you with a
        warning message such as this one:</p><pre class="screen2">14/10/29 11:54:43 WARN hive.TableDefWriter: 
Column design_date had to be
cast to a less precise type in Hive</pre></div><p class="calibre2">This three-step process of importing data to HDFS, creating the Hive table, and then
        loading the HDFS-resident data into Hive can be shortened to one step if you know that you
        want to import straight from a database directly into Hive. During an import, Sqoop can
        generate the Hive table definition and then load in the data. Had we not already performed
        the import, we could have executed this command, which creates the <code class="literal">widgets</code> table in Hive
        based on the copy in MySQL:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">sqoop import --connect jdbc:mysql://localhost/hadoopguide \</code></strong>
<code class="literal">&gt;</code> <strong class="userinput"><code class="calibre9">--table widgets -m 1 --hive-import</code></strong></pre><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Running <code class="literal">sqoop import</code> with the <code class="literal">--hive-import</code> argument will load the data directly from the
          source database into Hive; it infers a Hive schema automatically based on the schema for
          the table in the source database. Using this, you can get started working with your data
          in Hive with only one command.</p></div><p class="calibre2">Regardless of which data import route we chose, we can now use the
      <code class="literal">widgets</code> dataset and the <code class="literal">sales</code> dataset together to calculate the most
      profitable zip code. Let’s do so, and also save the result of this query
      in another table for <a class="calibre" id="calibre_link-3509"></a><a class="calibre" id="calibre_link-2022"></a><a class="calibre" id="calibre_link-2086"></a>later:</p><pre class="screen1"><code class="literal">hive&gt;</code> <strong class="userinput"><code class="calibre9">CREATE TABLE zip_profits</code></strong>
<code class="literal">    &gt;</code> <strong class="userinput"><code class="calibre9">AS</code></strong>
<code class="literal">    &gt;</code> <strong class="userinput"><code class="calibre9">SELECT SUM(w.price * s.qty) AS sales_vol, s.zip  FROM SALES s</code></strong>
<code class="literal">    &gt;</code> <strong class="userinput"><code class="calibre9">JOIN widgets w ON (s.widget_id = w.id) GROUP BY s.zip;</code></strong>
...
Moving data to: hdfs://localhost/user/hive/warehouse/zip_profits
...
OK

<code class="literal">hive&gt;</code> <strong class="userinput"><code class="calibre9">SELECT * FROM zip_profits ORDER BY sales_vol DESC;</code></strong>
...
OK
403.71  90210
28.0    10005
20.0    95014</pre></div></div><div class="book" title="Importing Large Objects"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-639">Importing Large Objects</h2></div></div></div><p class="calibre2">Most databases <a class="calibre" id="calibre_link-3496"></a><a class="calibre" id="calibre_link-2087"></a>provide the capability to store large amounts of data in a
    single field. Depending on whether this data is textual or binary in
    nature, it is usually represented as a <code class="literal">CLOB</code> or <code class="literal">BLOB</code>
    column in the table. These “large objects” are often handled specially by
    the database itself. In particular, most tables are physically laid out on
    disk as in <a class="ulink" href="#calibre_link-403" title="Figure&nbsp;15-2.&nbsp;Database tables are typically physically represented as an array of rows, with all the columns in a row stored adjacent to one another">Figure&nbsp;15-2</a>. When scanning through rows
    to determine which rows match the criteria for a particular query, this
    typically involves reading all columns of each row from disk. If large
    objects were stored “inline” in this fashion, they would adversely affect
    the performance of such scans. Therefore, large objects are often stored
    externally from their rows, as in <a class="ulink" href="#calibre_link-404" title="Figure&nbsp;15-3.&nbsp;Large objects are usually held in a separate area of storage; the main row storage contains indirect references to the large objects">Figure&nbsp;15-3</a>.
    Accessing a large object often requires “opening” it through the reference
    contained in the row.</p><div class="book"><div class="figure"><a id="calibre_link-403" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4454" class="calibre"></a><img alt="Database tables are typically physically represented as an array of rows, with all the columns in a row stored adjacent to one another" src="images/000044.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;15-2.&nbsp;Database tables are typically physically represented as an array
      of rows, with all the columns in a row stored adjacent to one
      another</div></div></div><div class="book"><div class="figure"><a id="calibre_link-404" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4455" class="calibre"></a><img alt="Large objects are usually held in a separate area of storage; the main row storage contains indirect references to the large objects" src="images/000052.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;15-3.&nbsp;Large objects are usually held in a separate area of storage; the
      main row storage contains indirect references to the large
      objects</div></div></div><p class="calibre2">The difficulty of working with large objects in a database suggests
    that a system such as Hadoop, which is much better suited to storing and
    processing large, complex data objects, is an ideal repository for such
    information. Sqoop can extract large objects from tables and store them in
    HDFS for further processing.</p><p class="calibre2">As in a database, MapReduce typically
    <span class="calibre"><em class="calibre10">materializes</em></span> every record before passing it along to
    the mapper. If individual records are truly large, this can be very
    inefficient.</p><p class="calibre2">As shown earlier, records imported by Sqoop are laid out on disk in
    a fashion very similar to a database’s internal structure: an array of
    records with all fields of a record concatenated together. When running a
    MapReduce program over imported records, each map task must fully
    materialize all fields of each record in its input split. If the contents
    of a large object field are relevant only for a small subset of the total
    number of records used as input to a MapReduce program, it would be
    inefficient to fully materialize all these records. Furthermore, depending
    on the size of the large object, full materialization in memory may be
    impossible.</p><p class="calibre2">To overcome these difficulties, Sqoop will store imported large
    objects in a separate file called a <code class="literal">LobFile</code>, if they are larger than a
    threshold size of 16 MB (configurable via the <code class="literal">sqoop.inline.lob.length.max</code> setting, in bytes).
      The <code class="literal">LobFile</code> format can store individual records of very large size (a
    64-bit address space is used). Each record in a <code class="literal">LobFile</code> holds a single
    large object. The <code class="literal">LobFile</code> format allows clients to hold a reference to a
    record without accessing the record contents. When records are accessed,
    this is done through a <code class="literal">java.io.InputStream</code> (for
    binary objects) or <code class="literal">java.io.Reader</code> (for
    character-based objects).</p><p class="calibre2">When a record is imported, the “normal” fields will be materialized
    together in a text file, along with a reference to the <code class="literal">LobFile</code> where a
      <code class="literal">CLOB</code> or <code class="literal">BLOB</code> column is stored. For example, suppose our <code class="literal">widgets</code> table contained a <code class="literal">BLOB</code> field named
    <code class="literal">schematic</code> holding the actual schematic
    diagram for each widget.</p><p class="calibre2">An imported record might then look like:</p><pre class="screen1">2,gizmo,4.00,2009-11-30,4,null,externalLob(lf,lobfile0,100,5011714)</pre><p class="calibre2">The <code class="literal">externalLob(...)</code> text is a
      reference to an externally stored large object, stored in <code class="literal">LobFile</code> format
    (<code class="literal">lf</code>) in a file named <em class="calibre10">lobfile0</em>, with the specified byte offset and
    length inside that file.</p><p class="calibre2">When working with this record, the
    <code class="literal">Widget.get_schematic()</code> method would return an
    object of type <code class="literal">BlobRef</code> referencing the <code class="literal">schematic</code> column, but not actually containing
    its contents. The <code class="literal">BlobRef.getDataStream()</code> method
      actually opens the <code class="literal">LobFile</code> and returns an
    <code class="literal">InputStream</code>, allowing you to access the <code class="literal">schematic</code> field’s contents.</p><p class="calibre2">When running a MapReduce job processing many
    <code class="literal">Widget</code> records, you might need to access the
    <code class="literal">schematic</code> fields of only a handful of
    records. This system allows you to incur the I/O costs of accessing only
    the required large object entries—a big savings, as individual schematics may be several
    megabytes or more of data.</p><p class="calibre2">The <code class="literal">BlobRef</code> and
    <code class="literal">ClobRef</code> classes cache references to underlying
      <code class="literal">LobFiles</code> within a map task. If you do access the <code class="literal">schematic</code> fields of several sequentially ordered
    records, they will take advantage of the existing file pointer’s alignment
    on the <a class="calibre" id="calibre_link-3497"></a><a class="calibre" id="calibre_link-2088"></a>next record body.</p></div><div class="book" title="Performing an Export"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-400">Performing an Export</h2></div></div></div><p class="calibre2">In Sqoop, an <em class="calibre10">import</em> <a class="calibre" id="calibre_link-3488"></a><a class="calibre" id="calibre_link-1590"></a>refers to the movement of data from a database system into
    HDFS. By contrast, an <em class="calibre10">export</em> uses HDFS as the
    source of data and a remote database as the destination. In the previous
    sections, we imported some data and then performed some analysis using
    Hive. We can export the results of this analysis to a database for
    consumption by other tools.</p><p class="calibre2">Before exporting a table from HDFS to a database, we must prepare
    the database to receive the data by creating the target table. Although
    Sqoop can infer which Java types are appropriate to hold SQL data types,
    this translation does not work in both directions (for example, there are
    several possible SQL column definitions that can hold data in a Java
    <code class="literal">String</code>; this could be <code class="literal">CHAR(64)</code>, <code class="literal">VARCHAR(200)</code>, or something else entirely).
    Consequently, you must determine which types are most appropriate.</p><p class="calibre2">We are going to export the <code class="literal">zip_profits</code> table from <a class="calibre" id="calibre_link-2020"></a><a class="calibre" id="calibre_link-1592"></a>Hive. We need to create a table in MySQL that has target
    columns in the same order, with the appropriate SQL types:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">mysql hadoopguide</code></strong>
<code class="literal">mysql&gt;</code> <strong class="userinput"><code class="calibre9">CREATE TABLE sales_by_zip (volume DECIMAL(8,2), zip INTEGER);</code></strong>
Query OK, 0 rows affected (0.01 sec)</pre><p class="calibre2">Then we run the export command:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">sqoop export --connect jdbc:mysql://localhost/hadoopguide -m 1 \</code></strong>
<code class="literal">&gt;</code> <strong class="userinput"><code class="calibre9">--table sales_by_zip --export-dir /user/hive/warehouse/zip_profits \</code></strong>
<code class="literal">&gt;</code> <strong class="userinput"><code class="calibre9">--input-fields-terminated-by '\0001'</code></strong>
...
14/10/29 12:05:08 INFO mapreduce.ExportJobBase: Transferred 176 bytes in 13.5373 
seconds (13.0011 bytes/sec)
14/10/29 12:05:08 INFO mapreduce.ExportJobBase: Exported 3 records.</pre><p class="calibre2">Finally, we can verify that the export worked by checking
    MySQL:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">mysql hadoopguide -e 'SELECT * FROM sales_by_zip'</code></strong>
+--------+-------+
| volume | zip   |
+--------+-------+
|  28.00 | 10005 | 
| 403.71 | 90210 | 
|  20.00 | 95014 | 
+--------+-------+</pre><p class="calibre2">When we created the <code class="literal">zip_profits</code>
    table in Hive, we did not specify any delimiters. So Hive used its default
    delimiters: a Ctrl-A character (Unicode <code class="literal">0x0001</code>) between fields and a newline at the end
    of each record. When we used Hive to access the contents of this table (in
    a <code class="literal">SELECT</code> statement), Hive converted
    this to a tab-delimited representation for display on the console. But
    when reading the tables directly from files, we need to tell Sqoop which
    delimiters to use. Sqoop assumes records are newline-delimited by default,
    but needs to be told about the Ctrl-A field delimiters. The <code class="literal">--input-fields-terminated-by</code> argument to
    <code class="literal">sqoop export</code> specified this
    information. Sqoop supports several escape sequences, which start with a
    backslash (\) character, when specifying delimiters.</p><p class="calibre2">In the example syntax, the escape sequence is enclosed in single
    quotes to ensure that the shell processes it literally. Without the
    quotes, the leading backslash itself may need to be escaped (e.g.,
    <code class="literal">--input-fields-terminated-by \\0001</code>).
    The escape sequences supported by Sqoop <a class="calibre" id="calibre_link-1576"></a><a class="calibre" id="calibre_link-3487"></a>are listed in <a class="ulink" href="#calibre_link-405" title="Table&nbsp;15-1.&nbsp;Escape sequences that can be used to specify nonprintable characters as field and record delimiters in Sqoop">Table&nbsp;15-1</a>.</p><div class="table"><a id="calibre_link-405" class="calibre"></a><div class="table-title">Table&nbsp;15-1.&nbsp;Escape sequences that can be used to specify nonprintable characters as field and
        record delimiters in Sqoop</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Escape</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">\b</code></td><td class="calibre25"><p class="calibre2">Backspaces.</p></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">\n</code></td><td class="calibre25"><p class="calibre2">Newline.</p></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">\r</code></td><td class="calibre25"><p class="calibre2">Carriage return.</p></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">\t</code></td><td class="calibre25"><p class="calibre2">Tab.</p></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">\'</code></td><td class="calibre25"><p class="calibre2">Single quote.</p></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">\"</code></td><td class="calibre25"><p class="calibre2">Double quote.</p></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">\\</code></td><td class="calibre25"><p class="calibre2">Backslash.</p></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">\0</code></td><td class="calibre25"><p class="calibre2"><code class="uri">NUL</code>. This will
            insert <code class="uri">NUL</code> characters between
            fields or lines, or will disable enclosing/escaping if used for
            one of the <code class="uri">--enclosed-by</code>,
            <code class="uri">--optionally-enclosed-by</code>, or
            <code class="uri">--escaped-by</code>
            arguments.</p></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">\0<em class="replaceable"><code class="calibre44">ooo</code></em></code></td><td class="calibre25"><p class="calibre2">The octal representation of a Unicode character’s
            code point. The actual character is specified by the octal value
            <code class="uri">ooo</code>.</p></td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">\0x<em class="replaceable"><code class="calibre44">hhh</code></em></code></td><td class="calibre28"><p class="calibre2">The hexadecimal representation of a Unicode
            character’s code point. This should be of the form <code class="uri">\0x<em class="replaceable"><code class="calibre44">hhh</code></em></code>, where
            <code class="uri"><em class="replaceable"><code class="calibre44">hhh</code></em></code>
            is the hex value. For example, <code class="uri">--fields-terminated-by '\0x10'</code> specifies
            the carriage return character.</p></td></tr></tbody></table></div></div></div><div class="book" title="Exports: A Deeper Look"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4456">Exports: A Deeper Look</h2></div></div></div><p class="calibre2">The Sqoop performs exports is very similar in
    nature to how Sqoop performs imports (see <a class="ulink" href="#calibre_link-406" title="Figure&nbsp;15-4.&nbsp;Exports are performed in parallel using MapReduce">Figure&nbsp;15-4</a>). Before performing the export, Sqoop
    picks a strategy based on the database connect string. For most systems,
    Sqoop uses <a class="calibre" id="calibre_link-2204"></a><a class="calibre" id="calibre_link-2159"></a>JDBC. Sqoop then generates a Java class based on the target
    table definition. This generated class has the ability to parse records
    from text files and insert values of the appropriate types into a table
    (in addition to the ability to read the columns from a
    <code class="literal">ResultSet</code>). A <a class="calibre" id="calibre_link-2508"></a><a class="calibre" id="calibre_link-3500"></a>MapReduce job is then launched that reads the source
    datafiles from HDFS, parses the records using the generated class, and
    executes the chosen export strategy.</p><p class="calibre2">The JDBC-based export strategy builds up batch <code class="literal">INSERT</code> statements that will each add multiple
    records to the target table. Inserting many records per statement performs
    much better than executing many single-row <code class="literal">INSERT</code> statements on most database systems.
    Separate threads are used to read from HDFS and communicate with the
    database, to ensure that I/O operations involving different systems are
    overlapped as much as possible.</p><div class="book"><div class="figure"><a id="calibre_link-406" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4457" class="calibre"></a><img alt="Exports are performed in parallel using MapReduce" src="images/000061.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;15-4.&nbsp;Exports are performed in parallel using MapReduce</div></div></div><p class="calibre2">For MySQL, Sqoop can employ a direct-mode strategy using <code class="literal">mysqlimport</code>. Each map task <a class="calibre" id="calibre_link-2739"></a>spawns a <code class="literal">mysqlimport</code>
    process that it communicates with via a named FIFO file on the local
    filesystem. Data is then streamed into <code class="literal">mysqlimport</code> via the FIFO channel, and from there
    into the database.</p><p class="calibre2">Whereas most MapReduce jobs reading from HDFS pick the degree of
    parallelism (number of map tasks) based on the number and size of the
    files to process, Sqoop’s export system allows users explicit control over
    the number of tasks. The performance of the export can be affected by the
    number of parallel writers to the database, so Sqoop uses the
    <code class="literal">CombineFileInputFormat</code> class to group the input
    files into a smaller number of map <a class="calibre" id="calibre_link-1591"></a>tasks.</p><div class="book" title="Exports and Transactionality"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4458">Exports and Transactionality</h3></div></div></div><p class="calibre2">Due to the parallel nature of <a class="calibre" id="calibre_link-1594"></a>the process, often an export is not an atomic operation.
      Sqoop will spawn multiple tasks to export slices of the data in
      parallel. These tasks can complete at different times, meaning that even
      though transactions are used inside tasks, results from one task may be
      visible before the results of another task. Moreover, databases often
      use fixed-size buffers to store transactions. As a result, one
      transaction cannot necessarily contain the entire set of operations
      performed by a task. Sqoop commits results every few thousand rows, to
      ensure that it does not run out of memory. These intermediate results
      are visible while the export continues. Applications that will use the
      results of an export should not be started until the export process is
      complete, or they may see partial results.</p><p class="calibre2">To solve this problem, Sqoop can export to a temporary staging table and then, at the
        end of the job—if the export has succeeded—move the staged data into the destination table
        in a single transaction. You can specify a staging table with the <code class="literal">--staging-table</code> option. The staging table must already exist and have the same
        schema as the destination. It must also be empty, unless the <code class="literal">--clear-staging-table</code> option is also supplied.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Using a staging table is slower, since the data must be written
        twice: first to the staging table, then to the destination table. The
        export process also uses more space while it is running, since there
        are two copies of the data while the staged data is being copied to
        the destination.</p></div></div><div class="book" title="Exports and SequenceFiles"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4459">Exports and SequenceFiles</h3></div></div></div><p class="calibre2">The example export reads <a class="calibre" id="calibre_link-1593"></a><a class="calibre" id="calibre_link-3330"></a>source data from a Hive table, which is stored in HDFS as
      a delimited text file. Sqoop can also export delimited text files that
      were not Hive tables. For example, it can export text files that are the
      output of a MapReduce job.</p><p class="calibre2">Sqoop can export records stored in <code class="literal">SequenceFile</code>s to an output table
        too, although some restrictions apply. A <code class="literal">SequenceFile</code> cannot contain
        arbitrary record types. Sqoop’s export tool will read objects from
          <code class="literal">SequenceFile</code>s and send them directly to the
          <code class="literal">OutputCollector</code>, which passes the objects to the database export
          <code class="literal">OutputFormat</code>. To work with Sqoop, the record must be stored in the
        “value” portion of the <code class="literal">SequenceFile</code>’s key-value pair format and must
        subclass the <code class="literal">org.apache.sqoop.lib.SqoopRecord</code> abstract class (as is
        done by all classes generated by Sqoop).</p><p class="calibre2">If you use the <a class="calibre" id="calibre_link-1165"></a>codegen tool (<span class="calibre">sqoop-codegen</span>) to generate a
          <code class="literal">SqoopRecord</code> implementation for a record based on your export target
        table, you can write a MapReduce program that populates instances of this class and writes
        them to <code class="literal">SequenceFile</code>s. <code class="literal">sqoop-export</code>
        can then export these <code class="literal">SequenceFile</code>s to the table. Another means by which
        data may be in <code class="literal">SqoopRecord</code> instances in
          <code class="literal">SequenceFile</code>s is if data is imported from a database table to HDFS and
        modified in some fashion, and then the results are stored in
        <code class="literal">SequenceFile</code>s holding records of the same data type.</p><p class="calibre2">In this case, Sqoop should reuse the existing class definition to read data from
          <code class="literal">SequenceFile</code>s, rather than generating a new (temporary) record
        container class to perform the export, as is done when converting text-based records to
        database rows. You can suppress code generation and instead use an existing record class and
        JAR by providing the <code class="literal">--class-name</code> and <code class="literal">--jar-file</code> arguments to Sqoop. Sqoop will use the specified
        class, loaded from the specified JAR, when exporting records.</p><p class="calibre2">In the following example, we reimport the <code class="literal">widgets</code> table as <code class="literal">SequenceFile</code>s, and then
      export it back to the database in a different table:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">sqoop import --connect jdbc:mysql://localhost/hadoopguide \</code></strong>
<code class="literal">&gt;</code> <strong class="userinput"><code class="calibre9">--table widgets -m 1 --class-name WidgetHolder --as-sequencefile \</code></strong>
<code class="literal">&gt;</code> <strong class="userinput"><code class="calibre9">--target-dir widget_sequence_files --bindir .</code></strong>
...
14/10/29 12:25:03 INFO mapreduce.ImportJobBase: Retrieved 3 records.

<code class="literal">%</code> <strong class="userinput"><code class="calibre9">mysql hadoopguide</code></strong>
<code class="literal">mysql&gt;</code> <strong class="userinput"><code class="calibre9">CREATE TABLE widgets2(id INT, widget_name VARCHAR(100),</code></strong> 
<code class="literal">    -&gt;</code> <strong class="userinput"><code class="calibre9">price DOUBLE, designed DATE, version INT, notes VARCHAR(200));</code></strong>
Query OK, 0 rows affected (0.03 sec)

mysql&gt; <strong class="userinput"><code class="calibre9">exit;</code></strong>

<code class="literal">%</code> <strong class="userinput"><code class="calibre9">sqoop export --connect jdbc:mysql://localhost/hadoopguide \</code></strong>
<code class="literal">&gt;</code> <strong class="userinput"><code class="calibre9">--table widgets2 -m 1 --class-name WidgetHolder \</code></strong>
<code class="literal">&gt;</code> <strong class="userinput"><code class="calibre9">--jar-file WidgetHolder.jar --export-dir widget_sequence_files</code></strong>
...
14/10/29 12:28:17 INFO mapreduce.ExportJobBase: Exported 3 records.</pre><p class="calibre2">During the import, we specified the <code class="literal">SequenceFile</code> format and indicated
        that we wanted the JAR file to be placed in the current directory (with <code class="literal">--bindir</code>) so we can reuse it. Otherwise, it would be placed in a
        temporary directory. We then created a destination table for the export, which had a
        slightly different schema (albeit one that is compatible with the original data). Finally,
        we ran an export that used the existing generated code to read the records from the
          <code class="literal">SequenceFile</code> and write them to the <a class="calibre" id="calibre_link-3489"></a>database.</p></div></div><div class="book" title="Further Reading"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4460">Further Reading</h2></div></div></div><p class="calibre2">For more <a class="calibre" id="calibre_link-3484"></a>information on using Sqoop, consult the <span class="calibre"><em class="calibre10"><a class="ulink" href="http://shop.oreilly.com/product/0636920029519.do" target="_top">Apache Sqoop
    Cookbook</a></em></span> by Kathleen Ting and <a class="calibre" id="calibre_link-2309"></a>Jarek Jarcec Cecho (O’Reilly, 2013).</p></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" type="footnote" id="calibre_link-394"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-407">94</a>] </sup>Of course, in a production deployment we’d need to be much more careful about access
          control, but this serves for demonstration purposes. The grant privilege shown in the
          example also assumes you’re running a pseudodistributed Hadoop instance. If you’re working
          with a distributed Hadoop cluster, you’d need to enable remote access by at least one
          user, whose account would be used to perform imports and exports via Sqoop.</p></div><div class="footnote" id="calibre_link-396"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-408">95</a>] </sup>Available from the <a class="ulink" href="http://sqoop.apache.org/" target="_top">Apache Software Foundation
              website</a>.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-520"><section type="chapter" id="calibre_link-4461" title="Chapter&nbsp;16.&nbsp;Pig"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;16.&nbsp;Pig</h2></div></div></div><p class="calibre2"><a class="ulink" href="http://pig.apache.org/" target="_top">Apache Pig</a> raises the level of
  <a class="calibre" id="calibre_link-2990"></a>abstraction for processing large datasets. MapReduce allows
  you, as the programmer, to specify a map function followed by a reduce
  function, but working out how to fit your data processing into this pattern,
  which often requires multiple MapReduce stages, can be a challenge. With
  Pig, the data structures are much richer, typically being multivalued and
  nested, and the transformations you can apply to the data are much more
  powerful. They include joins, for example, which are not for the faint of
  heart in MapReduce.</p><p class="calibre2">Pig is made up of two pieces:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">The language used to express data flows, called <em class="calibre10">Pig
      Latin</em>.</p></li><li class="listitem"><p class="calibre2">The execution environment to run Pig Latin programs. There are
      currently two environments: local execution in a single JVM and
      distributed execution on a Hadoop cluster.</p></li></ul></div><p class="calibre2">A Pig Latin program <a class="calibre" id="calibre_link-3012"></a>is made up of a series of operations, or transformations, that
  are applied to the input data to produce output. Taken as a whole, the
  operations describe a data flow, which the Pig execution environment
  translates into an executable representation and then runs. Under the
  covers, Pig turns the transformations into a series of MapReduce jobs, but
  as a programmer you are mostly unaware of this, which allows you to focus on
  the data rather than the nature of the execution.</p><p class="calibre2">Pig is a scripting language for exploring large datasets. One
  criticism of MapReduce is that the development cycle is very long. Writing
  the mappers and reducers, compiling and packaging the code, submitting the
  job(s), and retrieving the results is a time-consuming business, and even with Streaming,
  which removes the compile and package step, the experience is still
  involved. Pig’s sweet spot is its ability to process terabytes of data
  in response to a half-dozen lines of Pig Latin issued from the console. Indeed,
  it was created at Yahoo! to make it easier for researchers and engineers to
  mine the huge datasets there. Pig is very supportive of a programmer writing
  a query, since it provides several commands for introspecting the data
  structures in your program as it is written. Even more useful, it can
  perform a sample run on a representative subset of your input data, so you
  can see whether there are errors in the processing before unleashing it on
  the full dataset.</p><p class="calibre2">Pig was designed to be extensible. Virtually all parts of the
  processing path are customizable: loading, storing, filtering, grouping, and
  joining can all be altered by <a class="calibre" id="calibre_link-4462"></a><a class="calibre" id="calibre_link-3732"></a>user-defined functions (UDFs). These functions operate on
  Pig’s nested data model, so they can integrate very deeply with Pig’s
  operators. As another benefit, UDFs tend to be more reusable than the
  libraries developed for writing MapReduce programs.</p><p class="calibre2">In some cases, Pig doesn’t perform as well as programs written in
  MapReduce. However, the gap is narrowing with each release, as the Pig team
  implements sophisticated algorithms for applying Pig’s relational operators.
  It’s fair to say that unless you are willing to invest a lot of effort
  optimizing Java MapReduce code, writing queries in Pig Latin will save you
  time.</p><div class="book" title="Installing and Running Pig"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4463">Installing and Running Pig</h2></div></div></div><p class="calibre2">Pig runs as a client-side <a class="calibre" id="calibre_link-3000"></a>application. Even if you want to run Pig on a Hadoop
    cluster, there is nothing extra to install on the cluster: Pig launches
    jobs and interacts with HDFS (or other Hadoop filesystems) from your
    workstation.</p><p class="calibre2">Installation is straightforward. Download a stable release from
    <a class="ulink" href="http://pig.apache.org/releases.html" target="_top">http://pig.apache.org/releases.html</a>, and unpack the tarball
    in a suitable place on your workstation:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">tar xzf pig-<em class="replaceable1"><code class="calibre46">x.y.z</code></em>.tar.gz</code></strong></pre><p class="calibre2">It’s convenient to add Pig’s binary directory to your command-line
    path. For example:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">export PIG_HOME=~/sw/pig-<em class="replaceable1"><code class="calibre46">x.y.z</code></em></code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">export PATH=$PATH:$PIG_HOME/bin</code></strong></pre><p class="calibre2">You also need to set the <code class="literal">JAVA_HOME</code> environment <a class="calibre" id="calibre_link-2200"></a>variable to point to a suitable Java installation.</p><p class="calibre2">Try typing <code class="literal">pig -help</code> to get usage
    instructions.</p><div class="book" title="Execution Types"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4464">Execution Types</h3></div></div></div><p class="calibre2">Pig has two execution <a class="calibre" id="calibre_link-2998"></a>types or modes: local mode and MapReduce mode. Execution
      modes for Apache Tez and Spark (see <a class="ulink" href="#calibre_link-352" title="Chapter&nbsp;19.&nbsp;Spark">Chapter&nbsp;19</a>) were
      both under development at the time of writing. Both promise significant
      performance gains over MapReduce mode, so try them if they are available
      in the version of Pig you are using.</p><div class="book" title="Local mode"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4465">Local mode</h4></div></div></div><p class="calibre2">In local mode, Pig <a class="calibre" id="calibre_link-2344"></a>runs in a single JVM and accesses the local filesystem.
        This mode is suitable only for small datasets and when trying out
        Pig.</p><p class="calibre2">The execution type is set using the <code class="literal">-x</code> or <code class="literal">-exectype</code> option. To run in local mode, set
        the option to <code class="literal">local</code>:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">pig -x local</code></strong>
<code class="literal">grunt&gt; </code></pre><p class="calibre2">This starts Grunt, the Pig interactive shell, which is discussed
        in more detail shortly.</p></div><div class="book" title="MapReduce mode"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4466">MapReduce mode</h4></div></div></div><p class="calibre2">In MapReduce mode, <a class="calibre" id="calibre_link-2522"></a>Pig translates queries into MapReduce jobs and runs them
        on a Hadoop cluster. The cluster may be a pseudo- or fully distributed
        cluster. MapReduce mode (with a fully distributed cluster) is what you
        use when you want to run Pig on large datasets.</p><p class="calibre2">To use MapReduce mode, you first need to check that the version
        of Pig you downloaded is compatible with the version of Hadoop you are
        using. Pig releases will only work against particular versions of
        Hadoop; this is documented in the release notes.</p><p class="calibre2">Pig honors the <code class="literal">HADOOP_HOME</code>
        environment variable <a class="calibre" id="calibre_link-1871"></a>for finding which Hadoop client to run. However, if it
        is not set, Pig will use a bundled copy of the Hadoop libraries. Note
        that these may not match the version of Hadoop running on your
        cluster, so it is best to explicitly set <code class="literal">HADOOP_HOME</code>.</p><p class="calibre2">Next, you need to point Pig at the cluster’s namenode and
        resource manager. If the installation of Hadoop at <code class="literal">HADOOP_HOME</code> is already configured for this,
        then there is nothing more to do. Otherwise, you can <a class="calibre" id="calibre_link-1869"></a>set <code class="literal">HADOOP_CONF_DIR</code>
        to a directory containing the Hadoop site file (or files) that
        <a class="calibre" id="calibre_link-1738"></a><a class="calibre" id="calibre_link-3880"></a><a class="calibre" id="calibre_link-2533"></a>define <code class="literal">fs.defaultFS</code>,
        <code class="literal">yarn.resourcemanager.address</code>, and
        <code class="literal">mapreduce.framework.name</code> (the
        latter should be set to <code class="literal">yarn</code>).</p><p class="calibre2">Alternatively, you can set these properties in the <em class="calibre10">pig.properties</em> file in Pig’s <em class="calibre10">conf</em> directory (or the directory specified
        <a class="calibre" id="calibre_link-3035"></a>by <code class="literal">PIG_CONF_DIR</code>).
        Here’s an example for a pseudo-distributed setup:</p><pre class="screen1">fs.defaultFS=hdfs://localhost/
mapreduce.framework.name=yarn
yarn.resourcemanager.address=localhost:8032</pre><p class="calibre2">Once you have configured Pig to connect to a Hadoop cluster, you
        can launch Pig, setting the <code class="literal">-x</code>
        option to <code class="literal">mapreduce</code> or omitting it
        entirely, as MapReduce mode is the default. We’ve used the <code class="literal">-brief</code> option to stop timestamps from being
        logged:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">pig -brief</code></strong>
Logging error messages to: /Users/tom/pig_1414246949680.log
Default bootup file /Users/tom/.pigbootup not found
Connecting to hadoop file system at: hdfs://localhost/
<code class="literal">grunt&gt; </code></pre><p class="calibre2">As you can see from the output, Pig reports the filesystem (but
        not the YARN resource manager) that it has connected to.</p><p class="calibre2">In MapReduce mode, you can optionally enable
        <em class="calibre10">auto-local mode</em> (by <a class="calibre" id="calibre_link-3030"></a>setting <code class="literal">pig</code><code class="literal">.auto.local.enabled</code> to <code class="literal">true</code>), which is an optimization
        that runs small jobs locally if the input is less than 100 MB
        (<a class="calibre" id="calibre_link-3031"></a>set by <code class="literal">pig.auto.local.input.maxbytes</code>, default
        100,000,000) and no more than one reducer is being <a class="calibre" id="calibre_link-2999"></a>used.</p></div></div><div class="book" title="Running Pig Programs"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4467">Running Pig Programs</h3></div></div></div><p class="calibre2">There are three ways of executing Pig programs, all of which work
      in both local and MapReduce mode:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Script</span></dt><dd class="calibre8"><p class="calibre2">Pig can <a class="calibre" id="calibre_link-3296"></a>run a script file that contains Pig commands. For
            example, <code class="literal">pig</code> <code class="literal">script.pig</code> runs the commands in the
            local file <em class="calibre10">script.pig</em>.
            Alternatively, for very short scripts, you can use the <code class="literal">-e</code> option to run a script specified as a
            string on the command line.</p></dd><dt class="calibre7"><span class="term">Grunt</span></dt><dd class="calibre8"><p class="calibre2">Grunt is an <a class="calibre" id="calibre_link-1829"></a>interactive shell for running Pig commands. Grunt is
            started when no file is specified for Pig to run and the <code class="literal">-e</code> option is not used. It is also
            possible to run Pig scripts from within Grunt using <code class="literal">run</code> and <code class="literal">exec</code>.</p></dd><dt class="calibre7"><span class="term">Embedded</span></dt><dd class="calibre8"><p class="calibre2">You can run Pig programs from <a class="calibre" id="calibre_link-2167"></a>Java using <a class="calibre" id="calibre_link-3033"></a>the <code class="literal">PigServer</code>
            class, much like you can use JDBC to run SQL programs from Java.
            For programmatic access to <a class="calibre" id="calibre_link-3032"></a>Grunt, use <code class="literal">PigRunner</code>.</p></dd></dl></div></div><div class="book" title="Grunt"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4468">Grunt</h3></div></div></div><p class="calibre2">Grunt has line-editing facilities like those found in GNU Readline
      (used in the bash shell and many other command-line applications). For
      instance, the Ctrl-E key combination will move the cursor to the end of
      the line. Grunt remembers command history, too,<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-521" id="calibre_link-556">96</a>]</sup> and you can recall lines in the history buffer using
      Ctrl-P or Ctrl-N (for previous and next), or equivalently, the up or
      down cursor keys.</p><p class="calibre2">Another handy feature is Grunt’s completion mechanism, which will
      try to complete Pig Latin keywords and functions when you press the Tab
      key. For example, consider the following incomplete line:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">a = foreach b ge</code></strong></pre><p class="calibre2">If you press the Tab key at this point, <code class="literal">ge</code> will expand to <code class="literal">generate</code>, a Pig Latin keyword:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">a = foreach b generate</code></strong></pre><p class="calibre2">You can customize the completion tokens by creating a file named
      <em class="calibre10">autocomplete</em> and placing it on
      Pig’s classpath (such as in the <em class="calibre10">conf</em> directory in Pig’s <em class="calibre10">install</em> directory) or in the directory you
      invoked Grunt from. The file should have one token per line, and tokens
      must not contain any whitespace. Matching is case sensitive. It can be
      very handy to add commonly used file paths (especially because Pig does
      not perform filename completion) or the names of any user-defined
      functions you have created.</p><p class="calibre2">You can get a list of commands using the <code class="literal">help</code> command. When you’ve finished your Grunt
      session, you can exit with the <code class="literal">quit</code>
      command, or the equivalent shortcut <code class="literal">\q</code>.</p></div><div class="book" title="Pig Latin Editors"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4469">Pig Latin Editors</h3></div></div></div><p class="calibre2">There are <a class="calibre" id="calibre_link-3017"></a>Pig Latin syntax highlighters available for a variety of
      editors, including Eclipse, IntelliJ IDEA, Vim, Emacs, and TextMate.
      Details are available on the <a class="ulink" href="https://cwiki.apache.org/confluence/display/PIG/PigTools" target="_top">Pig
      wiki</a>.</p><p class="calibre2">Many Hadoop distributions come with the <a class="ulink" href="http://gethue.com/" target="_top">Hue web interface</a>, which has a Pig
      script editor <a class="calibre" id="calibre_link-3001"></a>and launcher.</p></div></div><div class="book" title="An Example"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4470">An Example</h2></div></div></div><p class="calibre2">Let’s look at a <a class="calibre" id="calibre_link-3010"></a>simple example by writing the program to calculate the
    maximum recorded temperature by year
    for the weather dataset in Pig Latin (just like we did using MapReduce in
    <a class="ulink" href="#calibre_link-462" title="Chapter&nbsp;2.&nbsp;MapReduce">Chapter&nbsp;2</a>). The complete program is only a few
    lines long:</p><pre class="screen1"><code class="c1">-- max_temp.pig: Finds the maximum temperature by year</code>
records <code class="o">=</code> <code class="k">LOAD</code> <code class="sb">'input/ncdc/micro-tab/sample.txt'</code>
  <code class="k">AS</code> <code class="p">(</code>year:<code class="kt">chararray</code><code class="o">,</code> temperature:<code class="kt">int</code><code class="o">,</code> quality:<code class="kt">int</code><code class="p">);</code>
filtered_records <code class="o">=</code> <code class="k">FILTER</code> records <code class="k">BY</code> temperature <code class="o">!=</code> <code class="mi">9999</code> <code class="k">AND</code>
  quality <code class="nf">IN</code> <code class="p">(</code><code class="mi">0</code><code class="o">,</code> <code class="mi">1</code><code class="o">,</code> <code class="mi">4</code><code class="o">,</code> <code class="mi">5</code><code class="o">,</code> <code class="mi">9</code><code class="p">);</code>
grouped_records <code class="o">=</code> <code class="k">GROUP</code> filtered_records <code class="k">BY</code> year;
max_temp <code class="o">=</code> <code class="k">FOREACH</code> grouped_records <code class="k">GENERATE</code> <code class="k">group</code><code class="o">,</code>
  <code class="nb">MAX</code><code class="p">(</code>filtered_records.temperature<code class="p">);</code>
<code class="k">DUMP</code> max_temp;</pre><p class="calibre2">To explore what’s going on, we’ll use Pig’s Grunt interpreter, which
    allows us to enter lines and interact with the program to understand what
    it’s doing. Start up Grunt in local mode, and then enter the first line of
    the Pig script:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">records = LOAD 'input/ncdc/micro-tab/sample.txt'</code></strong>
<code class="literal">&gt;&gt; </code><strong class="userinput"><code class="calibre9">  AS (year:chararray, temperature:int, quality:int);</code></strong></pre><p class="calibre2">For simplicity, the program assumes that the input is tab-delimited
    text, with each line having just year, temperature, and quality fields.
    (Pig actually has more flexibility than this with regard to the input
    formats it accepts, as we’ll see later.) This line describes the input
    data we want to process. The <code class="literal">year:chararray</code> notation describes the field’s
    name and type; <code class="literal">chararray</code> is like a Java
    <code class="literal">String</code>, and an <code class="literal">int</code>
    is like a Java <code class="literal">int</code>. The
    <code class="literal">LOAD</code> operator takes a URI argument; here we are just
    using a local file, but we could refer to an HDFS URI. The
    <code class="literal">AS</code> clause (which is optional) gives the fields names to
    make it convenient to refer to them in subsequent statements.</p><p class="calibre2">The result of the <code class="literal">LOAD</code> operator, and indeed any
    operator in Pig Latin, is a <em class="calibre10">relation</em>, which is just
    a set of tuples. A <em class="calibre10">tuple</em> is just like a row of data
    in a database table, with multiple fields in a particular order. In this
    example, the <code class="literal">LOAD</code> function produces a set of (year,
    temperature, quality) tuples that are present in the input file. We write
    a relation with one tuple per line, where tuples are represented as
    comma-separated items in parentheses:</p><pre class="screen1">(1950,0,1)
(1950,22,1)
(1950,-11,1)
(1949,111,1)</pre><p class="calibre2">Relations are given names, or <em class="calibre10">aliases</em>, so
    they can be referred to. This relation is given the <code class="literal">records</code> alias. We can examine the contents of an
    alias using the <code class="literal">DUMP</code> operator:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP records;</code></strong>
(1950,0,1)
(1950,22,1)
(1950,-11,1)
(1949,111,1)
(1949,78,1)</pre><p class="calibre2">We can also see the structure of a relation—the relation’s
    <em class="calibre10">schema</em>—using <a class="calibre" id="calibre_link-1430"></a>the <code class="literal">DESCRIBE</code> operator on the
    relation’s alias:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DESCRIBE records;</code></strong>
records: {year: chararray,temperature: int,quality: int}</pre><p class="calibre2">This tells us that <code class="literal">records</code> has three fields, with aliases <code class="literal">year</code>, <code class="literal">temperature</code>, and <code class="literal">quality</code>, which are the names we gave
    them in the <code class="literal">AS</code> clause. The fields have the types given
    to them in the <code class="literal">AS</code> clause, too. We examine types in Pig
    in more detail later.</p><p class="calibre2">The second statement removes records that have a missing temperature
    (indicated by a value of 9999) or an unsatisfactory quality reading. For
    this small dataset, no records are filtered out:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">filtered_records = FILTER records BY temperature != 9999 AND</code></strong>
<code class="literal">&gt;&gt; </code><strong class="userinput"><code class="calibre9">  quality IN (0, 1, 4, 5, 9);</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP filtered_records;</code></strong>
(1950,0,1)
(1950,22,1)
(1950,-11,1)
(1949,111,1)
(1949,78,1)</pre><p class="calibre2">The third statement uses the <code class="literal">GROUP</code> function to
    group the <code class="literal">records</code> relation by the
    <code class="literal">year</code> field. Let’s use
    <code class="literal">DUMP</code> to see what it produces:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">grouped_records = GROUP filtered_records BY year;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP grouped_records;</code></strong>
(1949,{(1949,78,1),(1949,111,1)})
(1950,{(1950,-11,1),(1950,22,1),(1950,0,1)})</pre><p class="calibre2">We now have two rows, or tuples: one for each year in the input
    data. The first field in each tuple is the field being grouped by (the
    year), and the second field has a bag of tuples for that year. A
    <em class="calibre10">bag</em> is just an unordered collection of tuples,
    which in Pig Latin is represented using curly braces.</p><p class="calibre2">By grouping the data in this way, we have created a row per year, so
    now all that remains is to find the maximum temperature for the tuples in
    each bag. Before we do this, let’s understand the structure of the
    <code class="literal">grouped_records</code> relation:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DESCRIBE grouped_records;</code></strong>
grouped_records: {group: chararray,filtered_records: {year: chararray,
temperature: int,quality: int}}</pre><p class="calibre2">This tells us that the grouping field is given the alias <code class="literal">group</code> by Pig, and the second field is the same
    structure as the <code class="literal">filtered_records</code>
    relation that was being grouped. With this information, we can try the
    fourth transformation:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">max_temp = FOREACH grouped_records GENERATE group,</code></strong>
<code class="literal">&gt;&gt; </code><strong class="userinput"><code class="calibre9">  MAX(filtered_records.temperature);</code></strong></pre><p class="calibre2"><code class="literal">FOREACH</code> processes every row to generate a derived
    set of rows, using a GENERATE clause to define the fields in each derived
    row. In this example, the first field is <code class="literal">group</code>,
    which is just the year. The second field is a little more
    complex. The <code class="literal">filtered_records.temperature</code>
    reference is to the <code class="literal">temperature</code> field
    of the <code class="literal">filtered_records</code> bag in the <code class="literal">grouped_records</code> relation. <code class="literal">MAX</code> is a built-in function for calculating the
    maximum value of fields in a bag. In this case, it calculates the maximum
    temperature for the fields in each <code class="literal">filtered_records</code> bag. Let’s check the
    result:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP max_temp;</code></strong>
(1949,111)
(1950,22)</pre><p class="calibre2">We’ve successfully calculated the maximum temperature for each
    year.</p><div class="book" title="Generating Examples"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4471">Generating Examples</h3></div></div></div><p class="calibre2">In this example, we’ve used a small sample dataset with just a
      handful of rows to make it easier to follow the data flow and aid
      debugging. Creating a cut-down dataset is an art, as ideally it should
      be rich enough to cover all the cases to exercise your queries (the
      <em class="calibre10">completeness</em> property), yet small enough to make
      sense to the programmer (the <em class="calibre10">conciseness</em>
      property). Using a random sample doesn’t work well in general because
      join and filter operations tend to remove all random data, leaving an
      empty result, which is not illustrative of the general data flow.</p><p class="calibre2">With the <a class="calibre" id="calibre_link-2074"></a><code class="literal">ILLUSTRATE</code> operator, Pig provides a
      tool for generating a reasonably complete and concise sample dataset.
      Here is the output from running <code class="literal">ILLUSTRATE</code> on our
      dataset (slightly reformatted to fit the page):</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">ILLUSTRATE max_temp;</code></strong>
-------------------------------------------------------------------------------
| records     | year:chararray      | temperature:int      | quality:int      | 
-------------------------------------------------------------------------------
|             | 1949                | 78                   | 1                | 
|             | 1949                | 111                  | 1                | 
|             | 1949                | 9999                 | 1                | 
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
| filtered_records   | year:chararray    | temperature:int    | quality:int   | 
-------------------------------------------------------------------------------
|                    | 1949              | 78                 | 1             | 
|                    | 1949              | 111                | 1             | 
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
| grouped_records  | group:chararray   | filtered_records:bag{:tuple(         |
                                           year:chararray,temperature:int,    |
                                           quality:int)}                      |
-------------------------------------------------------------------------------
|                  | 1949              | {(1949, 78, 1), (1949, 111, 1)}      |
-------------------------------------------------------------------------------
---------------------------------------------------
| max_temp     | group:chararray      | :int      | 
---------------------------------------------------
|              | 1949                 | 111       | 
---------------------------------------------------</pre><p class="calibre2">Notice that Pig used some of the original data (this is important
      to keep the generated dataset realistic), as well as creating some new
      data. It noticed the special value 9999 in the query and created a tuple
      containing this value to exercise the <code class="literal">FILTER</code>
      statement.</p><p class="calibre2">In summary, the output of <code class="literal">ILLUSTRATE</code> is easy to
      follow and can help you understand what your query is <a class="calibre" id="calibre_link-3011"></a>doing.</p></div></div><div class="book" title="Comparison with Databases"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4472">Comparison with Databases</h2></div></div></div><p class="calibre2">Having seen Pig in <a class="calibre" id="calibre_link-2993"></a>action, it might seem that Pig Latin is similar to SQL. The
    presence of such operators as <a class="calibre" id="calibre_link-1814"></a><a class="calibre" id="calibre_link-1431"></a><code class="literal">GROUP BY</code> and <code class="literal">DESCRIBE</code>
    reinforces this impression. However, there are several differences between
    the two languages, and between Pig and <a class="calibre" id="calibre_link-3144"></a>relational database management systems (RDBMSs) in
    general.</p><p class="calibre2">The most significant difference is that Pig Latin is a data flow
    programming language, whereas SQL is a declarative programming language.
    In other words, a Pig Latin program is a step-by-step set of operations on
    an input relation, in which each step is a single transformation. By
    contrast, SQL statements are a set of constraints that, taken together,
    define the output. In many ways, programming in Pig Latin is like working
    at the level of an RDBMS query planner, which figures out how to turn a
    declarative statement into a system of steps.</p><p class="calibre2">RDBMSs store data in tables, with tightly predefined schemas. Pig is
    more relaxed about the data that it processes: you can define a schema at
    runtime, but it’s optional. Essentially, it will operate on any source of
    tuples (although the source should support being read in parallel, by
    being in multiple files, for example), where a UDF is used to read the
    tuples from their raw representation.<sup class="calibre6">[<a class="firstname" href="#calibre_link-522" id="calibre_link-557">97</a>]</sup> The most common representation is a text file with
    tab-separated fields, and Pig provides a built-in load function for this
    format. Unlike with a traditional database, there is no data import
    process to load the data into the RDBMS. The data is loaded from the
    filesystem (usually HDFS) as the first step in the processing.</p><p class="calibre2">Pig’s support for complex, nested data structures further
    differentiates it from SQL, which operates on flatter data structures.
    Also, Pig’s ability to use UDFs and streaming operators that are tightly
    integrated with the language and Pig’s nested data structures makes Pig
    Latin more customizable than most SQL dialects.</p><p class="calibre2">RDBMSs have several features to support online, low-latency queries,
    such as transactions and indexes, that are absent in Pig. Pig does not
    support random reads or queries on the order of tens of milliseconds. Nor
    does it support random writes to update small portions of data; all writes
    are bulk streaming writes, just like with MapReduce.</p><p class="calibre2">Hive (covered in <a class="ulink" href="#calibre_link-402" title="Chapter&nbsp;17.&nbsp;Hive">Chapter&nbsp;17</a>) sits between Pig and
    conventional RDBMSs. Like Pig, Hive is designed to use HDFS for storage,
    but otherwise there are some significant differences. Its query language,
    HiveQL, is based on SQL, and anyone who is familiar with SQL will have
    little trouble writing queries in HiveQL. Like RDBMSs, Hive mandates that
    all data be stored in tables, with a schema under its management; however,
    it can associate a schema with preexisting data in HDFS, so the load step
    is optional. Pig is able to work
    with Hive tables using HCatalog; this is discussed <a class="calibre" id="calibre_link-2994"></a>further in <a class="ulink" href="#calibre_link-523" title="Using Hive tables with HCatalog">Using Hive tables with HCatalog</a>.</p></div><div class="book" title="Pig Latin"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4473">Pig Latin</h2></div></div></div><p class="calibre2">This section gives an <a class="calibre" id="calibre_link-3013"></a>informal description of the syntax and semantics of the Pig
    Latin programming language.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-524" id="calibre_link-558">98</a>]</sup> It is not meant to offer a complete reference to the
    language,<sup class="calibre6">[<a class="firstname" href="#calibre_link-525" id="calibre_link-559">99</a>]</sup> but there should be enough here for you to get a good
    understanding of Pig Latin’s constructs.</p><div class="book" title="Structure"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4474">Structure</h3></div></div></div><p class="calibre2">A Pig Latin program <a class="calibre" id="calibre_link-3029"></a>consists of a collection of statements. A statement can be
      thought of as an operation or a command.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-526" id="calibre_link-560">100</a>]</sup> For example, a <code class="literal">GROUP</code> <a class="calibre" id="calibre_link-1815"></a>operation is a type of statement:</p><pre class="screen1">grouped_records <code class="o">=</code> <code class="k">GROUP</code> records <code class="k">BY</code> year;</pre><p class="calibre2">The command to list the files in a Hadoop filesystem is another
      example of a statement:</p><pre class="screen1">ls /</pre><p class="calibre2">Statements are usually terminated with a <a class="calibre" id="calibre_link-3325"></a>semicolon, as in the example of the
      <code class="literal">GROUP</code> statement. In fact, this is an example of a
      statement that must be terminated with a semicolon; it is a syntax error
      to omit it. The <code class="literal">ls</code> command,
      <a class="calibre" id="calibre_link-2369"></a>on the other hand, does not have to be terminated with a
      semicolon. As a general guideline, statements or commands for
      interactive use in Grunt do not need the terminating semicolon. This
      group includes the interactive Hadoop commands, as well as the
      diagnostic operators such as <code class="literal">DESCRIBE</code>. It’s never an
      error to add a terminating semicolon, so if in doubt, it’s simplest to
      add one.</p><p class="calibre2">Statements that have to be terminated with a semicolon can be
      split across multiple lines for readability:</p><pre class="screen1">records <code class="o">=</code> <code class="k">LOAD</code> <code class="sb">'input/ncdc/micro-tab/sample.txt'</code>
  <code class="k">AS</code> <code class="p">(</code>year:<code class="kt">chararray</code><code class="o">,</code> temperature:<code class="kt">int</code><code class="o">,</code> quality:<code class="kt">int</code><code class="p">);</code></pre><p class="calibre2">Pig Latin has two forms of <a class="calibre" id="calibre_link-1196"></a>comments. Double hyphens are used for single-line
      comments. Everything from the
      first hyphen to the end of the line is ignored by the Pig Latin interpreter:</p><pre class="screen1"><code class="c1">-- My program</code>
<code class="k">DUMP</code> A; <code class="c1">-- What's in A?</code></pre><p class="calibre2">C-style comments are more flexible since they delimit the
      beginning and end of the comment block with <code class="literal">/*</code> and <code class="literal">*/</code>
      markers. They can span lines or be embedded in a single line:</p><pre class="screen1"><code class="c2">/* </code>
<code class="c2"> * Description of my program spanning</code>
<code class="c2"> * multiple lines.</code>
<code class="c2"> */</code>
A <code class="o">=</code> <code class="k">LOAD</code> <code class="sb">'input/pig/join/A'</code><code class="p">;</code>
B <code class="o">=</code> <code class="k">LOAD</code> <code class="sb">'input/pig/join/B'</code><code class="p">;</code>
C <code class="o">=</code> <code class="k">JOIN</code> A <code class="k">BY</code> $0, <code class="c2">/* ignored */</code> B <code class="k">BY</code> $1;
<code class="k">DUMP</code> C;</pre><p class="calibre2">Pig Latin has a list of <a class="calibre" id="calibre_link-2306"></a>keywords that have a special meaning in the language and
      cannot be used as identifiers. These include the operators
      (<code class="literal">LOAD</code>, <code class="literal">ILLUSTRATE</code>), commands
      (<code class="literal">cat</code>, <code class="literal">ls</code>), expressions (<code class="literal">matches</code>, <code class="literal">FLATTEN</code>), and functions (<code class="literal">DIFF</code>, <code class="literal">MAX</code>)—all of which are covered in the following
      sections.</p><p class="calibre2">Pig Latin has mixed rules on <a class="calibre" id="calibre_link-1063"></a>case sensitivity. Operators and commands are not case
      sensitive (to make interactive use more forgiving); however, aliases and
      function names are case sensitive.</p></div><div class="book" title="Statements"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4475">Statements</h3></div></div></div><p class="calibre2">As a Pig Latin program is <a class="calibre" id="calibre_link-3027"></a><a class="calibre" id="calibre_link-3520"></a>executed, each statement is parsed in turn. If there are
      syntax errors or other (semantic) problems, such as undefined aliases,
      the interpreter will halt and display an error message. The interpreter
      builds a <em class="calibre10">logical plan</em> for every relational
      operation, which forms the core of a Pig Latin program. The logical plan
      for the statement is added to the logical plan for the program so far,
      and then the interpreter moves on to the next statement.</p><p class="calibre2">It’s important to note that no data processing takes place while
      the logical plan of the program is being constructed. For example,
      consider again the Pig Latin program from the first example:</p><a id="calibre_link-4476" class="calibre"></a><pre class="screen1"><code class="c1">-- max_temp.pig: Finds the maximum temperature by year</code>
records <code class="o">=</code> <code class="k">LOAD</code> <code class="sb">'input/ncdc/micro-tab/sample.txt'</code>
  <code class="k">AS</code> <code class="p">(</code>year:<code class="kt">chararray</code><code class="o">,</code> temperature:<code class="kt">int</code><code class="o">,</code> quality:<code class="kt">int</code><code class="p">);</code>
filtered_records <code class="o">=</code> <code class="k">FILTER</code> records <code class="k">BY</code> temperature <code class="o">!=</code> <code class="mi">9999</code> <code class="k">AND</code>
  quality <code class="nf">IN</code> <code class="p">(</code><code class="mi">0</code><code class="o">,</code> <code class="mi">1</code><code class="o">,</code> <code class="mi">4</code><code class="o">,</code> <code class="mi">5</code><code class="o">,</code> <code class="mi">9</code><code class="p">);</code>
grouped_records <code class="o">=</code> <code class="k">GROUP</code> filtered_records <code class="k">BY</code> year;
max_temp <code class="o">=</code> <code class="k">FOREACH</code> grouped_records <code class="k">GENERATE</code> <code class="k">group</code><code class="o">,</code>
  <code class="nb">MAX</code><code class="p">(</code>filtered_records.temperature<code class="p">);</code>
<code class="k">DUMP</code> max_temp;</pre><p class="calibre2">When the Pig Latin interpreter sees the first line containing the
      <a class="calibre" id="calibre_link-2340"></a><code class="literal">LOAD</code> statement, it confirms that it is
      syntactically and semantically correct and adds it to the logical plan,
      but it does <span class="calibre">not</span> load the data from
      the file (or even check whether the file exists). Indeed, where would it
      load it? Into memory? Even if it did fit into memory, what would it do
      with the data? Perhaps not all the input data is needed (because later
      statements filter it, for example), so it would be pointless to load it.
      The point is that it makes no sense to start any processing until the
      whole flow is defined. Similarly, Pig validates the <a class="calibre" id="calibre_link-1816"></a><code class="literal">GROUP</code> and
      <code class="literal">FOREACH...GENERATE</code> statements, <a class="calibre" id="calibre_link-1726"></a>and adds them to the logical plan without executing them.
      The trigger for Pig to start execution is the <a class="calibre" id="calibre_link-1551"></a><code class="literal">DUMP</code> statement. At that point, the
      logical plan is compiled into a physical plan and executed.</p><div class="sidebar"><a id="calibre_link-533" class="calibre"></a><div class="sidebar-title">Multiquery Execution</div><p class="calibre2">Because <code class="literal">DUMP</code> is a diagnostic tool, it will
        always trigger execution. However, <a class="calibre" id="calibre_link-3531"></a>the <code class="literal">STORE</code> command is different. In
        interactive mode, <code class="literal">STORE</code> acts like
        <code class="literal">DUMP</code> and will always trigger execution (this
        <a class="calibre" id="calibre_link-3253"></a>includes the <code class="literal">run</code>
        command), but in batch mode it will not (this <a class="calibre" id="calibre_link-1584"></a>includes the <code class="literal">exec</code>
        command). The reason for this is efficiency. In batch mode, Pig will
        parse the whole script to see whether there are any optimizations that
        could be made to limit the amount of data to be written to or read
        from disk. Consider the following simple example:</p><a id="calibre_link-4477" class="calibre"></a><pre class="screen2">A <code class="o">=</code> <code class="k">LOAD</code> <code class="sb">'input/pig/multiquery/A'</code><code class="p">;</code>
B <code class="o">=</code> <code class="k">FILTER</code> A <code class="k">BY</code> $1 <code class="o">==</code> <code class="sb">'banana'</code><code class="p">;</code>
C <code class="o">=</code> <code class="k">FILTER</code> A <code class="k">BY</code> $1 <code class="o">!=</code> <code class="sb">'banana'</code><code class="p">;</code>
<code class="k">STORE</code> B <code class="k">INTO</code> <code class="sb">'output/b'</code><code class="p">;</code>
<code class="k">STORE</code> C <code class="k">INTO</code> <code class="sb">'output/c'</code><code class="p">;</code></pre><p class="calibre2">Relations <code class="literal">B</code> and <code class="literal">C</code> are both
        derived from <code class="literal">A</code>, so to save reading
        <code class="literal">A</code> twice, Pig can run this script as a single
        MapReduce job by reading <code class="literal">A</code> once and writing two
        output files from the job, one for each of <code class="literal">B</code> and
        <code class="literal">C</code>. This feature is <a class="calibre" id="calibre_link-2727"></a>called <em class="calibre10">multiquery
        execution</em>.</p><p class="calibre2">In previous versions of Pig that did not have multiquery
        execution, each <code class="literal">STORE</code> statement in a script run in
        batch mode triggered execution, resulting in a job for each
        <code class="literal">STORE</code> statement. It is possible to restore the old
        behavior by disabling multiquery execution with the <code class="literal">-M</code> or <code class="literal">-no_multiquery</code> option to <code class="literal">pig</code>.</p></div><p class="calibre2">The physical plan that Pig prepares is a series of MapReduce jobs,
      which in local mode Pig runs in the local JVM and in MapReduce mode Pig
      runs on a Hadoop cluster.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">You can see the logical and physical plans created by Pig using
        the <code class="literal">EXPLAIN</code> command on a relation
        (<code class="literal">EXPLAIN max_temp;</code>, for
        example).</p><p class="calibre2"><code class="literal">EXPLAIN</code> will also show the MapReduce plan,
        which shows how the physical operators are grouped into MapReduce
        jobs. This is a good way to find out how many MapReduce jobs Pig will
        run for your query.</p></div><p class="calibre2">The relational operators that can be a part of a logical plan in
      Pig are summarized in <a class="ulink" href="#calibre_link-527" title="Table&nbsp;16-1.&nbsp;Pig Latin relational operators">Table&nbsp;16-1</a>. We go
      through the operators in more detail in <a class="ulink" href="#calibre_link-528" title="Data Processing Operators">Data Processing Operators</a>.</p><div class="table"><a id="calibre_link-527" class="calibre"></a><div class="table-title">Table&nbsp;16-1.&nbsp;Pig Latin relational operators</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Category</td><td class="calibre20">Operator</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td rowspan="3" class="calibre23">Loading and storing</td><td class="calibre23"><code class="uri">LOAD</code></td><td class="calibre25">Loads <a class="calibre" id="calibre_link-2341"></a>data from the filesystem or other storage into a
              relation</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">STORE</code></td><td class="calibre25">Saves a <a class="calibre" id="calibre_link-3532"></a>relation to the filesystem or other
              storage</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">DUMP</code> (<code class="uri">\d</code>)</td><td class="calibre25">Prints a <a class="calibre" id="calibre_link-1552"></a>relation to the console</td></tr><tr class="calibre26"><td rowspan="7" class="calibre23">Filtering</td><td class="calibre23"><code class="uri">FILTER</code></td><td class="calibre25">Removes <a class="calibre" id="calibre_link-1694"></a>unwanted rows from a relation</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">DISTINCT</code></td><td class="calibre25">Removes <a class="calibre" id="calibre_link-1516"></a>duplicate rows from a relation</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">FOREACH...GENERATE</code></td><td class="calibre25">Adds or <a class="calibre" id="calibre_link-1727"></a>removes fields to or from a relation</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">MAPREDUCE</code></td><td class="calibre25">Runs a <a class="calibre" id="calibre_link-2524"></a>MapReduce job using a relation as input</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">STREAM</code></td><td class="calibre25">Transforms <a class="calibre" id="calibre_link-3536"></a>a relation using an external program</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">SAMPLE</code></td><td class="calibre25">Selects a <a class="calibre" id="calibre_link-3262"></a>random sample of a relation</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">ASSERT</code></td><td class="calibre25">Ensures a <a class="calibre" id="calibre_link-921"></a>condition is true for all rows in a relation;
              otherwise, fails</td></tr><tr class="calibre19"><td rowspan="5" class="calibre23">Grouping and joining</td><td class="calibre23"><code class="uri">JOIN</code></td><td class="calibre25">Joins two <a class="calibre" id="calibre_link-2274"></a>or more relations</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">COGROUP</code></td><td class="calibre25">Groups the <a class="calibre" id="calibre_link-1167"></a>data in two or more relations</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">GROUP</code></td><td class="calibre25">Groups the <a class="calibre" id="calibre_link-1817"></a>data in a single relation</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">CROSS</code></td><td class="calibre25">Creates the <a class="calibre" id="calibre_link-1289"></a>cross product of two or more relations</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">CUBE</code></td><td class="calibre25">Creates <a class="calibre" id="calibre_link-1315"></a>aggregations for all combinations of specified
              columns in a relation</td></tr><tr class="calibre26"><td rowspan="3" class="calibre23">Sorting</td><td class="calibre23"><code class="uri">ORDER</code></td><td class="calibre25">Sorts a <a class="calibre" id="calibre_link-2852"></a>relation by one or more fields</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">RANK</code></td><td class="calibre25">Assign <a class="calibre" id="calibre_link-3133"></a>a rank to each tuple in a relation, optionally
              sorting by fields first</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">LIMIT</code></td><td class="calibre25">Limits <a class="calibre" id="calibre_link-2322"></a>the size of a relation to a maximum number of
              tuples</td></tr><tr class="calibre19"><td rowspan="2" class="calibre27">Combining and splitting</td><td class="calibre23"><code class="uri">UNION</code></td><td class="calibre25">Combines <a class="calibre" id="calibre_link-3739"></a>two or more relations into one</td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">SPLIT</code></td><td class="calibre28">Splits a <a class="calibre" id="calibre_link-3479"></a>relation into two or more relations</td></tr></tbody></table></div></div><p class="calibre2">There are other types of statements that are not added to the
      logical plan. For example, the diagnostic
      operators—<code class="literal">DESCRIBE</code>, <code class="literal">EXPLAIN</code>, and
      <code class="literal">ILLUSTRATE</code>—are provided to allow the user to interact
      with the logical plan for debugging purposes (see <a class="ulink" href="#calibre_link-529" title="Table&nbsp;16-2.&nbsp;Pig Latin diagnostic operators">Table&nbsp;16-2</a>). <code class="literal">DUMP</code> is a sort of
      diagnostic operator, too, since it is used only to allow interactive
      debugging of small result sets or in combination with
      <code class="literal">LIMIT</code> to retrieve a few rows from a larger relation.
      The <code class="literal">STORE</code> statement should be used when the size of
      the output is more than a few lines, as it writes to a file rather than
      to the console.</p><div class="table"><a id="calibre_link-529" class="calibre"></a><div class="table-title">Table&nbsp;16-2.&nbsp;Pig Latin diagnostic operators</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Operator (Shortcut)</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">DESCRIBE</code> (<code class="uri">\de</code>)</td><td class="calibre25">Prints a <a class="calibre" id="calibre_link-1432"></a>relation’s schema</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">EXPLAIN</code> (<code class="uri">\e</code>)</td><td class="calibre25">Prints the <a class="calibre" id="calibre_link-1589"></a>logical and physical plans</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">ILLUSTRATE</code> (<code class="uri">\i</code>)</td><td class="calibre28">Shows a <a class="calibre" id="calibre_link-2075"></a>sample execution of the logical plan, using a
              generated subset of the input</td></tr></tbody></table></div></div><p class="calibre2">Pig Latin also provides three
      statements—<code class="literal">REGISTER</code>, <code class="literal">DEFINE</code>, and
      <code class="literal">IMPORT</code>—that make it possible to incorporate macros
      and user-defined functions into Pig scripts (see <a class="ulink" href="#calibre_link-530" title="Table&nbsp;16-3.&nbsp;Pig Latin macro and UDF statements">Table&nbsp;16-3</a>).</p><div class="table"><a id="calibre_link-530" class="calibre"></a><div class="table-title">Table&nbsp;16-3.&nbsp;Pig Latin macro and UDF statements</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Statement</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">REGISTER</code></td><td class="calibre25">Registers a <a class="calibre" id="calibre_link-3209"></a>JAR file with the Pig runtime</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">DEFINE</code></td><td class="calibre25">Creates an <a class="calibre" id="calibre_link-1413"></a>alias for a macro, UDF, streaming script, or
              command specification</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">IMPORT</code></td><td class="calibre28">Imports <a class="calibre" id="calibre_link-2095"></a>macros defined in a separate file into a
              script</td></tr></tbody></table></div></div><p class="calibre2">Because they do not process relations, commands are not added to
      the logical plan; instead, they are executed immediately. Pig provides
      commands to interact with Hadoop filesystems (which are <a class="calibre" id="calibre_link-3016"></a><a class="calibre" id="calibre_link-1191"></a>very handy for moving data around before or after
      processing with Pig) and MapReduce, as well as a few utility commands
      (described in <a class="ulink" href="#calibre_link-531" title="Table&nbsp;16-4.&nbsp;Pig Latin commands">Table&nbsp;16-4</a>).</p><div class="table"><a id="calibre_link-531" class="calibre"></a><div class="table-title">Table&nbsp;16-4.&nbsp;Pig Latin commands</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Category</td><td class="calibre20">Command</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td rowspan="12" class="calibre23">Hadoop filesystem</td><td class="calibre23"><code class="uri">cat</code></td><td class="calibre25">Prints the <a class="calibre" id="calibre_link-1070"></a>contents of one or more files</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">cd</code></td><td class="calibre25">Changes<a class="calibre" id="calibre_link-1071"></a> the current directory</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">copyFromLocal</code></td><td class="calibre25">Copies a <a class="calibre" id="calibre_link-1256"></a>local file or directory to a Hadoop
              filesystem</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">copyToLocal</code></td><td class="calibre25">Copies a <a class="calibre" id="calibre_link-1257"></a>file or directory on a Hadoop filesystem to the
              local filesystem</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">cp</code></td><td class="calibre25">Copies a <a class="calibre" id="calibre_link-1277"></a>file or directory to another directory</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">fs</code></td><td class="calibre25">Accesses <a class="calibre" id="calibre_link-1731"></a>Hadoop’s filesystem shell</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">ls</code></td><td class="calibre25">Lists <a class="calibre" id="calibre_link-2370"></a>files</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mkdir</code></td><td class="calibre25">Creates a <a class="calibre" id="calibre_link-2702"></a>new directory</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mv</code></td><td class="calibre25">Moves a file <a class="calibre" id="calibre_link-2732"></a>or directory to another directory</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">pwd</code></td><td class="calibre25">Prints the <a class="calibre" id="calibre_link-3094"></a>path of the current working directory</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">rm</code></td><td class="calibre25">Deletes a <a class="calibre" id="calibre_link-3242"></a>file or directory</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">rmf</code></td><td class="calibre25">Forcibly <a class="calibre" id="calibre_link-3243"></a>deletes a file or directory (does not fail if the
              file or directory does not exist)</td></tr><tr class="calibre19"><td class="calibre23">Hadoop MapReduce</td><td class="calibre23"><code class="uri">kill</code></td><td class="calibre25">Kills a <a class="calibre" id="calibre_link-2307"></a>MapReduce job</td></tr><tr class="calibre26"><td rowspan="8" class="calibre27">Utility</td><td class="calibre23"><code class="uri">clear</code></td><td class="calibre25">Clears the <a class="calibre" id="calibre_link-1099"></a>screen in Grunt</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">exec</code></td><td class="calibre25">Runs a <a class="calibre" id="calibre_link-1585"></a>script in a new Grunt shell in batch mode</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">help</code></td><td class="calibre25">Shows <a class="calibre" id="calibre_link-1988"></a>the available commands and options</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">history</code></td><td class="calibre25">Prints <a class="calibre" id="calibre_link-1997"></a>the query statements run in the current Grunt
              session</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">quit</code> (<code class="uri">\q</code>)</td><td class="calibre25">Exits the <a class="calibre" id="calibre_link-3123"></a>interpreter</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">run</code></td><td class="calibre25">Runs a<a class="calibre" id="calibre_link-3254"></a> script within the existing Grunt shell</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">set</code></td><td class="calibre25">Sets <a class="calibre" id="calibre_link-3379"></a>Pig options and MapReduce job properties</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">sh</code></td><td class="calibre28">Runs a <a class="calibre" id="calibre_link-3384"></a>shell command from within Grunt</td></tr></tbody></table></div></div><p class="calibre2">The filesystem commands can operate on files or directories in any
      Hadoop filesystem, and they are very similar to the <code class="literal">hadoop fs</code> commands (which is not surprising,
      as both are simple wrappers around the Hadoop <code class="literal">FileSystem</code> interface). You can access all of
      the Hadoop filesystem shell commands using Pig’s <code class="literal">fs</code> command. For example, <code class="literal">fs -ls</code>
      will show a file listing, and <code class="literal">fs -help</code> will show help on all the
      available commands.</p><p class="calibre2">Precisely which Hadoop filesystem is used is determined by the
      <code class="literal">fs.defaultFS</code> property in the site
      file for Hadoop Core. See <a class="ulink" href="#calibre_link-532" title="The Command-Line Interface">The Command-Line Interface</a> for more details on how to
      configure this property.</p><p class="calibre2">These commands are mostly self-explanatory, except <code class="literal">set</code>, which is used to set options that control
      Pig’s behavior (including arbitrary MapReduce job properties). The
      <code class="literal">debug</code> option is used to turn debug
      logging on or off from within a script (you can also control the log
      level when launching Pig, using the <code class="literal">-d</code> or <code class="literal">-debug</code> option):</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">set debug on</code></strong></pre><p class="calibre2">Another useful option is the <code class="literal">job.name</code> option, which gives a Pig job a
      meaningful name, making it easier to pick out your Pig MapReduce jobs
      when running on a shared Hadoop cluster. If Pig is running a script
      (rather than operating as an interactive query from Grunt), its job name
      defaults to a value based on the script name.</p><p class="calibre2">There are two commands in <a class="ulink" href="#calibre_link-531" title="Table&nbsp;16-4.&nbsp;Pig Latin commands">Table&nbsp;16-4</a> for running
      a Pig script, <code class="literal">exec</code> and <code class="literal">run</code>. The difference is that <code class="literal">exec</code> runs the script in batch mode in a new
      Grunt shell, so any aliases defined in the script are not accessible to
      the shell after the script has completed. On the other hand, when
      running a script with <code class="literal">run</code>, it is as
      if the contents of the script had been entered manually, so the command
      history of the invoking shell contains all the statements from the
      script. Multiquery execution, where Pig executes a batch of statements
      in one go (see <a class="ulink" href="#calibre_link-533" title="Multiquery Execution">Multiquery Execution</a>), is used only
      <a class="calibre" id="calibre_link-3028"></a><a class="calibre" id="calibre_link-3521"></a>by <code class="literal">exec</code>, not <code class="literal">run</code>.</p><div class="sidebar"><div class="sidebar-title">Control Flow</div><p class="calibre2">By design, <a class="calibre" id="calibre_link-1251"></a><a class="calibre" id="calibre_link-3522"></a>Pig Latin lacks native control flow statements. The
        recommended approach for writing programs that have conditional logic
        or loop constructs is to embed Pig Latin in another language, such as
        Python, JavaScript, or Java, and manage the control flow from there.
        In this model, the host script uses a compile-bind-run API to execute
        Pig scripts and retrieve their status. Consult the Pig documentation
        for details of the API.</p><p class="calibre2">Embedded Pig programs always run in a JVM, so for Python and
        JavaScript you use the <code class="literal">pig</code> command
        followed by the name of your script, and the appropriate Java
        scripting engine will be selected (Jython for Python, Rhino for
        JavaScript).</p></div></div><div class="book" title="Expressions"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4478">Expressions</h3></div></div></div><p class="calibre2">An expression is something that <a class="calibre" id="calibre_link-3018"></a><a class="calibre" id="calibre_link-1595"></a>is evaluated to yield a value. Expressions can be used in
      Pig as a part of a statement <a class="calibre" id="calibre_link-3523"></a>containing a relational operator. Pig has a rich variety
      of expressions, many of which will be familiar from other programming
      languages. They are listed in <a class="ulink" href="#calibre_link-534" title="Table&nbsp;16-5.&nbsp;Pig Latin expressions">Table&nbsp;16-5</a>, with brief
      descriptions and examples. We will see examples of many of these
      expressions throughout the chapter.</p><div class="table"><a id="calibre_link-534" class="calibre"></a><div class="table-title">Table&nbsp;16-5.&nbsp;Pig Latin expressions</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Category</td><td class="calibre20">Expressions</td><td class="calibre20">Description</td><td class="calibre21">Examples</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">Constant</td><td class="calibre23">Literal</td><td class="calibre23">Constant value (see also the “Literal example” column in
              <a class="ulink" href="#calibre_link-535" title="Table&nbsp;16-6.&nbsp;Pig Latin types">Table&nbsp;16-6</a>)</td><td class="calibre25"><code class="uri">1.0</code>, <code class="uri">'a'</code></td></tr><tr class="calibre26"><td class="calibre23">Field (by <span class="calibre">position</span>)</td><td class="calibre23"><code class="uri">$<em class="replaceable"><code class="calibre44">n</code></em></code></td><td class="calibre23">Field in position <code class="uri"><em class="replaceable"><code class="calibre44">n</code></em></code>
              (zero-based)</td><td class="calibre25"><code class="uri">$0</code></td></tr><tr class="calibre19"><td class="calibre23">Field (by name)</td><td class="calibre23"><code class="uri"> <em class="replaceable"><code class="calibre44">f</code></em>
              </code></td><td class="calibre23">Field named <code class="uri"><em class="replaceable"><code class="calibre44">f</code></em></code></td><td class="calibre25"><code class="uri">year</code></td></tr><tr class="calibre26"><td class="calibre23">Field (disambiguate)</td><td class="calibre23"><code class="uri">
              <em class="replaceable"><code class="calibre44">r</code></em>::<em class="replaceable"><code class="calibre44">f</code></em>
              </code></td><td class="calibre23">Field named <code class="uri"><em class="replaceable"><code class="calibre44">f</code></em></code> from
              relation <code class="uri"><em class="replaceable"><code class="calibre44">r</code></em></code> after
              grouping or joining</td><td class="calibre25"><code class="uri">A::year</code></td></tr><tr class="calibre19"><td class="calibre23">Projection</td><td class="calibre23"><code class="uri"><em class="replaceable"><code class="calibre44">c</code></em>.$<em class="replaceable"><code class="calibre44">n</code></em></code>,
              <code class="uri"><em class="replaceable"><code class="calibre44">c</code></em>.<em class="replaceable"><code class="calibre44">f</code></em></code></td><td class="calibre23">Field in container <code class="uri"><em class="replaceable"><code class="calibre44">c</code></em></code>
              (relation, bag, or tuple) by position, by name</td><td class="calibre25"><code class="uri">records.$0</code>, <code class="uri">records.year</code></td></tr><tr class="calibre26"><td class="calibre23">Map lookup</td><td class="calibre23"><code class="uri"><em class="replaceable"><code class="calibre44">m</code></em>#<em class="replaceable"><code class="calibre44">k</code></em></code></td><td class="calibre23">Value associated with key <code class="uri"><em class="replaceable"><code class="calibre44">k</code></em></code> in map
              <code class="uri"><em class="replaceable"><code class="calibre44">m</code></em></code></td><td class="calibre25"><code class="uri">items#'Coat'</code></td></tr><tr class="calibre19"><td class="calibre23">Cast</td><td class="calibre23"><code class="uri">(<em class="replaceable"><code class="calibre44">t</code></em>)
              <em class="replaceable"><code class="calibre44">f</code></em></code></td><td class="calibre23">Cast of field <code class="uri"><em class="replaceable"><code class="calibre44">f</code></em></code> to type
              <code class="uri"><em class="replaceable"><code class="calibre44">t</code></em></code></td><td class="calibre25"><code class="uri">(int) year</code></td></tr><tr class="calibre26"><td rowspan="4" class="calibre23">Arithmetic</td><td class="calibre23"><code class="uri"><em class="replaceable"><code class="calibre44">x</code></em> +
              <em class="replaceable"><code class="calibre44">y</code></em></code>, <code class="uri"><em class="replaceable"><code class="calibre44">x</code></em> -
              <em class="replaceable"><code class="calibre44">y</code></em></code></td><td class="calibre23">Addition, subtraction</td><td class="calibre25"><code class="uri">$1 + $2</code>, <code class="uri">$1 - $2</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri"><em class="replaceable"><code class="calibre44">x</code></em> *
              <em class="replaceable"><code class="calibre44">y</code></em></code>, <code class="uri"><em class="replaceable"><code class="calibre44">x</code></em> /
              <em class="replaceable"><code class="calibre44">y</code></em></code></td><td class="calibre23">Multiplication, division</td><td class="calibre25"><code class="uri">$1 * $2</code>, <code class="uri">$1 / $2</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri"><em class="replaceable"><code class="calibre44">x</code></em> %
              <em class="replaceable"><code class="calibre44">y</code></em></code></td><td class="calibre23">Modulo, the remainder of <code class="uri"><em class="replaceable"><code class="calibre44">x</code></em></code> divided
              by <code class="uri"><em class="replaceable"><code class="calibre44">y</code></em></code></td><td class="calibre25"><code class="uri">$1 % $2</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">+<em class="replaceable"><code class="calibre44">x</code></em></code>,
              <code class="uri">-<em class="replaceable"><code class="calibre44">x</code></em></code></td><td class="calibre23">Unary positive, negation</td><td class="calibre25"><code class="uri">+1</code>, <code class="uri">–</code><code class="uri">1</code></td></tr><tr class="calibre26"><td rowspan="2" class="calibre23">Conditional</td><td class="calibre23"><code class="uri"><em class="replaceable"><code class="calibre44">x</code></em> ?
              <em class="replaceable"><code class="calibre44">y</code></em> :
              <em class="replaceable"><code class="calibre44">z</code></em></code></td><td class="calibre23">Bincond/ternary; <code class="uri"><em class="replaceable"><code class="calibre44">y</code></em></code> if
              <code class="uri"><em class="replaceable"><code class="calibre44">x</code></em></code>
              evaluates to true, <code class="uri"><em class="replaceable"><code class="calibre44">z</code></em></code>
              otherwise</td><td class="calibre25"><code class="uri">quality == 0 ? 0 :
              1</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">CASE</code></td><td class="calibre23">Multi-case conditional</td><td class="calibre25"><code class="uri">CASE q WHEN 0 THEN 'good' ELSE
              'bad' END</code></td></tr><tr class="calibre26"><td rowspan="6" class="calibre23">Comparison</td><td class="calibre23"><code class="uri"><em class="replaceable"><code class="calibre44">x</code></em> ==
              <em class="replaceable"><code class="calibre44">y</code></em></code>, <code class="uri"><em class="replaceable"><code class="calibre44">x</code></em> !=
              <em class="replaceable"><code class="calibre44">y</code></em></code></td><td class="calibre23">Equals, does not equal</td><td class="calibre25"><code class="uri">quality == 0</code>, <code class="uri">temperature != 9999</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri"><em class="replaceable"><code class="calibre44">x</code></em>
              &gt; <em class="replaceable"><code class="calibre44">y</code></em></code>, <code class="uri"><em class="replaceable"><code class="calibre44">x</code></em> &lt;
              <em class="replaceable"><code class="calibre44">y</code></em></code></td><td class="calibre23">Greater than, less than</td><td class="calibre25"><code class="uri">quality &gt; 0</code>,
              <code class="uri">quality &lt; 10</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri"><em class="replaceable"><code class="calibre44">x</code></em>
              &gt;= <em class="replaceable"><code class="calibre44">y</code></em></code>, <code class="uri"><em class="replaceable"><code class="calibre44">x</code></em> &lt;=
              <em class="replaceable"><code class="calibre44">y</code></em></code></td><td class="calibre23">Greater than or equal to, less than or equal to</td><td class="calibre25"><code class="uri">quality &gt;= 1</code>,
              <code class="uri">quality &lt;= 9</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri"><em class="replaceable"><code class="calibre44">x</code></em>
              matches <em class="replaceable"><code class="calibre44">y</code></em></code></td><td class="calibre23">Pattern matching with regular expression</td><td class="calibre25"><code class="uri">quality matches
              '[01459]'</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri"><em class="replaceable"><code class="calibre44">x</code></em> is
              null</code></td><td class="calibre23">Is <code class="uri">null</code></td><td class="calibre25"><code class="uri">temperature is
              null</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri"><em class="replaceable"><code class="calibre44">x</code></em> is
              not null</code></td><td class="calibre23">Is not <code class="uri">null</code></td><td class="calibre25"><code class="uri">temperature is not
              null</code></td></tr><tr class="calibre26"><td rowspan="4" class="calibre23">Boolean</td><td class="calibre23"><code class="uri"><em class="replaceable"><code class="calibre44">x</code></em> OR
              <em class="replaceable"><code class="calibre44">y</code></em></code></td><td class="calibre23">Logical <code class="uri">OR</code></td><td class="calibre25"><code class="uri">q == 0 OR q ==
              1</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri"><em class="replaceable"><code class="calibre44">x</code></em> AND
              <em class="replaceable"><code class="calibre44">y</code></em></code></td><td class="calibre23">Logical <code class="uri">AND</code></td><td class="calibre25"><code class="uri">q == 0 AND r ==
              0</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">NOT
              <em class="replaceable"><code class="calibre44">x</code></em></code></td><td class="calibre23">Logical negation</td><td class="calibre25"><code class="uri">NOT q matches
              '[01459]'</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">IN
              <em class="replaceable"><code class="calibre44">x</code></em></code></td><td class="calibre23">Set membership</td><td class="calibre25"><code class="uri">q IN (0, 1, 4, 5,
              9)</code></td></tr><tr class="calibre26"><td class="calibre23">Functional</td><td class="calibre23"><code class="uri"><em class="replaceable"><code class="calibre44">fn</code></em>(<em class="replaceable"><code class="calibre44">f1</code></em>,<em class="replaceable"><code class="calibre44">f2</code></em>,...)</code></td><td class="calibre23">Invocation of function <code class="uri"><em class="replaceable"><code class="calibre44">fn</code></em></code> on
              fields <code class="uri"><em class="replaceable"><code class="calibre44">f1</code></em></code>,
              <code class="uri"><em class="replaceable"><code class="calibre44">f2</code></em></code>,
              etc.</td><td class="calibre25"><code class="uri">isGood(quality)</code></td></tr><tr class="calibre19"><td class="calibre27">Flatten</td><td class="calibre27"><code class="uri">FLATTEN(<em class="replaceable"><code class="calibre44">f</code></em>)</code></td><td class="calibre27">Removal of a level of <a class="calibre" id="calibre_link-3019"></a><a class="calibre" id="calibre_link-1596"></a><a class="calibre" id="calibre_link-3524"></a>nesting from bags and tuples</td><td class="calibre28"><code class="uri">FLATTEN(group)</code></td></tr></tbody></table></div></div></div><div class="book" title="Types"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4479">Types</h3></div></div></div><p class="calibre2">So far you have seen some of <a class="calibre" id="calibre_link-3014"></a>the simple types in Pig, such as <code class="literal">int</code> and <code class="literal">chararray</code>. Here we will discuss Pig’s built-in
      types in more detail.</p><p class="calibre2">Pig has a <code class="literal">boolean</code> type and six
      numeric types: <code class="literal">int</code>, <code class="literal">long</code>, <code class="literal">float</code>, <code class="literal">double</code>, <code class="literal">biginteger</code>, and <code class="literal">bigdecimal</code>, which are identical to their Java
      counterparts. There is also a <code class="literal">byte</code><code class="literal">array</code> type, like Java’s <code class="literal">byte</code> array type for representing a blob of
      binary data, and <code class="literal">char</code><code class="literal">array</code>,
      which, like <code class="literal">java.lang.String</code>,
      represents textual data in UTF-16 format (although it can be loaded or
      stored in UTF-8 format). The <code class="literal">datetime</code>
      type is for storing a date and time with millisecond precision and
      including a time zone.</p><p class="calibre2">Pig does not have types corresponding to Java’s <code class="literal">byte</code>, <code class="literal">short</code>, or <code class="literal">char</code> primitive types. These are all easily
      represented using Pig’s <code class="literal">int</code> type, or
      <code class="literal">chararray</code> for <code class="literal">char</code>.</p><p class="calibre2">The Boolean, numeric, textual, binary, and temporal types are
      simple atomic types. Pig Latin also has three complex types for
      representing nested structures: <code class="literal">tuple</code>, <code class="literal">bag</code>,
      and <code class="literal">map</code>. All of Pig Latin’s types are
      listed in <a class="ulink" href="#calibre_link-535" title="Table&nbsp;16-6.&nbsp;Pig Latin types">Table&nbsp;16-6</a>.</p><div class="table"><a id="calibre_link-535" class="calibre"></a><div class="table-title">Table&nbsp;16-6.&nbsp;Pig Latin types</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Category</td><td class="calibre20">Type</td><td class="calibre20">Description</td><td class="calibre21">Literal example</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">Boolean</td><td class="calibre23"><code class="uri">boolean</code></td><td class="calibre23">True/false value</td><td class="calibre25"><code class="uri">true</code></td></tr><tr class="calibre26"><td rowspan="6" class="calibre23">Numeric</td><td class="calibre23"><code class="uri">int</code></td><td class="calibre23">32-bit signed integer</td><td class="calibre25"><code class="uri">1</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">long</code></td><td class="calibre23">64-bit signed integer</td><td class="calibre25"><code class="uri">1L</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">float</code></td><td class="calibre23">32-bit floating-point number</td><td class="calibre25"><code class="uri">1.0F</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">double</code></td><td class="calibre23">64-bit floating-point number</td><td class="calibre25"><code class="uri">1.0</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">biginteger</code></td><td class="calibre23">Arbitrary-precision integer</td><td class="calibre25"><code class="uri">'10000000000'</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">bigdecimal</code></td><td class="calibre23">Arbitrary-precision signed decimal number</td><td class="calibre25"><code class="uri">'0.110001000000000000000001'</code></td></tr><tr class="calibre26"><td class="calibre23">Text</td><td class="calibre23"><code class="uri">chararray</code></td><td class="calibre23">Character array in UTF-16 format</td><td class="calibre25"><code class="uri">'a'</code></td></tr><tr class="calibre19"><td class="calibre23">Binary</td><td class="calibre23"><code class="uri">bytearray</code></td><td class="calibre23">Byte array</td><td class="calibre25">Not supported</td></tr><tr class="calibre26"><td class="calibre23">Temporal</td><td class="calibre23"><code class="uri">datetime</code></td><td class="calibre23">Date and time with time zone</td><td class="calibre25">Not supported, use <code class="uri">ToDate</code> built-in function</td></tr><tr class="calibre19"><td rowspan="3" class="calibre27">Complex</td><td class="calibre23"><code class="uri">tuple</code></td><td class="calibre23">Sequence of fields of any type</td><td class="calibre25"><code class="uri">(1,'pomegranate')</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">bag</code></td><td class="calibre23">Unordered collection of tuples, possibly with
              duplicates</td><td class="calibre25"><code class="uri">{(1,'pomegranate'),(2)}</code></td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">map</code></td><td class="calibre27">Set of key-value pairs; keys must be character arrays,
              but values may be any type</td><td class="calibre28"><code class="uri">['a'#'pomegranate']</code></td></tr></tbody></table></div></div><p class="calibre2">The complex types are usually loaded from files or constructed
      using relational operators. Be aware, however, that the literal form in
      <a class="ulink" href="#calibre_link-535" title="Table&nbsp;16-6.&nbsp;Pig Latin types">Table&nbsp;16-6</a> is used when a constant value is
      created from within a Pig Latin program. The raw form in a file is
      usually different when using the standard <code class="literal">PigStorage</code> loader. For example, the
      representation in a file of the bag in <a class="ulink" href="#calibre_link-535" title="Table&nbsp;16-6.&nbsp;Pig Latin types">Table&nbsp;16-6</a>
      would be <code class="literal">{(1,pomegranate),(2)}</code> (note
      the lack of quotation marks), and with a suitable schema, this would be
      loaded as a relation with a single field and row, whose value was the
      bag.</p><p class="calibre2">Pig provides the built-in <a class="calibre" id="calibre_link-3020"></a><a class="calibre" id="calibre_link-1774"></a><a class="calibre" id="calibre_link-3710"></a><a class="calibre" id="calibre_link-3693"></a><a class="calibre" id="calibre_link-3699"></a>functions <code class="literal">TOTUPLE</code>,
      <code class="literal">TOBAG</code>, and <code class="literal">TOMAP</code>, which are used for turning expressions
      into tuples, bags, and maps.</p><p class="calibre2">Although relations and bags are conceptually the same (unordered
      collections of tuples), in practice Pig treats them slightly
      differently. A relation is a top-level construct, whereas a bag has to
      be contained in a relation. Normally you don’t have to worry about this,
      but there are a few restrictions that can trip up the uninitiated. For
      example, it’s not possible to create a relation from a bag literal. So,
      the following statement fails:</p><pre class="screen1">A = {(1,2),(3,4)}; -- Error</pre><p class="calibre2">The simplest workaround in this case is to load the data from a
      file using the <code class="literal">LOAD</code> statement.</p><p class="calibre2">As another example, you can’t treat a relation like a bag and
      project a field into a new relation (<code class="literal">$0</code> refers to the first field of
      <code class="literal">A</code>, using the positional notation):</p><pre class="screen1">B = A.$0;</pre><p class="calibre2">Instead, you have to use a relational operator to turn the
      relation <code class="literal">A</code> into relation <code class="literal">B</code>:</p><pre class="screen1">B <code class="o">=</code> <code class="k">FOREACH</code> A <code class="k">GENERATE</code> $0;</pre><p class="calibre2">It’s possible that a future version of Pig Latin will remove these
      inconsistencies and treat relations and bags in the <a class="calibre" id="calibre_link-3015"></a>same way.</p></div><div class="book" title="Schemas"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4480">Schemas</h3></div></div></div><p class="calibre2">A relation in Pig <a class="calibre" id="calibre_link-3025"></a><a class="calibre" id="calibre_link-3291"></a>may have an associated schema, which gives the fields in
      the relation names and types. We’ve seen how an <code class="literal">AS</code>
      clause in a <code class="literal">LOAD</code> statement is used to attach a schema
      to a relation:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">records = LOAD 'input/ncdc/micro-tab/sample.txt'</code></strong>
<code class="literal">&gt;&gt; </code><strong class="userinput"><code class="calibre9">  AS (year:int, temperature:int, quality:int);</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DESCRIBE records;</code></strong>
records: {year: int,temperature: int,quality: int}</pre><p class="calibre2">This time we’ve declared the year to be an integer rather than a
      <code class="literal">chararray</code>, even though the file it is
      being loaded from is the same. An integer may be more appropriate if we
      need to manipulate the year arithmetically (to turn it into a timestamp,
      for example), whereas the <code class="literal">chararray</code>
      representation might be more appropriate when it’s being used as a
      simple identifier. Pig’s flexibility in the degree to which schemas are
      declared contrasts with schemas in traditional SQL databases, which are
      declared before the data is loaded into the system. Pig is designed for
      analyzing plain input files with no associated type information, so it
      is quite natural to choose types for fields later than you would with an
      RDBMS.</p><p class="calibre2">It’s possible to omit type declarations completely, too:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">records = LOAD 'input/ncdc/micro-tab/sample.txt'</code></strong>
<code class="literal">&gt;&gt; </code><strong class="userinput"><code class="calibre9">  AS (year, temperature, quality);</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DESCRIBE records;</code></strong>
records: {year: bytearray,temperature: bytearray,quality: bytearray}</pre><p class="calibre2">In this case, we have specified only the names of the fields in
      the schema: <code class="literal">year</code>, <code class="literal">temperature</code>, and <code class="literal">quality</code>. The types default to <code class="literal">bytearray</code>, the most general type, representing
      a binary string.</p><p class="calibre2">You don’t need to specify types for every field; you can leave
      some to default to <code class="literal">byte</code><code class="literal">array</code>,
      as we have done for <code class="literal">year</code> in this
      declaration:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">records = LOAD 'input/ncdc/micro-tab/sample.txt'</code></strong>
<code class="literal">&gt;&gt; </code><strong class="userinput"><code class="calibre9">  AS (year, temperature:int, quality:int);</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DESCRIBE records;</code></strong>
records: {year: bytearray,temperature: int,quality: int}</pre><p class="calibre2">However, if you specify a schema in this way, you do need to
      specify every field. Also, there’s no way to specify the type of a field
      without specifying the name. On the other hand, the schema is entirely
      optional and can be omitted by not specifying an <code class="literal">AS</code>
      clause:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">records = LOAD 'input/ncdc/micro-tab/sample.txt';</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DESCRIBE records;</code></strong>
Schema for records unknown.</pre><p class="calibre2">Fields in a relation with no schema can be referenced using only
      positional notation: <code class="literal">$0</code> refers to the
      first field in a relation, <code class="literal">$1</code> to the
      second, and so on. Their types default to <code class="literal">bytearray</code>:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">projected_records = FOREACH records GENERATE $0, $1, $2;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP projected_records;</code></strong>
(1950,0,1)
(1950,22,1)
(1950,-11,1)
(1949,111,1)
(1949,78,1)
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DESCRIBE projected_records;</code></strong>
projected_records: {bytearray,bytearray,bytearray}</pre><p class="calibre2">Although it can be convenient not to assign types to fields
      (particularly in the first stages of writing a query), doing so can
      improve the clarity and efficiency of Pig Latin programs and is
      generally recommended.</p><div class="book" title="Using Hive tables with HCatalog"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-523">Using Hive tables with HCatalog</h4></div></div></div><p class="calibre2">Declaring a schema <a class="calibre" id="calibre_link-2007"></a><a class="calibre" id="calibre_link-1921"></a>as a part of the query is flexible but doesn’t lend
        itself to schema reuse. A set of Pig queries over the same input data
        will often have the same schema repeated in each query. If the query
        processes a large number of
        fields, this repetition can become hard to maintain.</p><p class="calibre2">HCatalog (which is a component of Hive) solves this problem by
        providing access to Hive’s metastore, so that Pig queries can
        reference schemas by name, rather than specifying them in full each
        time. For example, after running through <a class="ulink" href="#calibre_link-536" title="An Example">An Example</a> to load data into a Hive table called
        <code class="literal">records</code>, Pig can access the table’s
        schema and data as follows:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">pig -useHCatalog</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">records = LOAD 'records' USING org.apache.hcatalog.pig.HCatLoader();</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DESCRIBE records;</code></strong>
records: {year: chararray,temperature: int,quality: int}
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP records;</code></strong>
(1950,0,1)
(1950,22,1)
(1950,-11,1)
(1949,111,1)
(1949,78,1)</pre></div><div class="book" title="Validation and nulls"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-554">Validation and nulls</h4></div></div></div><p class="calibre2">A SQL database will enforce the constraints in a table’s schema
        at load time; for example, trying to load a string into a column that
        is declared to be a numeric type will fail. In Pig, if the value
        cannot be cast to the type declared in the schema, it will substitute
        a <code class="literal">null</code> value. Let’s see how this
        works when we have the following input for the weather data, which has
        an “e” character in place of an integer:</p><pre class="screen1">1950	0   1
1950	22  1
1950	e   1
1949	111 1
1949	78  1</pre><p class="calibre2">Pig handles the corrupt line by producing a <code class="literal">null</code> for the offending value, which is
        displayed as the absence of a value when dumped to screen (and also
        when saved using <code class="literal">STORE</code>):</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">records = LOAD 'input/ncdc/micro-tab/sample_corrupt.txt'</code></strong>
<code class="literal">&gt;&gt; </code><strong class="userinput"><code class="calibre9">  AS (year:chararray, temperature:int, quality:int);</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP records;</code></strong>
(1950,0,1)
(1950,22,1)
(1950,,1)
(1949,111,1)
(1949,78,1)</pre><p class="calibre2">Pig produces a warning for the invalid field (not shown here)
        but does not halt its processing. For large datasets, it is very
        common to have corrupt, invalid, or merely unexpected data, and it is
        generally infeasible to incrementally fix every unparsable record.
        Instead, we can pull out all of the invalid records in one go so we
        can take action on them, perhaps by fixing our program (because they
        indicate that we have made a mistake) or by filtering them out
        (because the data is genuinely unusable):</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">corrupt_records = FILTER records BY temperature is null;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP corrupt_records;</code></strong>
(1950,,1)</pre><p class="calibre2">Note the use of the <code class="literal">is null</code>
        operator, <a class="calibre" id="calibre_link-2152"></a>which is analogous to SQL. In practice, we would include
        more information from the original record, such as an identifier and
        the value that could not be parsed, to help our analysis of the bad
        data.</p><p class="calibre2">We can find the number of corrupt records using the following
        idiom for counting the number of rows in a relation:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">grouped = GROUP corrupt_records ALL;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">all_grouped = FOREACH grouped GENERATE group, COUNT(corrupt_records);</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP all_grouped;</code></strong>
(all,1)</pre><p class="calibre2">(<a class="ulink" href="#calibre_link-537" title="GROUP">GROUP</a> explains grouping and the
        <code class="literal">ALL</code> operation in more detail.)</p><p class="calibre2">Another useful technique is to use the <code class="literal">SPLIT</code>
        operator to partition the data into “good” and “bad” relations, which
        can then be analyzed separately:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">SPLIT records INTO good_records IF temperature is not null,</code></strong>
<code class="literal">&gt;&gt; </code><strong class="userinput"><code class="calibre9">  bad_records OTHERWISE;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP good_records;</code></strong>
(1950,0,1)
(1950,22,1)
(1949,111,1)
(1949,78,1)
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP bad_records;</code></strong>
(1950,,1)</pre><p class="calibre2">Going back to the case in which <code class="literal">temperature</code>’s type was left undeclared, the
        corrupt data cannot be detected easily, since it doesn’t surface as a
        <code class="literal">null</code>:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">records = LOAD 'input/ncdc/micro-tab/sample_corrupt.txt'</code></strong>
<code class="literal">&gt;&gt; </code><strong class="userinput"><code class="calibre9">  AS (year:chararray, temperature, quality:int);</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP records;</code></strong>
(1950,0,1)
(1950,22,1)
(1950,e,1)
(1949,111,1)
(1949,78,1)
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">filtered_records = FILTER records BY temperature != 9999 AND</code></strong>
<code class="literal">&gt;&gt; </code><strong class="userinput"><code class="calibre9">  quality IN (0, 1, 4, 5, 9);</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">grouped_records = GROUP filtered_records BY year;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">max_temp = FOREACH grouped_records GENERATE group,</code></strong>
<code class="literal">&gt;&gt; </code><strong class="userinput"><code class="calibre9">  MAX(filtered_records.temperature);</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP max_temp;</code></strong>
(1949,111.0)
(1950,22.0)</pre><p class="calibre2">What happens in this case is that the <code class="literal">temperature</code> field is interpreted as a
        <code class="literal">bytearray</code>, so the corrupt field is
        not detected when the input is loaded. When passed to <a class="calibre" id="calibre_link-2658"></a>the <code class="literal">MAX</code> function, the
        <code class="literal">temperature</code> field is cast to a
        <code class="literal">double</code>, since <code class="literal">MAX</code> works only with numeric types. The
        corrupt field cannot be represented as a <code class="literal">double</code>, so it becomes a <code class="literal">null</code>, which <code class="literal">MAX</code> silently ignores. The best approach is
        generally to declare types for your data on loading and look for
        missing or corrupt values in the relations themselves before you do
        your main processing.</p><p class="calibre2">Sometimes corrupt data shows up as smaller tuples because fields
        are simply missing. You can filter these out by using <a class="calibre" id="calibre_link-3413"></a>the <code class="literal">SIZE</code> function as
        follows:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">A = LOAD 'input/pig/corrupt/missing_fields';</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP A;</code></strong>
(2,Tie)
(4,Coat)
(3)
(1,Scarf)
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">B = FILTER A BY SIZE(TOTUPLE(*)) &gt; 1;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP B;</code></strong>
(2,Tie)
(4,Coat)
(1,Scarf)</pre></div><div class="book" title="Schema merging"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4481">Schema merging</h4></div></div></div><p class="calibre2">In Pig, you don’t declare the schema for every new relation in
        the data flow. In most cases, Pig can figure out the resulting schema
        for the output of a relational operation by considering the schema of
        the input relation.</p><p class="calibre2">How are schemas propagated to new relations? Some relational
        operators don’t change the schema, so the relation produced by the
        <code class="literal">LIMIT</code> operator (which restricts a relation to a
        maximum number of tuples), for example, has the same schema as the
        relation it operates on. For other operators, the situation is more
        complicated. <code class="literal">UNION</code>, for example, combines two or more relations
        into one and tries to merge the input relations’ schemas. If the
        schemas are incompatible, due to different types or number of fields,
        then the schema of the result of the <code class="literal">UNION</code> is
        unknown.</p><p class="calibre2">You can find out the schema for any relation in the data flow
        using the <code class="literal">DESCRIBE</code> operator. If you want to
        redefine the schema for a relation, you can use the
        <code class="literal">FOREACH...GENERATE</code> operator with
        <code class="literal">AS</code> clauses to define the schema for some or all of
        the fields of the input relation.</p><p class="calibre2">See <a class="ulink" href="#calibre_link-538" title="User-Defined Functions">User-Defined Functions</a> for a further
        discussion of <a class="calibre" id="calibre_link-3026"></a><a class="calibre" id="calibre_link-3292"></a>schemas.</p></div></div><div class="book" title="Functions"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4482">Functions</h3></div></div></div><p class="calibre2">Functions in Pig <a class="calibre" id="calibre_link-3021"></a><a class="calibre" id="calibre_link-1777"></a>come in four types:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Eval function</span></dt><dd class="calibre8"><p class="calibre2">A function that <a class="calibre" id="calibre_link-1577"></a>takes one or more expressions and returns another
            expression. An example of a built-in eval function is <code class="literal">MAX</code>, which returns the maximum value of
            the entries in a bag. Some eval functions are <em class="calibre10">aggregate
            functions</em>, which means they operate on a bag of data
            to produce a scalar value; <code class="literal">MAX</code>
            is an example of an aggregate function. Furthermore, many
            aggregate functions are <em class="calibre10">algebraic</em>, which
            means that the result of the function may be calculated
            incrementally. In MapReduce terms, algebraic functions make use of
            the combiner and are much more efficient to calculate (see <a class="ulink" href="#calibre_link-539" title="Combiner Functions">Combiner Functions</a>). <code class="literal">MAX</code> is an algebraic function, whereas a
            function to calculate the median of a collection of values is an
            example of a function that is not algebraic.</p></dd><dt class="calibre7"><span class="term">Filter function</span></dt><dd class="calibre8"><p class="calibre2">A special type of eval <a class="calibre" id="calibre_link-1691"></a>function that returns a logical Boolean result. As
            the name suggests, filter functions are used in the
            <code class="literal">FILTER</code> operator to remove unwanted rows. They
            can also be used in other relational operators that take Boolean
            conditions, and in general, in expressions using Boolean or
            conditional expressions. An example of a built-in filter
            <a class="calibre" id="calibre_link-2153"></a>function is <code class="literal">IsEmpty</code>, which tests whether a bag or a
            map contains any items.</p></dd><dt class="calibre7"><span class="term">Load function</span></dt><dd class="calibre8"><p class="calibre2">A function that specifies <a class="calibre" id="calibre_link-2337"></a>how to load data into a relation from external
            storage.</p></dd><dt class="calibre7"><span class="term">Store function</span></dt><dd class="calibre8"><p class="calibre2">A function that specifies <a class="calibre" id="calibre_link-3530"></a>how to save the contents of a relation to external
            storage. Often, load and store functions are implemented by the
            same type. For example, <code class="literal">PigStorage</code>, which loads data from
            delimited text files, can store data in the same format.</p></dd></dl></div><p class="calibre2">Pig comes with a collection of built-in functions, a selection of
      which are listed in <a class="ulink" href="#calibre_link-540" title="Table&nbsp;16-7.&nbsp;A selection of Pig’s built-in functions">Table&nbsp;16-7</a>. The complete
      list of built-in functions, which includes a large number of standard
      math, string, date/time, and collection functions, can be found in the
      documentation for each Pig release.</p><div class="table"><a id="calibre_link-540" class="calibre"></a><div class="table-title">Table&nbsp;16-7.&nbsp;A selection of Pig’s built-in functions</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Category</td><td class="calibre20">Function</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">Eval</td><td class="calibre23"><code class="uri">AVG</code></td><td class="calibre25">Calculates the <a class="calibre" id="calibre_link-1775"></a><a class="calibre" id="calibre_link-932"></a>average (mean) value of entries in a bag.</td></tr><tr class="calibre26"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">CONCAT</code></td><td class="calibre25">Concatenates <a class="calibre" id="calibre_link-1225"></a>byte arrays or character arrays together.</td></tr><tr class="calibre19"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">COUNT</code></td><td class="calibre25">Calculates <a class="calibre" id="calibre_link-1261"></a>the number of non-<code class="uri">null</code> entries
              in a bag.</td></tr><tr class="calibre26"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">COUNT_STAR</code></td><td class="calibre25">Calculates <a class="calibre" id="calibre_link-1276"></a>the number of entries in a bag, including those
              that are <code class="uri">null</code>.</td></tr><tr class="calibre19"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">DIFF</code></td><td class="calibre25">Calculates <a class="calibre" id="calibre_link-1492"></a>the set difference of two bags. If the two
              arguments are not bags, returns a bag containing both if they
              are equal; otherwise, returns an empty bag.</td></tr><tr class="calibre26"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">MAX</code></td><td class="calibre25">Calculates <a class="calibre" id="calibre_link-2659"></a>the maximum value of entries in a bag.</td></tr><tr class="calibre19"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">MIN</code></td><td class="calibre25">Calculates <a class="calibre" id="calibre_link-2698"></a>the minimum value of entries in a bag.</td></tr><tr class="calibre26"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">SIZE</code></td><td class="calibre25">Calculates <a class="calibre" id="calibre_link-3414"></a>the size of a type. The size of numeric types is
              always 1; for character arrays, it is the number of characters;
              for byte arrays, the number of bytes; and for containers (tuple,
              bag, map), it is the number of entries.</td></tr><tr class="calibre19"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">SUM</code></td><td class="calibre25">Calculates <a class="calibre" id="calibre_link-3564"></a>the sum of the values of entries in a bag.</td></tr><tr class="calibre26"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">TOBAG</code></td><td class="calibre25">Converts <a class="calibre" id="calibre_link-3694"></a>one or more expressions to individual tuples,
              which are then put in a bag. A synonym for <code class="uri">()</code>.</td></tr><tr class="calibre19"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">TOKENIZE</code></td><td class="calibre25">Tokenizes <a class="calibre" id="calibre_link-3697"></a>a character array into a bag of its constituent
              words.</td></tr><tr class="calibre26"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">TOMAP</code></td><td class="calibre25">Converts an <a class="calibre" id="calibre_link-3700"></a>even number of expressions to a map of key-value
              pairs. A synonym for <code class="uri">[]</code>.</td></tr><tr class="calibre19"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">TOP</code></td><td class="calibre25">Calculates the <a class="calibre" id="calibre_link-3705"></a>top <span class="calibre">n</span> tuples in
              a bag.</td></tr><tr class="calibre26"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">TOTUPLE</code></td><td class="calibre25">Converts one or <a class="calibre" id="calibre_link-3711"></a>more expressions to a tuple. A synonym for
              <code class="uri">{}</code>.</td></tr><tr class="calibre19"><td class="calibre23">Filter</td><td class="calibre23"><code class="uri">IsEmpty</code></td><td class="calibre25">Tests whether a <a class="calibre" id="calibre_link-2154"></a>bag or map is empty.</td></tr><tr class="calibre26"><td class="calibre23">Load/Store</td><td class="calibre23"><code class="uri">PigStorage</code></td><td class="calibre25">Loads or <a class="calibre" id="calibre_link-3034"></a>stores relations using a field-delimited text
              format. Each line is broken into fields using a configurable
              field delimiter (defaults to a tab character) to be stored in
              the tuple’s fields. It is the default storage when none is
              specified.<sup class="calibre5">[<a class="firstname" href="#calibre_link-541" id="calibre_link-542">a</a>]</sup></td></tr><tr class="calibre19"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">TextLoader</code></td><td class="calibre25">Loads <a class="calibre" id="calibre_link-3679"></a>relations from a plain-text format. Each line
              corresponds to a tuple whose single field is the line of
              text.</td></tr><tr class="calibre26"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">JsonLoader</code>, <code class="uri">JsonStorage</code></td><td class="calibre25">Loads or <a class="calibre" id="calibre_link-2289"></a><a class="calibre" id="calibre_link-2290"></a>stores relations from or to a (Pig-defined) JSON
              format. Each tuple is stored on one line.</td></tr><tr class="calibre19"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">AvroStorage</code></td><td class="calibre25">Loads or <a class="calibre" id="calibre_link-967"></a>stores relations from or to Avro
              datafiles.</td></tr><tr class="calibre26"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">ParquetLoader</code>,
              <code class="uri">ParquetStorer</code></td><td class="calibre25">Loads or <a class="calibre" id="calibre_link-2938"></a><a class="calibre" id="calibre_link-2940"></a>stores relations from or to Parquet files.</td></tr><tr class="calibre19"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">OrcStorage</code></td><td class="calibre25">Loads or <a class="calibre" id="calibre_link-2850"></a>stores relations from or to Hive ORCFiles.</td></tr><tr class="calibre26"><td class="calibre27">&nbsp;</td><td class="calibre27"><code class="uri">HBaseStorage</code></td><td class="calibre28">Loads or <a class="calibre" id="calibre_link-1920"></a>stores relations from or to <a class="calibre" id="calibre_link-1776"></a>HBase tables.</td></tr></tbody><tbody class="calibre22"><tr class="calibre19"><td colspan="3" class="calibre28"><div class="footnote1" id="calibre_link-541"><p class="calibre2"><sup class="calibre37">[<a class="firstname" href="#calibre_link-542">a</a>] </sup>The default storage can be changed by setting <code class="literal2">pig.default.load.func</code> and <code class="literal2">pig.default.store.func</code> to the
                  fully qualified load and store function classnames.</p></div></td></tr></tbody></table></div></div><div class="book" title="Other libraries"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4483">Other libraries</h4></div></div></div><p class="calibre2">If the function you need is <a class="calibre" id="calibre_link-1778"></a><a class="calibre" id="calibre_link-3733"></a>not available, you can write your own user-defined
        function (or UDF for short), as explained in <a class="ulink" href="#calibre_link-538" title="User-Defined Functions">User-Defined Functions</a>. Before
        you do that, however, have a look in the <a class="ulink" href="https://cwiki.apache.org/confluence/display/PIG/PiggyBank?" target="_top">Piggy
        Bank</a>, a library of Pig functions shared by the Pig community
        and distributed as a part of Pig. For example, there are load and
        store functions in the Piggy Bank for CSV files, Hive RCFiles,
        sequence files, and XML files. The Piggy Bank JAR file comes with Pig,
        and you can use it with no further configuration. Pig’s API
        documentation includes a list of functions provided by the Piggy
        Bank.</p><p class="calibre2"><a class="ulink" href="http://datafu.incubator.apache.org/" target="_top">Apache
        DataFu</a> is another rich library of Pig UDFs. In addition to
        general utility functions, it includes functions for computing basic
        statistics, performing sampling and estimation, hashing, and working
        with web data (sessionization, link <a class="calibre" id="calibre_link-3022"></a>analysis).</p></div></div><div class="book" title="Macros"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4484">Macros</h3></div></div></div><p class="calibre2">Macros provide a way <a class="calibre" id="calibre_link-3023"></a><a class="calibre" id="calibre_link-2381"></a>to package reusable pieces of Pig Latin code from within
      Pig Latin itself. For example, we can extract the part of our Pig Latin
      program that performs grouping on a relation and then finds the maximum
      value in each group by defining a macro as follows:</p><a id="calibre_link-4485" class="calibre"></a><pre class="screen1"><code class="k">DEFINE</code> <code class="nf">max_by_group</code><code class="p">(</code>X, group_key, max_field<code class="p">)</code> <code class="k">RETURNS</code> Y <code class="p">{</code>
  A <code class="o">=</code> <code class="k">GROUP</code> $X <code class="k">by</code> $group_key;
  $Y <code class="o">=</code> <code class="k">FOREACH</code> A <code class="k">GENERATE</code> <code class="k">group</code><code class="o">,</code> <code class="nb">MAX</code><code class="p">(</code>$X.$max_field<code class="p">);</code>
<code class="p">};</code></pre><p class="calibre2">The macro, called <code class="literal">max_by_group</code>,
      takes three parameters: a relation, <code class="literal">X</code>, and two field names, <code class="literal">group_key</code> and <code class="literal">max_field</code>. It returns a single relation,
      <code class="literal">Y</code>. Within the macro body, parameters
      and return aliases are referenced with a <code class="literal">$</code> prefix, such as <code class="literal">$X</code>.</p><p class="calibre2">The macro is used as follows:</p><a id="calibre_link-4486" class="calibre"></a><pre class="screen1">records <code class="o">=</code> <code class="k">LOAD</code> <code class="sb">'input/ncdc/micro-tab/sample.txt'</code>
  <code class="k">AS</code> <code class="p">(</code>year:<code class="kt">chararray</code><code class="o">,</code> temperature:<code class="kt">int</code><code class="o">,</code> quality:<code class="kt">int</code><code class="p">);</code>
filtered_records <code class="o">=</code> <code class="k">FILTER</code> records <code class="k">BY</code> temperature <code class="o">!=</code> <code class="mi">9999</code> <code class="k">AND</code>
  quality <code class="nf">IN</code> <code class="p">(</code><code class="mi">0</code><code class="o">,</code> <code class="mi">1</code><code class="o">,</code> <code class="mi">4</code><code class="o">,</code> <code class="mi">5</code><code class="o">,</code> <code class="mi">9</code><code class="p">);</code>
<span class="calibre24"><strong class="calibre9">max_temp <code class="o1">=</code> <code class="nf1">max_by_group</code><code class="p1">(</code>filtered_records, year, temperature<code class="p1">);</code></strong></span>
<code class="k">DUMP</code> max_temp</pre><p class="calibre2">At runtime, Pig will expand the macro using the macro definition.
      After expansion, the program looks like the following, with the expanded
      section in bold:</p><a id="calibre_link-4487" class="calibre"></a><pre class="screen1">records <code class="o">=</code> <code class="k">LOAD</code> <code class="sb">'input/ncdc/micro-tab/sample.txt'</code>
  <code class="k">AS</code> <code class="p">(</code>year:<code class="kt">chararray</code><code class="o">,</code> temperature:<code class="kt">int</code><code class="o">,</code> quality:<code class="kt">int</code><code class="p">);</code>
filtered_records <code class="o">=</code> <code class="k">FILTER</code> records <code class="k">BY</code> temperature <code class="o">!=</code> <code class="mi">9999</code> <code class="k">AND</code>
  quality <code class="nf">IN</code> <code class="p">(</code><code class="mi">0</code><code class="o">,</code> <code class="mi">1</code><code class="o">,</code> <code class="mi">4</code><code class="o">,</code> <code class="mi">5</code><code class="o">,</code> <code class="mi">9</code><code class="p">);</code>
<span class="calibre24"><strong class="calibre9">macro_max_by_group_A_0 <code class="o1">=</code> <code class="kc">GROUP</code> filtered_records <code class="kc">by</code> <code class="p1">(</code>year<code class="p1">);</code></strong></span>
<span class="calibre24"><strong class="calibre9">max_temp <code class="o1">=</code> <code class="kc">FOREACH</code> macro_max_by_group_A_0 <code class="kc">GENERATE</code> <code class="kc">group</code><code class="o1">,</code></strong></span>
<span class="calibre24"><strong class="calibre9">  <code class="nb1">MAX</code><code class="p1">(</code>filtered_records.<code class="p1">(</code>temperature<code class="p1">));</code></strong></span>
<code class="k">DUMP</code> max_temp</pre><p class="calibre2">Normally you don’t see the expanded form, because Pig creates it
      internally; however, in some cases it is useful to see it when writing
      and debugging macros. You can get Pig to perform macro expansion only
      (without executing the script) by passing the <code class="literal">-dryrun</code> argument to <code class="literal">pig</code>.</p><p class="calibre2">Notice that the parameters that were passed to the macro (<code class="literal">filtered_records</code>, <code class="literal">year</code>, and <code class="literal">temperature</code>) have been substituted for the
      names in the macro definition. Aliases in the macro definition that
      don’t have a <code class="literal">$</code> prefix, such as
      <code class="literal">A</code> in this example, are local to the
      macro definition and are rewritten at expansion time to avoid conflicts
      with aliases in other parts of the program. In this case, <code class="literal">A</code> becomes <code class="literal">macro_max_by_group_A_0</code> in the expanded
      form.</p><p class="calibre2">To foster reuse, macros can be defined in separate files to Pig
      scripts, in which case they need to be imported into any script that
      uses them. An import statement looks like <a class="calibre" id="calibre_link-3024"></a><a class="calibre" id="calibre_link-2382"></a>this:</p><a id="calibre_link-4488" class="calibre"></a><pre class="screen1"><code class="k">IMPORT</code> <code class="sb">'./ch16-pig/src/main/pig/max_temp.macro'</code><code class="p">;</code></pre></div></div><div class="book" title="User-Defined Functions"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-538">User-Defined Functions</h2></div></div></div><p class="calibre2">Pig’s designers realized that <a class="calibre" id="calibre_link-3008"></a><a class="calibre" id="calibre_link-3734"></a><a class="calibre" id="calibre_link-1779"></a>the ability to plug in custom code is crucial for all but
    the most trivial data processing jobs. For this reason, they made it easy
    to define and use user-defined functions. We only cover Java UDFs in this
    section, but be aware that you can also write UDFs in Python, JavaScript,
    Ruby, or Groovy, all of which are run using the Java Scripting API.</p><div class="book" title="A Filter UDF"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4489">A Filter UDF</h3></div></div></div><p class="calibre2">Let’s demonstrate <a class="calibre" id="calibre_link-1692"></a>by writing a filter function for filtering out weather
      records that do not have a temperature quality reading of satisfactory
      (or better). The idea is to change this line:</p><a id="calibre_link-4490" class="calibre"></a><pre class="screen1">filtered_records <code class="o">=</code> <code class="k">FILTER</code> records <code class="k">BY</code> temperature <code class="o">!=</code> <code class="mi">9999</code> <code class="k">AND</code>
  quality <code class="nf">IN</code> <code class="p">(</code><code class="mi">0</code><code class="o">,</code> <code class="mi">1</code><code class="o">,</code> <code class="mi">4</code><code class="o">,</code> <code class="mi">5</code><code class="o">,</code> <code class="mi">9</code><code class="p">);</code></pre><p class="calibre2">to:</p><a id="calibre_link-4491" class="calibre"></a><pre class="screen1">filtered_records <code class="o">=</code> <code class="k">FILTER</code> records <code class="k">BY</code> temperature <code class="o">!=</code> <code class="mi">9999</code> <code class="k">AND</code> <code class="nf">isGood</code><code class="p">(</code>quality<code class="p">);</code></pre><p class="calibre2">This achieves two things: it makes the Pig script a little more
      concise, and it encapsulates the logic in one place so that it can be
      easily reused in other scripts. If we were just writing an ad hoc query,
      we probably wouldn’t bother to write a UDF. It’s when you start doing
      the same kind of processing over and over again that you see
      opportunities for reusable UDFs.</p><p class="calibre2">Filter UDFs are all subclasses of <code class="literal">FilterFunc</code>, which<a class="calibre" id="calibre_link-1697"></a> itself is a subclass <a class="calibre" id="calibre_link-1580"></a>of <code class="literal">EvalFunc</code>. We’ll look
      at <code class="literal">EvalFunc</code> in more detail later, but
      for the moment just note that, in essence, <code class="literal">EvalFunc</code> looks like the following
      class:</p><a id="calibre_link-4492" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">abstract</code> <code class="k">class</code> <code class="nc">EvalFunc</code><code class="o">&lt;</code><code class="n">T</code><code class="o">&gt;</code> <code class="o">{</code>
  <code class="k">public</code> <code class="k">abstract</code> <code class="n">T</code> <code class="nf">exec</code><code class="o">(</code><code class="n">Tuple</code> <code class="n">input</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">;</code>
<code class="o">}</code></pre><p class="calibre2"><code class="literal">EvalFunc</code>’s only abstract
      method, <code class="literal">exec()</code>, takes a tuple and returns a
      single value, the (parameterized) type <code class="literal">T</code>. The fields in the input tuple consist of
      the expressions passed to the function—in this case, a single integer.
      For <code class="literal">FilterFunc</code>, <code class="literal">T</code> is <code class="literal">Boolean</code>, so the method should return <code class="literal">true</code> only for those tuples that should not be
      filtered out.</p><p class="calibre2">For the quality filter, we write a class, <code class="literal">IsGoodQuality</code>, that extends <code class="literal">FilterFunc</code> and implements the
      <code class="literal">exec()</code> method (see <a class="ulink" href="#calibre_link-543" title="Example&nbsp;16-1.&nbsp;A FilterFunc UDF to remove records with unsatisfactory temperature quality readings">Example&nbsp;16-1</a>). The <code class="literal">Tuple</code> class is essentially a list of objects
      with associated types. Here we are concerned only with the first field
      (since the function only has a single argument), which we extract by
      index using the <code class="literal">get()</code> method on <code class="literal">Tuple</code>. The field is an integer, so if it’s not
      <code class="literal">null</code>, we cast it and check whether
      the value is one that signifies the temperature was a good reading,
      returning the appropriate value, <code class="literal">true</code>
      or <code class="literal">false</code>.</p><div class="example"><a id="calibre_link-543" class="calibre"></a><div class="example-title">Example&nbsp;16-1.&nbsp;A FilterFunc UDF to remove records with unsatisfactory
        temperature quality readings</div><div class="book"><pre class="screen"><code class="k">package</code> <code class="n">com</code><code class="o">.</code><code class="na">hadoopbook</code><code class="o">.</code><code class="na">pig</code><code class="o">;</code>

<code class="k">import</code> <code class="nn">java.io.IOException</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">java.util.ArrayList</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">java.util.List</code><code class="o">;</code>

<code class="k">import</code> <code class="nn">org.apache.pig.FilterFunc</code><code class="o">;</code>
 
<code class="k">import</code> <code class="nn">org.apache.pig.backend.executionengine.ExecException</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.pig.data.DataType</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.pig.data.Tuple</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.pig.impl.logicalLayer.FrontendException</code><code class="o">;</code>

<code class="k">public</code> <code class="k">class</code> <code class="nc">IsGoodQuality</code> <code class="k">extends</code> <code class="n">FilterFunc</code> <code class="o">{</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="n">Boolean</code> <code class="nf">exec</code><code class="o">(</code><code class="n">Tuple</code> <code class="n">tuple</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">tuple</code> <code class="o">==</code> <code class="k">null</code> <code class="o">||</code> <code class="n">tuple</code><code class="o">.</code><code class="na">size</code><code class="o">()</code> <code class="o">==</code> <code class="mi">0</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">return</code> <code class="k">false</code><code class="o">;</code>
    <code class="o">}</code>
    <code class="k">try</code> <code class="o">{</code>
      <code class="n">Object</code> <code class="n">object</code> <code class="o">=</code> <code class="n">tuple</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="mi">0</code><code class="o">);</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">object</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
        <code class="k">return</code> <code class="k">false</code><code class="o">;</code>
      <code class="o">}</code>
      <code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="o">(</code><code class="n">Integer</code><code class="o">)</code> <code class="n">object</code><code class="o">;</code>
      <code class="k">return</code> <code class="n">i</code> <code class="o">==</code> <code class="mi">0</code> <code class="o">||</code> <code class="n">i</code> <code class="o">==</code> <code class="mi">1</code> <code class="o">||</code> <code class="n">i</code> <code class="o">==</code> <code class="mi">4</code> <code class="o">||</code> <code class="n">i</code> <code class="o">==</code> <code class="mi">5</code> <code class="o">||</code> <code class="n">i</code> <code class="o">==</code> <code class="mi">9</code><code class="o">;</code>
    <code class="o">}</code> <code class="k">catch</code> <code class="o">(</code><code class="n">ExecException</code> <code class="n">e</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">throw</code> <code class="k">new</code> <code class="nf">IOException</code><code class="o">(</code><code class="n">e</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
 
<code class="o">}</code></pre></div></div><p class="calibre2">To use the new function, we first compile it and package it in a
      JAR file (the example code that accompanies this book comes with build
      instructions for how to do this). Then we tell Pig about the JAR file
      with the <code class="literal">REGISTER</code> operator, which is given the local
      path to the filename (and is <span class="calibre">not</span>
      enclosed in quotes):</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">REGISTER pig-examples.jar;</code></strong></pre><p class="calibre2">Finally, we can invoke the function:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">filtered_records = FILTER records BY temperature != 9999 AND</code></strong>
<code class="literal">&gt;&gt; </code><strong class="userinput"><code class="calibre9">  com.hadoopbook.pig.IsGoodQuality(quality);</code></strong></pre><p class="calibre2">Pig resolves function calls by treating the function’s name as a
      Java classname and attempting to load a class of that name. (This,
      incidentally, is why function names are case sensitive: because Java
      classnames are.) When searching for classes, Pig uses a classloader that
      includes the JAR files that have been registered. When running in
      distributed mode, Pig will ensure that your JAR files get shipped to the
      cluster.</p><p class="calibre2">For the UDF in this example, Pig looks for a class with the name
      <code class="literal">com.hadoopbook.pig.IsGoodQuality</code>,
      which it finds in the JAR file we registered.</p><p class="calibre2">Resolution of built-in functions proceeds in the same way, except
      for one difference: Pig has a set of built-in package names that it
      searches, so the function call does not have to be a fully qualified
      name. For example, the function <code class="literal">MAX</code>
      is actually implemented by a class <code class="literal">MAX</code> in the <a class="calibre" id="calibre_link-2870"></a>package <code class="literal">org.apache.pig.builtin</code>. This is one of the
      packages that Pig looks in, so we can write <code class="literal">MAX</code> rather than <code class="literal">org.apache.pig.builtin.MAX</code> in our Pig
      programs.</p><p class="calibre2">We can add our package name to the search path by invoking Grunt
      with this command-line argument: <code class="literal">-Dudf.import.list=com.hadoopbook.pig</code>.
      Alternatively, we can shorten the function name <a class="calibre" id="calibre_link-881"></a>by defining an
      alias, using the <code class="literal">DEFINE</code> <a class="calibre" id="calibre_link-1414"></a>operator:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DEFINE isGood com.hadoopbook.pig.IsGoodQuality();</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">filtered_records = FILTER records BY temperature != 9999 AND</code></strong>
<code class="literal">&gt;&gt; </code><strong class="userinput"><code class="calibre9">  isGood(quality);</code></strong></pre><p class="calibre2">Defining an alias is a good idea if you want to use the function
      several times in the same script. It’s also necessary if you want to
      pass arguments to the constructor of the UDF’s implementation
      class.</p><div class="note" title="Tip"><h3 class="title3">Tip</h3><p class="calibre2">If you add the lines to register JAR files and define function
        aliases to the <em class="calibre10">.pigbootup</em> file in
        your home directory, they will be run whenever you start Pig.</p></div><div class="book" title="Leveraging types"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4493">Leveraging types</h4></div></div></div><p class="calibre2">The filter works when the quality field is declared to be of
        type <code class="literal">int</code>, but if the type
        information is absent, the UDF fails! This happens because the field
        is the default type, <code class="literal">bytearray</code>,
        represented by the <code class="literal">DataByteArray</code>
        class. Because <code class="literal">DataByteArray</code> is not
        an <code class="literal">Integer</code>, the cast fails.</p><p class="calibre2">The obvious way to fix this is to convert the field to an
        integer in the <code class="literal">exec()</code> method.
        However, there is a better way, which is to tell Pig the types of the
        fields that the function expects. The
        <code class="literal">getArgToFuncMapping()</code> method on <code class="literal">EvalFunc</code> is provided for precisely this
        reason. We can override it to tell Pig that the first field should be
        an integer:</p><a id="calibre_link-4494" class="calibre"></a><pre class="screen1">  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="n">List</code><code class="o">&lt;</code><code class="n">FuncSpec</code><code class="o">&gt;</code> <code class="nf">getArgToFuncMapping</code><code class="o">()</code> <code class="k">throws</code> <code class="n">FrontendException</code> <code class="o">{</code>
    <code class="n">List</code><code class="o">&lt;</code><code class="n">FuncSpec</code><code class="o">&gt;</code> <code class="n">funcSpecs</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ArrayList</code><code class="o">&lt;</code><code class="n">FuncSpec</code><code class="o">&gt;();</code>
    <code class="n">funcSpecs</code><code class="o">.</code><code class="na">add</code><code class="o">(</code><code class="k">new</code> <code class="n">FuncSpec</code><code class="o">(</code><code class="k">this</code><code class="o">.</code><code class="na">getClass</code><code class="o">().</code><code class="na">getName</code><code class="o">(),</code>
        <code class="k">new</code> <code class="nf">Schema</code><code class="o">(</code><code class="k">new</code> <code class="n">Schema</code><code class="o">.</code><code class="na">FieldSchema</code><code class="o">(</code><code class="k">null</code><code class="o">,</code> <code class="n">DataType</code><code class="o">.</code><code class="na">INTEGER</code><code class="o">))));</code>

    <code class="k">return</code> <code class="n">funcSpecs</code><code class="o">;</code>
  <code class="o">}</code></pre><p class="calibre2">This method returns a <code class="literal">FuncSpec</code> object <a class="calibre" id="calibre_link-1762"></a>corresponding to each of the fields of the tuple that
        are passed to the <code class="literal">exec()</code> method. Here there
        is a single field, and we construct an anonymous <code class="literal">FieldSchema</code> (the name is passed as <code class="literal">null</code>, since Pig ignores the name when doing
        type conversion). The type is specified using the <code class="literal">INTEGER</code> constant on Pig’s <code class="literal">DataType</code> class.</p><p class="calibre2">With the amended function, Pig will attempt to convert the
        argument passed to the function to an integer. If the field cannot be
        converted, then a <code class="literal">null</code> is passed
        for the field. The <code class="literal">exec()</code> method always
        returns <code class="literal">false</code> when the field is
          <code class="literal">null</code>. For this application, this
        behavior is appropriate, as we want to filter out records whose
        quality field is <a class="calibre" id="calibre_link-1693"></a>unintelligible.</p></div></div><div class="book" title="An Eval UDF"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4495">An Eval UDF</h3></div></div></div><p class="calibre2">Writing an eval function is a <a class="calibre" id="calibre_link-1578"></a>small step up from writing a filter function. Consider the
      UDF in <a class="ulink" href="#calibre_link-544" title="Example&nbsp;16-2.&nbsp;An EvalFunc UDF to trim leading and trailing whitespace from chararray values">Example&nbsp;16-2</a>, which trims the leading and trailing
      whitespace from <code class="literal">chararray</code> values using the
      <code class="literal">trim()</code> method on <code class="literal">java.lang.String</code>.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-545" id="calibre_link-561">101</a>]</sup></p><div class="example"><a id="calibre_link-544" class="calibre"></a><div class="example-title">Example&nbsp;16-2.&nbsp;An EvalFunc UDF to trim leading and trailing whitespace from
        chararray values</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">Trim</code> <code class="k">extends</code> <code class="n">PrimitiveEvalFunc</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;</code> <code class="o">{</code>
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="n">String</code> <code class="nf">exec</code><code class="o">(</code><code class="n">String</code> <code class="n">input</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">return</code> <code class="n">input</code><code class="o">.</code><code class="na">trim</code><code class="o">();</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">In this case, we have taken advantage <a class="calibre" id="calibre_link-3059"></a>of <code class="literal">PrimitiveEvalFunc</code>, which is a
      specialization of <code class="literal">EvalFunc</code> for when the input is
      a single primitive (atomic) type. For the <code class="literal">Trim</code> UDF, the input and output types are both
      of type <code class="literal">String</code>.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-546" id="calibre_link-562">102</a>]</sup></p><p class="calibre2">In general, when you write an eval function, you need to consider
      what the output’s schema looks like. In the following statement, the
      schema of <code class="literal">B</code> is determined by the function <code class="literal">udf</code>:</p><pre class="screen1">B <code class="o">=</code> <code class="k">FOREACH</code> A <code class="k">GENERATE</code> <code class="nf">udf</code><code class="p">(</code>$0<code class="p">);</code></pre><p class="calibre2">If <code class="literal">udf</code> creates tuples with
      scalar fields, then Pig can determine <code class="literal">B</code>’s schema
      through reflection. For complex types such as bags, tuples, or maps, Pig
      needs more help, and you should implement the
      <code class="literal">outputSchema()</code> method to give Pig the
      information about the output schema.</p><p class="calibre2">The <code class="literal">Trim</code> UDF returns a string,
      which Pig translates as a <code class="literal">chararray</code>,
      as can be seen from the following session:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP A;</code></strong>
( pomegranate)
(banana  )
(apple)
(  lychee )
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DESCRIBE A;</code></strong>
A: {fruit: chararray}
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">B = FOREACH A GENERATE com.hadoopbook.pig.Trim(fruit);</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP B;</code></strong>
(pomegranate)
(banana)
(apple)
(lychee)
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DESCRIBE B;</code></strong>
B: {chararray}</pre><p class="calibre2"><code class="literal">A</code> has <code class="literal">chararray</code> fields that have leading and
      trailing spaces. We create <code class="literal">B</code> from
      <code class="literal">A</code> by applying the <code class="literal">Trim</code> function to the first field in
      <code class="literal">A</code> (named <code class="literal">fruit</code>).
      <code class="literal">B</code>’s fields are correctly inferred to be of type
      <code class="literal">chararray</code>.</p><div class="book" title="Dynamic invokers"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4496">Dynamic invokers</h4></div></div></div><p class="calibre2">Sometimes you want to use a function that is provided by a Java
        library, but without going to the effort of writing a UDF. Dynamic
        invokers allow you to do this by calling Java methods directly from a
        Pig script. The trade-off is that method calls are made via
        reflection, which can impose significant overhead when calls are made
        for every record in a large dataset. So for scripts that are run
        repeatedly, a dedicated UDF is normally preferred.</p><p class="calibre2">The following snippet shows how we could define and use a
        <code class="literal">trim</code> UDF that uses the Apache Commons<a class="calibre" id="calibre_link-3561"></a> Lang <code class="literal">StringUtils</code>
        class:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DEFINE trim InvokeForString('org.apache.commons.lang.StringUtils.trim',</code></strong>
<code class="literal">&gt;&gt; </code><strong class="userinput"><code class="calibre9">  'String');</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">B = FOREACH A GENERATE trim(fruit);</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP B;</code></strong>
(pomegranate)
(banana)
(apple)
(lychee)</pre><p class="calibre2">The <code class="literal">InvokeForString</code>
        invoker<a class="calibre" id="calibre_link-2145"></a><a class="calibre" id="calibre_link-2143"></a><a class="calibre" id="calibre_link-2144"></a><a class="calibre" id="calibre_link-2141"></a><a class="calibre" id="calibre_link-2142"></a> is used because the return type of the method is a
        <code class="literal">String</code>. (There are also <code class="literal">InvokeForInt</code>, <code class="literal">InvokeForLong</code>, <code class="literal">InvokeForDouble</code>, and <code class="literal">InvokeForFloat</code> invokers.) The first argument
        to the invoker constructor is the fully qualified method to be
        invoked. The second is a space-separated list of the method argument
        <a class="calibre" id="calibre_link-1579"></a>classes.</p></div></div><div class="book" title="A Load UDF"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4497">A Load UDF</h3></div></div></div><p class="calibre2">We’ll demonstrate a <a class="calibre" id="calibre_link-2338"></a>custom load function that can read plain-text column
      ranges as fields, very much like the Unix <code class="literal">cut</code> command.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-547" id="calibre_link-563">103</a>]</sup> It is used as follows:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">records = LOAD 'input/ncdc/micro/sample.txt'</code></strong>
<code class="literal">&gt;&gt; </code><strong class="userinput"><code class="calibre9">  USING com.hadoopbook.pig.CutLoadFunc('16-19,88-92,93-93')</code></strong>
<code class="literal">&gt;&gt; </code><strong class="userinput"><code class="calibre9">  AS (year:int, temperature:int, quality:int);</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP records;</code></strong>
(1950,0,1)
(1950,22,1)
(1950,-11,1)
(1949,111,1)
(1949,78,1)</pre><p class="calibre2">The string passed to <code class="literal">CutLoadFunc</code> is the column specification; each
      comma-separated range defines a field, which is assigned a name and type
      in the <code class="literal">AS</code> clause. Let’s examine the implementation of
      <code class="literal">CutLoadFunc</code>, shown in <a class="ulink" href="#calibre_link-548" title="Example&nbsp;16-3.&nbsp;A LoadFunc UDF to load tuple fields as column ranges">Example&nbsp;16-3</a>.</p><div class="example"><a id="calibre_link-548" class="calibre"></a><div class="example-title">Example&nbsp;16-3.&nbsp;A LoadFunc UDF to load tuple fields as column ranges</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">CutLoadFunc</code> <code class="k">extends</code> <code class="n">LoadFunc</code> <code class="o">{</code>

  <code class="k">private</code> <code class="k">static</code> <code class="k">final</code> <code class="n">Log</code> <code class="n">LOG</code> <code class="o">=</code> <code class="n">LogFactory</code><code class="o">.</code><code class="na">getLog</code><code class="o">(</code><code class="n">CutLoadFunc</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>

  <code class="k">private</code> <code class="k">final</code> <code class="n">List</code><code class="o">&lt;</code><code class="n">Range</code><code class="o">&gt;</code> <code class="n">ranges</code><code class="o">;</code>
  <code class="k">private</code> <code class="k">final</code> <code class="n">TupleFactory</code> <code class="n">tupleFactory</code> <code class="o">=</code> <code class="n">TupleFactory</code><code class="o">.</code><code class="na">getInstance</code><code class="o">();</code>
  <code class="k">private</code> <code class="n">RecordReader</code> <code class="n">reader</code><code class="o">;</code>

  <code class="k">public</code> <code class="nf">CutLoadFunc</code><code class="o">(</code><code class="n">String</code> <code class="n">cutPattern</code><code class="o">)</code> <code class="o">{</code>
    <code class="n">ranges</code> <code class="o">=</code> <code class="n">Range</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">cutPattern</code><code class="o">);</code>
  <code class="o">}</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">setLocation</code><code class="o">(</code><code class="n">String</code> <code class="n">location</code><code class="o">,</code> <code class="n">Job</code> <code class="n">job</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="n">FileInputFormat</code><code class="o">.</code><code class="na">setInputPaths</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="n">location</code><code class="o">);</code>
  <code class="o">}</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="n">InputFormat</code> <code class="nf">getInputFormat</code><code class="o">()</code> <code class="o">{</code>
    <code class="k">return</code> <code class="k">new</code> <code class="nf">TextInputFormat</code><code class="o">();</code>
  <code class="o">}</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">prepareToRead</code><code class="o">(</code><code class="n">RecordReader</code> <code class="n">reader</code><code class="o">,</code> <code class="n">PigSplit</code> <code class="n">split</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">this</code><code class="o">.</code><code class="na">reader</code> <code class="o">=</code> <code class="n">reader</code><code class="o">;</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="n">Tuple</code> <code class="nf">getNext</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="k">try</code> <code class="o">{</code>
      <code class="k">if</code> <code class="o">(!</code><code class="n">reader</code><code class="o">.</code><code class="na">nextKeyValue</code><code class="o">())</code> <code class="o">{</code>
        <code class="k">return</code> <code class="k">null</code><code class="o">;</code>
      <code class="o">}</code>
      <code class="n">Text</code> <code class="n">value</code> <code class="o">=</code> <code class="o">(</code><code class="n">Text</code><code class="o">)</code> <code class="n">reader</code><code class="o">.</code><code class="na">getCurrentValue</code><code class="o">();</code>
      <code class="n">String</code> <code class="n">line</code> <code class="o">=</code> <code class="n">value</code><code class="o">.</code><code class="na">toString</code><code class="o">();</code>
      <code class="n">Tuple</code> <code class="n">tuple</code> <code class="o">=</code> <code class="n">tupleFactory</code><code class="o">.</code><code class="na">newTuple</code><code class="o">(</code><code class="n">ranges</code><code class="o">.</code><code class="na">size</code><code class="o">());</code>
      <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="n">ranges</code><code class="o">.</code><code class="na">size</code><code class="o">();</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>
        <code class="n">Range</code> <code class="n">range</code> <code class="o">=</code> <code class="n">ranges</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="n">i</code><code class="o">);</code>
        <code class="k">if</code> <code class="o">(</code><code class="n">range</code><code class="o">.</code><code class="na">getEnd</code><code class="o">()</code> <code class="o">&gt;</code> <code class="n">line</code><code class="o">.</code><code class="na">length</code><code class="o">())</code> <code class="o">{</code>
          <code class="n">LOG</code><code class="o">.</code><code class="na">warn</code><code class="o">(</code><code class="n">String</code><code class="o">.</code><code class="na">format</code><code class="o">(</code>
              <code class="sb">"Range end (%s) is longer than line length (%s)"</code><code class="o">,</code>
              <code class="n">range</code><code class="o">.</code><code class="na">getEnd</code><code class="o">(),</code> <code class="n">line</code><code class="o">.</code><code class="na">length</code><code class="o">()));</code>
          <code class="k">continue</code><code class="o">;</code>
        <code class="o">}</code>
        <code class="n">tuple</code><code class="o">.</code><code class="na">set</code><code class="o">(</code><code class="n">i</code><code class="o">,</code> <code class="k">new</code> <code class="n">DataByteArray</code><code class="o">(</code><code class="n">range</code><code class="o">.</code><code class="na">getSubstring</code><code class="o">(</code><code class="n">line</code><code class="o">)));</code>
      <code class="o">}</code>
      <code class="k">return</code> <code class="n">tuple</code><code class="o">;</code>
    <code class="o">}</code> <code class="k">catch</code> <code class="o">(</code><code class="n">InterruptedException</code> <code class="n">e</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">throw</code> <code class="k">new</code> <code class="nf">ExecException</code><code class="o">(</code><code class="n">e</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">In Pig, like in Hadoop, data loading takes place before the mapper
      runs, so it is important that the input can be split into portions that
      are handled independently by each mapper (see <a class="ulink" href="#calibre_link-549" title="Input Splits and Records">Input Splits and Records</a> for background). <a class="calibre" id="calibre_link-2342"></a>A <code class="literal">LoadFunc</code> will
      typically use an existing underlying Hadoop <code class="literal">InputFormat</code> to create records, with the <code class="literal">LoadFunc</code> providing the logic for turning the
      records into Pig tuples.</p><p class="calibre2"><code class="literal">CutLoadFunc</code> is constructed with
      a string that specifies the column ranges to use for each field. The
      logic for parsing this string and creating a list of internal <code class="literal">Range</code> objects that encapsulates these ranges
      is contained in the <code class="literal">Range</code> class, and
      is not shown here (it is available in the example code that accompanies
      this book).</p><p class="calibre2">Pig calls <code class="literal">setLocation()</code> on a
      <code class="literal">LoadFunc</code> to pass the input location to the
      loader. Since <code class="literal">CutLoadFunc</code> uses a
      <code class="literal">TextInputFormat</code> to break the input into lines, we
      just pass the location to set the input path using a static method on
      <code class="literal">FileInputFormat</code>.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Pig uses the new MapReduce API, so we use the input and output
        formats and associated classes from the <code class="literal">org.apache.hadoop</code><code class="literal">.mapreduce</code>
        package.</p></div><p class="calibre2">Next, Pig calls the <code class="literal">getInputFormat()</code>
      method to create a <code class="literal">RecordReader</code> for each split,
      just like in MapReduce. Pig passes each
      <code class="literal">RecordReader</code> to the
      <code class="literal">prepareToRead()</code> method of <code class="literal">CutLoadFunc</code>, which we store a reference to, so
      we can use it in the <code class="literal">getNext()</code> method for
      iterating through the records.</p><p class="calibre2">The Pig runtime calls <code class="literal">getNext()</code>
      repeatedly, and the load function reads tuples from the reader until the
      reader reaches the last record in its split. At this point, it returns
      <code class="literal">null</code> to signal that there are no more
      tuples to be read.</p><p class="calibre2">It is the responsibility of the <code class="literal">getNext()</code>
      implementation to turn lines of the input file into <code class="literal">Tuple</code> objects. It does this by means of a
      <code class="literal">TupleFactory</code>, a Pig class for
      creating <code class="literal">Tuple</code> instances. The
      <code class="literal">newTuple()</code> method creates a new tuple with the
      required number of fields, which is just the number of <code class="literal">Range</code> classes, and the fields are populated
      using substrings of the line, which are determined by the <code class="literal">Range</code> objects.</p><p class="calibre2">We need to think about what to do when the line is shorter than
      the range asked for. One option is to throw an exception and stop
      further processing. This is appropriate if your application cannot
      tolerate incomplete or corrupt records. In many cases, it is better to
      return a tuple with <code class="literal">null</code> fields and
      let the Pig script handle the incomplete data as it sees fit. This is
      the approach we take here; by exiting the <code class="literal">for</code> loop if
      the range end is past the end of the line, we leave the current field
      and any subsequent fields in the tuple with their default values of
      <code class="literal">null</code>.</p><div class="book" title="Using a schema"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4498">Using a schema</h4></div></div></div><p class="calibre2">Let’s now consider the <a class="calibre" id="calibre_link-3293"></a>types of the fields being loaded. If the user has
        specified a schema, then the fields need to be converted to the
        relevant types. However, this is performed lazily by Pig, so the
        loader should always construct tuples of type <code class="literal">bytearrary</code>, using the <code class="literal">DataByteArray</code> type. The load function still
        has the opportunity to do the conversion, however, by overriding
        <code class="literal">getLoadCaster()</code> to return a custom
        implementation of the <code class="literal">LoadCaster</code> interface,
        which provides a collection of conversion methods for this purpose.</p><p class="calibre2"><code class="literal">CutLoadFunc</code> doesn’t override
        <code class="literal">getLoadCaster()</code> because the default
        implementation returns <code class="literal">Utf8StorageConverter</code>, which provides
        standard conversions between UTF-8–encoded data and Pig data
        types.</p><p class="calibre2">In some cases, the load function itself can determine the
        schema. For example, if we were loading self-describing data such as
        XML or JSON, we could create a schema for Pig by looking at the data.
        Alternatively, the load function may determine the schema in another
        way, such as from an external file, or by being passed information in
        its constructor. To support such cases, the load function should
        implement the <code class="literal">LoadMetadata</code> interface (in
        addition to the <code class="literal">LoadFunc</code> interface) so it can
        supply a schema to the Pig runtime. Note, however, that if a user
        supplies a schema in the <code class="literal">AS</code> clause of
        <code class="literal">LOAD</code>, then it takes precedence over the schema
        specified through the <code class="literal">LoadMetadata</code>
        interface.</p><p class="calibre2">A load function may additionally implement the
        <code class="literal">LoadPushDown</code> interface as a means for finding
        out which columns the query is asking for. This can be a useful
        optimization for column-oriented storage, so that the loader loads
        only the columns that are needed by the query. There is no obvious way
        for <code class="literal">CutLoadFunc</code> to load only a
        subset of columns, because it reads the whole line for each tuple, so
        we don’t use this <a class="calibre" id="calibre_link-3009"></a><a class="calibre" id="calibre_link-3735"></a><a class="calibre" id="calibre_link-1780"></a><a class="calibre" id="calibre_link-2339"></a>optimization.</p></div></div></div><div class="book" title="Data Processing Operators"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-528">Data Processing Operators</h2></div></div></div><div class="book" title="Loading and Storing Data"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4499">Loading and Storing Data</h3></div></div></div><p class="calibre2">Throughout this <a class="calibre" id="calibre_link-2996"></a><a class="calibre" id="calibre_link-2844"></a>chapter, we have seen how to load data from external
      storage for processing in Pig. Storing the results is straightforward,
      too. Here’s an example of using <code class="literal">PigStorage</code> to store
      tuples as plain-text values separated by a colon character:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">STORE A INTO 'out' USING PigStorage(':');</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">cat out</code></strong>
Joe:cherry:2
Ali:apple:3
Joe:banana:2
Eve:apple:7</pre><p class="calibre2">Other built-in storage functions were described in <a class="ulink" href="#calibre_link-540" title="Table&nbsp;16-7.&nbsp;A selection of Pig’s built-in functions">Table&nbsp;16-7</a>.</p></div><div class="book" title="Filtering Data"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4500">Filtering Data</h3></div></div></div><p class="calibre2">Once you have <a class="calibre" id="calibre_link-2840"></a>some data loaded into a relation, often the next step is
      to filter it to remove the data that you are not interested in. By
      filtering early in the processing pipeline, you minimize the amount of
      data flowing through the system, which can improve efficiency.</p><div class="book" title="FOREACH...GENERATE"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4501">FOREACH...GENERATE</h4></div></div></div><p class="calibre2">We have already seen how to remove rows from a relation using
        the <a class="calibre" id="calibre_link-1695"></a><code class="literal">FILTER</code> operator with simple
        expressions and a UDF. The <code class="literal">FOREACH...GENERATE</code>
        operator<a class="calibre" id="calibre_link-1728"></a> is used to act on every row in a relation. It can be
        used to remove fields or to generate new ones. In this example, we do
        both:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP A;</code></strong>
(Joe,cherry,2)
(Ali,apple,3)
(Joe,banana,2)
(Eve,apple,7)
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">B = FOREACH A GENERATE $0, $2+1, 'Constant';</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP B;</code></strong>
(Joe,3,Constant)
(Ali,4,Constant)
(Joe,3,Constant)
(Eve,8,Constant)</pre><p class="calibre2">Here we have created a new relation, <code class="literal">B</code>, with
        three fields. Its first field is a projection of the first field
        (<code class="literal">$0</code>) of <code class="literal">A</code>.
        <code class="literal">B</code>’s second field is the third field of
        <code class="literal">A</code> (<code class="literal">$2</code>) with 1
        added to it. <code class="literal">B</code>’s third field is a constant field
        (every row in <code class="literal">B</code> has the same third field) with the
        <code class="literal">chararray</code> value <code class="literal">Constant</code>.</p><p class="calibre2">The <code class="literal">FOREACH...GENERATE</code> operator has a nested
        form to support more complex processing. In the following example, we
        compute various statistics for the weather dataset:</p><pre class="screen1"><code class="c1">-- year_stats.pig</code>
<code class="k">REGISTER</code> <code class="k">pig</code><code class="o">-</code>examples.jar;
<code class="k">DEFINE</code> isGood com.hadoopbook.pig.IsGoodQuality<code class="p">();</code>
records <code class="o">=</code> <code class="k">LOAD</code> <code class="sb">'input/ncdc/all/19{1,2,3,4,5}0*'</code>
  <code class="k">USING</code> com.hadoopbook.pig.CutLoadFunc<code class="p">(</code><code class="sb">'5-10,11-15,16-19,88-92,93-93'</code><code class="p">)</code>
  <code class="k">AS</code> <code class="p">(</code>usaf:<code class="kt">chararray</code><code class="o">,</code> wban:<code class="kt">chararray</code><code class="o">,</code> year:<code class="kt">int</code><code class="o">,</code> temperature:<code class="kt">int</code><code class="o">,</code> quality:<code class="kt">int</code><code class="p">);</code>
  
grouped_records <code class="o">=</code> <code class="k">GROUP</code> records <code class="k">BY</code> year <code class="k">PARALLEL</code> <code class="mi">30</code><code class="p">;</code>

year_stats <code class="o">=</code> <code class="k">FOREACH</code> grouped_records <code class="p">{</code>
  uniq_stations <code class="o">=</code> <code class="k">DISTINCT</code> records.usaf;
  good_records <code class="o">=</code> <code class="k">FILTER</code> records <code class="k">BY</code> <code class="nf">isGood</code><code class="p">(</code>quality<code class="p">);</code>
  <code class="k">GENERATE</code> <code class="k">FLATTEN</code><code class="p">(</code><code class="k">group</code><code class="p">)</code><code class="o">,</code> <code class="nb">COUNT</code><code class="p">(</code>uniq_stations<code class="p">)</code> <code class="k">AS</code> station_count,
    <code class="nb">COUNT</code><code class="p">(</code>good_records<code class="p">)</code> <code class="k">AS</code> good_record_count, <code class="nb">COUNT</code><code class="p">(</code>records<code class="p">)</code> <code class="k">AS</code> record_count;
<code class="p">}</code>

<code class="k">DUMP</code> year_stats;</pre><p class="calibre2">Using the cut UDF we developed earlier, we load various fields
        from the input dataset into the <code class="literal">records</code> relation. Next, we group <code class="literal">records</code> by year. Notice the <a class="calibre" id="calibre_link-2901"></a><code class="literal">PARALLEL</code> keyword for setting the
        number of reducers to use; this is vital when running on a cluster.
        Then we process each group using a nested
        <code class="literal">FOREACH...GENERATE</code> operator. The first nested
        statement creates a relation for the distinct USAF identifiers for
        stations using the <a class="calibre" id="calibre_link-1517"></a><code class="literal">DISTINCT</code> operator. The second nested
        statement creates a relation for the records with “good” readings
        using the <code class="literal">FILTER</code> operator and a UDF. The final
        nested statement is a <code class="literal">GENERATE</code> statement (a nested
        <code class="literal">FOREACH...GENERATE</code> must always have a
        <code class="literal">GENERATE</code> statement as the last nested statement)
        that generates the summary fields of interest using the grouped
        records, as well as the relations created in the nested block.</p><p class="calibre2">Running it on a few years’ worth of data, we get the
        following:</p><pre class="screen1">(1920,8L,8595L,8595L)
(1950,1988L,8635452L,8641353L)
(1930,121L,89245L,89262L)
(1910,7L,7650L,7650L)
(1940,732L,1052333L,1052976L)</pre><p class="calibre2">The fields are year, number of unique stations, total number of
        good readings, and total number of readings. We can see how the number
        of weather stations and readings grew over time.</p></div><div class="book" title="STREAM"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4502">STREAM</h4></div></div></div><p class="calibre2">The <code class="literal">STREAM</code> operator <a class="calibre" id="calibre_link-3537"></a>allows you to transform data in a relation using an
        external program or script. It is named by analogy with Hadoop
        Streaming, which provides a similar capability for MapReduce (see
        <a class="ulink" href="#calibre_link-550" title="Hadoop Streaming">Hadoop Streaming</a>).</p><p class="calibre2"><code class="literal">STREAM</code> can use built-in commands with
        arguments. Here is an example that uses the Unix <code class="literal">cut</code> command to extract the second field of
        each tuple in <code class="literal">A</code>. Note that the command and its
        arguments are enclosed in backticks:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">C = STREAM A THROUGH `cut -f 2`;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP C;</code></strong>
(cherry)
(apple)
(banana)
(apple)</pre><p class="calibre2">The <code class="literal">STREAM</code> operator uses PigStorage to
        serialize and deserialize relations to and from the program’s standard
        input and output streams. Tuples in <code class="literal">A</code> are converted
        to tab-delimited lines that are passed to the script. The output of
        the script is read one line at a time and split on tabs to create new
        tuples for the output relation <code class="literal">C</code>. You can provide a
        custom serializer and deserializer by subclassing
        <code class="literal">PigStreamingBase</code> (in the <code class="literal">org.apache.pig</code> package), then using the
        <code class="literal">DEFINE</code> <a class="calibre" id="calibre_link-1415"></a>operator.</p><p class="calibre2">Pig streaming is most powerful when you write custom processing
        scripts. The following Python script filters out bad weather
        records:</p><pre class="screen1"><code class="c1">#!/usr/bin/env python</code>

<code class="k">import</code> <code class="nn">re</code>
<code class="k">import</code> <code class="nn">sys</code>

<code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">sys</code><code class="o">.</code><code class="n">stdin</code><code class="p">:</code>
  <code class="p">(</code><code class="n">year</code><code class="p">,</code> <code class="n">temp</code><code class="p">,</code> <code class="n">q</code><code class="p">)</code> <code class="o">=</code> <code class="n">line</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>
  <code class="k">if</code> <code class="p">(</code><code class="n">temp</code> <code class="o">!=</code> <code class="sb">"9999"</code> <code class="ow">and</code> <code class="n">re</code><code class="o">.</code><code class="n">match</code><code class="p">(</code><code class="sb">"[01459]"</code><code class="p">,</code> <code class="n">q</code><code class="p">)):</code>
    <code class="k">print</code> <code class="sb">"</code><code class="si">%s</code><code class="se">\t</code><code class="si">%s</code><code class="sb">"</code> <code class="o">%</code> <code class="p">(</code><code class="n">year</code><code class="p">,</code> <code class="n">temp</code><code class="p">)</code></pre><p class="calibre2">To use the script, you need to ship it to the cluster. This is
        achieved via a <code class="literal">DEFINE</code> clause, which also creates an
        alias for the <code class="literal">STREAM</code> command. The
        <code class="literal">STREAM</code> statement can then refer to the alias, as
        the following Pig script <a class="calibre" id="calibre_link-2841"></a>shows:</p><pre class="screen1"><code class="c1">-- max_temp_filter_stream.pig</code>
<code class="k">DEFINE</code> is_good_quality `is_good_quality.py`
  <code class="k">SHIP</code> <code class="p">(</code><code class="sb">'ch16-pig/src/main/python/is_good_quality.py'</code><code class="p">);</code>
records <code class="o">=</code> <code class="k">LOAD</code> <code class="sb">'input/ncdc/micro-tab/sample.txt'</code>
  <code class="k">AS</code> <code class="p">(</code>year:<code class="kt">chararray</code><code class="o">,</code> temperature:<code class="kt">int</code><code class="o">,</code> quality:<code class="kt">int</code><code class="p">);</code>
filtered_records <code class="o">=</code> <code class="k">STREAM</code> records <code class="k">THROUGH</code> is_good_quality
  <code class="k">AS</code> <code class="p">(</code>year:<code class="kt">chararray</code><code class="o">,</code> temperature:<code class="kt">int</code><code class="p">);</code>
grouped_records <code class="o">=</code> <code class="k">GROUP</code> filtered_records <code class="k">BY</code> year;
max_temp <code class="o">=</code> <code class="k">FOREACH</code> grouped_records <code class="k">GENERATE</code> <code class="k">group</code><code class="o">,</code>
  <code class="nb">MAX</code><code class="p">(</code>filtered_records.temperature<code class="p">);</code>
<code class="k">DUMP</code> max_temp;</pre></div></div><div class="book" title="Grouping and Joining Data"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4503">Grouping and Joining Data</h3></div></div></div><p class="calibre2">Joining datasets in <a class="calibre" id="calibre_link-2842"></a><a class="calibre" id="calibre_link-2283"></a><a class="calibre" id="calibre_link-1819"></a>MapReduce takes some work on the part of the programmer
      (see <a class="ulink" href="#calibre_link-262" title="Joins">Joins</a>), whereas Pig has very good built-in
      support for join operations, making it much more approachable. Since the
      large datasets that are suitable for analysis by Pig (and MapReduce in general)
      are not normalized, however, joins are used more infrequently in Pig
      than they are in SQL.</p><div class="book" title="JOIN"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4504">JOIN</h4></div></div></div><p class="calibre2">Let’s look at an <a class="calibre" id="calibre_link-2275"></a>example of an inner join. Consider the relations
        <code class="literal">A</code> and <code class="literal">B</code>:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP A;</code></strong>
(2,Tie)
(4,Coat)
(3,Hat)
(1,Scarf)
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP B;</code></strong>
(Joe,2)
(Hank,4)
(Ali,0)
(Eve,3)
(Hank,2)</pre><p class="calibre2">We can join the two relations on the numerical (identity) field
        in each:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">C = JOIN A BY $0, B BY $1;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP C;</code></strong>
(2,Tie,Hank,2)
(2,Tie,Joe,2)
(3,Hat,Eve,3)
(4,Coat,Hank,4)</pre><p class="calibre2">This is a classic inner join, where each match between the two
        relations corresponds to a row in the result. (It’s actually an
        equijoin because the join predicate is equality.) The result’s fields
        are made up of all the fields of all the input relations.</p><p class="calibre2">You should use the general join operator when all the relations
        being joined are too large to fit in memory. If one of the relations
        is small enough to fit in memory, you can use a special type of join
        called a <em class="calibre10">fragment replicate join</em>, which is
        implemented by distributing the small input to all the mappers and
        performing a map-side join using an in-memory lookup table against the
        (fragmented) larger relation. There is a special syntax for telling
        Pig to use a fragment replicate join:<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-551" id="calibre_link-564">104</a>]</sup></p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">C = JOIN A BY $0, B BY $1 USING 'replicated';</code></strong></pre><p class="calibre2">The first relation must be the large one, followed by one or
        more small ones (all of which must fit in memory).</p><p class="calibre2">Pig also supports outer joins using a syntax that is similar to
        SQL’s (this is covered for Hive in <a class="ulink" href="#calibre_link-552" title="Outer joins">Outer joins</a>).
        For example:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">C = JOIN A BY $0 LEFT OUTER, B BY $1;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP C;</code></strong>
(1,Scarf,,)
(2,Tie,Hank,2)
(2,Tie,Joe,2)
(3,Hat,Eve,3)
(4,Coat,Hank,4)</pre></div><div class="book" title="COGROUP"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4505">COGROUP</h4></div></div></div><p class="calibre2"><code class="literal">JOIN</code> always gives a <a class="calibre" id="calibre_link-1168"></a>flat structure: a set of tuples. The
        <code class="literal">COGROUP</code> statement is similar to
        <code class="literal">JOIN</code>, but instead creates a nested set of output
        tuples. This can be useful if you want to exploit the structure in
        subsequent statements:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">D = COGROUP A BY $0, B BY $1;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP D;</code></strong>
(0,{},{(Ali,0)})
(1,{(1,Scarf)},{})
(2,{(2,Tie)},{(Hank,2),(Joe,2)})
(3,{(3,Hat)},{(Eve,3)})
(4,{(4,Coat)},{(Hank,4)})</pre><p class="calibre2"><code class="literal">COGROUP</code> generates a tuple for each unique
        grouping key. The first field of each tuple is the key, and the
        remaining fields are bags of tuples from the relations with a matching
        key. The first bag contains the matching tuples from relation
        <code class="literal">A</code> with the same key. Similarly, the second bag
        contains the matching tuples from relation <code class="literal">B</code> with
        the same key.</p><p class="calibre2">If for a particular key a relation has no matching key, the bag
        for that relation is empty. For example, since no one has bought a
        scarf (with ID 1), the second bag in the tuple for that row is empty.
        This is an example of an outer join, which is the default type for
        <code class="literal">COGROUP</code>. It can be made explicit using the
        <code class="literal">OUTER</code> keyword, making this
        <code class="literal">COGROUP</code> statement the same as the previous
        one:</p><pre class="screen1">D = COGROUP A BY $0 <span class="calibre24"><strong class="calibre9">OUTER</strong></span>, B BY $1 <span class="calibre24"><strong class="calibre9">OUTER</strong></span>;</pre><p class="calibre2">You can suppress rows with empty bags by using the
        <code class="literal">INNER</code> keyword, which gives the
        <code class="literal">COGROUP</code> inner join semantics. The
        <code class="literal">INNER</code> keyword is applied per relation, so the
        following suppresses rows only when relation <code class="literal">A</code> has
        no match (dropping the unknown product 0 here):</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">E = COGROUP A BY $0 INNER, B BY $1;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP E;</code></strong>
(1,{(1,Scarf)},{})
(2,{(2,Tie)},{(Hank,2),(Joe,2)})
(3,{(3,Hat)},{(Eve,3)})
(4,{(4,Coat)},{(Hank,4)})</pre><p class="calibre2">We can flatten this structure to discover who bought each of the
        items in relation <code class="literal">A</code>:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">F = FOREACH E GENERATE FLATTEN(A), B.$0;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP F;</code></strong>
(1,Scarf,{})
(2,Tie,{(Hank),(Joe)})
(3,Hat,{(Eve)})
(4,Coat,{(Hank)})</pre><p class="calibre2">Using a combination of <code class="literal">COGROUP</code>,
        <code class="literal">INNER</code>, and <code class="literal">FLATTEN</code> (which
        removes nesting) it’s possible to simulate an (inner)
        <code class="literal">JOIN</code>:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">G = COGROUP A BY $0 INNER, B BY $1 INNER;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">H = FOREACH G GENERATE FLATTEN($1), FLATTEN($2);</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP H;</code></strong>
(2,Tie,Hank,2)
(2,Tie,Joe,2)
(3,Hat,Eve,3)
(4,Coat,Hank,4)</pre><p class="calibre2">This gives the same result as <code class="literal">JOIN A BY
        $0, B BY $1</code>.</p><p class="calibre2">If the join key is composed of several fields, you can specify
        them all in the <code class="literal">BY</code> clauses of the
        <code class="literal">JOIN</code> or <code class="literal">COGROUP</code> statement. Make
        sure that the number of fields in each <code class="literal">BY</code> clause is
        the same.</p><p class="calibre2">Here’s another example of a join in Pig, in a script for
        calculating the maximum temperature for every station over a time
        period controlled by the input:</p><pre class="screen1"><code class="c1">-- max_temp_station_name.pig</code>
<code class="k">REGISTER</code> <code class="k">pig</code><code class="o">-</code>examples.jar;
<code class="k">DEFINE</code> isGood com.hadoopbook.pig.IsGoodQuality<code class="p">();</code>

stations <code class="o">=</code> <code class="k">LOAD</code> <code class="sb">'input/ncdc/metadata/stations-fixed-width.txt'</code>
  <code class="k">USING</code> com.hadoopbook.pig.CutLoadFunc<code class="p">(</code><code class="sb">'1-6,8-12,14-42'</code><code class="p">)</code>
  <code class="k">AS</code> <code class="p">(</code>usaf:<code class="kt">chararray</code><code class="o">,</code> wban:<code class="kt">chararray</code><code class="o">,</code> name:<code class="kt">chararray</code><code class="p">);</code>
  
trimmed_stations <code class="o">=</code> <code class="k">FOREACH</code> stations <code class="k">GENERATE</code> usaf, wban, <code class="nf">TRIM</code><code class="p">(</code>name<code class="p">);</code>    

records <code class="o">=</code> <code class="k">LOAD</code> <code class="sb">'input/ncdc/all/191*'</code>
  <code class="k">USING</code> com.hadoopbook.pig.CutLoadFunc<code class="p">(</code><code class="sb">'5-10,11-15,88-92,93-93'</code><code class="p">)</code>
  <code class="k">AS</code> <code class="p">(</code>usaf:<code class="kt">chararray</code><code class="o">,</code> wban:<code class="kt">chararray</code><code class="o">,</code> temperature:<code class="kt">int</code><code class="o">,</code> quality:<code class="kt">int</code><code class="p">);</code>
  
filtered_records <code class="o">=</code> <code class="k">FILTER</code> records <code class="k">BY</code> temperature <code class="o">!=</code> <code class="mi">9999</code> <code class="k">AND</code> <code class="nf">isGood</code><code class="p">(</code>quality<code class="p">);</code>
grouped_records <code class="o">=</code> <code class="k">GROUP</code> filtered_records <code class="k">BY</code> <code class="p">(</code>usaf, wban<code class="p">)</code> <code class="k">PARALLEL</code> <code class="mi">30</code><code class="p">;</code>
max_temp <code class="o">=</code> <code class="k">FOREACH</code> grouped_records <code class="k">GENERATE</code> <code class="k">FLATTEN</code><code class="p">(</code><code class="k">group</code><code class="p">)</code><code class="o">,</code>
  <code class="nb">MAX</code><code class="p">(</code>filtered_records.temperature<code class="p">);</code>
max_temp_named <code class="o">=</code> <code class="k">JOIN</code> max_temp <code class="k">BY</code> <code class="p">(</code>usaf, wban<code class="p">)</code><code class="o">,</code> trimmed_stations <code class="k">BY</code> <code class="p">(</code>usaf, wban<code class="p">)</code>
  <code class="k">PARALLEL</code> <code class="mi">30</code><code class="p">;</code>
max_temp_result <code class="o">=</code> <code class="k">FOREACH</code> max_temp_named <code class="k">GENERATE</code> $0, $1, $5, $2;

<code class="k">STORE</code> max_temp_result <code class="k">INTO</code> <code class="sb">'max_temp_by_station'</code><code class="p">;</code></pre><p class="calibre2">We use the cut UDF we developed earlier to load one relation
        holding the station IDs (USAF and WBAN identifiers) and names, and one
        relation holding all the weather records, keyed by station ID. We
        group the filtered weather records by station ID and aggregate by
        maximum temperature before joining with the stations. Finally, we
        project out the fields we want in the final result: USAF, WBAN,
        station name, and maximum temperature.</p><p class="calibre2">Here are a few results for the 1910s:</p><pre class="screen1">228020        99999        SORTAVALA      322
029110        99999        VAASA AIRPORT  300
040650        99999        GRIMSEY        378</pre><p class="calibre2">This query could be made more efficient by using a fragment
        replicate join, as the station metadata is <a class="calibre" id="calibre_link-1169"></a>small.</p></div><div class="book" title="CROSS"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4506">CROSS</h4></div></div></div><p class="calibre2">Pig Latin includes <a class="calibre" id="calibre_link-1290"></a>the cross-product operator (also known as the Cartesian
        product), <code class="literal">CROSS</code>, which joins every tuple in a
        relation with every tuple in a second relation (and with every tuple
        in further relations, if supplied). The size of the output is the
        product of the size of the inputs, potentially making the output very
        large:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">I = CROSS A, B;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP I;</code></strong>
(2,Tie,Joe,2)
(2,Tie,Hank,4)
(2,Tie,Ali,0)
(2,Tie,Eve,3)
(2,Tie,Hank,2)
(4,Coat,Joe,2)
(4,Coat,Hank,4)
(4,Coat,Ali,0)
(4,Coat,Eve,3)
(4,Coat,Hank,2)
(3,Hat,Joe,2)
(3,Hat,Hank,4)
(3,Hat,Ali,0)
(3,Hat,Eve,3)
(3,Hat,Hank,2)
(1,Scarf,Joe,2)
(1,Scarf,Hank,4)
(1,Scarf,Ali,0)
(1,Scarf,Eve,3)
(1,Scarf,Hank,2)</pre><p class="calibre2">When dealing with large datasets, you should try to avoid
        operations that generate intermediate representations that are
        quadratic (or worse) in size. Computing the cross product of the whole
        input dataset is rarely needed, if ever.</p><p class="calibre2">For example, at first blush, one might expect that calculating
        pairwise document similarity in a corpus of documents would require
        every document pair to be generated before calculating their
        similarity. However, if we start with the insight that most document
        pairs have a similarity score of zero (i.e., they are unrelated), then
        we can find a way to a better algorithm.</p><p class="calibre2">In this case, the key idea is to focus on the entities that we
        are using to calculate similarity (terms in a document, for example)
        and make them the center of the algorithm. In practice, we also remove
        terms that don’t help discriminate between documents (stopwords), and
        this reduces the problem space still further. Using this technique to
        analyze a set of roughly one million (10<sup class="calibre6">6</sup>)
        documents generates on the order of one billion
        (10<sup class="calibre6">9</sup>) intermediate pairs,<sup class="calibre6">[<a class="firstname" href="#calibre_link-553" id="calibre_link-565">105</a>]</sup> rather than the one trillion
        (10<sup class="calibre6">12</sup>) produced by the naive approach
        (generating the cross product of the input) or the approach with no
        stopword removal.</p></div><div class="book" title="GROUP"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-537">GROUP</h4></div></div></div><p class="calibre2">Where <code class="literal">COGROUP</code> groups <a class="calibre" id="calibre_link-1818"></a>the data in two or more relations, the
        <code class="literal">GROUP</code> statement groups the data in a single
        relation. <code class="literal">GROUP</code> supports grouping by more than
        equality of keys: you can use an expression or user-defined function
        as the group key. For example, consider the following relation
        <code class="literal">A</code>:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP A;</code></strong>
(Joe,cherry)
(Ali,apple)
(Joe,banana)
(Eve,apple)</pre><p class="calibre2">Let’s group by the number of characters in the second
        field:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">B = GROUP A BY SIZE($1);</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP B;</code></strong>
(5,{(Eve,apple),(Ali,apple)})
(6,{(Joe,banana),(Joe,cherry)})</pre><p class="calibre2"><code class="literal">GROUP</code> creates a relation whose first field is
        the grouping field, which is given the alias <code class="literal">group</code>. The second field is a bag containing
        the grouped fields with the same schema as the original relation (in
        this case, <code class="literal">A</code>).</p><p class="calibre2">There are also two special grouping operations:
        <code class="literal">ALL</code> and <code class="literal">ANY</code>.
        <code class="literal">ALL</code> groups all the tuples in a relation in a single
        group, as if the <code class="literal">GROUP</code> function were a
        constant:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">C = GROUP A ALL;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP C;</code></strong>
(all,{(Eve,apple),(Joe,banana),(Ali,apple),(Joe,cherry)})</pre><p class="calibre2">Note that there is no <code class="literal">BY</code> in this form of the
        <code class="literal">GROUP</code> statement. The <code class="literal">ALL</code>
        grouping is commonly used to count the number of tuples in a relation,
        as shown in <a class="ulink" href="#calibre_link-554" title="Validation and nulls">Validation and nulls</a>.</p><p class="calibre2">The <code class="literal">ANY</code> keyword is used to group the tuples
        in a relation randomly, which can be useful for <a class="calibre" id="calibre_link-2843"></a><a class="calibre" id="calibre_link-2284"></a><a class="calibre" id="calibre_link-1820"></a>sampling.</p></div></div><div class="book" title="Sorting Data"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-745">Sorting Data</h3></div></div></div><p class="calibre2">Relations are unordered in Pig. Consider a relation
      <code class="literal">A</code>:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP A;</code></strong>
(2,3)
(1,2)
(2,4)</pre><p class="calibre2">There is no <a class="calibre" id="calibre_link-2845"></a><a class="calibre" id="calibre_link-3438"></a>guarantee which order the rows will be processed in. In
      particular, when retrieving the contents of <code class="literal">A</code> using
      <a class="calibre" id="calibre_link-1553"></a><code class="literal">DUMP</code> or <a class="calibre" id="calibre_link-3533"></a><code class="literal">STORE</code>, the rows may be written in any
      order. If you want to impose an order on the output, you can use the
      <a class="calibre" id="calibre_link-2853"></a><code class="literal">ORDER</code> operator to sort a relation by
      one or more fields. The default sort order compares fields of the same
      type using the natural ordering, and different types are given an
      arbitrary, but deterministic, ordering (a tuple is always “less than” a
      bag, for example).</p><p class="calibre2">The following example sorts <code class="literal">A</code> by the first
      field in ascending order and by the second field in descending
      order:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">B = ORDER A BY $0, $1 DESC;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP B;</code></strong>
(1,2)
(2,4)
(2,3)</pre><p class="calibre2">Any further processing on a sorted relation is not guaranteed to
      retain its order. For example:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">C = FOREACH B GENERATE *;</code></strong></pre><p class="calibre2">Even though relation <code class="literal">C</code> has the same contents as
      relation <code class="literal">B</code>, its tuples may be emitted in any order by
      a <code class="literal">DUMP</code> or a <code class="literal">STORE</code>. It is for this
      reason that it is usual to perform the <code class="literal">ORDER</code>
      operation just before retrieving the output.</p><p class="calibre2">The <code class="literal">LIMIT</code> statement <a class="calibre" id="calibre_link-2323"></a>is useful for limiting the number of results as a
      quick-and-dirty way to get a sample of a relation. (Although random
      sampling using the <code class="literal">SAMPLE</code> operator, or prototyping
      with <a class="calibre" id="calibre_link-2076"></a>the <code class="literal">ILLUSTRATE</code> command, should be
      preferred for generating more representative samples of the data.) It
      can be used immediately after the <code class="literal">ORDER</code> statement to
      retrieve the first <span class="calibre">n</span> tuples. Usually,
      <code class="literal">LIMIT</code> will select any <span class="calibre">n</span> tuples from a relation, but when used
      immediately after an <code class="literal">ORDER</code> statement, the order is
      retained (in an exception to the rule that processing a relation does
      not retain its order):</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">D = LIMIT B 2;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP D;</code></strong>
(1,2)
(2,4)</pre><p class="calibre2">If the limit is greater than the number of tuples in the relation,
      all tuples are returned (so <code class="literal">LIMIT</code> has no
      effect).</p><p class="calibre2">Using <code class="literal">LIMIT</code> can improve the performance of a
      query because Pig tries to apply the limit as early as possible in the
      processing pipeline, to minimize the amount of data that needs to be
      processed. For this reason, you should always use
      <code class="literal">LIMIT</code> if you are not interested in the entire
      output.</p></div><div class="book" title="Combining and Splitting Data"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4507">Combining and Splitting Data</h3></div></div></div><p class="calibre2">Sometimes you have <a class="calibre" id="calibre_link-2839"></a>several relations that you would like to combine into one.
      For this, the <code class="literal">UNION</code> statement is <a class="calibre" id="calibre_link-3740"></a>used. For example:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP A;</code></strong>
(2,3)
(1,2)
(2,4)
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP B;</code></strong>
(z,x,8)
(w,y,1)
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">C = UNION A, B;</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP C;</code></strong>
(2,3)
(z,x,8)
(1,2)
(w,y,1)
(2,4)</pre><p class="calibre2"><code class="literal">C</code> is the union of relations
      <code class="literal">A</code> and <code class="literal">B</code>, and because relations are
      unordered, the order of the tuples in <code class="literal">C</code> is undefined.
      Also, it’s possible to form the union of two relations with different
      schemas or with different numbers of fields, as we have done here. Pig
      attempts to merge the schemas from the relations that
      <code class="literal">UNION</code> is operating on. In this case, they are
      incompatible, so <code class="literal">C</code> has no schema:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DESCRIBE A;</code></strong>
A: {f0: int,f1: int}
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DESCRIBE B;</code></strong>
B: {f0: chararray,f1: chararray,f2: int}
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DESCRIBE C;</code></strong>
Schema for C unknown.</pre><p class="calibre2">If the output relation has no schema, your script needs to be able
      to handle tuples that vary in the number of fields and/or types.</p><p class="calibre2">The <code class="literal">SPLIT</code> operator<a class="calibre" id="calibre_link-3480"></a> is the opposite of <code class="literal">UNION</code>: it
      partitions a relation into two or more relations. See <a class="ulink" href="#calibre_link-554" title="Validation and nulls">Validation and nulls</a> for an example of how to use <a class="calibre" id="calibre_link-2997"></a>it.</p></div></div><div class="book" title="Pig in Practice"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4508">Pig in Practice</h2></div></div></div><p class="calibre2">There are some <a class="calibre" id="calibre_link-3005"></a>practical techniques that are worth knowing about when you
    are developing and running Pig
    programs. This section covers some of them.</p><div class="book" title="Parallelism"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4509">Parallelism</h3></div></div></div><p class="calibre2">When running in <a class="calibre" id="calibre_link-3002"></a><a class="calibre" id="calibre_link-2523"></a>MapReduce mode, it’s important that the degree of
      parallelism matches the size of the dataset. By default, Pig sets the
      number of reducers by looking at the size of the input and using one
      reducer per 1 GB of input, up to a maximum of 999 reducers. You can override these parameters by
      setting <code class="literal">pig.exec.reducers</code><code class="literal">.bytes.per.reducer</code> (the default is
      1,000,000,000 bytes) and <code class="literal">pig.exec.reducers</code><code class="literal">.max</code> (the
      default is 999).</p><p class="calibre2">To explicitly set the number of reducers you want for each job,
      you can use <a class="calibre" id="calibre_link-2902"></a>a <code class="literal">PARALLEL</code> clause for operators that
      run in the reduce phase. These include all the grouping and joining
      operators (<code class="literal">GROUP</code>, <code class="literal">COGROUP</code>,
      <code class="literal">JOIN</code>, <code class="literal">CROSS</code>), as well as
      <code class="literal">DISTINCT</code> and <code class="literal">ORDER</code>. The following
      line sets the number of reducers to 30 for the
      <code class="literal">GROUP</code>:</p><a id="calibre_link-4510" class="calibre"></a><pre class="screen1">grouped_records <code class="o">=</code> <code class="k">GROUP</code> records <code class="k">BY</code> year <code class="k">PARALLEL</code> <code class="mi">30</code><code class="p">;</code></pre><p class="calibre2">Alternatively, you can set the <code class="literal">default_parallel</code> option, and it will take
      effect for all subsequent jobs:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">set default_parallel 30</code></strong></pre><p class="calibre2">See <a class="ulink" href="#calibre_link-555" title="Choosing the Number of Reducers">Choosing the Number of Reducers</a> for further
      discussion.</p><p class="calibre2">The number of map tasks is set by the size of the input (with one
      map per HDFS block) and is not affected by the
      <code class="literal">PARALLEL</code> clause.</p></div><div class="book" title="Anonymous Relations"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4511">Anonymous Relations</h3></div></div></div><p class="calibre2">You usually apply a diagnostic <a class="calibre" id="calibre_link-2992"></a><a class="calibre" id="calibre_link-1554"></a><a class="calibre" id="calibre_link-1433"></a>operator like <code class="literal">DUMP</code> or
      <code class="literal">DESCRIBE</code> to the most recently defined relation. Since
      this is so common, Pig has a shortcut to refer to the previous relation:
      <code class="literal">@</code>. Similarly, it can be tiresome to
      have to come up with a name for each relation when using the
      interpreter. Pig allows you to use the special syntax <code class="literal">=&gt;</code> to create a relation with no alias,
      which can only be referred to with <code class="literal">@</code>.
      For example:</p><pre class="screen1"><code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">=&gt; LOAD 'input/ncdc/micro-tab/sample.txt';</code></strong>
<code class="literal">grunt&gt; </code><strong class="userinput"><code class="calibre9">DUMP @</code></strong>
(1950,0,1)
(1950,22,1)
(1950,-11,1)
(1949,111,1)
(1949,78,1)</pre></div><div class="book" title="Parameter Substitution"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4512">Parameter Substitution</h3></div></div></div><p class="calibre2">If you have a Pig script <a class="calibre" id="calibre_link-3003"></a><a class="calibre" id="calibre_link-2906"></a>that you run on a regular basis, it’s quite common to want
      to be able to run the same script with different parameters. For
      example, a script that runs daily may use the date to determine which
      input files it runs over. Pig supports <em class="calibre10">parameter
      substitution</em>, where parameters in the script are substituted
      with values supplied at runtime.
      Parameters are denoted by identifiers prefixed with a <code class="literal">$</code> character; for example, <code class="literal">$input</code> and <code class="literal">$output</code> are used in the following script to
      specify the input and output paths:</p><a id="calibre_link-4513" class="calibre"></a><pre class="screen1"><code class="c1">-- max_temp_param.pig</code>
records <code class="o">=</code> <code class="k">LOAD</code> <code class="sb">'$input'</code> <code class="k">AS</code> <code class="p">(</code>year:<code class="kt">chararray</code><code class="o">,</code> temperature:<code class="kt">int</code><code class="o">,</code> quality:<code class="kt">int</code><code class="p">);</code>
filtered_records <code class="o">=</code> <code class="k">FILTER</code> records <code class="k">BY</code> temperature <code class="o">!=</code> <code class="mi">9999</code> <code class="k">AND</code>
  quality <code class="nf">IN</code> <code class="p">(</code><code class="mi">0</code><code class="o">,</code> <code class="mi">1</code><code class="o">,</code> <code class="mi">4</code><code class="o">,</code> <code class="mi">5</code><code class="o">,</code> <code class="mi">9</code><code class="p">);</code>
grouped_records <code class="o">=</code> <code class="k">GROUP</code> filtered_records <code class="k">BY</code> year;
max_temp <code class="o">=</code> <code class="k">FOREACH</code> grouped_records <code class="k">GENERATE</code> <code class="k">group</code><code class="o">,</code>
  <code class="nb">MAX</code><code class="p">(</code>filtered_records.temperature<code class="p">);</code>
<code class="k">STORE</code> max_temp <code class="k">into</code> <code class="sb">'$output'</code><code class="p">;</code></pre><p class="calibre2">Parameters can be specified when launching Pig using the <code class="literal">-param</code> option, once for each parameter:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">pig -param input=/user/tom/input/ncdc/micro-tab/sample.txt \</code></strong>
<code class="literal">&gt; </code><strong class="userinput"><code class="calibre9">    -param output=/tmp/out \</code></strong>
<code class="literal">&gt; </code><strong class="userinput"><code class="calibre9">    ch16-pig/src/main/pig/max_temp_param.pig</code></strong></pre><p class="calibre2">You can also put parameters in a file and pass them to Pig using
      the <code class="literal">-param_file</code> option. For example,
      we can achieve the same result as the previous command by placing the
      parameter definitions in a file:</p><a id="calibre_link-4514" class="calibre"></a><pre class="screen1"># Input file
input=/user/tom/input/ncdc/micro-tab/sample.txt
# Output file
output=/tmp/out</pre><p class="calibre2">The <em class="calibre10">pig</em> invocation then
      becomes:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">pig -param_file ch16-pig/src/main/pig/max_temp_param.param \</code></strong>
<code class="literal">&gt; </code><strong class="userinput"><code class="calibre9">    ch16-pig/src/main/pig/max_temp_param.pig</code></strong></pre><p class="calibre2">You can specify multiple parameter files by using <code class="literal">-param_file</code> repeatedly. You can also use a
      combination of <code class="literal">-param</code> and <code class="literal">-param_file</code> options; if any parameter is
      defined both in a parameter file and on the command line, the last value
      on the command line takes precedence.</p><div class="book" title="Dynamic parameters"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4515">Dynamic parameters</h4></div></div></div><p class="calibre2">For parameters that are supplied using the <code class="literal">-param</code> option, it is easy to make the value
        dynamic by running a command or script. Many Unix shells support
        command substitution for a command enclosed in backticks, and we can
        use this to make the output directory date-based:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">pig -param input=/user/tom/input/ncdc/micro-tab/sample.txt \</code></strong>
<code class="literal">&gt; </code><strong class="userinput"><code class="calibre9">    -param output=/tmp/`date "+%Y-%m-%d"`/out \</code></strong>
<code class="literal">&gt; </code><strong class="userinput"><code class="calibre9">    ch16-pig/src/main/pig/max_temp_param.pig</code></strong></pre><p class="calibre2">Pig also supports backticks in parameter files by executing the
        enclosed command in a shell and using the shell output as the
        substituted value. If the command or script exits with a nonzero exit
        status, then the error message is reported and execution halts.
        Backtick support in parameter files is a useful feature; it means that
        parameters can be defined in the same way in a file or on the command
        line.</p></div><div class="book" title="Parameter substitution processing"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4516">Parameter substitution processing</h4></div></div></div><p class="calibre2">Parameter substitution occurs as a preprocessing step before the
        script is run. You can see the substitutions that the preprocessor
        made by executing Pig with the <code class="literal">-dryrun</code> option. In dry run mode, Pig
        performs parameter substitution (and macro expansion) and generates a
        copy of the original script with substituted values, but does not
        execute the script. You can inspect the generated script and check
        that the substitutions look sane (because they are dynamically
        generated, for example) before running it in normal <a class="calibre" id="calibre_link-3006"></a><a class="calibre" id="calibre_link-3004"></a><a class="calibre" id="calibre_link-2907"></a>mode.</p></div></div></div><div class="book" title="Further Reading"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4517">Further Reading</h2></div></div></div><p class="calibre2">This chapter <a class="calibre" id="calibre_link-2991"></a>provided a basic introduction to using Pig. For a more
    detailed guide, see <span class="calibre"><a class="ulink" href="http://shop.oreilly.com/product/0636920018087.do" target="_top">Programming
    Pig</a></span> by Alan Gates (O’Reilly, 2011).</p></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" type="footnote" id="calibre_link-521"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-556">96</a>] </sup>History is stored in a file called <em class="calibre10">.pig_history</em> in your home
          directory.</p></div><div class="footnote" id="calibre_link-522"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-557">97</a>] </sup>Or as the <a class="ulink" href="http://pig.apache.org/philosophy.html" target="_top">Pig
        Philosophy</a> has it, “Pigs eat anything.”</p></div><div class="footnote" type="footnote" id="calibre_link-524"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-558">98</a>] </sup>Not to be confused with Pig Latin, the language game. English
        words are translated into Pig Latin by moving the initial consonant
        sound to the end of the word and adding an “ay” sound. For example,
        “pig” becomes “ig-pay,” and “Hadoop” becomes “Adoop-hay.”</p></div><div class="footnote" id="calibre_link-525"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-559">99</a>] </sup>Pig Latin does not have a formal language definition as such,
        but there is a comprehensive guide to the language that you can find
        through a link on the <a class="ulink" href="http://pig.apache.org/" target="_top">Pig
        website</a>.</p></div><div class="footnote" type="footnote" id="calibre_link-526"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-560">100</a>] </sup>You sometimes see these terms being used interchangeably in
          documentation on Pig Latin: for example, “<code class="literal">GROUP</code>
          command,” “<code class="literal">GROUP</code> operation,”
          “<code class="literal">GROUP</code> statement.”</p></div><div class="footnote" type="footnote" id="calibre_link-545"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-561">101</a>] </sup>Pig actually comes with an equivalent built-in function called
          <code class="literal">TRIM</code>.</p></div><div class="footnote" type="footnote" id="calibre_link-546"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-562">102</a>] </sup>Although not relevant for this example, eval functions that
          operate on a bag may additionally implement Pig’s
          <code class="literal">Algebraic</code> or
          <code class="literal">Accumulator</code> interfaces for more efficient
          processing of the bag in chunks.</p></div><div class="footnote" type="footnote" id="calibre_link-547"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-563">103</a>] </sup>There is a more fully featured UDF for doing the same thing in
          the Piggy Bank called <code class="literal">FixedWidthLoader</code>.</p></div><div class="footnote" type="footnote" id="calibre_link-551"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-564">104</a>] </sup>There are more keywords that may be used in the
            <code class="literal">USING</code> clause, including <code class="literal">'skewed'</code> (for large datasets with a
            skewed keyspace), <code class="literal">'merge'</code> (to
            effect a merge join for inputs that are already sorted on the join
            key), and <code class="literal">'merge-sparse'</code> (where
            1% or less of data is matched). See Pig’s documentation for
            details on how to use these specialized joins.</p></div><div class="footnote" id="calibre_link-553"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-565">105</a>] </sup>Tamer Elsayed, Jimmy Lin, and Douglas W. Oard, <a class="ulink" href="http://bit.ly/doc_similarity" target="_top">“Pairwise
            Document Similarity in Large Collections with MapReduce,”</a>
            <span class="calibre"><em class="calibre10">Proceedings of the 46th Annual Meeting of the
            Association of Computational Linguistics</em></span>, June
            2008.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-402"><section type="chapter" id="calibre_link-4518" title="Chapter&nbsp;17.&nbsp;Hive"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;17.&nbsp;Hive</h2></div></div></div><p class="calibre2">In “Information Platforms and the Rise of the Data
  Scientist,”<sup class="calibre6">[<a class="firstname" href="#calibre_link-609" id="calibre_link-657">106</a>]</sup> <br>Jeff Hammerbacher<a class="calibre" id="calibre_link-1883"></a><a class="calibre" id="calibre_link-1998"></a> describes Information Platforms <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;as “the locus of their
  organization’s efforts to ingest, process, and generate information,” and<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;how they “serve to accelerate the process of learning from empirical
  data.”</p><p class="calibre2">One of the biggest ingredients in the Information Platform built by
  Jeff’s team at Facebook <br>was <a class="ulink" href="https://hive.apache.org/" target="_top">Apache Hive</a>, a framework for data
  warehousing on top of Hadoop. <br>Hive grew from a need to manage and learn from
  the huge volumes of data <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;that Facebook was producing every day from its
  burgeoning social network. <br>After trying a few different systems, the team
  chose Hadoop for storage and processing, <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;since it was cost effective and met
  the scalability requirements.</p><p class="calibre2"><span style="
    text-shadow: 0 0 .38em;
">Hive was created to make it possible for analysts with strong SQL
  skills (but meager Java programming skills) <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;to run queries on the huge
  volumes of data that Facebook stored in HDFS. </span><br>Today, Hive is a successful
  Apache project used by many organizations as a general-purpose, scalable
  data processing platform.</p><p class="calibre2">Of course, SQL isn’t ideal for every big data problem—it’s not a good
  fit for building complex machine-learning algorithms, for example—<br>but it’s
  great for many analyses, <br>and it has the huge advantage of being very well
  known in the industry. <br>What’s more, SQL is the <span class="calibre">lingua franca</span> in business intelligence tools (ODBC
  is a common bridge, for example), <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; so Hive is well placed to integrate with
  these products.</p><p class="calibre2">This chapter is an introduction to using Hive. <br>It assumes that you
  have working knowledge of SQL and general database architecture; <br>as we go
  through Hive’s features, we’ll often compare them to the equivalent in a
  traditional RDBMS.</p><div class="book" title="Installing Hive"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4519">Installing Hive</h2></div></div></div><p class="calibre2">In normal use, <a class="calibre" id="calibre_link-2008"></a>Hive runs on your workstation and <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span style="
    text-shadow: 0 0 1em red;
    color: red;
">convert</span>s your SQL query
    <span style="
    text-shadow: 0 0 1em red;
    color: red;
">into</span> a series of jobs for execution on a Hadoop cluster. <br>Hive organizes
    data into tables, which provide a means for attaching structure to data
    stored in HDFS.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // 🧡<br>Metadata—such as table schemas—is stored in a database
    called <a class="calibre" id="calibre_link-2677"></a><a class="calibre" id="calibre_link-2687"></a>the <em class="calibre10" style="
    text-shadow: 0 0 1em;
">metastore</em>.</p><p class="calibre2">When starting out with Hive, it is convenient to run the <span style="
    text-shadow: 0 0 1em;
">metastore</span>
    on your local machine. <br>In this configuration, which is the default,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // 🤎<br>the
    Hive table definitions that you create will be local to your machine, so
    you can’t share them with other users.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;// 🤎<br>We’ll see how to configure a shared
    remote <span style="
    text-shadow: 0 0 1em;
">metastore</span>, which is the norm in production environments, in <a class="ulink" href="#calibre_link-610" title="The Metastore">The Metastore</a>. // 💚</p><p class="calibre2">Installation of Hive is straightforward. <br>As a prerequisite, you need
    to have <u style="
    text-decoration: underline red wavy;
">the same version of</u> Hadoop installed locally that your cluster is
    running.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-611" id="calibre_link-658">107</a>]</sup> <br>Of course, you may choose to run Hadoop locally, either in
    standalone or pseudodistributed mode, while getting started with Hive.
<br>These options are all covered in <a class="ulink" href="#calibre_link-28" title="Appendix&nbsp;A.&nbsp;Installing Apache Hadoop">Appendix&nbsp;A</a>.</p><div class="sidebar"><a id="calibre_link-4520" class="calibre"></a><div class="sidebar-title">Which Versions of Hadoop Does Hive Work With?</div><p class="calibre2">Any given <a class="calibre" id="calibre_link-2039"></a><a class="calibre" id="calibre_link-3761"></a>release of Hive is designed to work with multiple versions
      of Hadoop. <br>Generally, Hive works with the latest stable release of
      Hadoop, as well as supporting a number of older versions, listed in the
      release notes. <br>You don’t need to do anything special to tell Hive which
      version of Hadoop you are using, <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; beyond making sure that the <em class="calibre10">hadoop</em> executable is on the path or <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; setting
      <a class="calibre" id="calibre_link-1872"></a>the <code class="literal">HADOOP_HOME</code>
      environment variable.</p></div><p class="calibre2">Download a <a class="ulink" href="http://hive.apache.org/downloads.html" target="_top">release</a>, and unpack the tarball in a
    suitable place on your workstation:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">tar xzf apache-hive-<em class="replaceable1"><code class="calibre46">x.y.z</code></em>-bin.tar.gz</code></strong></pre><p class="calibre2">It’s handy to put Hive on your path to make it easy to
    launch:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">export HIVE_HOME=~/sw/apache-hive-<em class="replaceable1"><code class="calibre46">x.y.z</code></em>-bin</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">export PATH=$PATH:$HIVE_HOME/bin</code></strong></pre><p class="calibre2">Now type <code class="literal">hive</code> to launch the Hive
    shell:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hive</code></strong>
<code class="literal">hive&gt; </code></pre><div class="book" title="The Hive Shell"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4521">The Hive Shell</h3></div></div></div><p class="calibre2">The shell is <a class="calibre" id="calibre_link-2048"></a>the primary way that we will interact with Hive, by
      issuing commands in <em class="calibre10">HiveQL</em>. <br>HiveQL is Hive’s
      query language, a dialect of SQL. <br>It is heavily influenced by MySQL, so
      if you are familiar with <a class="calibre" id="calibre_link-2736"></a>MySQL, you should feel at home using Hive.</p><p class="calibre2">When starting Hive for the first time, we can check that it is
      working by listing its tables—there should be none. <br>The command must be
      terminated with a semicolon to tell Hive to execute it:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SHOW TABLES;</code></strong>
OK
Time taken: 0.473 seconds</pre><p class="calibre2">Like SQL, HiveQL is generally <a class="calibre" id="calibre_link-1061"></a><b>case insensitive</b> (except for string comparisons), so
      <code class="literal">show tables;</code> works equally well here.
<br>The Tab key will autocomplete Hive keywords and functions.</p><p class="calibre2">For a fresh install, the command takes a few seconds to run as it
      lazily creates the metastore database on your machine. <br>(The database
      stores its files in a directory called <em class="calibre10">metastore_db</em>, which is relative to the
      location from which you ran the <code class="literal">hive</code>
      command.)</p><p class="calibre2">You can also run the Hive shell in noninteractive mode. <br>The
      <code class="literal">-f</code> option runs the commands in the
      specified file, which is <em class="calibre10">script.q</em>
      in this example:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hive -f script.q</code></strong></pre><p class="calibre2">For short scripts, you can use the <code class="literal">-e</code> option to specify the commands inline, in
      which case the final semicolon is not required:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hive -e 'SELECT * FROM dummy'</code></strong>
OK
X
Time taken: 1.22 seconds, Fetched: 1 row(s)</pre><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">It’s useful to have a small table of data to <a class="calibre" id="calibre_link-3651"></a>test queries against, <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; such as trying out functions in
        <code class="literal">SELECT</code> expressions using literal
        data (see <a class="ulink" href="#calibre_link-612" title="Operators and Functions">Operators and Functions</a>). <br>Here’s one way of
        populating a single-row table:</p><pre class="screen2"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">echo 'X' &gt; /tmp/dummy.txt</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">hive -e "CREATE TABLE dummy (value STRING); \</code></strong>
  <strong class="userinput"><code class="calibre9">LOAD DATA LOCAL INPATH '/tmp/dummy.txt' \</code></strong>
  <strong class="userinput"><code class="calibre9">OVERWRITE INTO TABLE dummy"</code></strong></pre></div><p class="calibre2">In both interactive and noninteractive mode, Hive will print
      information to standard error—such as the time taken to run a
      query—during the course of operation. You can suppress these messages
      using the <code class="literal">-S</code> option at launch
      time, which has the effect of showing only
      the output result for queries:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hive -S -e 'SELECT * FROM dummy'</code></strong>
X</pre><p class="calibre2">Other useful Hive shell features include the ability to run
      commands on the host operating system by using a <code class="literal">!</code> prefix to the command and the ability to
      access Hadoop filesystems using <a class="calibre" id="calibre_link-2009"></a>the <code class="literal">dfs</code> command.</p></div></div><div class="book" title="An Example"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-536">An Example</h2></div></div></div><p class="calibre2">Let’s see how to use <a class="calibre" id="calibre_link-2040"></a>Hive to run a query on the weather dataset we explored in
    earlier chapters. The first step is to load the data into Hive’s managed
    storage. Here we’ll have Hive use the local filesystem for storage; later
    we’ll see how to store tables in HDFS.</p><p class="calibre2">Just like an RDBMS, Hive organizes its data into tables. We create a
    table to hold the <a class="calibre" id="calibre_link-1284"></a>weather data using the <code class="literal">CREATE
    TABLE</code> statement:</p><pre class="screen1">CREATE TABLE records (year STRING, temperature INT, quality INT)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\t';</pre><p class="calibre2">The first line declares a <code class="literal">records</code>
    table with three columns: <code class="literal">year</code>,
    <code class="literal">temperature</code>, and <code class="literal">quality</code>. The type of each
    column must be specified, too. Here the year is a string, while the other
    two columns are integers.</p><p class="calibre2">So far, the SQL is familiar. The <code class="literal">ROW
    FORMAT</code> clause, <a class="calibre" id="calibre_link-3244"></a>however, is particular to HiveQL. This declaration is saying
    that each row in the data file is tab-delimited text. Hive expects there
    to be three fields in each row, corresponding to the table columns, with
    fields separated by tabs and rows by newlines.</p><p class="calibre2">Next, we can populate Hive with the data. This is just a small
    sample, for exploratory purposes:</p><pre class="screen1">LOAD DATA LOCAL INPATH 'input/ncdc/micro-tab/sample.txt'
OVERWRITE INTO TABLE records;</pre><p class="calibre2">Running this command tells Hive to put the specified local file in
    its warehouse directory. This is a simple filesystem operation. There is
    no attempt, for example, to parse the file and store it in an internal
    database format, because Hive does not mandate any particular file format.
    Files are stored verbatim; they are not modified by Hive.</p><p class="calibre2">In this example, we are storing Hive tables on the local filesystem
    (<code class="literal">fs.defaultFS</code> is set to its default
    value of <code class="literal">file:///</code>). Tables are stored
    as directories under Hive’s warehouse directory, which is controlled by
    the <code class="literal">hive.metastore.warehouse.dir</code>
    property and defaults to <em class="calibre10">/user/hive/warehouse</em>.</p><p class="calibre2">Thus, the files for the <code class="literal">records</code>
    table are found in the <em class="calibre10">/user/hive/warehouse/records</em> directory on the local filesystem:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">ls /user/hive/warehouse/records/</code></strong>
sample.txt</pre><p class="calibre2">In this case, there is only one file, <em class="calibre10">sample.txt</em>, but in general there can be more,
    and Hive will read all of them when querying the table.</p><p class="calibre2">The <code class="literal">OVERWRITE</code> keyword<a class="calibre" id="calibre_link-2888"></a> in the <code class="literal">LOAD DATA</code>
    statement <a class="calibre" id="calibre_link-2334"></a>tells Hive to delete any existing files in the directory for
    the table. If it is omitted, the new files are simply added to the table’s
    directory (unless they have the same names, in which case they replace the
    old files).</p><p class="calibre2">Now that the data is in Hive, we can run a query against it:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT year, MAX(temperature)</code></strong>
<code class="literal">    &gt; </code><strong class="userinput"><code class="calibre9">FROM records</code></strong>
<code class="literal">    &gt; </code><strong class="userinput"><code class="calibre9">WHERE temperature != 9999 AND quality IN (0, 1, 4, 5, 9)</code></strong>
<code class="literal">    &gt; </code><strong class="userinput"><code class="calibre9">GROUP BY year;</code></strong>
1949    111
1950    22</pre><p class="calibre2">This SQL query is unremarkable. It <a class="calibre" id="calibre_link-3316"></a><a class="calibre" id="calibre_link-1812"></a>is a <code class="literal">SELECT</code> statement
    with a <code class="literal">GROUP BY</code> clause for grouping
    rows into years, which uses the <code class="literal">MAX</code>
    aggregate function to find the maximum temperature for each year group.
    The remarkable thing is that Hive transforms this query into a job, which
    it executes on our behalf, then prints the results to the console. There
    are some nuances, such as the SQL constructs that Hive supports and the
    format of the data that we can query—and we explore some of these in this
    chapter—but it is the ability to execute SQL queries against our raw data
    that gives Hive its <a class="calibre" id="calibre_link-2041"></a>power.</p></div><div class="book" title="Running Hive"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4522">Running Hive</h2></div></div></div><p class="calibre2">In this section, we look at some more practical aspects of running
    Hive, including how to set up Hive to run against a Hadoop cluster and a
    shared metastore. In doing so, we’ll see Hive’s architecture in some
    detail.</p><div class="book" title="Configuring Hive"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4523">Configuring Hive</h3></div></div></div><p class="calibre2">Hive is <a class="calibre" id="calibre_link-2002"></a>configured using an XML configuration file like Hadoop’s.
      The file is called <em class="calibre10">hive-site.xml</em>
      and is located in Hive’s <em class="calibre10">conf</em>
      directory. This file is where you can set properties that you want to
      set every time you run Hive. The same directory contains <em class="calibre10">hive-default.xml</em>, which documents the
      properties that Hive exposes and their default values.</p><p class="calibre2">You can override the configuration directory that Hive looks for
      in <em class="calibre10">hive-site.xml</em> by passing the
      <code class="literal">--config</code> option to the <code class="literal">hive</code> command:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hive --config /Users/tom/dev/hive-conf</code></strong></pre><p class="calibre2">Note that this option specifies the containing directory, not
      <em class="calibre10">hive-site.xml</em> itself. It can be
      useful when you have multiple site files—for different clusters,
      say—that you switch between on a regular basis. Alternatively, you can
      set the <code class="literal">HIVE_CONF_DIR</code> environment
      variable to the configuration directory for the same effect.</p><p class="calibre2">The <em class="calibre10">hive-site.xml</em> file is a
      natural place to put the cluster connection details: you can specify the
      filesystem and resource manager using the usual Hadoop <a class="calibre" id="calibre_link-1737"></a><a class="calibre" id="calibre_link-3879"></a>properties, <code class="literal">fs.defaultFS</code> and <code class="literal">yarn.resourcemanager.address</code> (see <a class="ulink" href="#calibre_link-28" title="Appendix&nbsp;A.&nbsp;Installing Apache Hadoop">Appendix&nbsp;A</a> for more details on configuring Hadoop). If not set,
      they default to the local filesystem and the local (in-process) job
      runner—just like they do in Hadoop—which is very handy when trying out
      Hive on small trial datasets. Metastore configuration settings (covered
      in <a class="ulink" href="#calibre_link-610" title="The Metastore">The Metastore</a>) are commonly found in <em class="calibre10">hive-site.xml</em>, too.</p><p class="calibre2">Hive also permits you to set properties on a per-session basis, by
      passing the <code class="literal">-hiveconf</code> option to the <code class="literal">hive</code> command. For example, the following
      command sets the cluster (in this case, to a pseudodistributed cluster)
      for the duration of the session:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hive -hiveconf fs.defaultFS=hdfs://localhost \
  -hiveconf mapreduce.framework.name=yarn \
  -hiveconf yarn.resourcemanager.address=localhost:8032 </code></strong></pre><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">If you plan to have more than one Hive user sharing a Hadoop
        cluster, you need to make the directories that Hive uses writable by
        all users. The following commands will create the directories and set
        their permissions appropriately:</p><pre class="screen2"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -mkdir /tmp</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -chmod a+w /tmp</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -mkdir -p /user/hive/warehouse</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -chmod a+w /user/hive/warehouse</code></strong></pre><p class="calibre2">If all users are in the same group, then permissions <code class="literal">g+w</code> are sufficient on the warehouse
        directory.</p></div><p class="calibre2">You can change settings from within a session, too, using
      <a class="calibre" id="calibre_link-3378"></a>the <code class="literal">SET</code> command. This
      is useful for changing Hive settings for a particular query. For
      example, the following command ensures buckets are populated according
      to the table definition (see <a class="ulink" href="#calibre_link-613" title="Buckets">Buckets</a>):</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SET hive.enforce.bucketing=true;</code></strong></pre><p class="calibre2">To see the current value of any property, use <code class="literal">SET</code> with just the property name:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SET hive.enforce.bucketing;</code></strong>
hive.enforce.bucketing=true</pre><p class="calibre2">By itself, <code class="literal">SET</code> will list all
      the properties (and their values) set by Hive. Note that the list will
      not include Hadoop defaults, unless they have been explicitly overridden
      in one of the ways covered in this section. Use <code class="literal">SET -v</code> to list all the properties in the
      system, including Hadoop defaults.</p><p class="calibre2">There is a precedence hierarchy to setting properties. In the
      following list, lower numbers take precedence over higher
      numbers:</p><div class="book"><ol class="orderedlist"><li class="listitem"><p class="calibre2">The Hive <code class="literal">SET</code> command</p></li><li class="listitem"><p class="calibre2">The command-line <code class="literal">-hiveconf</code>
          option</p></li><li class="listitem"><p class="calibre2"><em class="calibre10">hive-site.xml</em> and the
          Hadoop site files (<em class="calibre10">core-site.xml</em>, <em class="calibre10">hdfs-site.xml</em>, <em class="calibre10">mapred-site.xml</em>, and <em class="calibre10">yarn-site.xml</em>)</p></li><li class="listitem"><p class="calibre2">The Hive defaults and the Hadoop default files (<em class="calibre10">core-default.xml</em>, <em class="calibre10">hdfs-default.xml</em>, <em class="calibre10">mapred-default.xml</em>, and <em class="calibre10">yarn-default.xml</em>)</p></li></ol></div><p class="calibre2">Setting configuration properties for Hadoop is covered in more
      detail in <a class="ulink" href="#calibre_link-201" title="Which Properties Can I Set?">Which Properties Can I Set?</a>.</p><div class="book" title="Execution engines"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-620">Execution engines</h4></div></div></div><p class="calibre2">Hive was originally <a class="calibre" id="calibre_link-2480"></a><a class="calibre" id="calibre_link-2006"></a>written to use MapReduce as its execution engine, and
        that is still the default. It is now also possible to run Hive using
        <a class="ulink" href="http://tez.apache.org/" target="_top">Apache Tez</a> as its
        <a class="calibre" id="calibre_link-900"></a>execution engine, and work is underway to support Spark
        <a class="calibre" id="calibre_link-3456"></a>(see <a class="ulink" href="#calibre_link-352" title="Chapter&nbsp;19.&nbsp;Spark">Chapter&nbsp;19</a>), too. Both Tez and
        Spark are general directed acyclic graph (DAG) engines that offer more
        flexibility and higher performance than MapReduce. For example, unlike
        MapReduce, where intermediate job output is materialized to HDFS, Tez
        and Spark can avoid replication overhead by writing the intermediate
        output to local disk, or even store it in memory (at the request of
        the Hive planner).</p><p class="calibre2">The execution engine is controlled <a class="calibre" id="calibre_link-2043"></a>by the <code class="literal">hive.execution.engine</code> property, which
        defaults to <code class="literal">mr</code> (for MapReduce).
        It’s easy to switch the execution engine on a per-query basis, so you
        can see the effect of a different engine on a particular query. Set
        Hive to use Tez as follows:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SET hive.execution.engine=tez;</code></strong></pre><p class="calibre2">Note that Tez needs to be installed on the Hadoop cluster first;
        see the Hive documentation for up-to-date details on how to do
        this.</p></div><div class="book" title="Logging"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4524">Logging</h4></div></div></div><p class="calibre2">You can find Hive’s <a class="calibre" id="calibre_link-2360"></a>error log on the local filesystem at <em class="calibre10">${java.io.tmpdir}/${user.name}/hive.log</em>. It
        can be very useful when trying to diagnose configuration problems or
        other types of error. Hadoop’s MapReduce task logs are also a useful
        resource for troubleshooting; see <a class="ulink" href="#calibre_link-34" title="Hadoop Logs">Hadoop Logs</a> for
        where to find them.</p><p class="calibre2">On many systems, <code class="literal">${java.io.tmpdir}</code> is <em class="calibre10">/tmp</em>, but if it’s not, or if you want to
        set the logging directory to be another location, then use the
        following:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hive -hiveconf hive.log.dir='/tmp/${user.name}'</code></strong></pre><p class="calibre2">The logging configuration is in <em class="calibre10">conf/hive-log4j.properties</em>, and you can
        edit this file to change log levels and other logging-related
        settings. However, often it’s more convenient to set logging
        configuration for the session. For example, the following handy
        invocation will send debug messages to the <a class="calibre" id="calibre_link-2003"></a>console:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hive -hiveconf hive.root.logger=DEBUG,console</code></strong></pre></div></div><div class="book" title="Hive Services"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-618">Hive Services</h3></div></div></div><p class="calibre2">The Hive shell <a class="calibre" id="calibre_link-2016"></a>is only one of several services that you can run using the
      <code class="literal">hive</code> command. You can specify the
      service to run using the <code class="literal">--service</code>
      option. Type <code class="literal">hive --service help</code> to
      get a list of available service names; some of the most useful ones
      <a class="calibre" id="calibre_link-1189"></a>are described in the following list:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term"><code class="literal1">cli</code></span></dt><dd class="calibre8"><p class="calibre2">The command-line interface to Hive (the shell). This is the
            default service.</p></dd><dt class="calibre7"><span class="term"><code class="literal1">hiveserver2</code></span></dt><dd class="calibre8"><p class="calibre2">Runs Hive as a <a class="calibre" id="calibre_link-2054"></a>server exposing a Thrift service, enabling access
            from a range of clients written in different languages. HiveServer
            2 improves on the original HiveServer by supporting authentication
            and multiuser concurrency. Applications using the Thrift, JDBC,
            and ODBC connectors need to run a Hive server to communicate with
            Hive. Set the <code class="literal">hive.server2.thrift.port</code> configuration
            <a class="calibre" id="calibre_link-2046"></a>property to specify the port the server will listen
            on (defaults to 10000).</p></dd><dt class="calibre7"><span class="term"><code class="literal1">beeline</code></span></dt><dd class="calibre8"><p class="calibre2">A command-line <a class="calibre" id="calibre_link-982"></a>interface to Hive that works in embedded mode (like
            the regular CLI), or by connecting to a HiveServer 2 process using
            JDBC.</p></dd><dt class="calibre7"><span class="term"><code class="literal1">hwi</code></span></dt><dd class="calibre8"><p class="calibre2">The Hive Web Interface. A <a class="calibre" id="calibre_link-2042"></a><a class="calibre" id="calibre_link-2064"></a>simple web interface that can be used as an
            alternative to the CLI without having to install any client
            software. See also <a class="ulink" href="http://gethue.com/" target="_top">Hue</a> for
            a more fully featured Hadoop web interface that includes
            applications for running Hive queries and browsing the Hive
            metastore.</p></dd><dt class="calibre7"><span class="term"><code class="literal1">jar</code></span></dt><dd class="calibre8"><p class="calibre2">The Hive <a class="calibre" id="calibre_link-2157"></a>equivalent of <code class="literal">hadoop
            jar</code>, a convenient way to run Java applications that
            includes both Hadoop and Hive classes on the classpath.</p></dd><dt class="calibre7"><span class="term"><code class="literal1">metastore</code></span></dt><dd class="calibre8"><p class="calibre2">By default, the <a class="calibre" id="calibre_link-2688"></a><a class="calibre" id="calibre_link-2678"></a>metastore is run in the same process as the Hive
            service. Using this service, it is possible to run the metastore
            as a standalone (remote) process. Set <a class="calibre" id="calibre_link-2691"></a>the <code class="literal">METASTORE_PORT</code> environment variable (or
            use the <code class="literal">-p</code> command-line option)
            to specify the port the server will listen on (defaults to
            9083).</p></dd></dl></div><div class="book" title="Hive clients"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4525">Hive clients</h4></div></div></div><p class="calibre2">If you run Hive as a server (<code class="literal">hive
        --service hiveserver2</code>), there are a number of different
        mechanisms for connecting to it from applications (the relationship
        between Hive clients and Hive services is illustrated in <a class="ulink" href="#calibre_link-614" title="Figure&nbsp;17-1.&nbsp;Hive architecture">Figure&nbsp;17-1</a>):</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Thrift Client</span></dt><dd class="calibre8"><p class="calibre2">The Hive server <a class="calibre" id="calibre_link-3686"></a>is exposed as a Thrift service, so it’s possible
              to interact with it using any programming language that supports
              Thrift. There are third-party projects providing clients for
              Python and Ruby; for more details, see the <a class="ulink" href="http://bit.ly/hive_server" target="_top">Hive
              wiki</a>.</p></dd><dt class="calibre7"><span class="term">JDBC driver</span></dt><dd class="calibre8"><p class="calibre2">Hive <a class="calibre" id="calibre_link-2205"></a>provides a Type 4 (pure Java) JDBC driver, defined
              in the class <code class="literal">org.apache.</code><code class="literal">hadoop.hive.jdbc.HiveDriver</code>.
              When <a class="calibre" id="calibre_link-2047"></a>configured with a JDBC URI of the form <code class="literal">jdbc:hive2://<em class="replaceable"><code class="replaceable">host</code></em>:<em class="replaceable"><code class="replaceable">port</code></em>/<em class="replaceable"><code class="replaceable">dbname</code></em></code>,
              a Java application will connect to a Hive server running in a
              separate process at the given host and port. (The driver makes
              calls to an interface implemented by the Hive Thrift Client
              using the Java Thrift bindings.)</p><p class="calibre2">You may alternatively choose to connect to Hive via JDBC
              in <em class="calibre10">embedded mode</em> using the URI <code class="literal">jdbc:hive2://</code>. In this mode, Hive runs
              in the same JVM as the application invoking it; there is no need
              to launch it as a standalone server, since it does not use the
              Thrift service or the Hive Thrift Client.</p><p class="calibre2">The Beeline CLI uses the JDBC driver to communicate with
              Hive.</p></dd><dt class="calibre7"><span class="term">ODBC driver</span></dt><dd class="calibre8"><p class="calibre2">An ODBC driver <a class="calibre" id="calibre_link-2827"></a>allows applications that support the ODBC protocol
              (such as business intelligence software) to connect to Hive. <br>The
              Apache Hive distribution does not ship with an ODBC driver, but
              several vendors make one freely available. <br>(Like the JDBC
              driver, ODBC drivers use Thrift to communicate with the
              <a class="calibre" id="calibre_link-2017"></a><a class="calibre" id="calibre_link-1190"></a>Hive server.)</p></dd></dl></div><div class="book"><div class="figure"><a id="calibre_link-614" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4526" class="calibre"></a><img alt="Hive architecture" src="images/000069.png" class="calibre29" style="
    width: 45em;
"></div></div><div class="figure-title">Figure&nbsp;17-1.&nbsp;Hive architecture</div></div></div></div></div><div class="book" title="The Metastore"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-610">The Metastore</h3></div></div></div><p class="calibre2">The <em class="calibre10">metastore</em> is the <a class="calibre" id="calibre_link-2010"></a><a class="calibre" id="calibre_link-2679"></a><a class="calibre" id="calibre_link-2689"></a>central repository of Hive metadata. <br>The metastore is
      divided into two pieces: a service and the backing store for the data.
<br>By default, the metastore service runs in the same JVM as the Hive
      service and<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;contains an embedded Derby database instance backed by the
      local disk. <br>This is called <a class="calibre" id="calibre_link-1565"></a>the <em class="calibre10">embedded metastore</em>
      configuration (see <a class="ulink" href="#calibre_link-615" title="Figure&nbsp;17-2.&nbsp;Metastore configurations">Figure&nbsp;17-2</a>).&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;// 💤</p><p class="calibre2">Using an embedded metastore is a simple way to get started with
      Hive; <br>however, only one embedded Derby database can access the database
      files on disk at any one time, <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;which means you can have only one Hive
      session open at a time that accesses the same metastore. <br>Trying to start a
      second session produces an error when it attempts to open a connection
      to the metastore.</p><p class="calibre2">The solution to supporting multiple sessions (and therefore
      multiple users) is to use a standalone database. <br>This configuration is
      referred to as a <em class="calibre10">local metastore</em>,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;// 💦<br>since the
      metastore service still runs in the same process as the Hive service <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; but
      connects to a database running in a separate process, either on the same
      machine or on a remote <span class="calibre">machine. <br>Any
      JDBC-compliant database may be used by setting the <code class="literal">javax.jd</code></span><code class="literal">o</code><code class="literal">.option.*</code>
      configuration properties listed in <a class="ulink" href="#calibre_link-616" title="Table&nbsp;17-1.&nbsp;Important metastore configuration properties">Table&nbsp;17-1</a>.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-617" id="calibre_link-659">108</a>]</sup></p><div class="figure"><a id="calibre_link-615" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4527" class="calibre"></a><img alt="Metastore configurations" src="images/000077.png" class="calibre29" style="
    width: 35em;
"></div></div><div class="figure-title">Figure&nbsp;17-2.&nbsp;Metastore configurations</div></div><p class="calibre2">MySQL is a popular <a class="calibre" id="calibre_link-2735"></a>choice for the standalone metastore. In this case, the
      <code class="literal">javax.jdo.option.ConnectionURL</code> property is set to
      <code class="literal">jdbc:mysql://host/dbname?createDatabaseIfNotExist=true</code>,
      and <code class="literal">javax.jdo.option.ConnectionDriverName</code> is set to
      <code class="literal">com.mysql.jdbc.Driver</code>. (The username
      and password should be set too, of course.) The JDBC driver JAR file for
      MySQL (Connector/J) must be on Hive’s classpath, which is simply
      achieved by placing it in Hive’s <em class="calibre10">lib</em> directory.</p><p class="calibre2">Going a step further, there’s another metastore configuration
      called a <em class="calibre10">remote metastore</em>, where one or more
      metastore servers run in separate processes to the Hive service. This
      brings better manageability and security because the database tier can
      be completely firewalled off, and the clients no longer need the
      database credentials.</p><p class="calibre2">A Hive service is configured to use a remote metastore by setting
      <code class="literal">hive.metastore</code><code class="literal">.uris</code> to the
      metastore server URI(s), separated by commas if there is more than one.
      Metastore server URIs are of the
      form <code class="literal">thrift://<em class="replaceable"><code class="replaceable">host</code></em>:<em class="replaceable"><code class="replaceable">port</code></em></code>,
        where the port corresponds to the one set by <code class="literal">METASTORE_PORT</code> when starting the metastore
      server (see <a class="ulink" href="#calibre_link-618" title="Hive Services">Hive Services</a>).</p><div class="table"><a id="calibre_link-616" class="calibre"></a><div class="table-title">Table&nbsp;17-1.&nbsp;Important metastore configuration properties</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Property name</td><td class="calibre20">Type</td><td class="calibre20">Default
              value</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">hive.metastore</code> <code class="uri">.</code> <code class="uri">warehouse.dir</code></td><td class="calibre23">URI</td><td class="calibre23"><code class="uri">/user/hive/</code> <code class="uri">warehouse</code></td><td class="calibre25">The <a class="calibre" id="calibre_link-2045"></a>directory relative to <code class="uri">fs.defaultFS</code> where managed tables
              are stored.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">hive.metastore.uris</code></td><td class="calibre23">Comma-separated URIs</td><td class="calibre23">Not set</td><td class="calibre25">If not set <a class="calibre" id="calibre_link-2044"></a>(the default), use an in-process metastore;
              otherwise, connect to one or more remote metastores, specified
              by a list of URIs. Clients connect in a round-robin fashion when
              there are multiple remote
              servers.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">javax.jdo.option.</code>
              <code class="uri">ConnectionURL</code></td><td class="calibre23">URI</td><td class="calibre23"> <code class="uri">jdbc:derby:;database</code> 
              <code class="uri">Name=metastore_db;</code> <code class="uri">create=true</code></td><td class="calibre25">The JDBC URL of<a class="calibre" id="calibre_link-1243"></a> the metastore database.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">javax.jdo.option.</code>
              <code class="uri">ConnectionDriverName</code></td><td class="calibre23"><code class="uri">String</code></td><td class="calibre23"><code class="uri">org.apache.derby.</code>
              <code class="uri">jdbc.EmbeddedDriver</code></td><td class="calibre25">The JDBC <a class="calibre" id="calibre_link-1240"></a>driver classname.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">javax.jdo.option.</code>
              <code class="uri">ConnectionUserName</code></td><td class="calibre23"><code class="uri">String</code></td><td class="calibre23"><code class="uri">APP</code></td><td class="calibre25">The JDBC <a class="calibre" id="calibre_link-1244"></a>username.</td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">javax.jdo.option.</code>
              <code class="uri">ConnectionPassword</code></td><td class="calibre27"><code class="uri">String</code></td><td class="calibre27"><code class="uri">mine</code></td><td class="calibre28">The <a class="calibre" id="calibre_link-1242"></a>JDBC <a class="calibre" id="calibre_link-2011"></a><a class="calibre" id="calibre_link-2680"></a><a class="calibre" id="calibre_link-2690"></a>password.</td></tr></tbody></table></div></div></div></div><div class="book" title="Comparison with Traditional Databases"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4528">Comparison with Traditional Databases</h2></div></div></div><p class="calibre2">Although Hive <a class="calibre" id="calibre_link-2004"></a>resembles a traditional database in many ways (such as
    supporting a SQL interface), <br>its original HDFS and MapReduce underpinnings&nbsp; &nbsp; &nbsp; &nbsp; <font color="#800080" style="
    font-size: .7em;
">// underpinning&nbsp; 支柱</font><br>mean that <span style="
    text-shadow: 0 0 1em darkred;
">there are a number of <u>architectural difference</u>s that have
    directly influenced the features that Hive supports</span>. <br>Over time, however,
    these limitations have been (and continue to be) removed, <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; with the result
    that Hive looks and feels more like a traditional database with every year
    that passes.</p><div class="book" title="Schema on Read Versus Schema on Write"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4529">Schema on Read Versus Schema on Write</h3></div></div></div><p class="calibre2">In a traditional database, a table’s schema <a class="calibre" id="calibre_link-3283"></a><a class="calibre" id="calibre_link-3284"></a>is enforced at data load time. <br>If the data being loaded
      doesn’t conform to the schema, then it is rejected. <br>This design is
      sometimes called <em class="calibre10" style="
    text-shadow: 0 0 1em darkcyan;
">schema on write</em> because the data
      is checked against the schema when it is written into the
      database.</p><p class="calibre2">Hive, on the other hand, doesn’t verify the data when it is
      loaded, but rather when a query is issued. <br>This is called
      <em class="calibre10" style="
    text-shadow: 0 0 1em cyan;
">schema on read</em>.</p><p class="calibre2">There are trade-offs between the two approaches. <br>Schema on read
      makes for a very fast initial load, <br>since the data does not have to be
      read, parsed, and serialized to disk in the database’s internal format.
<br>The load operation is just a file copy or move. <br>It is more flexible,
      too: consider having two schemas for the same underlying data, depending
      on the analysis being performed. <br>(This is possible in Hive using
      external tables; see <a class="ulink" href="#calibre_link-619" title="Managed Tables and External Tables">Managed Tables and External Tables</a>.)</p><p class="calibre2">Schema on write makes query time performance faster <br>because the
      database can index columns and perform compression on the data. <br>The
      trade-off, however, is that it takes longer to load data into the
      database. <br>Furthermore, there are many scenarios where the schema is not
      known at load time,<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; so there are no indexes to apply, because the
      queries have not been formulated yet. <br>These scenarios are where Hive
      shines.</p></div><div class="book" title="Updates, Transactions, and Indexes"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-625">Updates, Transactions, and Indexes</h3></div></div></div><p class="calibre2">Updates, transactions, and indexes are mainstays of traditional
      databases. <br>Yet, until recently, these features have not been considered
      a part of Hive’s feature set. <br>This is because Hive was built to operate
      over HDFS data using MapReduce, <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;where full-table scans are the norm and
<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;a table update is achieved by transforming the data into a new table.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // 🖤<br>For a data warehousing application that runs over large portions of the
      dataset, this works well.</p><p class="calibre2">Hive has long supported adding new rows in bulk to an existing
      table by <a class="calibre" id="calibre_link-2127"></a>using <code class="literal">INSERT INTO</code> to
      add new data files to a table. <br>From release 0.14.0, finer-grained
      changes are possible, <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;so you can call <code class="literal">INSERT
      INTO TABLE...VALUES</code> to insert small batches of values computed
      in SQL. <br>In addition, it is <a class="calibre" id="calibre_link-3747"></a><a class="calibre" id="calibre_link-1425"></a>possible to <code class="literal">UPDATE</code> and
      <code class="literal">DELETE</code> rows in a table.</p><p class="calibre2">HDFS does <span style="background-color: red;">not</span> provide <u style="
    text-decoration: underline wavy;
">in-place</u> file updates, so changes resulting
      from inserts, updates, and deletes are stored in small <u style="
    text-decoration: underline grey .14em;
">delta file</u>s.&nbsp; &nbsp; // 💔<br><u style="
    text-decoration: underline grey 0.14em;
">Delta file</u>s are periodically merged into the base table files by
      MapReduce jobs that are run in the background by the metastore. <br>These
      features only work in the context of transactions (introduced in Hive
      0.13.0), <br>so the table they are being used on needs to have transactions
      enabled on it. <br>Queries reading the table are guaranteed to see a
      consistent snapshot of the table.</p><p class="calibre2">Hive also has support for table- and partition-level locking.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;// 🔒<br>Locks prevent, for example, one process from dropping a table while
      another is reading from it. <br>Locks are managed transparently using
      ZooKeeper, <br>so the user doesn’t have to acquire or release them, <br>although
      it is possible to get information about which locks are being held
      <a class="calibre" id="calibre_link-3392"></a>via the <code class="literal">SHOW LOCKS</code>
      statement. <br>By <font color="#ffa500">default</font>, locks are <span style="background-color: red;">not</span> enabled.</p><p class="calibre2">Hive indexes <a class="calibre" id="calibre_link-2096"></a>can speed up queries in certain cases. <br>A query <a class="calibre" id="calibre_link-3317"></a>such as <code class="literal">SELECT * from t WHERE x =
      a</code>, for example, can take advantage of an index on column
      <code class="literal">x</code>, <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; since only a small portion of the
      table’s files need to be scanned. <br>There are currently two index types:
      <em class="calibre10"><u>compact</u></em> and <em class="calibre10"><u>bitmap</u></em>. <br>(The
      index implementation was designed to be pluggable,<br>&nbsp;so it’s expected that
      a variety of implementations will emerge for different use
      cases.)</p><p class="calibre2"><u>Compact index</u>es store the HDFS block numbers of each value, rather
      than each file offset, <br>so they don’t take up much disk space <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; but are
      still effective for the case where values are clustered together in
      nearby rows. <br><u>Bitmap index</u>es use compressed bitsets to efficiently store
      the rows that a particular value appears in, <br>and they are usually
      appropriate for low-cardinality columns (such as gender or
      country).</p></div><div class="book" title="SQL-on-Hadoop Alternatives"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-83">SQL-on-Hadoop Alternatives</h3></div></div></div><p class="calibre2">In the years since Hive was created, many other SQL-on-Hadoop
      engines have emerged to address some of Hive’s limitations. <br><a class="ulink" href="http://impala.io/" target="_top">Cloudera Impala</a>, an open<a class="calibre" id="calibre_link-2079"></a> source interactive SQL engine, was one of the first,
      giving an order of magnitude performance boost compared to Hive running
      on MapReduce. <br>Impala uses a dedicated daemon that runs on each datanode
      in the cluster. <br>When a client runs a query it contacts an arbitrary node
      running an Impala daemon, which acts as a coordinator node for the
      query. The coordinator sends work to other Impala daemons in the cluster
      and combines their results into the full result set for the query.
<br>Impala uses the Hive metastore and supports Hive formats and most HiveQL
      constructs (plus SQL-92), so in practice it is straightforward to
      migrate between the two systems, or to run both on the same
      cluster.</p><p class="calibre2">Hive has not stood still, though, and since Impala was launched,
      the “Stinger” initiative by Hortonworks has improved the performance of
      Hive through support for Tez as an execution engine, and the addition of
      a vectorized query engine among other improvements.</p><p class="calibre2">Other prominent open source Hive alternatives include <a class="ulink" href="http://prestodb.io/" target="_top">Presto from Facebook</a>, <a class="ulink" href="http://drill.apache.org/" target="_top">Apache Drill</a>, and <a class="ulink" href="https://spark.apache.org/sql/" target="_top">Spark SQL</a>. <br>Presto and Drill
      have similar architectures to Impala, although Drill targets SQL:2011
      rather than HiveQL. <br>Spark SQL uses Spark as its underlying engine, and
      lets you embed SQL queries in Spark programs.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Spark SQL is different to using the Spark execution engine from
        within Hive (“Hive on Spark,” see <a class="ulink" href="#calibre_link-620" title="Execution engines">Execution engines</a>). <br>Hive, on Spark provides all the
        features of Hive since it is a part of the Hive project. <br>Spark SQL, on
        the other hand, is a new SQL engine that offers some level of Hive
        compatibility.</p></div><p class="calibre2"><a class="ulink" href="http://phoenix.apache.org/" target="_top">Apache Phoenix</a>
      takes a <a class="calibre" id="calibre_link-896"></a>different approach entirely: it provides SQL on HBase. <br>SQL
      access is through a JDBC driver that turns queries into HBase scans and
      takes advantage of HBase coprocessors to perform server-side
      aggregation. <br>Metadata is stored in <a class="calibre" id="calibre_link-2005"></a>HBase, too.</p></div></div><div class="book" title="HiveQL"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4530">HiveQL</h2></div></div></div><p class="calibre2">Hive’s SQL dialect, <a class="calibre" id="calibre_link-2018"></a><a class="calibre" id="calibre_link-2052"></a>called HiveQL, is a mixture of SQL-92, MySQL, and Oracle’s
    SQL dialect. <br>The level of SQL-92 support has improved over time, and will
    likely continue to get better. <br>HiveQL also provides features from later
    SQL standards, such as window functions (also known as analytic functions)
    from SQL:2003. <br>Some of Hive’s non-standard extensions to SQL were inspired
    by MapReduce, <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;such as multitable inserts (see <a class="ulink" href="#calibre_link-621" title="Multitable insert">Multitable insert</a>) and <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; the <code class="literal">TRANSFORM</code>, <code class="literal">MAP</code>, and <code class="literal">REDUCE</code> clauses (see <a class="ulink" href="#calibre_link-622" title="MapReduce Scripts">MapReduce Scripts</a>).</p><p class="calibre2">This chapter does not provide a complete reference to HiveQL; for
    that, see the <a class="ulink" href="http://bit.ly/languagemanual" target="_top">Hive
    documentation</a>. <br>Instead, we focus on commonly used features and <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;pay
    particular attention to features that diverge from either SQL-92 or
    popular databases such as MySQL. <br><a class="ulink" href="#calibre_link-623" title="Table&nbsp;17-2.&nbsp;A high-level comparison of SQL and HiveQL">Table&nbsp;17-2</a> provides a
    high-level comparison of SQL and HiveQL.</p><div class="table"><a id="calibre_link-623" class="calibre"></a><div class="table-title">Table&nbsp;17-2.&nbsp;A high-level comparison of SQL and HiveQL</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Feature</td><td class="calibre20">SQL</td><td class="calibre20">HiveQL</td><td class="calibre21">References</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">Updates</td><td class="calibre23"><code class="uri">UPDATE</code>, <code class="uri">INSERT</code>, <code class="uri">DELETE</code></td><td class="calibre23"><code class="uri">UPDATE</code>, <code class="uri">INSERT</code>, <code class="uri">DELETE</code></td><td rowspan="3" class="calibre25"><a class="ulink" href="#calibre_link-624" title="Inserts">Inserts</a>; <a class="ulink" href="#calibre_link-625" title="Updates, Transactions, and Indexes">Updates, Transactions, and Indexes</a></td></tr><tr class="calibre26"><td class="calibre23">Transactions</td><td class="calibre23">Supported</td><td class="calibre23">Limited support</td></tr><tr class="calibre19"><td class="calibre23">Indexes</td><td class="calibre23">Supported</td><td class="calibre23">Supported</td></tr><tr class="calibre26"><td class="calibre23">Data types</td><td class="calibre23">Integral, <br>floating-point, <br>fixed-point, <br>text and binary
            strings, <br>temporal</td><td class="calibre23">Boolean, <br>integral, <br>floating-point, <br>fixed-point, <br>text and
            binary strings, <br>temporal, <br>array, <br>map, <br>struct</td><td class="calibre25"><a class="ulink" href="#calibre_link-626" title="Data Types">Data Types</a></td></tr><tr class="calibre19"><td class="calibre23">Functions</td><td class="calibre23">Hundreds of built-in functions</td><td class="calibre23">Hundreds of built-in functions</td><td class="calibre25"><a class="ulink" href="#calibre_link-612" title="Operators and Functions">Operators and Functions</a></td></tr><tr class="calibre26"><td class="calibre23">Multitable
            inserts</td><td class="calibre23">Not supported</td><td class="calibre23">Supported</td><td class="calibre25"><a class="ulink" href="#calibre_link-621" title="Multitable insert">Multitable insert</a></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">CREATE TABLE...AS SELECT</code></td><td class="calibre23">Not valid SQL-92, <br>but found in some databases</td><td class="calibre23">Supported</td><td class="calibre25"><a class="ulink" href="#calibre_link-627" title="CREATE TABLE...AS SELECT">CREATE TABLE...AS SELECT</a></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">SELECT</code></td><td class="calibre23">SQL-92</td><td class="calibre23">SQL-92. <br><code class="uri">SORT BY</code> for
            partial ordering, <br><code class="uri">LIMIT</code> to
            limit number of rows returned</td><td class="calibre25"><a class="ulink" href="#calibre_link-628" title="Querying Data">Querying Data</a></td></tr><tr class="calibre19"><td class="calibre23">Joins</td><td class="calibre23">SQL-92, or <br>variants (join <span class="calibre">tables</span> in the
            <code class="uri">FROM</code> clause, <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; join condition in
            the <code class="uri">WHERE</code> clause)</td><td class="calibre23">Inner joins, <br>outer joins, <br>semi joins, <br>map joins, <br>cross
            joins</td><td class="calibre25"><a class="ulink" href="#calibre_link-629" title="Joins">Joins</a></td></tr><tr class="calibre26"><td class="calibre23">Subqueries</td><td class="calibre23">In any clause (correlated or noncorrelated)</td><td class="calibre23">In the <code class="uri">FROM</code>, <code class="uri">WHERE</code>, or <code class="uri">HAVING</code> clauses (uncorrelated subqueries
            not supported)</td><td class="calibre25"><a class="ulink" href="#calibre_link-630" title="Subqueries">Subqueries</a></td></tr><tr class="calibre19"><td class="calibre23">Views</td><td class="calibre23">Updatable (materialized or
            nonmaterialized)</td><td class="calibre23">Read-only (materialized views not supported)</td><td class="calibre25"><a class="ulink" href="#calibre_link-631" title="Views">Views</a></td></tr><tr class="calibre26"><td class="calibre27">Extension points</td><td class="calibre27">User-defined functions, <br>stored procedures</td><td class="calibre27">User-defined <a class="calibre" id="calibre_link-2053"></a>functions, <br>MapReduce scripts</td><td class="calibre28"><a class="ulink" href="#calibre_link-632" title="User-Defined Functions">User-Defined Functions</a>; <a class="ulink" href="#calibre_link-622" title="MapReduce Scripts">MapReduce Scripts</a></td></tr></tbody></table></div></div><div class="book" title="Data Types"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-626">Data Types</h3></div></div></div><p class="calibre2">Hive supports both <a class="calibre" id="calibre_link-2049"></a>primitive and complex data types. <br>Primitives include
      numeric, Boolean, string, and timestamp types. <br>The complex data types
      include arrays, maps, and structs. <br>Hive’s data types are listed in <a class="ulink" href="#calibre_link-633" title="Table&nbsp;17-3.&nbsp;Hive data types">Table&nbsp;17-3</a>. <br>Note that the literals shown are those used
      from within HiveQL; <br>they are not the serialized forms used in the
      table’s storage format (see <a class="ulink" href="#calibre_link-634" title="Storage Formats">Storage Formats</a>).</p><div class="table"><a id="calibre_link-633" class="calibre"></a><div class="table-title">Table&nbsp;17-3.&nbsp;Hive data types</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Category</td><td class="calibre20">Type</td><td class="calibre20">Description</td><td class="calibre21">Literal examples</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td rowspan="14" class="calibre23">Primitive</td><td class="calibre23"><code class="uri">BOOLEAN</code></td><td class="calibre23">True/false value.</td><td class="calibre25"><code class="uri">TRUE</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">TINYINT</code></td><td class="calibre23">1-byte (8-bit) signed integer, <br>from –128 to 127.</td><td class="calibre25"><code class="uri">1Y</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">SMALLINT</code></td><td class="calibre23">2-byte (16-bit) signed integer, <br>from –32,768 to
              32,767.</td><td class="calibre25"><code class="uri">1S</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">INT</code></td><td class="calibre23">4-byte (32-bit) signed integer, <br>from –2,147,483,648 to
              2,147,483,647.</td><td class="calibre25"><code class="uri">1</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">BIGINT</code></td><td class="calibre23">8-byte (64-bit) signed integer, <br>from
              –9,223,372,036,854,775,808 to 9,223,372,036,854,775,807.</td><td class="calibre25"><code class="uri">1L</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">FLOAT</code></td><td class="calibre23">4-byte (32-bit) single-precision floating-point
              number.</td><td class="calibre25"><code class="uri">1.0</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">DOUBLE</code></td><td class="calibre23">8-byte (64-bit) double-precision floating-point
              number.</td><td class="calibre25"><code class="uri">1.0</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">DECIMAL</code></td><td class="calibre23">Arbitrary-precision signed decimal number.</td><td class="calibre25"><code class="uri">1.0</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">STRING</code></td><td class="calibre23">Unbounded variable-length character string.</td><td class="calibre25"><code class="uri">'a'</code>, <code class="uri">"a"</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">VARCHAR</code></td><td class="calibre23">Variable-length character string.</td><td class="calibre25"><code class="uri">'a'</code>, <code class="uri">"a"</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">CHAR</code></td><td class="calibre23">Fixed-length character string.</td><td class="calibre25"><code class="uri">'a'</code>, <code class="uri">"a"</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">BINARY</code></td><td class="calibre23">Byte array.</td><td class="calibre25">Not supported</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">TIMESTAMP</code></td><td class="calibre23">Timestamp with nanosecond precision.</td><td class="calibre25"><code class="uri">1325502245000</code>,
              <code class="uri">'2012-01-02
              03:04:05.123456789'</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">DATE</code></td><td class="calibre23">Date.</td><td class="calibre25"><code class="uri">'2012-01-02'</code></td></tr><tr class="calibre19"><td rowspan="4" class="calibre27">Complex</td><td class="calibre23"><code class="uri">ARRAY</code></td><td class="calibre23">An ordered collection of fields. <br>The fields must all be
              of the same type.</td><td class="calibre25"><code class="uri">array(1, 2)</code> <sup class="calibre5">[<a class="firstname" href="#calibre_link-635" id="calibre_link-637">a</a>]</sup></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">MAP</code></td><td class="calibre23">An unordered collection of key-value pairs. <br>Keys must be
              primitives; values may be any type. <br>For a particular map, <br>the
              keys must be the same type, and the values must be the same
              type.</td><td class="calibre25"><code class="uri">map('a', 1, 'b',
              2)</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">STRUCT</code></td><td class="calibre23">A collection of named fields. <br>The fields may be of
              different types.</td><td class="calibre25"><code class="uri">struct('a', 1,
              1.0)</code>,<sup class="calibre5">[<a class="firstname" href="#calibre_link-636" id="calibre_link-638">b</a>]</sup> <br><code class="uri">named_struct('col1', 'a',
              'col2', 1, 'col3', 1.0)</code></td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">UNION</code></td><td class="calibre27">A value that may be one of a number of defined data
              types. <br>The value is tagged with an integer (zero-indexed) <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; representing its data
              type in the union.</td><td class="calibre28"><code class="uri">create_union(1, 'a',
              63)</code></td></tr></tbody><tbody class="calibre22"><tr class="calibre19"><td colspan="4" class="calibre28"><div class="footnote1" id="calibre_link-635"><p class="calibre2"><sup class="calibre37">[<a class="firstname" href="#calibre_link-637">a</a>] </sup>The literal forms for arrays, maps, structs, and unions are provided as
                    functions. <br>&nbsp; &nbsp; That is, <code class="literal2">array</code>, <code class="literal2">map</code>, <code class="literal2">struct</code>, and
                      <code class="literal2">create_union</code> are built-in Hive
                    functions.</p></div><div class="footnote1" id="calibre_link-636"><p class="calibre2"><sup class="calibre37">[<a class="firstname" href="#calibre_link-638">b</a>] </sup>The columns are named <code class="literal2">col1</code>, <code class="literal2">col2</code>, <code class="literal2">col3</code>, etc.</p></div></td></tr></tbody></table></div></div><div class="book" title="Primitive types"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4531">Primitive types</h4></div></div></div><p class="calibre2">Hive’s primitive types correspond roughly to Java’s, although
        some names are influenced by MySQL’s type names (some of which, in
        turn, overlap with SQL-92’s). <br>There is a <code class="literal">BOOLEAN</code> type for storing true and false
        values. <br>There are four signed integral types: <code class="literal">TINYINT</code>, <code class="literal">SMALLINT</code>, <code class="literal">INT</code>, and <code class="literal">BIGINT</code>, which are equivalent to Java’s
        <code class="literal">byte</code>, <code class="literal">short</code>, <code class="literal">int</code>, and <code class="literal">long</code> primitive types, respectively (they are
        1-byte, 2-byte, 4-byte, and 8-byte signed integers).</p><p class="calibre2">Hive’s floating-point types, <code class="literal">FLOAT</code> and <code class="literal">DOUBLE</code>, correspond to Java’s <code class="literal">float</code> and <code class="literal">double</code>, which are 32-bit and 64-bit
        floating-point numbers.</p><p class="calibre2">The <code class="literal">DECIMAL</code> data type is used
        to represent arbitrary-precision decimals, like Java’s
        <code class="literal">BigDecimal</code>, and are commonly used for
        representing currency values. <br><code class="literal">DECIMAL</code> values are stored as unscaled
        integers. <br>The <em class="calibre10">precision</em> is the number of digits
        in the unscaled value, and the <em class="calibre10">scale</em> is the
        number of digits to the right of the decimal point. <br>So, for example,
        <code class="literal">DECIMAL(5,2)</code> stores numbers between
        −999.99 and 999.99. <br>If the scale is omitted then it defaults to 0, so
        <code class="literal">DECIMAL(5)</code>stores numbers in the
        range −99,999 to 99,999 (i.e., integers). <br>If the precision is omitted
        then it defaults to 10, so <code class="literal">DECIMAL</code>
        is equivalent to <code class="literal">DECIMAL(10,0)</code>. <br>The
        maximum allowed precision is 38, and the scale must be no larger than
        the precision.</p><p class="calibre2">There are three Hive data types for storing text. <br><code class="literal">STRING</code> is a variable-length character string
        with no declared maximum length. <br>(The theoretical maximum size
        <code class="literal">STRING</code> that may be stored is 2 GB,
        although in practice it may be inefficient to materialize such large
        values. <br>&nbsp;Sqoop has large object support; see <a class="ulink" href="#calibre_link-639" title="Importing Large Objects">Importing Large Objects</a>.) <br><code class="literal">VARCHAR</code> types are similar except they are
        declared with a maximum length between 1 and 65355; for example,
        <code class="literal">VARCHAR(100)</code>. <br><code class="literal">CHAR</code> types are fixed-length strings that are
        padded with trailing spaces if necessary; for example, <code class="literal">CHAR(100)</code>. <br>Trailing spaces are ignored for
        the purposes of string comparison of <code class="literal">CHAR</code> values.</p><p class="calibre2">The <code class="literal">BINARY</code> data type is for
        storing variable-length binary data.</p><p class="calibre2">The <code class="literal">TIMESTAMP</code> data type
        stores timestamps with nanosecond precision. Hive comes with UDFs for
        converting between Hive timestamps, Unix timestamps (seconds since the
        Unix epoch), and strings, which makes most common date operations
        tractable. <br><code class="literal">TIMESTAMP</code> does not
        encapsulate a time zone; however, the <code class="literal">to_utc_timestamp</code> and <code class="literal">from_utc_timestamp</code> functions make it
        possible to do time zone conversions.</p><p class="calibre2">The <code class="literal">DATE</code> data type stores a
        date with year, month, and day components.</p></div><div class="book" title="Complex types"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4532">Complex types</h4></div></div></div><p class="calibre2">Hive has four complex types: <code class="literal">ARRAY</code>, <code class="literal">MAP</code>, <code class="literal">STRUCT</code>, and <code class="literal">UNION</code>. <code class="literal">ARRAY</code> and <code class="literal">MAP</code> are like their namesakes in Java,
        whereas a <code class="literal">STRUCT</code> is a record type
        that encapsulates a set of named fields. A <code class="literal">UNION</code> specifies a choice of data types;
        values must match exactly one of these types.</p><p class="calibre2">Complex types permit an arbitrary level of nesting. Complex type
        declarations must specify the type of the fields in the collection,
        using an angled bracket notation, as illustrated in this table
        definition with three columns (one for each complex type):</p><pre class="screen1">CREATE TABLE complex (
  c1 ARRAY&lt;INT&gt;,
  c2 MAP&lt;STRING, INT&gt;,
  c3 STRUCT&lt;a:STRING, b:INT, c:DOUBLE&gt;,
  c4 UNIONTYPE&lt;STRING, INT&gt;
);</pre><p class="calibre2">If we load the table with one row of data for <code class="literal">ARRAY</code>, <code class="literal">MAP</code>, <code class="literal">STRUCT</code>, and <code class="literal">UNION</code>, as shown in the “Literal examples”
        column in <a class="ulink" href="#calibre_link-633" title="Table&nbsp;17-3.&nbsp;Hive data types">Table&nbsp;17-3</a> (we’ll see the file format
        needed to do this in <a class="ulink" href="#calibre_link-634" title="Storage Formats">Storage Formats</a>), the following
        query demonstrates the field accessor operators for each <a class="calibre" id="calibre_link-2050"></a>type:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT c1[0], c2['b'], c3.c, c4 FROM complex;</code></strong>
1    2    1.0    {1:63}</pre></div></div><div class="book" title="Operators and Functions"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-612">Operators and Functions</h3></div></div></div><p class="calibre2">The usual set of SQL operators <a class="calibre" id="calibre_link-2838"></a><a class="calibre" id="calibre_link-1767"></a><a class="calibre" id="calibre_link-2051"></a>is provided by Hive: relational operators (such as
      <code class="literal">x = 'a'</code> for testing equality,
      <code class="literal">x IS NULL</code> for testing nullity, and
      <code class="literal">x LIKE 'a%'</code> for pattern matching),
      arithmetic operators (such as <code class="literal">x + 1</code>
      for addition), and logical operators (such as <code class="literal">x</code> <code class="literal">OR</code>
      <code class="literal">y</code> for logical <code class="literal">OR</code>).
      The operators match those in MySQL, which deviates from SQL-92 because
      <code class="literal">||</code> is logical <code class="literal">OR</code>,
      not string concatenation. Use the <code class="literal">concat</code> function for the latter in both MySQL
      and Hive.</p><p class="calibre2">Hive comes with a large number of built-in functions—too many to
      list here—divided into categories that include mathematical and
      statistical functions, string functions, date functions (for operating
      on string representations of dates), conditional functions, aggregate
      functions, and functions for working with XML (using the <code class="literal">xpath</code> function) and JSON.</p><p class="calibre2">You can retrieve a list of functions from the Hive shell by
      <a class="calibre" id="calibre_link-3391"></a>typing <code class="literal">SHOW
      FUNCTIONS</code>.<sup class="calibre6">[<a class="firstname" href="#calibre_link-640" id="calibre_link-660">109</a>]</sup> To get brief usage instructions for a particular function,
      use <a class="calibre" id="calibre_link-1434"></a>the <code class="literal">DESCRIBE</code>
      command:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">DESCRIBE FUNCTION length;</code></strong>
length(str | binary) - Returns the length of str or number of bytes in binary
 data</pre><p class="calibre2">In the case when there is no built-in function that does what you
      want, you can write your own; see <a class="ulink" href="#calibre_link-632" title="User-Defined Functions">User-Defined Functions</a>.</p><div class="book" title="Conversions"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4533">Conversions</h4></div></div></div><p class="calibre2">Primitive types form a <a class="calibre" id="calibre_link-1254"></a>hierarchy that dictates the implicit type conversions
        Hive will perform in function and operator expressions. For example, a
        <code class="literal">TINYINT</code> will be converted to an
        <code class="literal">INT</code> if an expression expects an
        <code class="literal">INT</code>; however, the reverse
        conversion will not occur, and Hive will return an error unless the
        <code class="literal">CAST</code> operator is used.</p><p class="calibre2">The implicit conversion rules can be summarized as follows. Any
        numeric type can be implicitly converted to a wider type, or to a text
        type (<code class="literal">STRING</code>, <code class="literal">VARCHAR</code>, <code class="literal">CHAR</code>). All the text types can be implicitly
        converted to another text type. Perhaps surprisingly, they can also be
        converted to <code class="literal">DOUBLE</code> or <code class="literal">DECIMAL</code>. <code class="literal">BOOLEAN</code> types cannot be converted to any
        other type, and they cannot be implicitly converted to any other type
        in expressions. <code class="literal">TIMESTAMP</code> and
        <code class="literal">DATE</code> can be implicitly converted to
        a text type.</p><p class="calibre2">You can perform explicit type conversion using <code class="literal">CAST</code>. For example, <code class="literal">CAST('1' AS INT)</code> will convert the
        string <code class="literal">'1'</code> to the integer value 1.
        If the cast fails—as it does in <code class="literal">CAST('X' AS INT)</code>, for example—the
        expression <a class="calibre" id="calibre_link-2019"></a>returns <code class="literal">NULL</code>.</p></div></div></div><div class="book" title="Tables"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4534">Tables</h2></div></div></div><p class="calibre2">A Hive table is <a class="calibre" id="calibre_link-2023"></a><a class="calibre" id="calibre_link-3600"></a>logically made up of the data being stored and the
    associated metadata describing the layout of the data in the table. <br>The
    data typically resides in HDFS, although it may reside in any Hadoop
    filesystem, including the local filesystem or S3. <br>Hive stores the metadata
    in a <a class="calibre" id="calibre_link-3143"></a>relational database and not in, say, HDFS (see <a class="ulink" href="#calibre_link-610" title="The Metastore">The Metastore</a>).</p><p class="calibre2">In this section, we look in more detail at how to create tables, the
    different physical storage formats that Hive offers, and how to import
    data into tables.</p><div class="sidebar"><a id="calibre_link-4535" class="calibre"></a><div class="sidebar-title">Multiple Database/Schema Support</div><p class="calibre2">Many relational databases have a facility for multiple namespaces,
      which allows users and applications to be segregated into different
      databases or schemas. Hive supports the same facility and provides
      <a class="calibre" id="calibre_link-1280"></a><a class="calibre" id="calibre_link-1544"></a>commands such as <code class="literal">CREATE DATABASE
      <em class="replaceable"><code class="replaceable">dbname</code></em></code>, <code class="literal">USE <em class="replaceable"><code class="replaceable">dbname</code></em></code>,
      and <code class="literal">DROP DATABASE
      <em class="replaceable"><code class="replaceable">dbname</code></em></code>. You can fully qualify a
      table by writing <code class="literal"><em class="replaceable"><code class="replaceable">dbname</code></em>.<em class="replaceable"><code class="replaceable">tablename</code></em></code>.
      If no database is specified, tables belong to the <code class="literal">default</code> database.</p></div><div class="book" title="Managed Tables and External Tables"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-619">Managed Tables and External Tables</h3></div></div></div><p class="calibre2">When you create a <a class="calibre" id="calibre_link-2033"></a><a class="calibre" id="calibre_link-2029"></a><a class="calibre" id="calibre_link-3610"></a><a class="calibre" id="calibre_link-3606"></a><a class="calibre" id="calibre_link-2384"></a><a class="calibre" id="calibre_link-1598"></a>table in Hive, by default Hive will manage the data, which
      means that Hive moves the data into its warehouse directory.
      Alternatively, you may create an <em class="calibre10">external table</em>,
      which tells Hive to refer to the data that is at an existing location
      outside the warehouse directory.</p><p class="calibre2">The difference between the two table types is seen in <a class="calibre" id="calibre_link-1285"></a><a class="calibre" id="calibre_link-2335"></a>the <code class="literal">LOAD</code> and <code class="literal">DROP</code> semantics. <br>Let’s consider a managed table
      first.</p><p class="calibre2">When you load data into a managed table, it is moved into Hive’s
      warehouse directory. <br>For example, this:</p><pre class="screen1">CREATE TABLE managed_table (dummy STRING);
LOAD DATA INPATH '/user/tom/data.txt' INTO table managed_table;</pre><p class="calibre2">will <span class="calibre">move</span> the file <span class="calibre">hdfs://user/tom/data.txt</span> into Hive’s warehouse
      directory for the <code class="literal">managed_table</code> table, <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;which is
      <span class="calibre">hdfs://user/hive/warehouse/managed_table</span>.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-641" id="calibre_link-661">110</a>]</sup></p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">The load operation is very fast because it is just a move or
        rename within a filesystem. <br>However, bear in mind that Hive does not
        check that the files in the table directory conform to the schema
        declared for the table, even for managed tables. <br>If there is a
        mismatch, this will become apparent at query time, often by the query
        returning <code class="literal">NULL</code> for a missing field.
<br>You can check that the data is being parsed correctly by issuing a
        simple <code class="literal">SELECT</code> statement to retrieve
        a few rows directly from the table.</p></div><p class="calibre2">If the table is later dropped, using:</p><pre class="screen1">DROP TABLE managed_table;</pre><p class="calibre2">the table, including its <a class="calibre" id="calibre_link-1546"></a>metadata <span class="calibre">and its data</span>,
      is deleted. It bears repeating that since the initial <code class="literal">LOAD</code> performed a move operation, and the
      <code class="literal">DROP</code> performed a delete operation,
      the data no longer exists anywhere. This is what it means for Hive to
      manage the data.</p><p class="calibre2">An external table behaves differently. You control the creation
      and deletion of the data. The location of the external data is specified
      at table creation time:</p><pre class="screen1">CREATE <span class="calibre24"><strong class="calibre9">EXTERNAL</strong></span> TABLE external_table (dummy STRING)
  LOCATION '/user/tom/external_table';
LOAD DATA INPATH '/user/tom/data.txt' INTO TABLE external_table;</pre><p class="calibre2">With the <code class="literal">EXTERNAL</code> keyword,
      <a class="calibre" id="calibre_link-1597"></a>Hive knows that it is not managing the data, so it doesn’t
      move it to its warehouse directory. Indeed, it doesn’t even check
      whether the external location exists at the time it is defined. This is
      a useful feature because it means you can create the data lazily after
      creating the table.</p><p class="calibre2">When you drop an external table, Hive will leave the data
      untouched and only delete the metadata.</p><p class="calibre2">So how do you choose which type of table to use? In most cases,
      there is not much difference between the two (except of course for the
      difference in <code class="literal">DROP</code> semantics), so it
      is a just a matter of preference. As a rule of thumb, if you are doing
      all your processing with Hive, then use managed tables, but if you wish
      to use Hive and other tools on the same dataset, then use external
      tables. A common pattern is to use an external table to access an
      initial dataset stored in HDFS (created by another process), then use a
      Hive transform to move the data into a managed Hive table. This works
      the other way around, too; an external table (not necessarily on HDFS)
      can be used to export data from Hive for other applications to
      <a class="calibre" id="calibre_link-2129"></a>use.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-642" id="calibre_link-662">111</a>]</sup></p><p class="calibre2">Another reason for using external tables is when you wish to
      associate multiple schemas with the same <a class="calibre" id="calibre_link-2034"></a><a class="calibre" id="calibre_link-2030"></a><a class="calibre" id="calibre_link-3611"></a><a class="calibre" id="calibre_link-3607"></a><a class="calibre" id="calibre_link-2385"></a><a class="calibre" id="calibre_link-1599"></a>dataset.</p></div><div class="book" title="Partitions and Buckets"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-297">Partitions and Buckets</h3></div></div></div><p class="calibre2">Hive organizes <a class="calibre" id="calibre_link-2035"></a><a class="calibre" id="calibre_link-3612"></a><a class="calibre" id="calibre_link-2948"></a>tables into <em class="calibre10">partitions</em>—a way of
      dividing a table into coarse-grained parts based on the value of a
      <em class="calibre10">partition column</em>, such as a date. Using
      partitions can make it faster to do queries on slices of the
      data.</p><p class="calibre2">Tables or partitions may be subdivided further <a class="calibre" id="calibre_link-2025"></a><a class="calibre" id="calibre_link-3602"></a><a class="calibre" id="calibre_link-1024"></a>into <em class="calibre10">buckets</em> to give extra
      structure to the data that may be used for more efficient queries. For
      example, bucketing by user ID means we can quickly evaluate a user-based
      query by running it on a randomized sample of the total set of
      users.</p><div class="book" title="Partitions"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4536">Partitions</h4></div></div></div><p class="calibre2">To take an example where partitions are commonly used, imagine
        logfiles where each record includes a timestamp. If we partition by
        date, then records for the same date will be stored in the same
        partition. The advantage to this scheme is that queries that are
        restricted to a particular date or set of dates can run much more
        efficiently, because they only need to scan the files in the
        partitions that the query pertains to. Notice that partitioning
        doesn’t preclude more wide-ranging queries: it is still feasible to
        query the entire dataset across many partitions.</p><p class="calibre2">A table may be partitioned in multiple dimensions. For example,
        in addition to partitioning logs by date, we might also
        <em class="calibre10">subpartition</em> each date partition by country to
        permit efficient queries by location.</p><p class="calibre2">Partitions are defined at table creation time using <a class="calibre" id="calibre_link-882"></a><a class="calibre" id="calibre_link-2945"></a>the <code class="literal">PARTITIONED BY</code>
        clause,<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-643" id="calibre_link-663">112</a>]</sup> which takes a list of column definitions. For the
        hypothetical logfiles example, we might define a table with records
        comprising a timestamp and the log line itself:</p><pre class="screen1">CREATE TABLE logs (ts BIGINT, line STRING)
PARTITIONED BY (dt STRING, country STRING);</pre><p class="calibre2">When we load data into a partitioned table, the partition values
        are specified explicitly:</p><pre class="screen1">LOAD DATA LOCAL INPATH 'input/hive/partitions/file1'
INTO TABLE logs
PARTITION (dt='2001-01-01', country='GB');</pre><p class="calibre2">At the filesystem level, partitions are simply nested
        subdirectories of the table directory. After loading a few more files
        into the <code class="literal">logs</code> table, the directory
        structure might look like this:</p><pre class="screen1">/user/hive/warehouse/logs
├── dt=2001-01-01/
│&nbsp;&nbsp; ├── country=GB/
│&nbsp;&nbsp; │&nbsp;&nbsp; ├── file1
│&nbsp;&nbsp; │&nbsp;&nbsp; └── file2
│&nbsp;&nbsp; └── country=US/
│&nbsp;&nbsp;     └── file3
└── dt=2001-01-02/
    ├── country=GB/
    │&nbsp;&nbsp; └── file4
    └── country=US/
        ├── file5
        └── file6</pre><p class="calibre2">The <code class="literal">logs</code> table has two date
        partitions (<code class="literal">2001-01-01</code> and <code class="literal">2001-01-02</code>, corresponding to subdirectories
        called <em class="calibre10">dt=2001-01-01</em> and
        <em class="calibre10">dt=2001-01-02</em>); and two country
        subpartitions (<code class="literal">GB</code> and <code class="literal">US</code>, corresponding to nested subdirectories
        called <em class="calibre10">country=GB</em> and <em class="calibre10">country=US</em>). The
        datafiles reside in the leaf directories.</p><p class="calibre2">We can ask Hive for the partitions in a table <a class="calibre" id="calibre_link-3393"></a>using <code class="literal">SHOW
        PARTITIONS</code>:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SHOW PARTITIONS logs;</code></strong>
dt=2001-01-01/country=GB
dt=2001-01-01/country=US
dt=2001-01-02/country=GB
dt=2001-01-02/country=US</pre><p class="calibre2">One thing to bear in mind is that the column definitions in the
        <code class="literal">PARTITIONED BY</code> clause are
        full-fledged table columns, called <span class="calibre">partition
        columns</span>; however, the datafiles do not contain values for
        these columns, since they are derived from the directory names.</p><p class="calibre2">You can use partition columns in <code class="literal">SELECT</code> statements in the usual way. Hive
        performs <em class="calibre10">input pruning</em> to scan only the
        relevant partitions. For example:</p><pre class="screen1">SELECT ts, dt, line
FROM logs
WHERE country='GB';</pre><p class="calibre2">will only scan <em class="calibre10">file1</em>,
        <em class="calibre10">file2</em>, and <em class="calibre10">file4</em>. Notice, too, that the query returns
        the values of the <code class="literal">dt</code> partition
        column, which Hive reads from the directory names since they are not
        in the <a class="calibre" id="calibre_link-2036"></a><a class="calibre" id="calibre_link-3613"></a><a class="calibre" id="calibre_link-2949"></a>datafiles.</p></div><div class="book" title="Buckets"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-613">Buckets</h4></div></div></div><p class="calibre2">There are <a class="calibre" id="calibre_link-2026"></a><a class="calibre" id="calibre_link-3603"></a><a class="calibre" id="calibre_link-1025"></a>two reasons why you might want to organize your tables
        (or partitions) into buckets. The first is to enable more efficient
        queries. Bucketing imposes extra structure on the table, which Hive
        can take advantage of when performing certain queries. In particular,
        a join of two tables that are bucketed on the same columns—which
        include the join columns—can be efficiently implemented as a map-side
        join.</p><p class="calibre2">The second reason to bucket a table is to make sampling more
        efficient. When working with large datasets, it is very convenient to
        try out queries on a fraction of your dataset while you are in the
        process of developing or refining them. We will see how to do
        efficient sampling at the end of this section.</p><p class="calibre2">First, let’s see how to tell Hive that a table should be
        bucketed. We use <a class="calibre" id="calibre_link-1106"></a>the <code class="literal">CLUSTERED BY</code>
        clause to specify the columns to bucket on and the number of
        buckets:</p><pre class="screen1">CREATE TABLE bucketed_users (id INT, name STRING)
CLUSTERED BY (id) INTO 4 BUCKETS;</pre><p class="calibre2">Here we are using the user ID to determine the bucket (which
        Hive does by hashing the value and reducing modulo the number of
        buckets), so any particular bucket will effectively have a random set
        of users in it.</p><p class="calibre2">In the map-side join case, where the two tables are bucketed in
        the same way, a mapper processing a bucket of the left table knows
        that the matching rows in the right table are in its corresponding
        bucket, so it need only retrieve that bucket (which is a small
        fraction of all the data stored in the right table) to effect the
        join. This optimization also works when the number of buckets in the
        two tables are multiples of each other; they do not have to have
        exactly the same number of buckets. The HiveQL for joining two
        bucketed tables is shown in <a class="ulink" href="#calibre_link-644" title="Map joins">Map joins</a>.</p><p class="calibre2">The data within a bucket may additionally be sorted by one or
        more columns. This allows even more efficient map-side joins, since
        the join of each bucket becomes an efficient merge sort. The syntax
        for declaring that a table has sorted buckets is:</p><pre class="screen1">CREATE TABLE bucketed_users (id INT, name STRING)
CLUSTERED BY (id) <span class="calibre24"><strong class="calibre9">SORTED BY (id ASC)</strong></span> INTO 4 BUCKETS;</pre><p class="calibre2">How can we make sure the data in our table is bucketed? Although
        it’s possible to load data generated outside Hive into a bucketed
        table, it’s often easier to get Hive to do the bucketing, usually from
        an existing table.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">Hive does not check that the buckets in the datafiles on disk
          are consistent with the buckets in the table definition (either in
          number or on the basis of bucketing columns). If there is a
          mismatch, you may get an error or undefined behavior at query time.
          For this reason, it is advisable to get Hive to perform the
          bucketing.</p></div><p class="calibre2">Take an unbucketed users table:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT * FROM users;</code></strong>
0    Nat
2    Joe
3    Kay
4    Ann</pre><p class="calibre2">To populate the bucketed table, we need to set the <code class="literal">hive.enforce.bucketing</code> property to <code class="literal">true</code> so that Hive knows to create the number
        of buckets declared in the table definition. Then it is just a matter
        of <a class="calibre" id="calibre_link-2130"></a>using the <code class="literal">INSERT</code>
        command:</p><pre class="screen1">INSERT OVERWRITE TABLE bucketed_users
SELECT * FROM users;</pre><p class="calibre2">Physically, each bucket is just a file in the table (or
        partition) directory. The filename is not important, but bucket
        <span class="calibre">n</span> is the <span class="calibre">n</span>th file when arranged in lexicographic
        order. In fact, buckets correspond to MapReduce output file
        partitions: a job will produce as many buckets (output files) as
        reduce tasks. We can see this by looking at the layout of the <code class="literal">bucketed_users</code> table we just created.
        Running this command:</p><pre class="screen1">hive&gt; <strong class="userinput"><code class="calibre9">dfs -ls /user/hive/warehouse/bucketed_users;</code></strong></pre><p class="calibre2">shows that four files were created, with the following names
        (the names are generated by Hive):</p><pre class="screen1">000000_0
000001_0
000002_0
000003_0</pre><p class="calibre2">The first bucket contains the users with IDs 0 and 4, since for
        an <code class="literal">INT</code> the hash is the integer
          itself, and the value is reduced modulo the number of buckets—four, in
        this case:<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-645" id="calibre_link-664">113</a>]</sup></p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">dfs -cat /user/hive/warehouse/bucketed_users/000000_0;</code></strong>
0Nat
4Ann</pre><p class="calibre2">We can see the same thing by sampling the table using <a class="calibre" id="calibre_link-3617"></a>the <code class="literal">TABLESAMPLE</code>
        clause, which restricts the query to a fraction of the buckets in the
        table rather than the whole table:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT * FROM bucketed_users</code></strong>
<code class="literal">    &gt; </code><strong class="userinput"><code class="calibre9">TABLESAMPLE(BUCKET 1 OUT OF 4 ON id);</code></strong>
4    Ann
0    Nat</pre><p class="calibre2">Bucket numbering is 1-based, so this query retrieves all the
        users from the first of four buckets. For a large, evenly distributed
        dataset, approximately one-quarter of the table’s rows would be
        returned. It’s possible to sample a number of buckets by specifying a
        different proportion (which need not be an exact multiple of the
        number of buckets, as sampling is not intended to be a precise
        operation). For example, this query returns half of the
        buckets:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT * FROM bucketed_users</code></strong>
<code class="literal">    &gt; </code><strong class="userinput"><code class="calibre9">TABLESAMPLE(BUCKET 1 OUT OF 2 ON id);</code></strong>
4    Ann
0    Nat
2    Joe</pre><p class="calibre2">Sampling a bucketed table is very efficient because the query
        only has to read the buckets that match the <code class="literal">TABLESAMPLE</code> clause. Contrast this with
        sampling a nonbucketed table using the <code class="literal">rand()</code> function, where the whole input
        dataset is scanned, even if only a very small sample is <a class="calibre" id="calibre_link-2027"></a><a class="calibre" id="calibre_link-3604"></a><a class="calibre" id="calibre_link-1026"></a>needed:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT * FROM users</code></strong>
<code class="literal">    &gt;</code> <strong class="userinput"><code class="calibre9">TABLESAMPLE(BUCKET 1 OUT OF 4 ON rand());</code></strong>
2    Joe</pre></div></div><div class="book" title="Storage Formats"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-634">Storage Formats</h3></div></div></div><p class="calibre2">There are two <a class="calibre" id="calibre_link-2037"></a><a class="calibre" id="calibre_link-3614"></a><a class="calibre" id="calibre_link-1352"></a>dimensions that govern table storage in Hive: the
      <em class="calibre10">row format</em> and the <em class="calibre10">file
      format</em>. The row format dictates how rows, and the fields in
      a particular row, are stored. In Hive parlance, the row format is
      defined<a class="calibre" id="calibre_link-3349"></a><a class="calibre" id="calibre_link-3374"></a> by a <em class="calibre10">SerDe</em>, a portmanteau word for
      a <span class="calibre">Ser</span>ializer-<span class="calibre">De</span>serializer.</p><p class="calibre2">When acting as a deserializer, which is the case when querying a
      table, a SerDe will deserialize a row of data from the bytes in the file
      to objects used internally by Hive to operate on that row of data. When
      used as a serializer, which is the case when performing an <code class="literal">INSERT</code> or CTAS (see <a class="ulink" href="#calibre_link-646" title="Importing Data">Importing Data</a>), the table’s SerDe will serialize Hive’s
      internal representation of a row of data into the bytes that are written
      to the output file.</p><p class="calibre2">The file format dictates the container format for fields in a row.
      The simplest format is a plain-text file, but there are row-oriented and
      column-oriented binary formats available, too.</p><div class="book" title="The default storage format: Delimited text"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4537">The default storage format: Delimited text</h4></div></div></div><p class="calibre2">When you create a table <a class="calibre" id="calibre_link-1427"></a>with no <code class="literal">ROW FORMAT</code> or
        <code class="literal">STORED AS</code> clauses, the default
        format is delimited text with one row per line.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-647" id="calibre_link-665">114</a>]</sup></p><p class="calibre2">The default row delimiter is not a tab character, but the
        Ctrl-A character from the set of ASCII control codes (it has ASCII
        code 1). The choice of Ctrl-A, sometimes written as
        <code class="literal">^A</code> in documentation, came about because it is less
        likely to be a part of the field text than a tab character. There is
        no means for escaping delimiter characters in Hive, so it is important
        to choose ones that don’t occur in data fields.</p><p class="calibre2">The default collection item delimiter is a Ctrl-B character,
        used to delimit items in an <code class="literal">ARRAY</code>
        or <code class="literal">STRUCT</code>, or in key-value pairs in
        a <code class="literal">MAP</code>. The default map key
        delimiter is a Ctrl-C character, used to delimit the key and value
        in a <code class="literal">MAP</code>. Rows in a table are
        delimited by a newline character.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">The preceding description of delimiters is correct for the
          usual case of flat data structures, where the complex types contain
          only primitive types. For nested types, however, this isn’t the
          whole story, and in fact the <span class="calibre">level</span> of the nesting determines the
          delimiter.</p><p class="calibre2">For an array of arrays, for example, the delimiters for the
          outer array are Ctrl-B characters, as expected, but for the inner
          array they are Ctrl-C
          characters, the next delimiter in the list. If you are unsure which
          delimiters Hive uses for a particular nested structure, you can run
          a command like:</p><pre class="screen2">CREATE TABLE nested
AS
SELECT array(array(1, 2), array(3, 4))
FROM dummy;</pre><p class="calibre2">and then use <code class="literal">hexdump</code> or
          <a class="calibre" id="calibre_link-1990"></a>something similar to examine the delimiters in the
          output file.</p><p class="calibre2">Hive actually supports eight levels of delimiters,
          corresponding to ASCII codes 1, 2, ... 8, but you can override only
          the first three.</p></div><p class="calibre2">Thus, the statement:</p><a id="calibre_link-4538" class="calibre"></a><pre class="screen1">CREATE TABLE <em class="replaceable"><code class="replaceable">...</code></em>;</pre><p class="calibre2">is identical to the more explicit:</p><a id="calibre_link-4539" class="calibre"></a><pre class="screen1">CREATE TABLE <em class="replaceable"><code class="replaceable">...</code></em>
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\001'
  COLLECTION ITEMS TERMINATED BY '\002'
  MAP KEYS TERMINATED BY '\003'
  LINES TERMINATED BY '\n'
STORED AS TEXTFILE;</pre><p class="calibre2">Notice that the octal form of the delimiter characters can be
        used—001 for Ctrl-A, for instance.</p><p class="calibre2">Internally, Hive uses a SerDe called
        <code class="literal">LazySimpleSerDe</code> for this delimited format,
        along with the line-oriented MapReduce text input and output formats
        we saw in <a class="ulink" href="#calibre_link-568" title="Chapter&nbsp;8.&nbsp;MapReduce Types and Formats">Chapter&nbsp;8</a>. The “lazy” prefix comes
        about because it deserializes fields lazily—only as they are accessed.
        However, it is not a compact format because fields are stored in a
        verbose textual format, so a Boolean value, for instance, is written
        as the literal string <code class="literal">true</code> or
        <code class="literal">false</code>.</p><p class="calibre2">The simplicity of the format has a lot going for it, such as
        making it easy to process with other tools, including MapReduce
        programs or Streaming, but there are more compact and performant
        binary storage formats that you might consider using. <a class="calibre" id="calibre_link-1428"></a>These are discussed next.</p></div><div class="book" title="Binary storage formats: Sequence files, Avro datafiles, Parquet files, RCFiles, and ORCFiles"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4540">Binary storage formats: Sequence files, Avro datafiles, Parquet
        files, RCFiles, and ORCFiles</h4></div></div></div><p class="calibre2">Using a binary <a class="calibre" id="calibre_link-985"></a><a class="calibre" id="calibre_link-936"></a><a class="calibre" id="calibre_link-2912"></a>format is as simple as <a class="calibre" id="calibre_link-3534"></a>changing the <code class="literal">STORED
        AS</code> clause in the <code class="literal">CREATE
        TABLE</code> statement. <a class="calibre" id="calibre_link-1286"></a>In this case, the <code class="literal">ROW
        FORMAT</code> is not specified, since the format is controlled by
        the underlying binary file format.</p><p class="calibre2">Binary formats can be divided into two categories: row-oriented
        formats and column-oriented formats. Generally speaking,
        column-oriented formats work well when queries access only a small
        number of columns in the table, whereas row-oriented formats are
        appropriate when a large number of columns of a single row are needed
        for processing at the same time.</p><p class="calibre2">The two row-oriented formats supported natively in Hive are Avro
        datafiles (see <a class="ulink" href="#calibre_link-133" title="Chapter&nbsp;12.&nbsp;Avro">Chapter&nbsp;12</a>) and sequence files (see
        <a class="ulink" href="#calibre_link-141" title="SequenceFile">SequenceFile</a>). Both are general-purpose,
        splittable, compressible formats; in addition, Avro supports schema
        evolution and multiple language bindings. From Hive 0.14.0, a table
        can be stored in Avro format using:</p><pre class="screen1">SET hive.exec.compress.output=true;
SET avro.output.codec=snappy;
CREATE TABLE <em class="replaceable"><code class="replaceable">...</code></em> STORED AS AVRO;</pre><p class="calibre2">Notice that compression is enabled on the table by setting the
        relevant properties.</p><p class="calibre2">Similarly, the declaration <code class="literal">STORED AS
        </code><span class="calibre"><code class="literal">SEQUENCEFILE</code> can
        be used to store sequence files in Hive.</span> The properties for
        compression are listed in <a class="ulink" href="#calibre_link-599" title="Using Compression in MapReduce">Using Compression in MapReduce</a>.</p><p class="calibre2">Hive has native support for the Parquet (see <a class="ulink" href="#calibre_link-208" title="Chapter&nbsp;13.&nbsp;Parquet">Chapter&nbsp;13</a>), RCFile, and ORCFile <a class="calibre" id="calibre_link-2849"></a><a class="calibre" id="calibre_link-2847"></a>column-oriented binary formats (see <a class="ulink" href="#calibre_link-369" title="Other File Formats and Column-Oriented Formats">Other File Formats and Column-Oriented Formats</a>). Here is an
        example of creating a copy of a table in Parquet format using
        <code class="literal">CREATE TABLE...AS SELECT</code> (see <a class="ulink" href="#calibre_link-627" title="CREATE TABLE...AS SELECT">CREATE TABLE...AS SELECT</a>):</p><pre class="screen1">CREATE TABLE users_parquet STORED AS PARQUET
AS
SELECT * FROM users;</pre></div><div class="book" title="Using a custom SerDe: RegexSerDe"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4541">Using a custom SerDe: RegexSerDe</h4></div></div></div><p class="calibre2">Let’s see how to use a custom SerDe for loading data. We’ll use
        a contrib SerDe that uses a <a class="calibre" id="calibre_link-3210"></a>regular expression for reading the fixed-width station
        metadata from a text file:</p><pre class="screen1">CREATE TABLE stations (usaf STRING, wban STRING, name STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "(\\d{6}) (\\d{5}) (.{29}) .*"
);</pre><p class="calibre2">In previous examples, we have used <a class="calibre" id="calibre_link-1426"></a>the <code class="literal">DELIMITED</code> keyword
        to refer to delimited text in <a class="calibre" id="calibre_link-3245"></a>the <code class="literal">ROW FORMAT</code>
        clause. In this example, we instead specify a SerDe with <a class="calibre" id="calibre_link-3351"></a>the <code class="literal">SERDE</code> keyword and
        the fully qualified classname of the Java class that implements the
        <a class="calibre" id="calibre_link-3207"></a>SerDe, <code class="literal">org.apache.hadoop.hive.contrib.serde2.RegexSerDe</code>.</p><p class="calibre2">SerDes can be configured with extra properties <a class="calibre" id="calibre_link-3786"></a>using the <code class="literal">WITH
        SERDEPROPERTIES</code> clause. Here we set the <code class="literal">input.regex</code> property, which is specific to
        <code class="literal">RegexSerDe</code>.</p><p class="calibre2"><code class="literal">input.regex</code> is the regular
        expression pattern to be used during deserialization to turn the line
        of text forming the row into a set of columns. <a class="ulink" href="http://bit.ly/java_regex" target="_top">Java
        regular expression syntax</a> is used for the matching, and
        columns are formed from capturing groups of parentheses.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-648" id="calibre_link-666">115</a>]</sup> In this example, there are three capturing groups for
        <code class="literal">usaf</code> (a six-digit identifier),
        <code class="literal">wban</code> (a five-digit identifier), and
        <code class="literal">name</code> (a fixed-width column of 29
        characters).</p><p class="calibre2">To populate the table, we <a class="calibre" id="calibre_link-2336"></a>use a <code class="literal">LOAD DATA</code>
        statement as before:</p><pre class="screen1">LOAD DATA LOCAL INPATH "input/ncdc/metadata/stations-fixed-width.txt"
INTO TABLE stations;</pre><p class="calibre2">Recall that <code class="literal">LOAD DATA</code> copies
        or moves the files to Hive’s warehouse directory (in this case, it’s a
        copy because the source is the local filesystem). The table’s SerDe is
        not used for the load operation.</p><p class="calibre2">When we retrieve data from the table the SerDe is invoked for
        deserialization, as we can see from this simple query, which correctly
        parses the fields for each row:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT * FROM stations LIMIT 4;</code></strong>
010000    99999    BOGUS NORWAY                 
010003    99999    BOGUS NORWAY                 
010010    99999    JAN MAYEN                    
010013    99999    ROST</pre><p class="calibre2">As this example demonstrates, <code class="literal">RegexSerDe</code>
        can be useful for getting data into Hive, but due to its inefficiency
        it should not be used for general-purpose storage. Consider copying
        the data into a binary storage <a class="calibre" id="calibre_link-3350"></a><a class="calibre" id="calibre_link-3375"></a>format instead.</p></div><div class="book" title="Storage handlers"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4542">Storage handlers</h4></div></div></div><p class="calibre2">Storage handlers are <a class="calibre" id="calibre_link-3529"></a>used for storage systems that Hive<a class="calibre" id="calibre_link-1907"></a> cannot access natively, such as HBase. Storage handlers
        are specified <a class="calibre" id="calibre_link-3535"></a>using a <code class="literal">STORED BY</code>
        clause, instead of the <code class="literal">ROW FORMAT</code>
        and <code class="literal">STORED AS</code> clauses. For more
        information on HBase integration<a class="calibre" id="calibre_link-2038"></a><a class="calibre" id="calibre_link-3615"></a><a class="calibre" id="calibre_link-1353"></a>, see the <a class="ulink" href="http://bit.ly/hbase_int" target="_top">Hive
        wiki</a>.</p></div></div><div class="book" title="Importing Data"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-646">Importing Data</h3></div></div></div><p class="calibre2">We’ve already <a class="calibre" id="calibre_link-2031"></a><a class="calibre" id="calibre_link-3608"></a><a class="calibre" id="calibre_link-2080"></a>seen how to use the <code class="literal">LOAD
      DATA</code> operation to import data into a Hive table (or partition)
      by copying or moving files to the table’s directory. You can also
      populate a table with data from another Hive table using an <code class="literal">INSERT</code> statement, or at creation time using
      the <em class="calibre10">CTAS</em> construct, which is an abbreviation used
      to refer to <code class="literal">CREATE TABLE...AS
      SELECT</code>.</p><p class="calibre2">If you want to import data from a relational database directly
      into Hive, have a look at Sqoop; this is covered in <a class="ulink" href="#calibre_link-649" title="Imported Data and Hive">Imported Data and Hive</a>.</p><div class="book" title="Inserts"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-624">Inserts</h4></div></div></div><p class="calibre2">Here’s an example of <a class="calibre" id="calibre_link-2131"></a>an <code class="literal">INSERT</code>
        statement:</p><pre class="screen1">INSERT OVERWRITE TABLE target
SELECT col1, col2
  FROM source;</pre><p class="calibre2">For partitioned tables, you can specify the partition to insert
        into by supplying <a class="calibre" id="calibre_link-2944"></a>a <code class="literal">PARTITION</code> clause:</p><pre class="screen1">INSERT OVERWRITE TABLE target
<span class="calibre24"><strong class="calibre9">PARTITION (dt='2001-01-01')</strong></span>
SELECT col1, col2
  FROM source;</pre><p class="calibre2">The <code class="literal">OVERWRITE</code> keyword means
        that the contents of the <code class="literal">target</code>
        table (for the first example) or the <code class="literal">2001-01-01</code> partition (for the second
        example) are replaced by the results of the <code class="literal">SELECT</code> statement. If you want to add records
        to an already populated nonpartitioned table or partition, <a class="calibre" id="calibre_link-2128"></a>use <code class="literal">INSERT INTO
        TABLE</code>.</p><p class="calibre2">You can specify the partition dynamically by determining the
        partition value from <a class="calibre" id="calibre_link-3318"></a>the <code class="literal">SELECT</code>
        statement:</p><pre class="screen1">INSERT OVERWRITE TABLE target
<span class="calibre24"><strong class="calibre9">PARTITION (dt)</strong></span>
SELECT col1, col2, <span class="calibre24"><strong class="calibre9">dt</strong></span>
  FROM source;</pre><p class="calibre2">This is known as <a class="calibre" id="calibre_link-1557"></a>a <em class="calibre10">dynamic partition
        insert</em>.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">From Hive 0.14.0, you can use the <code class="literal">INSERT INTO TABLE...VALUES</code> statement for
          inserting a small collection of records specified in literal
          form.</p></div></div><div class="book" title="Multitable insert"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-621">Multitable insert</h4></div></div></div><p class="calibre2">In HiveQL, you can turn the <code class="literal">INSERT</code> statement around and start with the
        <code class="literal">FROM</code> clause for the same
        effect:</p><pre class="screen1">FROM source
INSERT OVERWRITE TABLE target
  SELECT col1, col2;</pre><p class="calibre2">The reason for this syntax becomes clear when you see that it’s
        possible to have multiple <code class="literal">INSERT</code>
        clauses in the same query. <a class="calibre" id="calibre_link-2728"></a>This so-called <em class="calibre10">multitable insert</em>
        is more efficient than multiple <code class="literal">INSERT</code> statements because the source table
        needs to be scanned only once to produce the multiple disjoint
        outputs.</p><p class="calibre2">Here’s an example that computes various statistics over the
        weather dataset:</p><a id="calibre_link-4543" class="calibre"></a><pre class="screen1">FROM records2
INSERT OVERWRITE TABLE stations_by_year
  SELECT year, COUNT(DISTINCT station)
  GROUP BY year 
INSERT OVERWRITE TABLE records_by_year
  SELECT year, COUNT(1)
  GROUP BY year
INSERT OVERWRITE TABLE good_records_by_year
  SELECT year, COUNT(1)
  WHERE temperature != 9999 AND quality IN (0, 1, 4, 5, 9)
  GROUP BY year;</pre><p class="calibre2">There is a single source table (<code class="literal">records2</code>), but three tables to hold the
        results from three different queries over the source.</p></div><div class="book" title="CREATE TABLE...AS SELECT"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-627">CREATE TABLE...AS SELECT</h4></div></div></div><p class="calibre2">It’s often very <a class="calibre" id="calibre_link-1287"></a>convenient to store the output of a Hive query in a new
        table, perhaps because it is too large to be dumped to the console or
        because there are further processing steps to carry out on the
        result.</p><p class="calibre2">The new table’s column definitions are derived from the columns
        retrieved by the <code class="literal">SELECT</code> clause. In
        the following query, the <code class="literal">target</code>
        table has two columns named <code class="literal">col1</code>
        and <code class="literal">col2</code> whose types are the same
        as the ones in the <code class="literal">source</code>
        table:</p><pre class="screen1">CREATE TABLE target
AS
SELECT col1, col2
FROM source;</pre><p class="calibre2">A CTAS operation is atomic, so if the <code class="literal">SELECT</code> query fails for some reason, the
        table is <a class="calibre" id="calibre_link-2032"></a><a class="calibre" id="calibre_link-2081"></a><a class="calibre" id="calibre_link-3609"></a>not created.</p></div></div><div class="book" title="Altering Tables"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4544">Altering Tables</h3></div></div></div><p class="calibre2">Because Hive <a class="calibre" id="calibre_link-2024"></a><a class="calibre" id="calibre_link-3601"></a>uses the schema-on-read approach, it’s flexible in
      permitting a table’s definition to change after the table has been
      created. The general caveat, however, is that in many cases, it is up to
      you to ensure that the data is changed to reflect the new
      structure.</p><p class="calibre2">You can rename a table <a class="calibre" id="calibre_link-883"></a>using the <code class="literal">ALTER TABLE</code>
      statement:</p><pre class="screen1">ALTER TABLE source RENAME TO target;</pre><p class="calibre2">In addition to updating the table metadata, <code class="literal">ALTER TABLE</code> moves the underlying table
      directory so that it reflects the new name. In the current example,
      <em class="calibre10">/user/hive/warehouse/source</em> is
      renamed to <em class="calibre10">/user/hive/warehouse/target</em>. (An external
      table’s underlying directory is
      not moved; only the metadata is updated.)</p><p class="calibre2">Hive allows you to change the definition for columns, add new
      columns, or even replace all existing columns in a table with a new
      set.</p><p class="calibre2">For example, consider adding a new column:</p><pre class="screen1">ALTER TABLE target ADD COLUMNS (col3 STRING);</pre><p class="calibre2">The new column <code class="literal">col3</code> is added
      after the existing (nonpartition) columns. The datafiles are not
      updated, so queries will return <code class="literal">null</code>
      for all values of <code class="literal">col3</code> (unless of
      course there were extra fields already present in the files). Because
      Hive does not permit updating existing records, you will need to arrange
      for the underlying files to be updated by another mechanism. For this
      reason, it is more common to create a new table that defines new columns
      and populates them using a <code class="literal">SELECT</code>
      statement.</p><p class="calibre2">Changing a column’s metadata, such as a column’s name or data
      type, is more straightforward, assuming that the old data type can be
      interpreted as the new data type.</p><p class="calibre2">To learn more about how to alter a table’s structure, including
      adding and dropping partitions, changing and replacing columns, and
      changing table and SerDe properties, see the <a class="ulink" href="http://bit.ly/data_def_lang" target="_top">Hive wiki</a>.</p></div><div class="book" title="Dropping Tables"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4545">Dropping Tables</h3></div></div></div><p class="calibre2">The <code class="literal">DROP TABLE</code> statement
      <a class="calibre" id="calibre_link-2028"></a><a class="calibre" id="calibre_link-3605"></a><a class="calibre" id="calibre_link-1547"></a>deletes the data and metadata for a table. In the case of
      external tables, only the metadata is deleted; the data is left
      untouched.</p><p class="calibre2">If you want to delete all the data in a table but keep the table
      definition, <a class="calibre" id="calibre_link-3719"></a>use <code class="literal">TRUNCATE TABLE</code>. For
      example:</p><pre class="screen1">TRUNCATE TABLE my_table;</pre><p class="calibre2">This doesn’t work for external tables; instead, use <code class="literal">dfs -rmr</code> (from the Hive shell) to remove the
      external table directory directly.</p><p class="calibre2">In a similar vein, if you want to create a new, empty table with
      the same schema as another table, then use the <code class="literal">LIKE</code> keyword:</p><pre class="screen1">CREATE TABLE new_table LIKE existing_table;</pre></div></div><div class="book" title="Querying Data"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-628">Querying Data</h2></div></div></div><p class="calibre2">This section discusses <a class="calibre" id="calibre_link-2014"></a>how to use various forms of the <code class="literal">SELECT</code> statement to retrieve data from
    Hive.</p><div class="book" title="Sorting and Aggregating"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-746">Sorting and Aggregating</h3></div></div></div><p class="calibre2">Sorting data in Hive can be achieved by using <a class="calibre" id="calibre_link-3104"></a><a class="calibre" id="calibre_link-3115"></a><a class="calibre" id="calibre_link-877"></a><a class="calibre" id="calibre_link-3431"></a><a class="calibre" id="calibre_link-2851"></a>a standard <code class="literal">ORDER BY</code>
      clause. <code class="literal">ORDER BY</code> performs a parallel
      total sort of the input (like that described in <a class="ulink" href="#calibre_link-80" title="Total Sort">Total Sort</a>). When a globally sorted result is not
      required—and in many cases it isn’t—you can use Hive’s nonstandard
      <a class="calibre" id="calibre_link-3422"></a>extension, <code class="literal">SORT BY</code>,
      instead. <code class="literal">SORT BY</code> produces a sorted
      file per reducer.</p><p class="calibre2">In some cases, you want to control which reducer a particular row
      goes to—typically so you can perform some subsequent aggregation. This
      is what Hive’s <code class="literal">DISTRIBUTE BY</code> clause
      <a class="calibre" id="calibre_link-1518"></a>does. Here’s an example to sort the weather dataset by
      year and temperature, in such a way as to ensure that all the rows for a
      given year end up in the same reducer partition:<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-650" id="calibre_link-667">116</a>]</sup></p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">FROM records2</code></strong>
<code class="literal">    &gt; </code><strong class="userinput"><code class="calibre9">SELECT year, temperature</code></strong>
<code class="literal">    &gt; </code><strong class="userinput"><code class="calibre9">DISTRIBUTE BY year</code></strong>
<code class="literal">    &gt; </code><strong class="userinput"><code class="calibre9">SORT BY year ASC, temperature DESC;</code></strong>
1949    111
1949    78
1950    22
1950    0
1950    -11</pre><p class="calibre2">A follow-on query (or a query that nests this query as a subquery;
      see <a class="ulink" href="#calibre_link-630" title="Subqueries">Subqueries</a>) would be able to use the fact that
      each year’s temperatures were grouped and sorted (in descending order)
      in the same file.</p><p class="calibre2">If the columns for <code class="literal">SORT BY</code> and
      <code class="literal">DISTRIBUTE BY</code> are the same, you can
      <a class="calibre" id="calibre_link-1104"></a>use <code class="literal">CLUSTER BY</code> as a
      shorthand for specifying both.</p></div><div class="book" title="MapReduce Scripts"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-622">MapReduce Scripts</h3></div></div></div><p class="calibre2">Using an approach like Hadoop Streaming,<a class="calibre" id="calibre_link-3114"></a><a class="calibre" id="calibre_link-3295"></a><a class="calibre" id="calibre_link-1849"></a><a class="calibre" id="calibre_link-3714"></a><a class="calibre" id="calibre_link-2386"></a><a class="calibre" id="calibre_link-3176"></a><a class="calibre" id="calibre_link-2495"></a> the <code class="literal">TRANSFORM</code>,
      <code class="literal">MAP</code>, and <code class="literal">REDUCE</code> clauses make it possible to invoke an
      external script or program from Hive. Suppose we want to use a script to
      filter out rows that don’t meet some condition, such as the script in
      <a class="ulink" href="#calibre_link-651" title="Example&nbsp;17-1.&nbsp;Python script to filter out poor-quality weather records">Example&nbsp;17-1</a>, which removes poor-quality
      <a class="calibre" id="calibre_link-3297"></a><a class="calibre" id="calibre_link-3099"></a>readings.</p><div class="example"><a id="calibre_link-651" class="calibre"></a><div class="example-title">Example&nbsp;17-1.&nbsp;Python script to filter out poor-quality weather
        records</div><div class="book"><pre class="screen"><code class="c1">#!/usr/bin/env python</code>

<code class="k">import</code> <code class="nn">re</code>
<code class="k">import</code> <code class="nn">sys</code>

<code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">sys</code><code class="o">.</code><code class="n">stdin</code><code class="p">:</code>
  <code class="p">(</code><code class="n">year</code><code class="p">,</code> <code class="n">temp</code><code class="p">,</code> <code class="n">q</code><code class="p">)</code> <code class="o">=</code> <code class="n">line</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>
  <code class="k">if</code> <code class="p">(</code><code class="n">temp</code> <code class="o">!=</code> <code class="sb">"9999"</code> <code class="ow">and</code> <code class="n">re</code><code class="o">.</code><code class="n">match</code><code class="p">(</code><code class="sb">"[01459]"</code><code class="p">,</code> <code class="n">q</code><code class="p">)):</code>
    <code class="k">print</code> <code class="sb">"</code><code class="si">%s</code><code class="se">\t</code><code class="si">%s</code><code class="sb">"</code> <code class="o">%</code> <code class="p">(</code><code class="n">year</code><code class="p">,</code> <code class="n">temp</code><code class="p">)</code></pre></div></div><p class="calibre2">We can use the script as follows:</p><a id="calibre_link-4546" class="calibre"></a><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">ADD FILE /Users/tom/book-workspace/hadoop-book/ch17-hive/
src/main/python/is_good_quality.py;</code></strong>
<code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">FROM records2</code></strong>
<code class="literal">    &gt; </code><strong class="userinput"><code class="calibre9">SELECT TRANSFORM(year, temperature, quality)</code></strong>
<code class="literal">    &gt; </code><strong class="userinput"><code class="calibre9">USING 'is_good_quality.py'</code></strong>
<code class="literal">    &gt; </code><strong class="userinput"><code class="calibre9">AS year, temperature;</code></strong>
1950    0
1950    22
1950    -11
1949    111
1949    78</pre><p class="calibre2">Before running the query, we need to register the script with
      Hive. This is so Hive knows to ship the file to the Hadoop cluster (see
      <a class="ulink" href="#calibre_link-54" title="Distributed Cache">Distributed Cache</a>).</p><p class="calibre2">The query itself streams the <code class="literal">year</code>, <code class="literal">temperature</code>, and <code class="literal">quality</code> fields as a tab-separated line to the
      <em class="calibre10">is_good_quality.py</em> script, and
      parses the tab-separated output into <code class="literal">year</code> and <code class="literal">temperature</code> fields to form the output of the
      query.</p><p class="calibre2">This example has no reducers. If we use a nested form for the
      query, we can specify a map and a reduce function. This time we use the
      <code class="literal">MAP</code> and <code class="literal">REDUCE</code> keywords, but <code class="literal">SELECT TRANSFORM</code> in both cases would have the
      same result. (<a class="ulink" href="#calibre_link-484" title="Example&nbsp;2-10.&nbsp;Reduce function for maximum temperature in Python">Example&nbsp;2-10</a> includes the
      source for the <span class="calibre">max_temperature_reduce.py</span> script):</p><pre class="screen1">FROM (
  FROM records2
  <span class="calibre24"><strong class="calibre9">MAP</strong></span> year, temperature, quality
  USING 'is_good_quality.py'
  AS year, temperature) map_output
<span class="calibre24"><strong class="calibre9">REDUCE</strong></span> year, temperature
USING 'max_temperature_reduce.py'
AS year, temperature;</pre></div><div class="book" title="Joins"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-629">Joins</h3></div></div></div><p class="calibre2">One of the nice <a class="calibre" id="calibre_link-3112"></a><a class="calibre" id="calibre_link-2277"></a>things about using Hive, rather than raw MapReduce, is
      that Hive makes performing commonly used operations very simple. Join
      operations are a case in point, given how involved they are to implement
      in MapReduce (see <a class="ulink" href="#calibre_link-262" title="Joins">Joins</a>).</p><div class="book" title="Inner joins"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4547">Inner joins</h4></div></div></div><p class="calibre2">The simplest <a class="calibre" id="calibre_link-2100"></a><a class="calibre" id="calibre_link-2278"></a>kind of join is the inner join, where each match in the
        input tables results in a row in the output. Consider two small
        demonstration tables, <code class="literal">sales</code> (which
        lists the names of people and the IDs of the items they bought) and
        <code class="literal">things</code> (which lists the item IDs
        and their names):</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT * FROM sales;</code></strong>
Joe    2
Hank   4
Ali    0
Eve    3
Hank   2
<code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT * FROM things;</code></strong>
2    Tie
4    Coat
3    Hat
1    Scarf</pre><p class="calibre2">We can perform an inner join on the two tables as
        follows:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT sales.*, things.*</code></strong>
<code class="literal">    &gt; </code><strong class="userinput"><code class="calibre9">FROM sales JOIN things ON (sales.id = things.id);</code></strong>
Joe    2    2    Tie
Hank   4    4    Coat
Eve    3    3    Hat
Hank   2    2    Tie</pre><p class="calibre2">The table in the <code class="literal">FROM</code> clause
        (<code class="literal">sales</code>) is joined with the table in
        <a class="calibre" id="calibre_link-2273"></a><a class="calibre" id="calibre_link-1730"></a>the <code class="literal">JOIN</code> clause
        (<code class="literal">things</code>), using the predicate in
        the <code class="literal">ON</code> clause. Hive only supports
        equijoins, which means that only equality can be used in the join
        predicate, which here matches on the <code class="literal">id</code> column in both tables.</p><p class="calibre2">In Hive, you can join on multiple columns in the join predicate
        by specifying a series of expressions, separated by <code class="literal">AND</code> keywords. You can also join more than
        two tables by supplying additional <code class="literal">JOIN...ON...</code> clauses in the query. Hive is
        intelligent about trying to minimize the number of MapReduce jobs to
        perform the joins.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Hive (like MySQL and Oracle) allows you to list the join
          tables in the <code class="literal">FROM</code> clause and
          specify the join condition in the <code class="literal">WHERE</code> clause of a <code class="literal">SELECT</code> statement. For example, the
          following is another way of expressing the query we just saw:</p><pre class="screen2">SELECT sales.*, things.*
FROM sales, things
WHERE sales.id = things.id;</pre></div><p class="calibre2">A single join is implemented as a single MapReduce job, but
        multiple joins can be performed in less than one MapReduce job per
        join if the same column is used in the join condition.<sup class="calibre6">[<a class="firstname" href="#calibre_link-652" id="calibre_link-668">117</a>]</sup> You can see how many MapReduce jobs Hive will use for
        any particular query by prefixing it <a class="calibre" id="calibre_link-1588"></a>with the <code class="literal">EXPLAIN</code>
        keyword:</p><pre class="screen1"><span class="calibre24"><strong class="calibre9">EXPLAIN</strong></span>
SELECT sales.*, things.*
FROM sales JOIN things ON (sales.id = things.id);</pre><p class="calibre2">The <code class="literal">EXPLAIN</code> output includes
        many details about the execution plan for the query, including the
        abstract syntax tree, the dependency graph for the stages that Hive
        will execute, and information about each stage. Stages may be
        MapReduce jobs or operations such as file moves. For even more detail,
        prefix the query with <code class="literal">EXPLAIN
        EXTENDED</code>.</p><p class="calibre2">Hive currently uses a rule-based query optimizer for determining
        how to execute a query, but a cost-based optimizer is available from
        Hive 0.14.0.</p></div><div class="book" title="Outer joins"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-552">Outer joins</h4></div></div></div><p class="calibre2">Outer joins allow you to find<a class="calibre" id="calibre_link-2282"></a><a class="calibre" id="calibre_link-2873"></a> nonmatches in the tables being joined. In the current
        example, when we performed an inner join, the row for Ali did not
        appear in the output, because the ID of the item she purchased was not
        present in the <code class="literal">things</code> table. If we
        change the join type to <code class="literal">LEFT OUTER
        JOIN</code>, the <a class="calibre" id="calibre_link-2319"></a>query will return a row for every row in the left table
        (<code class="literal">sales</code>), even if there is no
        corresponding row in the table it is being joined to (<code class="literal">things</code>):</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT sales.*, things.*</code></strong>
<code class="literal">    &gt; </code><strong class="userinput"><code class="calibre9">FROM sales LEFT OUTER JOIN things ON (sales.id = things.id);</code></strong>
Joe    2    2    Tie
Hank   4    4    Coat
Ali    0    NULL NULL
Eve    3    3    Hat
Hank   2    2    Tie</pre><p class="calibre2">Notice that the row for Ali is now returned, and the columns
        from the <code class="literal">things</code> table are <code class="literal">NULL</code> because there is no match.</p><p class="calibre2">Hive also supports right outer joins, which reverses the roles
        of the tables relative to the left join. In this case, all items from
        the <code class="literal">things</code> table are included, even
        those that weren’t purchased by anyone (a scarf):</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT sales.*, things.*</code></strong>
<code class="literal">    &gt; </code><strong class="userinput"><code class="calibre9">FROM sales RIGHT OUTER JOIN things ON (sales.id = things.id);</code></strong>
Joe    2    2    Tie
Hank   2    2    Tie
Hank   4    4    Coat
Eve    3    3    Hat
NULL   NULL 1    Scarf</pre><p class="calibre2">Finally, there is a full outer join, where the output has a row
        for each row from both tables in the join:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT sales.*, things.*</code></strong>
<code class="literal">    &gt; </code><strong class="userinput"><code class="calibre9">FROM sales FULL OUTER JOIN things ON (sales.id = things.id);</code></strong>
Ali    0    NULL NULL
NULL   NULL 1    Scarf
Hank   2    2    Tie
Joe    2    2    Tie
Eve    3    3    Hat
Hank   4    4    Coat</pre></div><div class="book" title="Semi joins"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4548">Semi joins</h4></div></div></div><p class="calibre2">Consider this <code class="literal">IN</code> subquery,
        <a class="calibre" id="calibre_link-3323"></a><a class="calibre" id="calibre_link-2287"></a>which finds all the items in the <code class="literal">things</code> table that are in the <code class="literal">sales</code> table:</p><pre class="screen1">SELECT *
FROM things
WHERE things.id IN (SELECT id from sales);</pre><p class="calibre2">We can also express it as follows:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT *</code></strong>
<code class="literal">    &gt; </code><strong class="userinput"><code class="calibre9">FROM things LEFT SEMI JOIN sales ON (sales.id = things.id);</code></strong>
2    Tie
4    Coat
3    Hat</pre><p class="calibre2">There is a restriction that we must <a class="calibre" id="calibre_link-2320"></a>observe for <code class="literal">LEFT SEMI
        JOIN</code> queries: the right table (<code class="literal">sales</code>) may appear only in the <code class="literal">ON</code> clause. It cannot be referenced in a
        <code class="literal">SELECT</code> expression, for
        example.</p></div><div class="book" title="Map joins"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-644">Map joins</h4></div></div></div><p class="calibre2">Consider the original inner <a class="calibre" id="calibre_link-2281"></a>join again:</p><pre class="screen1">SELECT sales.*, things.*
FROM sales JOIN things ON (sales.id = things.id);</pre><p class="calibre2">If one table is small enough to fit in memory, as <code class="literal">things</code> is here, Hive can load it into memory
        to perform the join in each of the mappers. This is called a map
        join.</p><p class="calibre2">The job to execute this query has no reducers, so this query
        would not work for a <code class="literal">RIGHT</code> or
        <code class="literal">FULL OUTER JOIN</code>, since absence of
        matching can be detected only in an aggregating (reduce) step across
        all the inputs.</p><p class="calibre2">Map joins can take advantage of bucketed tables (see <a class="ulink" href="#calibre_link-613" title="Buckets">Buckets</a>), since a mapper working on a bucket of the left
        table needs to load only the corresponding buckets of the right table
        to perform the join. The syntax for the join is the same as for the
        in-memory case shown earlier; however, you also need to enable the
        optimization with the <a class="calibre" id="calibre_link-3113"></a>following:</p><pre class="screen1">SET hive.optimize.bucketmapjoin=true;</pre></div></div><div class="book" title="Subqueries"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-630">Subqueries</h3></div></div></div><p class="calibre2">A subquery is a <code class="literal">SELECT</code>
      statement <a class="calibre" id="calibre_link-3319"></a><a class="calibre" id="calibre_link-3116"></a><a class="calibre" id="calibre_link-3563"></a>that is embedded in another SQL statement. Hive has
      limited support for subqueries, permitting a subquery in the <code class="literal">FROM</code> clause of a <code class="literal">SELECT</code> statement, or in the <code class="literal">WHERE</code> clause in certain cases.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Hive allows uncorrelated subqueries, where the subquery is a
        self-contained query referenced by an <code class="literal">IN</code> or <code class="literal">EXISTS</code> statement in the <code class="literal">WHERE</code> clause. Correlated subqueries, where
        the subquery references the outer query, are not currently
        supported.</p></div><p class="calibre2">The following query finds the mean maximum temperature for every
      year and weather station:</p><pre class="screen1">SELECT station, year, AVG(max_temperature)
FROM (
  SELECT station, year, MAX(temperature) AS max_temperature
  FROM records2
  WHERE temperature != 9999 AND quality IN (0, 1, 4, 5, 9)
  GROUP BY station, year
) mt
GROUP BY station, year;</pre><p class="calibre2">The <code class="literal">FROM</code> subquery is used to
      find the maximum temperature for each station/date combination, and then
      the outer query uses the <code class="literal">AVG</code>
      aggregate function to find the average of the maximum temperature
      readings for each station/date combination.</p><p class="calibre2">The outer query accesses the results of the subquery like it does
      a table, which is why the subquery must be given an alias (<code class="literal">mt</code>). The columns of the subquery have to be
      given unique names so that the outer query can refer to them.</p></div><div class="book" title="Views"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-631">Views</h3></div></div></div><p class="calibre2">A view is a <a class="calibre" id="calibre_link-3117"></a><a class="calibre" id="calibre_link-3764"></a><a class="calibre" id="calibre_link-3616"></a>sort of “virtual table” that is defined by <a class="calibre" id="calibre_link-3320"></a>a <code class="literal">SELECT</code> statement.
      Views can be used to present data to users in a way that differs from
      the way it is actually stored on disk. Often, the data from existing
      tables is simplified or aggregated in a particular way that makes it
      convenient for further processing. Views may also be used to restrict
      users’ access to particular subsets of tables that they are authorized
      to see.</p><p class="calibre2">In Hive, a view is not materialized to disk when it is created;
      rather, the view’s <code class="literal">SELECT</code> statement
      is executed when the statement that refers to the view is run. If a view
      performs extensive transformations on the base tables or is used
      frequently, you may choose to manually materialize it by creating a new
      table that stores the contents of the view (see <a class="ulink" href="#calibre_link-627" title="CREATE TABLE...AS SELECT">CREATE TABLE...AS SELECT</a>).</p><p class="calibre2">We can use views to rework the query from the previous section for
      finding the mean maximum temperature for every year and weather station.
      First, let’s create a view for valid records—that is, records that have
      a particular <code class="literal">quality</code> value:</p><pre class="screen1">CREATE VIEW valid_records
AS
SELECT *
FROM records2
WHERE temperature != 9999 AND quality IN (0, 1, 4, 5, 9);</pre><p class="calibre2">When we create a view, the query is not run; it is simply stored
      in the metastore. Views are included in the output of <a class="calibre" id="calibre_link-3394"></a>the <code class="literal">SHOW TABLES</code>
      command, and you can see more details about a particular view, including
      the query used to define it, by issuing <a class="calibre" id="calibre_link-1435"></a>the <code class="literal">DESCRIBE EXTENDED
      <em class="replaceable"><code class="replaceable">view_name</code></em></code> command.</p><p class="calibre2">Next, let’s create a second view of maximum temperatures for each
      station and year. It is based on the <code class="literal">valid_records</code> view:</p><pre class="screen1">CREATE VIEW max_temperatures (station, year, max_temperature)
AS
SELECT station, year, MAX(temperature)
FROM valid_records
GROUP BY station, year;</pre><p class="calibre2">In this view definition, we list the column names explicitly. We
      do this because the maximum temperature column is an aggregate
      expression, and otherwise Hive would create a column alias for us (such
      as <code class="literal">_c2</code>). We could equally well have
      used an <code class="literal">AS</code> clause in the <code class="literal">SELECT</code> to name the column.</p><p class="calibre2">With the views in place, we can now use them by running a
      query:</p><pre class="screen1">SELECT station, year, AVG(max_temperature)
FROM max_temperatures
GROUP BY station, year;</pre><p class="calibre2">The result of the query is the same as that of running the one
      that uses a subquery. In particular, Hive creates the same number of
      MapReduce jobs for both: two in each case, one for <a class="calibre" id="calibre_link-1813"></a>each <code class="literal">GROUP BY</code>. This
      example shows that Hive can combine a query on a view into a sequence of
      jobs that is equivalent to writing the query without using a view. In
      other words, Hive won’t needlessly materialize a view, even at execution
      time.</p><p class="calibre2">Views in Hive are read-only, so there is no way to load or insert
      data into an underlying base table via a <a class="calibre" id="calibre_link-2015"></a>view.</p></div></div><div class="book" title="User-Defined Functions"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-632">User-Defined Functions</h2></div></div></div><p class="calibre2">Sometimes the query you want to <a class="calibre" id="calibre_link-1768"></a><a class="calibre" id="calibre_link-3730"></a>write can’t be expressed easily (or at all) using the
    built-in functions that Hive provides. By allowing you to write a
    <em class="calibre10">user-defined function</em> (UDF), Hive makes it easy to
    plug in your own processing code and invoke it from a Hive query.</p><p class="calibre2">UDFs have to be written in Java, the language that Hive itself is
    written in. For other languages, <a class="calibre" id="calibre_link-3321"></a>consider using a <code class="literal">SELECT
    TRANSFORM</code> query, which allows you to stream data through a
    user-defined script (<a class="ulink" href="#calibre_link-622" title="MapReduce Scripts">MapReduce Scripts</a>).</p><p class="calibre2">There are three types of UDF in Hive: (regular) UDFs, <a class="calibre" id="calibre_link-3753"></a><a class="calibre" id="calibre_link-3726"></a>user-defined aggregate functions (UDAFs), and <a class="calibre" id="calibre_link-3756"></a><a class="calibre" id="calibre_link-3736"></a>user-defined table-generating functions (UDTFs). They differ
    in the number of rows that they accept as input and produce as
    output:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">A UDF operates on a single row and produces a single row as its
        output. Most functions, such as mathematical functions and string
        functions, are of this type.</p></li><li class="listitem"><p class="calibre2">A UDAF works on multiple input rows and creates a single output
        row. Aggregate functions include such functions as <code class="literal">COUNT</code> and <code class="literal">MAX</code>.</p></li><li class="listitem"><p class="calibre2">A UDTF operates on a single row and produces multiple rows—a
        table—as output.</p></li></ul></div><p class="calibre2">Table-generating functions are less well known than the other two
    types, so let’s look at an example. Consider a table with a single column,
    <code class="literal">x</code>, which contains arrays of strings.
    It’s instructive to take a slight detour to see how the table is defined
    and populated:</p><pre class="screen1">CREATE TABLE arrays (x ARRAY&lt;STRING&gt;)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\001'
  COLLECTION ITEMS TERMINATED BY '\002';</pre><p class="calibre2">Notice that the <code class="literal">ROW FORMAT</code> clause
    <a class="calibre" id="calibre_link-3246"></a>specifies that the entries in the array are delimited by
    Ctrl-B characters. The example file that we are going to load has the
    following contents, where <code class="literal">^B</code> is a
    representation of the Ctrl-B character to make it suitable for
    printing:</p><pre class="screen1">a^Bb
c^Bd^Be</pre><p class="calibre2">After running a <code class="literal">LOAD DATA</code>
    command, the following query confirms that the data was loaded
    correctly:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT * FROM arrays;</code></strong>
["a","b"]
["c","d","e"]</pre><p class="calibre2">Next, we can use the <code class="literal">explode</code> UDTF
    to transform this table. This function emits a row for each entry in the
    array, so in this case the type of the output column <code class="literal">y</code> is <code class="literal">STRING</code>.
    The result is that the table is flattened into five rows:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT explode(x) AS y FROM arrays;</code></strong>
a
b
c
d
e</pre><p class="calibre2"><code class="literal">SELECT</code> statements using UDTFs
    have some restrictions (e.g., they cannot retrieve additional column
    expressions), which make them less useful in practice. For this reason,
    Hive <a class="calibre" id="calibre_link-2314"></a>supports <code class="literal">LATERAL VIEW</code>
    queries, which are more powerful. <code class="literal">LATERAL
    VIEW</code> queries are not covered here, but you may find out more
    about <a class="calibre" id="calibre_link-1769"></a>them in the <a class="ulink" href="http://bit.ly/lateral_view" target="_top">Hive
    wiki</a>.</p><div class="book" title="Writing a UDF"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4549">Writing a UDF</h3></div></div></div><p class="calibre2">To illustrate the <a class="calibre" id="calibre_link-1772"></a>process of writing and using a UDF, we’ll write a simple
      UDF to trim characters from the ends of strings. Hive already has a
      built-in function called <code class="literal">trim</code>, so
      we’ll call ours <code class="literal">strip</code>. The code for
      the <code class="literal">Strip</code> Java class is shown in
      <a class="ulink" href="#calibre_link-653" title="Example&nbsp;17-2.&nbsp;A UDF for stripping characters from the ends of strings">Example&nbsp;17-2</a>.</p><div class="example"><a id="calibre_link-653" class="calibre"></a><div class="example-title">Example&nbsp;17-2.&nbsp;A UDF for stripping characters from the ends of strings</div><div class="book"><pre class="screen"><code class="k">package</code> <code class="n">com</code><code class="o">.</code><code class="na">hadoopbook</code><code class="o">.</code><code class="na">hive</code><code class="o">;</code>

<code class="k">import</code> <code class="nn">org.apache.commons.lang.StringUtils</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.hive.ql.exec.UDF</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.io.Text</code><code class="o">;</code>

<code class="k">public</code> <code class="k">class</code> <code class="nc">Strip</code> <code class="k">extends</code> <code class="n">UDF</code> <code class="o">{</code>
  <code class="k">private</code> <code class="n">Text</code> <code class="n">result</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Text</code><code class="o">();</code>
  
  <code class="k">public</code> <code class="n">Text</code> <code class="nf">evaluate</code><code class="o">(</code><code class="n">Text</code> <code class="n">str</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">str</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">return</code> <code class="k">null</code><code class="o">;</code>
    <code class="o">}</code>
    <code class="n">result</code><code class="o">.</code><code class="na">set</code><code class="o">(</code><code class="n">StringUtils</code><code class="o">.</code><code class="na">strip</code><code class="o">(</code><code class="n">str</code><code class="o">.</code><code class="na">toString</code><code class="o">()));</code>
    <code class="k">return</code> <code class="n">result</code><code class="o">;</code>
  <code class="o">}</code>  <code class="k">public</code> <code class="n">Text</code> <code class="n">evaluate</code><code class="o">(</code><code class="n">Text</code> <code class="n">str</code><code class="o">,</code> <code class="n">String</code> <code class="n">stripChars</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">str</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">return</code> <code class="k">null</code><code class="o">;</code>
    <code class="o">}</code>
    <code class="n">result</code><code class="o">.</code><code class="na">set</code><code class="o">(</code><code class="n">StringUtils</code><code class="o">.</code><code class="na">strip</code><code class="o">(</code><code class="n">str</code><code class="o">.</code><code class="na">toString</code><code class="o">(),</code> <code class="n">stripChars</code><code class="o">));</code>
    <code class="k">return</code> <code class="n">result</code><code class="o">;</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">A UDF must satisfy the following two properties:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">A UDF must be a <a class="calibre" id="calibre_link-3729"></a>subclass of
          <code class="literal">org.apache.hadoop.hive.ql.exec.UDF</code>.</p></li><li class="listitem"><p class="calibre2">A UDF must implement at least one <code class="literal">evaluate()</code> method.</p></li></ul></div><p class="calibre2">The <code class="literal">evaluate()</code> method is not
      defined by an interface, since it may take an arbitrary number of
      arguments, of arbitrary types, and it may return a value of arbitrary
      type. Hive introspects the UDF to find the <code class="literal">evaluate()</code> method that matches the Hive
      function that was invoked.</p><p class="calibre2">The <code class="literal">Strip</code> class has two <code class="literal">evaluate()</code> methods. The first strips leading
      and trailing whitespace from the input, and the second can strip any of
      a set of supplied characters from the ends of the string. The actual
      string processing is delegated to the
      <code class="literal">String</code><code class="literal">Utils</code> class from
      the Apache Commons project, which makes the only noteworthy part of the
      code the use of <code class="literal">Text</code> from the Hadoop
      Writable library. Hive actually supports Java primitives in UDFs (and a
      few other types, such as <code class="literal">java.util.List</code> and <code class="literal">java.util.Map</code>), so a signature like:</p><pre class="screen1">public String evaluate(String str)</pre><p class="calibre2">would work equally well. However, by using <code class="literal">Text</code> we can take advantage of object reuse,
      which can bring efficiency savings, so this is preferred in
      general.</p><p class="calibre2">To use the UDF in Hive, we first need to package the compiled Java
      class in a JAR file. You can do this by typing <code class="literal">mvn package</code> with the book’s example code.
      Next, we register the function in the metastore and give it a name using
      <a class="calibre" id="calibre_link-1281"></a>the <code class="literal">CREATE FUNCTION</code>
      statement:</p><pre class="screen1">CREATE FUNCTION strip AS 'com.hadoopbook.hive.Strip'
USING JAR '/path/to/hive-examples.jar';</pre><p class="calibre2">When <a class="calibre" id="calibre_link-3757"></a>using Hive locally, a local file path is sufficient, but
      on a cluster you should copy the JAR file into HDFS and use an HDFS URI
      in the <code class="literal">USING JAR</code> clause.</p><p class="calibre2">The UDF is now ready to be used, just like a built-in
      function:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT strip('  bee  ') FROM dummy;</code></strong>
bee
<code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT strip('banana', 'ab') FROM dummy;</code></strong>
nan</pre><p class="calibre2">Notice that the UDF’s name is not <a class="calibre" id="calibre_link-1062"></a>case sensitive:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT STRIP('  bee  ') FROM dummy;</code></strong>
bee</pre><p class="calibre2">If you want to remove the function, use <a class="calibre" id="calibre_link-1545"></a>the <code class="literal">DROP FUNCTION</code>
      statement:</p><pre class="screen1">DROP FUNCTION strip;</pre><p class="calibre2">It’s also possible to create a function for the duration of the
      Hive session, so it is not persisted in the <a class="calibre" id="calibre_link-3645"></a>metastore, using the <code class="literal">TEMPORARY</code> keyword:</p><pre class="screen1">ADD JAR /path/to/hive-examples.jar;
CREATE TEMPORARY FUNCTION strip AS 'com.hadoopbook.hive.Strip';</pre><p class="calibre2">When using temporary functions, it may be useful to create a
      <em class="calibre10">.hiverc</em> file in your home directory
      containing the commands to define your UDFs. The file will be
      automatically run at the beginning of each Hive <a class="calibre" id="calibre_link-1773"></a>session.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">As an alternative to calling <code class="literal">ADD
        JAR</code> at launch time, you can specify a path where Hive looks
        for auxiliary JAR files to put on its classpath (including the task
        classpath). This technique is useful for automatically adding your own
        library of UDFs every time you run Hive.</p><p class="calibre2">There are two ways of specifying the path. Either pass the
        <code class="literal">--auxpath</code> option to the
        <code class="literal">hive</code> command:</p><pre class="screen2"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hive --auxpath /path/to/hive-examples.jar</code></strong></pre><p class="calibre2">or set the <code class="literal">HIVE_AUX_JARS_PATH</code>
        environment variable before invoking Hive. The auxiliary path may be a
        comma-separated list of JAR file paths or a directory containing JAR
        files.</p></div></div><div class="book" title="Writing a UDAF"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4550">Writing a UDAF</h3></div></div></div><p class="calibre2">An aggregate function is <a class="calibre" id="calibre_link-1770"></a><a class="calibre" id="calibre_link-3727"></a><a class="calibre" id="calibre_link-3754"></a>more difficult to write than a regular UDF. Values are
      aggregated in chunks (potentially across many tasks), so the
      implementation has to be capable of combining partial aggregations into
      a final result. The code to achieve this is best explained by example,
      so let’s look at the implementation of a simple UDAF for calculating the
      maximum of a collection of integers (<a class="ulink" href="#calibre_link-654" title="Example&nbsp;17-3.&nbsp;A UDAF for calculating the maximum of a collection of integers">Example&nbsp;17-3</a>).</p><div class="example"><a id="calibre_link-654" class="calibre"></a><div class="example-title">Example&nbsp;17-3.&nbsp;A UDAF for calculating the maximum of a collection of
        integers</div><div class="book"><pre class="screen"><code class="k">package</code> <code class="n">com</code><code class="o">.</code><code class="na">hadoopbook</code><code class="o">.</code><code class="na">hive</code><code class="o">;</code>

<code class="k">import</code> <code class="nn">org.apache.hadoop.hive.ql.exec.UDAF</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.hive.ql.exec.UDAFEvaluator</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.io.IntWritable</code><code class="o">;</code>

<code class="k">public</code> <code class="k">class</code> <code class="nc">Maximum</code> <code class="k">extends</code> <code class="n">UDAF</code> <code class="o">{</code>

  <code class="k">public</code> <code class="k">static</code> <code class="k">class</code> <code class="nc">MaximumIntUDAFEvaluator</code> <code class="k">implements</code> <code class="n">UDAFEvaluator</code> <code class="o">{</code>
    
    <code class="k">private</code> <code class="n">IntWritable</code> <code class="n">result</code><code class="o">;</code>
    
    <code class="k">public</code> <code class="kt">void</code> <code class="nf">init</code><code class="o">()</code> <code class="o">{</code>
      <code class="n">result</code> <code class="o">=</code> <code class="k">null</code><code class="o">;</code>
    <code class="o">}</code>

    <code class="k">public</code> <code class="kt">boolean</code> <code class="nf">iterate</code><code class="o">(</code><code class="n">IntWritable</code> <code class="n">value</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">value</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
        <code class="k">return</code> <code class="k">true</code><code class="o">;</code>
      <code class="o">}</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">result</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
        <code class="n">result</code> <code class="o">=</code> <code class="k">new</code> <code class="n">IntWritable</code><code class="o">(</code><code class="n">value</code><code class="o">.</code><code class="na">get</code><code class="o">());</code>
      <code class="o">}</code> <code class="k">else</code> <code class="o">{</code>
        <code class="n">result</code><code class="o">.</code><code class="na">set</code><code class="o">(</code><code class="n">Math</code><code class="o">.</code><code class="na">max</code><code class="o">(</code><code class="n">result</code><code class="o">.</code><code class="na">get</code><code class="o">(),</code> <code class="n">value</code><code class="o">.</code><code class="na">get</code><code class="o">()));</code>
      <code class="o">}</code>
      <code class="k">return</code> <code class="k">true</code><code class="o">;</code>
    <code class="o">}</code>

    <code class="k">public</code> <code class="n">IntWritable</code> <code class="nf">terminatePartial</code><code class="o">()</code> <code class="o">{</code>
      <code class="k">return</code> <code class="n">result</code><code class="o">;</code>
    <code class="o">}</code>

    <code class="k">public</code> <code class="kt">boolean</code> <code class="nf">merge</code><code class="o">(</code><code class="n">IntWritable</code> <code class="n">other</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">return</code> <code class="nf">iterate</code><code class="o">(</code><code class="n">other</code><code class="o">);</code>
    <code class="o">}</code>

    <code class="k">public</code> <code class="n">IntWritable</code> <code class="nf">terminate</code><code class="o">()</code> <code class="o">{</code>
      <code class="k">return</code> <code class="n">result</code><code class="o">;</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The class structure is slightly different from the one for UDFs. A
      UDAF must be a subclass of
      <code class="literal">org.apache.hadoop.hive.ql.exec.UDAF</code> (note the “A”
      in UDAF) <a class="calibre" id="calibre_link-3724"></a>and contain one or more nested static classes implementing
      <code class="literal">org.apache.hadoop.hive.ql.</code><code class="literal">exec.UDAFEvaluator</code>.
      In <a class="calibre" id="calibre_link-3725"></a>this example, there is a single nested class,
      <code class="literal">MaximumIntUDAFEvaluator</code>, but we could add more
      evaluators, such as <code class="literal">MaximumLongUDAFEvaluator</code>,
      <code class="literal">MaximumFloatUDAFEvaluator</code>, and so on, to provide
      overloaded forms of the UDAF for finding the maximum of a collection of
      longs, floats, and so on.</p><p class="calibre2">An evaluator must implement five methods, described in turn here
      (the flow is illustrated in <a class="ulink" href="#calibre_link-655" title="Figure&nbsp;17-3.&nbsp;Data flow with partial results for a UDAF">Figure&nbsp;17-3</a>):</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term"><code class="literal">init()</code></span></dt><dd class="calibre8"><p class="calibre2">The <code class="literal">init()</code> method initializes the
            evaluator and resets its internal state. In <code class="literal">MaximumIntUDAFEvaluator</code>,
            we set the <code class="literal">IntWritable</code> object holding the
            final result to <code class="literal">null</code>. We use
            <code class="literal">null</code> to indicate that no values
            have been aggregated yet, which has the desirable effect of making
            the maximum value of an empty set <code class="literal">NULL</code>.</p></dd><dt class="calibre7"><span class="term"><code class="literal">iterate()</code></span></dt><dd class="calibre8"><p class="calibre2">The <code class="literal">iterate()</code> method is called
            every time there is a new value to be aggregated. The evaluator
            should update its internal state with the result of performing the
            aggregation. The arguments that <code class="literal">iterate()</code>
            takes correspond to those in the Hive function from which it was
            called. In this example, there is only one argument. The value is
            first checked to see whether it is <code class="literal">null</code>, and if it is, it is ignored.
            Otherwise, the <code class="literal">result</code> instance
            variable is set either to <code class="literal">value</code>’s integer value (if this is the
            first value that has been seen) or to the larger of the current
            result and <code class="literal">value</code> (if one or
            more values have already been seen). We return <code class="literal">true</code> to indicate that the input value
            was valid.</p></dd><dt class="calibre7"><span class="term"><code class="literal">terminatePartial()</code></span></dt><dd class="calibre8"><p class="calibre2">The <code class="literal">terminatePartial()</code> method is
            called when Hive wants a result for the partial aggregation. The
            method must return an object that encapsulates the state of the
            aggregation. In this case, an <code class="literal">IntWritable</code>
            suffices because it encapsulates either the maximum value seen or
            <code class="literal">null</code> if no values have been
            processed.</p></dd><dt class="calibre7"><span class="term"><code class="literal">merge()</code></span></dt><dd class="calibre8"><p class="calibre2">The <code class="literal">merge()</code> method is called when
            Hive decides to combine one partial aggregation with another. The
            method takes a single object, whose type must correspond to the
            return type of the <code class="literal">terminatePartial()</code>
            method. In this example, the <code class="literal">merge()</code>
            method can simply delegate to the
            <code class="literal">iterate()</code> method because the partial
            aggregation is represented in the same way as a value being
            aggregated. This is not generally the case (we’ll see a more
            general example later), and the method should implement the logic
            to combine the evaluator’s state with the state of the partial
            aggregation.</p></dd><dt class="calibre7"><span class="term"><code class="literal">terminate()</code></span></dt><dd class="calibre8"><p class="calibre2">The <code class="literal">terminate()</code> method is called
            when the final result of the aggregation is needed. The evaluator
            should return its state as a value. In this case, we return the
            <code class="literal">result</code> instance
            variable.</p></dd></dl></div><p class="calibre2">Let’s exercise our new function:</p><pre class="screen1"><code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">CREATE TEMPORARY FUNCTION maximum AS 'com.hadoopbook.hive.Maximum';</code></strong>
<code class="literal">hive&gt; </code><strong class="userinput"><code class="calibre9">SELECT maximum(temperature) FROM records;</code></strong>
111</pre><div class="book"><div class="figure"><a id="calibre_link-655" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4551" class="calibre"></a><img alt="Data flow with partial results for a UDAF" src="images/000088.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;17-3.&nbsp;Data flow with partial results for a UDAF</div></div></div><div class="book" title="A more complex UDAF"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4552">A more complex UDAF</h4></div></div></div><p class="calibre2">The previous example is unusual in that a partial aggregation
        can be represented using the same type
        (<code class="literal">IntWritable</code>) as the final result. This is not
        generally the case for more
        complex aggregate functions, as can be seen by considering a UDAF for
        calculating the mean (average) of a collection of double values. It’s
        not mathematically possible to combine partial means into a final mean
        value (see <a class="ulink" href="#calibre_link-539" title="Combiner Functions">Combiner Functions</a>). Instead, we can
        represent the partial aggregation as a pair of numbers: the cumulative
        sum of the double values processed so far, and the number of
        values.</p><p class="calibre2">This idea is implemented in the UDAF shown in <a class="ulink" href="#calibre_link-656" title="Example&nbsp;17-4.&nbsp;A UDAF for calculating the mean of a collection of doubles">Example&nbsp;17-4</a>. Notice that the partial aggregation is implemented
        as a “struct” nested static class, called
        <code class="literal">PartialResult</code>, which Hive is intelligent enough
        to serialize and deserialize, since we are using field types that Hive
        can handle (Java primitives in this case).</p><p class="calibre2">In this example, the <code class="literal">merge()</code> method is
        different from <code class="literal">iterate()</code> because it combines
        the partial sums and partial counts by pairwise addition. In addition
        to this, the return type of <code class="literal">terminatePartial()</code>
        is <code class="literal">PartialResult</code>—which, of course, is never
        seen by the user calling the function—whereas the return type of
        <code class="literal">terminate()</code> is
        <code class="literal">DoubleWritable</code>, the final result seen by the
        <a class="calibre" id="calibre_link-3731"></a><a class="calibre" id="calibre_link-1771"></a><a class="calibre" id="calibre_link-3728"></a><a class="calibre" id="calibre_link-3755"></a>user.</p><div class="example"><a id="calibre_link-656" class="calibre"></a><div class="example-title">Example&nbsp;17-4.&nbsp;A UDAF for calculating the mean of a collection of
          doubles</div><div class="book"><pre class="screen"><code class="k">package</code> <code class="n">com</code><code class="o">.</code><code class="na">hadoopbook</code><code class="o">.</code><code class="na">hive</code><code class="o">;</code>

<code class="k">import</code> <code class="nn">org.apache.hadoop.hive.ql.exec.UDAF</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.hive.ql.exec.UDAFEvaluator</code><code class="o">;</code>
<code class="k">import</code> <code class="nn">org.apache.hadoop.hive.serde2.io.DoubleWritable</code><code class="o">;</code>

<code class="k">public</code> <code class="k">class</code> <code class="nc">Mean</code> <code class="k">extends</code> <code class="n">UDAF</code> <code class="o">{</code>

  <code class="k">public</code> <code class="k">static</code> <code class="k">class</code> <code class="nc">MeanDoubleUDAFEvaluator</code> <code class="k">implements</code> <code class="n">UDAFEvaluator</code> <code class="o">{</code>
    <code class="k">public</code> <code class="k">static</code> <code class="k">class</code> <code class="nc">PartialResult</code> <code class="o">{</code>
      <code class="kt">double</code> <code class="n">sum</code><code class="o">;</code>
      <code class="kt">long</code> <code class="n">count</code><code class="o">;</code>
    <code class="o">}</code>
    
    <code class="k">private</code> <code class="n">PartialResult</code> <code class="n">partial</code><code class="o">;</code>

    <code class="k">public</code> <code class="kt">void</code> <code class="nf">init</code><code class="o">()</code> <code class="o">{</code>
      <code class="n">partial</code> <code class="o">=</code> <code class="k">null</code><code class="o">;</code>
    <code class="o">}</code>

    <code class="k">public</code> <code class="kt">boolean</code> <code class="nf">iterate</code><code class="o">(</code><code class="n">DoubleWritable</code> <code class="n">value</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">value</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
        <code class="k">return</code> <code class="k">true</code><code class="o">;</code>
      <code class="o">}</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">partial</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
        <code class="n">partial</code> <code class="o">=</code> <code class="k">new</code> <code class="n">PartialResult</code><code class="o">();</code>
      <code class="o">}</code>
      <code class="n">partial</code><code class="o">.</code><code class="na">sum</code> <code class="o">+=</code> <code class="n">value</code><code class="o">.</code><code class="na">get</code><code class="o">();</code>
      <code class="n">partial</code><code class="o">.</code><code class="na">count</code><code class="o">++;</code>
      <code class="k">return</code> <code class="k">true</code><code class="o">;</code>
    <code class="o">}</code>

    <code class="k">public</code> <code class="n">PartialResult</code> <code class="nf">terminatePartial</code><code class="o">()</code> <code class="o">{</code>
      <code class="k">return</code> <code class="n">partial</code><code class="o">;</code>
    <code class="o">}</code>

    <code class="k">public</code> <code class="kt">boolean</code> <code class="nf">merge</code><code class="o">(</code><code class="n">PartialResult</code> <code class="n">other</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">other</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
        <code class="k">return</code> <code class="k">true</code><code class="o">;</code>
      <code class="o">}</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">partial</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
        <code class="n">partial</code> <code class="o">=</code> <code class="k">new</code> <code class="n">PartialResult</code><code class="o">();</code>
      <code class="o">}</code>
      <code class="n">partial</code><code class="o">.</code><code class="na">sum</code> <code class="o">+=</code> <code class="n">other</code><code class="o">.</code><code class="na">sum</code><code class="o">;</code>
      <code class="n">partial</code><code class="o">.</code><code class="na">count</code> <code class="o">+=</code> <code class="n">other</code><code class="o">.</code><code class="na">count</code><code class="o">;</code>
      <code class="k">return</code> <code class="k">true</code><code class="o">;</code>
    <code class="o">}</code>

    <code class="k">public</code> <code class="n">DoubleWritable</code> <code class="nf">terminate</code><code class="o">()</code> <code class="o">{</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">partial</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
        <code class="k">return</code> <code class="k">null</code><code class="o">;</code>
      <code class="o">}</code>
      <code class="k">return</code> <code class="k">new</code> <code class="nf">DoubleWritable</code><code class="o">(</code><code class="n">partial</code><code class="o">.</code><code class="na">sum</code> <code class="o">/</code> <code class="n">partial</code><code class="o">.</code><code class="na">count</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div></div></div></div><div class="book" title="Further Reading"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4553">Further Reading</h2></div></div></div><p class="calibre2">For more <a class="calibre" id="calibre_link-1999"></a>information about Hive, see <span class="calibre"><a class="ulink" href="http://shop.oreilly.com/product/0636920023555.do" target="_top">Programming
    Hive</a></span> by Edward Capriolo, Dean Wampler, and Jason
    Rutherglen (O’Reilly, 2012).</p></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" id="calibre_link-609"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-657">106</a>] </sup>Toby Segaran and Jeff Hammerbacher, <span class="calibre"><a class="ulink" href="http://oreilly.com/catalog/9780596157128/" target="_top">Beautiful Data: The
      Stories Behind Elegant Data Solutions</a></span> (O’Reilly,
      2009).</p></div><div class="footnote" type="footnote" id="calibre_link-611"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-658">107</a>] </sup>It is assumed that you have network connectivity from your
        workstation to the Hadoop cluster. You can <span class="calibre">test this</span>
        before running Hive by installing Hadoop locally and performing some
        HDFS operations with the <code class="literal">hadoop fs</code>
        command.</p></div><div class="footnote" type="footnote" id="calibre_link-617"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-659">108</a>] </sup>The properties have the <code class="literal">javax.jdo</code> prefix because the metastore
          implementation uses the Java Data Objects (JDO) API for persisting
          Java objects. Specifically, it uses the DataNucleus implementation
          of JDO.</p></div><div class="footnote" id="calibre_link-640"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-660">109</a>] </sup>Or see the <a class="ulink" href="http://bit.ly/languagemanual_udf" target="_top">Hive
          function reference</a>.</p></div><div class="footnote" type="footnote" id="calibre_link-641"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-661">110</a>] </sup>The move will succeed only if the source and target
          filesystems are the same. Also, there is a special case when the
          <code class="literal">LOCAL</code> keyword is used, where Hive
          will <span class="calibre">copy</span> the data from the local
          filesystem into Hive’s warehouse directory (even if it, too, is on
          the same local filesystem). In all other cases, though, <code class="literal">LOAD</code> is a move operation and is best
          thought of as such.</p></div><div class="footnote" type="footnote" id="calibre_link-642"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-662">111</a>] </sup>You can also use <code class="literal">INSERT OVERWRITE
          DIRECTORY</code> to export data to a Hadoop filesystem.</p></div><div class="footnote" type="footnote" id="calibre_link-643"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-663">112</a>] </sup>However, partitions may be added to or removed from a table
            after creation using an <code class="literal">ALTER
            TABLE</code> statement.</p></div><div class="footnote" type="footnote" id="calibre_link-645"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-664">113</a>] </sup>The fields appear to run together when displaying the raw
            file because the separator character in the output is a
            nonprinting control character. The control characters used are
            explained in the next section.</p></div><div class="footnote" type="footnote" id="calibre_link-647"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-665">114</a>] </sup>The default format can be changed by setting the property
            <code class="literal">hive.default.fileformat</code>.</p></div><div class="footnote" type="footnote" id="calibre_link-648"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-666">115</a>] </sup>Sometimes you need to use parentheses for regular expression
            constructs that you don’t want to count as a capturing group—for
            example, the pattern <code class="literal">(ab)+</code> for
            matching a string of one or more <code class="literal">ab</code> characters. The solution is to use a
            noncapturing group, which has a <code class="literal">?</code> character after the first parenthesis.
            There are various noncapturing group constructs (see the Java
            documentation), but in this example we could use <code class="literal">(?:ab)+</code> to avoid capturing the group as
            a Hive column.</p></div><div class="footnote" type="footnote" id="calibre_link-650"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-667">116</a>] </sup>This is a reworking in Hive of the discussion in <a class="ulink" href="#calibre_link-160" title="Secondary Sort">Secondary Sort</a>.</p></div><div class="footnote" id="calibre_link-652"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-668">117</a>] </sup>The order of the tables in the <code class="literal">JOIN</code> clauses is significant. It’s
            generally best to have the largest table last, but see the <a class="ulink" href="http://bit.ly/hive_joins_docs" target="_top">Hive wiki</a> for more
            details, including how to give hints to the Hive planner.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-283"><section type="chapter" id="calibre_link-4554" title="Chapter&nbsp;18.&nbsp;Crunch"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;18.&nbsp;Crunch</h2></div></div></div><p class="calibre2"><a class="ulink" href="https://crunch.apache.org/" target="_top">Apache Crunch</a> is a
  higher-level API for <a class="calibre" id="calibre_link-1292"></a><a class="calibre" id="calibre_link-4555"></a>writing MapReduce pipelines. The main advantages it offers
  over plain MapReduce are its focus on programmer-friendly Java types like
  <code class="literal">String</code> and plain old Java objects, a richer set of
  data transformation operations, and multistage pipelines (no need to
  explicitly manage individual MapReduce jobs in a workflow).</p><p class="calibre2">In these respects, Crunch looks a lot like a Java version of
  <a class="calibre" id="calibre_link-2995"></a>Pig. One day-to-day source of friction in using Pig, which
  Crunch avoids, is that the language used to write user-defined functions
  (Java or Python) is different from the language used to write Pig scripts
  (Pig Latin), which makes for a disjointed development experience as one
  switches between the two different representations and languages. By
  contrast, Crunch programs and UDFs are written in a single language (Java or
  Scala), and UDFs can be embedded right in the programs. The overall
  experience feels very like writing a non-distributed program. Although it
  has many parallels with Pig, Crunch was inspired by <a class="calibre" id="calibre_link-1724"></a>FlumeJava, the Java library developed at Google for building
  MapReduce pipelines.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">FlumeJava is not to be confused with <a class="calibre" id="calibre_link-1702"></a>Apache Flume, covered in <a class="ulink" href="#calibre_link-276" title="Chapter&nbsp;14.&nbsp;Flume">Chapter&nbsp;14</a>,
    which is a system for collecting streaming event data. You can read more
    about FlumeJava in <a class="ulink" href="http://bit.ly/data-parallel_pipelines" target="_top">“FlumeJava:
    Easy, Efficient Data-Parallel Pipelines”</a> by Craig Chambers et
    al.</p></div><p class="calibre2">Because they are high level, Crunch pipelines are highly composable
  and common functions can be extracted into libraries and reused in other
  programs. This is different from MapReduce, where it is very difficult to
  reuse code: most programs have custom mapper and reducer implementations,
  apart from simple cases such as where an identity function or a simple sum
  (<code class="literal">LongSumReducer</code>) is called for. Writing a library of
  mappers and reducers for different types of transformations, like sorting
  and joining operations, is not easy in MapReduce, whereas in Crunch it is
  very natural. For example, there is a library class,
  <code class="literal">org.apache.crunch.lib.Sort</code>, with <a class="calibre" id="calibre_link-3423"></a>a <code class="literal">sort()</code> method that will sort any
  Crunch collection that is passed to it.</p><p class="calibre2">Although Crunch was initially written to run using Hadoop’s <a class="calibre" id="calibre_link-2452"></a>MapReduce execution engine, it is not tied to it, and in fact
  you can run a Crunch pipeline using Apache Spark (see <a class="ulink" href="#calibre_link-352" title="Chapter&nbsp;19.&nbsp;Spark">Chapter&nbsp;19</a>) as the distributed execution engine. Different
  engines have different characteristics: Spark, for example, is more
  efficient than MapReduce if there is a lot of intermediate data to be passed
  between jobs, since it can retain the data in memory rather than
  materializing it to disk like MapReduce does. Being able to try a pipeline
  on different engines without rewriting the program is a powerful property,
  since it allows you to treat what the program <span class="calibre">does</span> separately from matters of runtime efficiency
  (which generally improve over time as the engines are tuned).</p><p class="calibre2">This chapter is an introduction to writing data processing programs in
  Crunch. You can find more information in the <a class="ulink" href="http://crunch.apache.org/user-guide.html" target="_top">Crunch User
  Guide</a>.</p><div class="book" title="An Example"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4556">An Example</h2></div></div></div><p class="calibre2">We’ll start with a <a class="calibre" id="calibre_link-1311"></a>simple Crunch pipeline to illustrate the basic concepts.
    <a class="ulink" href="#calibre_link-751" title="Example&nbsp;18-1.&nbsp;Application to find the maximum temperature, using Crunch">Example&nbsp;18-1</a> shows a Crunch version of the
    program to calculate the maximum temperature by year for the weather
    dataset, which we first met in <a class="ulink" href="#calibre_link-462" title="Chapter&nbsp;2.&nbsp;MapReduce">Chapter&nbsp;2</a>.</p><div class="example"><a id="calibre_link-751" class="calibre"></a><div class="example-title">Example&nbsp;18-1.&nbsp;Application to find the maximum temperature, using Crunch</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">MaxTemperatureCrunch</code> <code class="o">{</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="na">length</code> <code class="o">!=</code> <code class="mi">2</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="sb">"Usage: MaxTemperatureCrunch &lt;input path&gt; &lt;output path&gt;"</code><code class="o">);</code>
      <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(-</code><code class="mi">1</code><code class="o">);</code>
    <code class="o">}</code>

    <code class="n">Pipeline</code> <code class="n">pipeline</code> <code class="o">=</code> <code class="k">new</code> <code class="n">MRPipeline</code><code class="o">(</code><code class="n">getClass</code><code class="o">());</code>
    <code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">records</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">readTextFile</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]);</code>

    <code class="n">PTable</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;</code> <code class="n">yearTemperatures</code> <code class="o">=</code> <code class="n">records</code>
        <code class="o">.</code><code class="na">parallelDo</code><code class="o">(</code><code class="n">toYearTempPairsFn</code><code class="o">(),</code> <code class="n">tableOf</code><code class="o">(</code><code class="n">strings</code><code class="o">(),</code> <code class="n">ints</code><code class="o">()));</code>
    <code class="n">PTable</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;</code> <code class="n">maxTemps</code> <code class="o">=</code> <code class="n">yearTemperatures</code>
        <code class="o">.</code><code class="na">groupByKey</code><code class="o">()</code>
        <code class="o">.</code><code class="na">combineValues</code><code class="o">(</code><code class="n">Aggregators</code><code class="o">.</code><code class="na">MAX_INTS</code><code class="o">());</code>
    
    <code class="n">maxTemps</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">To</code><code class="o">.</code><code class="na">textFile</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">]));</code>
    <code class="n">PipelineResult</code> <code class="n">result</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">done</code><code class="o">();</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">result</code><code class="o">.</code><code class="na">succeeded</code><code class="o">()</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">);</code>
  <code class="o">}</code>

  <code class="k">static</code> <code class="n">DoFn</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Pair</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;&gt;</code> <code class="n">toYearTempPairsFn</code><code class="o">()</code> <code class="o">{</code>
    <code class="k">return</code> <code class="k">new</code> <code class="n">DoFn</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Pair</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;&gt;()</code> <code class="o">{</code>
      <code class="n">NcdcRecordParser</code> <code class="n">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="n">NcdcRecordParser</code><code class="o">();</code>
      <code class="nd">@Override</code>
      <code class="k">public</code> <code class="kt">void</code> <code class="nf">process</code><code class="o">(</code><code class="n">String</code> <code class="n">input</code><code class="o">,</code> <code class="n">Emitter</code><code class="o">&lt;</code><code class="n">Pair</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;&gt;</code> <code class="n">emitter</code><code class="o">)</code> <code class="o">{</code>
        <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">input</code><code class="o">);</code>
        <code class="k">if</code> <code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">isValidTemperature</code><code class="o">())</code> <code class="o">{</code>
          <code class="n">emitter</code><code class="o">.</code><code class="na">emit</code><code class="o">(</code><code class="n">Pair</code><code class="o">.</code><code class="na">of</code><code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">getYear</code><code class="o">(),</code> <code class="n">parser</code><code class="o">.</code><code class="na">getAirTemperature</code><code class="o">()));</code>
        <code class="o">}</code>
      <code class="o">}</code>
    <code class="o">};</code>
  <code class="o">}</code>

<code class="o">}</code></pre></div></div><p class="calibre2">After the customary checking of command-line arguments, the program
    starts by constructing a Crunch <code class="literal">Pipeline</code> object,
    which represents the computation that we want to run. As the name
    suggests, a pipeline can have multiple stages; pipelines with multiple
    inputs and outputs, branches, and iteration are all possible, although in
    this example we start with a single-stage pipeline. We’re going to use
    MapReduce to run the pipeline, so we create an
    <code class="literal">MRPipeline</code>, but we could have chosen to use a
    <code class="literal">MemPipeline</code> for running the pipeline in memory for
    testing purposes, or a <code class="literal">SparkPipeline</code> to run the
    same computation using Spark.</p><p class="calibre2">A pipeline receives data from one or more input sources, and in this
    example the source is a single text file whose name is specified by the
    first command-line argument, <code class="literal">args[0]</code>.
    The <code class="literal">Pipeline</code> class <a class="calibre" id="calibre_link-3048"></a>has a convenience method,
    <code class="literal">readTextFile()</code>, to convert a text file <a class="calibre" id="calibre_link-2960"></a>into a <code class="literal">PCollection</code> of
    <code class="literal">String</code> objects, where each
    <code class="literal">String</code> is a line from the text file.
    <code class="literal">PCollection&lt;S&gt;</code> is the most fundamental data
    type in Crunch, and represents an immutable, unordered, distributed
    collection of elements of type <code class="literal">S</code>. You
    can think of <code class="literal">PCollection&lt;S&gt;</code> as an
    unmaterialized analog of
    <code class="literal">java.util.Collection</code>—unmaterialized since its
    elements are not read into memory. In this example, the input is a
    distributed collection of the lines of a text file, and is represented by
    <code class="literal">PCollection&lt;String&gt;</code>.</p><p class="calibre2">A Crunch computation operates on a
    <code class="literal">PCollection</code>, and produces a new
    <code class="literal">PCollection</code>. The first thing we need to do is parse
    each line of the input file, and filter out any bad records. We do this by
    <a class="calibre" id="calibre_link-2965"></a>using the <code class="literal">parallelDo()</code> method on
    <code class="literal">PCollection</code>, which applies a function to every
    element in the <code class="literal">PCollection</code> and returns a new
    <code class="literal">PCollection</code>. The method signature looks like
    this:</p><pre class="screen1"><code class="o">&lt;</code><code class="n">T</code><code class="o">&gt;</code> <code class="n">PCollection</code><code class="o">&lt;</code><code class="n">T</code><code class="o">&gt;</code> <code class="n">parallelDo</code><code class="o">(</code><code class="n">DoFn</code><code class="o">&lt;</code><code class="n">S</code><code class="o">,</code><code class="n">T</code><code class="o">&gt;</code> <code class="n">doFn</code><code class="o">,</code> <code class="n">PType</code><code class="o">&lt;</code><code class="n">T</code><code class="o">&gt;</code> <code class="n">type</code><code class="o">);</code></pre><p class="calibre2">The idea is that we write a <code class="literal">DoFn</code>
    implementation <a class="calibre" id="calibre_link-1536"></a>that transforms an instance of type <code class="literal">S</code> into one or more instances of type <code class="literal">T</code>, and Crunch will apply the function to every
    element in the <code class="literal">PCollection</code>. It should be clear that
    the operation can be performed in parallel in the map task of a MapReduce
    job. The second argument to the <code class="literal">parallelDo()</code>
    method is a <code class="literal">PType&lt;T&gt;</code> object, which gives
    Crunch information about both the Java type used for <code class="literal">T</code> and how to serialize that type.</p><p class="calibre2">We are actually going to use an overloaded version of
    <code class="literal">parallelDo()</code> that creates an extension of
    <code class="literal">PCollection</code> called <code class="literal">PTable&lt;K,
    V&gt;</code>, which <a class="calibre" id="calibre_link-3080"></a>is a distributed <em class="calibre10">multi-map</em> of
    key-value pairs. (A multi-map is a map that can have duplicate key-value
    pairs.) This is so we can represent the year as the key and the
    temperature as the value, which will enable us to do grouping and
    aggregation later in the pipeline. The method signature is:</p><pre class="screen1"><code class="o">&lt;</code><code class="n">K</code><code class="o">,</code> <code class="n">V</code><code class="o">&gt;</code> <code class="n">PTable</code><code class="o">&lt;</code><code class="n">K</code><code class="o">,</code> <code class="n">V</code><code class="o">&gt;</code> <code class="n">parallelDo</code><code class="o">(</code><code class="n">DoFn</code><code class="o">&lt;</code><code class="n">S</code><code class="o">,</code> <code class="n">Pair</code><code class="o">&lt;</code><code class="n">K</code><code class="o">,</code> <code class="n">V</code><code class="o">&gt;&gt;</code> <code class="n">doFn</code><code class="o">,</code> <code class="n">PTableType</code><code class="o">&lt;</code><code class="n">K</code><code class="o">,</code> <code class="n">V</code><code class="o">&gt;</code> <code class="n">type</code><code class="o">);</code></pre><p class="calibre2">In this example, the <code class="literal">DoFn</code> parses a line of
    input and emits a year-temperature pair:</p><pre class="screen1"><code class="k">static</code> <code class="n">DoFn</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Pair</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;&gt;</code> <code class="n">toYearTempPairsFn</code><code class="o">()</code> <code class="o">{</code>
  <code class="k">return</code> <code class="k">new</code> <code class="n">DoFn</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Pair</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;&gt;()</code> <code class="o">{</code>
    <code class="n">NcdcRecordParser</code> <code class="n">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="n">NcdcRecordParser</code><code class="o">();</code>
    <code class="nd">@Override</code>
    <code class="k">public</code> <code class="kt">void</code> <code class="nf">process</code><code class="o">(</code><code class="n">String</code> <code class="n">input</code><code class="o">,</code> <code class="n">Emitter</code><code class="o">&lt;</code><code class="n">Pair</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;&gt;</code> <code class="n">emitter</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">input</code><code class="o">);</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">isValidTemperature</code><code class="o">())</code> <code class="o">{</code>
        <code class="n">emitter</code><code class="o">.</code><code class="na">emit</code><code class="o">(</code><code class="n">Pair</code><code class="o">.</code><code class="na">of</code><code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">getYear</code><code class="o">(),</code> <code class="n">parser</code><code class="o">.</code><code class="na">getAirTemperature</code><code class="o">()));</code>
      <code class="o">}</code>
    <code class="o">}</code>
  <code class="o">};</code>
<code class="o">}</code></pre><p class="calibre2">After applying the function we get a table of year-temperature
    pairs:</p><pre class="screen1"><code class="n">PTable</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;</code> <code class="n">yearTemperatures</code> <code class="o">=</code> <code class="n">records</code>
    <code class="o">.</code><code class="na">parallelDo</code><code class="o">(</code><code class="n">toYearTempPairsFn</code><code class="o">(),</code> <code class="n">tableOf</code><code class="o">(</code><code class="n">strings</code><code class="o">(),</code> <code class="n">ints</code><code class="o">()));</code></pre><p class="calibre2">The second argument to <code class="literal">parallelDo()</code> is a
    <code class="literal">PTableType&lt;K, V&gt;</code> instance, <a class="calibre" id="calibre_link-3088"></a>which is constructed using static methods on Crunch’s
    <code class="literal">Writables</code> class (since we have chosen to use Hadoop
    Writable serialization for any intermediate data that Crunch will write).
    The <code class="literal">tableOf()</code> method creates a
    <code class="literal">PTableType</code> with the given key and value types. The
    <code class="literal">strings()</code> method declares that keys are
    represented by Java <code class="literal">String</code> objects in memory, and
    serialized as Hadoop <code class="literal">Text</code>. The values are Java
    <code class="literal">int</code> types and are serialized as Hadoop
    <code class="literal">IntWritable</code>s.</p><p class="calibre2">At this point, we have a more structured representation of the data,
    but the number of records is still the same since every line in the input
    file corresponds to an entry in the <code class="literal">yearTemperatures</code> table. To calculate the maximum
    temperature reading for each year in the dataset, we need to group the
    table entries by year, then find the maximum temperature value for each
    year. Fortunately, Crunch provides exactly these operations as a part of
    <code class="literal">PTable</code>’s API. The
    <code class="literal">groupByKey()</code> method performs a MapReduce shuffle
    to group entries by key and returns the third type of <code class="literal">PCollection</code>, called
    <code class="literal">PGrouped</code><code class="literal">Table&lt;K, V&gt;</code>,
    which <a class="calibre" id="calibre_link-2982"></a>has a <code class="literal">combineValues()</code> method for
    performing aggregation of all the values for a key, just like a MapReduce
    reducer:</p><pre class="screen1"><code class="n">PTable</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;</code> <code class="n">maxTemps</code> <code class="o">=</code> <code class="n">yearTemperatures</code>
    <code class="o">.</code><code class="na">groupByKey</code><code class="o">()</code>
    <code class="o">.</code><code class="na">combineValues</code><code class="o">(</code><code class="n">Aggregators</code><code class="o">.</code><code class="na">MAX_INTS</code><code class="o">());</code></pre><p class="calibre2">The <code class="literal">combineValues()</code> method accepts an
    instance of a <a class="calibre" id="calibre_link-878"></a>Crunch <code class="literal">Aggregator</code>, a simple interface
    for expressing any kind of aggregation of a stream of values, and here we
    can take advantage of a built-in aggregator from the
    <code class="literal">Aggregators</code> class <a class="calibre" id="calibre_link-879"></a>called <code class="literal">MAX_INTS</code> that
    finds the maximum value from a set of integers.</p><p class="calibre2">The final step in the pipeline is writing the <code class="literal">maxTemps</code> table to a file by calling
    <code class="literal">write()</code> with a text file target object
    constructed using the <code class="literal">To</code> static factory. Crunch
    actually uses Hadoop’s <code class="literal">TextOutputFormat</code> for
    <a class="calibre" id="calibre_link-3682"></a>this operation, which means that the key and value in each
    line of output are separated by a tab:</p><pre class="screen1"><code class="n">maxTemps</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">To</code><code class="o">.</code><code class="na">textFile</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">]));</code></pre><p class="calibre2">The program so far has only been concerned with pipeline
    construction. To execute a pipeline, we have to call the
    <code class="literal">done()</code> method, at which point the program blocks
    until the pipeline completes. Crunch <a class="calibre" id="calibre_link-3053"></a>returns a <code class="literal">PipelineResult</code> object that
    encapsulates various statistics about the different jobs that were run in
    the pipeline, as well as whether the pipeline succeeded or not. We use the
    latter information to set the program’s exit code appropriately.</p><p class="calibre2">When we run the program on the sample dataset, we get the following
    <a class="calibre" id="calibre_link-1312"></a>result:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop jar crunch-examples.jar crunch.MaxTemperatureCrunch \
  input/ncdc/sample.txt output</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">cat output/part-r-00000</code></strong>
1949	111
1950	22</pre></div><div class="book" title="The Core Crunch API"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4557">The Core Crunch API</h2></div></div></div><p class="calibre2">This section presents the core interfaces in Crunch. Crunch’s API is
    high level by design, so the programmer can concentrate on the logical
    operations of the computation, rather than the details of how it is
    executed.</p><div class="book" title="Primitive Operations"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4558">Primitive Operations</h3></div></div></div><p class="calibre2">The core data <a class="calibre" id="calibre_link-1302"></a>structure in Crunch is
      <code class="literal">PCollection&lt;S&gt;</code>, an immutable, unordered,
      distributed collection of elements of type <code class="literal">S</code>. In this section, we examine the primitive
      operations on <code class="literal">PCollection</code> and its derived types,
      <code class="literal">PTable</code> and
      <code class="literal">PGroupedTable</code>.</p><div class="book" title="union()"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4559">union()</h4></div></div></div><p class="calibre2">The simplest primitive <a class="calibre" id="calibre_link-2973"></a>Crunch operation is <code class="literal">union()</code>,
        which returns a <code class="literal">PCollection</code> that contains all
        the elements of the <code class="literal">PCollection</code> it is invoked
        on and the <code class="literal">PCollection</code> supplied as an argument.
        For example:</p><pre class="screen1"><code class="n">PCollection</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">&gt;</code> <code class="n">a</code> <code class="o">=</code> <code class="n">MemPipeline</code><code class="o">.</code><code class="na">collectionOf</code><code class="o">(</code><code class="mi">1</code><code class="o">,</code> <code class="mi">3</code><code class="o">);</code>
<code class="n">PCollection</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">&gt;</code> <code class="n">b</code> <code class="o">=</code> <code class="n">MemPipeline</code><code class="o">.</code><code class="na">collectionOf</code><code class="o">(</code><code class="mi">2</code><code class="o">);</code>
<code class="n">PCollection</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">&gt;</code> <code class="n">c</code> <code class="o">=</code> <code class="n">a</code><code class="o">.</code><code class="na">union</code><code class="o">(</code><code class="n">b</code><code class="o">);</code>
<code class="n">assertEquals</code><code class="o">(</code><code class="sb">"{2,1,3}"</code><code class="o">,</code> <code class="n">dump</code><code class="o">(</code><code class="n">c</code><code class="o">));</code></pre><p class="calibre2"><code class="literal">MemPipeline</code>’s
        <code class="literal">collectionOf()</code> method <a class="calibre" id="calibre_link-2670"></a>is used to create a <code class="literal">PCollection</code>
        instance from a small number of elements, normally for the purposes of
        testing or demonstration. The <code class="literal">dump()</code> method
        is a utility method introduced here for rendering the contents of a
        small <code class="literal">PCollection</code> as a string (it’s not a part
        of Crunch, but you can find the implementation in the
        <code class="literal">PCollections</code> class in the example code that
        accompanies this book). Since <code class="literal">PCollection</code>s are
        unordered, the order of the elements in <code class="literal">c</code> is undefined.</p><p class="calibre2">When forming the union of two
        <code class="literal">PCollection</code>s, they must have been created from
        the same pipeline (or the operation will fail at runtime), and they
        must have the same type. The latter condition is enforced at compile
        time, since <code class="literal">PCollection</code> is a parameterized type
        and the type arguments for the <code class="literal">PCollection</code>s in
        the union must match.</p></div><div class="book" title="parallelDo()"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-754">parallelDo()</h4></div></div></div><p class="calibre2">The second primitive <a class="calibre" id="calibre_link-2966"></a>operation is <code class="literal">parallelDo()</code> for
        calling a function on every element in an input
        <code class="literal">PCollection&lt;S&gt;</code> and returning a new output
        <code class="literal">PCollection&lt;T&gt;</code> containing the results of
        the function calls. In its simplest form,
        <code class="literal">parallelDo()</code> takes two arguments: a
        <code class="literal">DoFn&lt;S, T&gt;</code> implementation that defines a
        function transforming elements of type <code class="literal">S</code> to type <code class="literal">T</code>, and <a class="calibre" id="calibre_link-3089"></a>a <code class="literal">PType&lt;T&gt;</code> instance to
        describe the output type <code class="literal">T</code>.
        (<code class="literal">PType</code>s are explained in more detail in the
        section <a class="ulink" href="#calibre_link-752" title="Types">Types</a>.)</p><p class="calibre2">The following code snippet shows how to use
        <code class="literal">parallelDo()</code> to apply a string length
        function to a <code class="literal">PCollection</code> of strings:</p><pre class="screen1"><code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">a</code> <code class="o">=</code> <code class="n">MemPipeline</code><code class="o">.</code><code class="na">collectionOf</code><code class="o">(</code><code class="sb">"cherry"</code><code class="o">,</code> <code class="sb">"apple"</code><code class="o">,</code> <code class="sb">"banana"</code><code class="o">);</code>
<code class="n">PCollection</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">&gt;</code> <code class="n">b</code> <code class="o">=</code> <code class="n">a</code><code class="o">.</code><code class="na">parallelDo</code><code class="o">(</code><code class="k">new</code> <code class="n">DoFn</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;()</code> <code class="o">{</code>
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">process</code><code class="o">(</code><code class="n">String</code> <code class="n">input</code><code class="o">,</code> <code class="n">Emitter</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">&gt;</code> <code class="n">emitter</code><code class="o">)</code> <code class="o">{</code>
    <code class="n">emitter</code><code class="o">.</code><code class="na">emit</code><code class="o">(</code><code class="n">input</code><code class="o">.</code><code class="na">length</code><code class="o">());</code>
  <code class="o">}</code>
<code class="o">},</code> <code class="n">ints</code><code class="o">());</code>
<code class="n">assertEquals</code><code class="o">(</code><code class="sb">"{6,5,6}"</code><code class="o">,</code> <code class="n">dump</code><code class="o">(</code><code class="n">b</code><code class="o">));</code></pre><p class="calibre2">In this case, the output <code class="literal">PCollection</code> of
        integers has the same number of elements as the input, so we could
        have used the <code class="literal">MapFn</code> subclass <a class="calibre" id="calibre_link-2411"></a>of <code class="literal">DoFn</code> for 1:1 mappings:</p><pre class="screen1"><code class="n">PCollection</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">&gt;</code> <code class="n">b</code> <code class="o">=</code> <code class="n">a</code><code class="o">.</code><code class="na">parallelDo</code><code class="o">(</code><code class="k">new</code> <code class="n">MapFn</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;()</code> <code class="o">{</code>
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="n">Integer</code> <code class="nf">map</code><code class="o">(</code><code class="n">String</code> <code class="n">input</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">return</code> <code class="n">input</code><code class="o">.</code><code class="na">length</code><code class="o">();</code>
  <code class="o">}</code>
<code class="o">},</code> <code class="n">ints</code><code class="o">());</code>
<code class="n">assertEquals</code><code class="o">(</code><code class="sb">"{6,5,6}"</code><code class="o">,</code> <code class="n">dump</code><code class="o">(</code><code class="n">b</code><code class="o">));</code></pre><p class="calibre2">One common use of <code class="literal">parallelDo()</code> is for
        filtering out data that is not needed in later processing steps.
        Crunch provides a <code class="literal">filter()</code> method for this
        purpose that takes a special <code class="literal">DoFn</code> called
        <code class="literal">FilterFn</code>. Implementors <a class="calibre" id="calibre_link-1696"></a>need only implement the
        <code class="literal">accept()</code> method to indicate whether an
        element should be in the output. For example, this code retains only
        those strings with an even number of characters:</p><pre class="screen1"><code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">b</code> <code class="o">=</code> <code class="n">a</code><code class="o">.</code><code class="na">filter</code><code class="o">(</code><code class="k">new</code> <code class="n">FilterFn</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;()</code> <code class="o">{</code>
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">boolean</code> <code class="nf">accept</code><code class="o">(</code><code class="n">String</code> <code class="n">input</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">return</code> <code class="n">input</code><code class="o">.</code><code class="na">length</code><code class="o">()</code> <code class="o">%</code> <code class="mi">2</code> <code class="o">==</code> <code class="mi">0</code><code class="o">;</code> <code class="c2">// even</code>
  <code class="o">}</code>
<code class="o">});</code>
<code class="n">assertEquals</code><code class="o">(</code><code class="sb">"{cherry,banana}"</code><code class="o">,</code> <code class="n">dump</code><code class="o">(</code><code class="n">b</code><code class="o">));</code></pre><p class="calibre2">Notice that there is no <code class="literal">PType</code> in the
        method signature for <code class="literal">filter()</code>, since the
        output <code class="literal">PCollection</code> has the same type as the
        input.</p><div class="note" title="Tip"><h3 class="title3">Tip</h3><p class="calibre2">If your <code class="literal">DoFn</code> significantly changes
          <a class="calibre" id="calibre_link-1538"></a>the size of the <code class="literal">PCollection</code> it
          is operating on, you can override its
          <code class="literal">scaleFactor()</code> method to give a hint to the
          Crunch planner about the estimated relative size of the output,
          which may improve its efficiency.</p><p class="calibre2"><code class="literal">FilterFn</code>’s
          <code class="literal">scaleFactor()</code> method returns 0.5; in other
          words, the assumption is that implementations will filter out about
          half of the elements in a <code class="literal">PCollection</code>. You
          can override this method if your filter function is significantly
          more or less selective than this.</p></div><p class="calibre2">There is an overloaded form of
        <code class="literal">parallelDo()</code> for generating a
        <code class="literal">PTable</code> from a
        <code class="literal">PCollection</code>. Recall from the opening example
        that a <code class="literal">PTable&lt;K, V&gt;</code> is a multi-map of
        key-value pairs; or, in the language of Java types,
        <code class="literal">PTable&lt;K, V&gt;</code> is a
        <code class="literal">PCollection&lt;Pair&lt;K, V&gt;&gt;</code>, where
        <code class="literal">Pair&lt;K, V&gt;</code> is Crunch’s <a class="calibre" id="calibre_link-2898"></a>pair class.</p><p class="calibre2">The following code creates a <code class="literal">PTable</code> by
        <a class="calibre" id="calibre_link-3082"></a>using a <code class="literal">DoFn</code> that turns an input
        string into a key-value pair (the key is the length of the string, and
        the value is the string itself):</p><a id="calibre_link-4560" class="calibre"></a><pre class="screen1"><code class="n">PTable</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;</code> <code class="n">b</code> <code class="o">=</code> <code class="n">a</code><code class="o">.</code><code class="na">parallelDo</code><code class="o">(</code>
    <code class="k">new</code> <code class="n">DoFn</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Pair</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;&gt;()</code> <code class="o">{</code>
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">process</code><code class="o">(</code><code class="n">String</code> <code class="n">input</code><code class="o">,</code> <code class="n">Emitter</code><code class="o">&lt;</code><code class="n">Pair</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;&gt;</code> <code class="n">emitter</code><code class="o">)</code> <code class="o">{</code>
    <code class="n">emitter</code><code class="o">.</code><code class="na">emit</code><code class="o">(</code><code class="n">Pair</code><code class="o">.</code><code class="na">of</code><code class="o">(</code><code class="n">input</code><code class="o">.</code><code class="na">length</code><code class="o">(),</code> <code class="n">input</code><code class="o">));</code>
  <code class="o">}</code>
<code class="o">},</code> <code class="n">tableOf</code><code class="o">(</code><code class="n">ints</code><code class="o">(),</code> <code class="n">strings</code><code class="o">()));</code>
<code class="n">assertEquals</code><code class="o">(</code><code class="sb">"{(6,cherry),(5,apple),(6,banana)}"</code><code class="o">,</code> <code class="n">dump</code><code class="o">(</code><code class="n">b</code><code class="o">));</code></pre><p class="calibre2">Extracting keys from a <code class="literal">PCollection</code> of
        values to form a <code class="literal">PTable</code> is a common enough task
        that Crunch provides a method for it, called
        <code class="literal">by()</code>. This method takes a
        <code class="literal">MapFn&lt;S, K&gt;</code> to map the input value
        <code class="literal">S</code> to its <a class="calibre" id="calibre_link-2967"></a>key <code class="literal">K</code>:</p><pre class="screen1"><code class="n">PTable</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;</code> <code class="n">b</code> <code class="o">=</code> <code class="n">a</code><code class="o">.</code><code class="na">by</code><code class="o">(</code><code class="k">new</code> <code class="n">MapFn</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;()</code> <code class="o">{</code>
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="n">Integer</code> <code class="nf">map</code><code class="o">(</code><code class="n">String</code> <code class="n">input</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">return</code> <code class="n">input</code><code class="o">.</code><code class="na">length</code><code class="o">();</code>
  <code class="o">}</code>
<code class="o">},</code> <code class="n">ints</code><code class="o">());</code>
<code class="n">assertEquals</code><code class="o">(</code><code class="sb">"{(6,cherry),(5,apple),(6,banana)}"</code><code class="o">,</code> <code class="n">dump</code><code class="o">(</code><code class="n">b</code><code class="o">));</code></pre></div><div class="book" title="groupByKey()"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4561">groupByKey()</h4></div></div></div><p class="calibre2">The third primitive operation is
        <code class="literal">groupByKey()</code>, for bringing together all the
        values <a class="calibre" id="calibre_link-3084"></a>in a <code class="literal">PTable&lt;K, V&gt;</code> that have
        the same key. This operation can be thought of as the MapReduce
        shuffle, and indeed that’s how it’s implemented for the MapReduce
        execution engine. In terms of Crunch types,
        <code class="literal">groupByKey()</code> returns a
        <code class="literal">PGroupedTable&lt;K, V&gt;</code>, which <a class="calibre" id="calibre_link-2983"></a>is a <code class="literal">PCollection&lt;Pair&lt;K,
        Iterable&lt;V&gt;&gt;&gt;</code>, or a multi-map where each key
        is paired with an iterable collection over its values.</p><p class="calibre2">Continuing from the previous code snippet, if we group the
        <code class="literal">PTable</code> of length-string mappings by key, we get
        the following (where the items in square brackets indicate an iterable
        collection):</p><pre class="screen1"><code class="n">PGroupedTable</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;</code> <code class="n">c</code> <code class="o">=</code> <code class="n">b</code><code class="o">.</code><code class="na">groupByKey</code><code class="o">();</code>
<code class="n">assertEquals</code><code class="o">(</code><code class="sb">"{(5,[apple]),(6,[banana,cherry])}"</code><code class="o">,</code> <code class="n">dump</code><code class="o">(</code><code class="n">c</code><code class="o">));</code></pre><p class="calibre2">Crunch uses information on the size of the table to set the
        number of partitions (reduce tasks in MapReduce) to use for the
        <code class="literal">groupByKey()</code> operation. Most of the time the
        default is fine, but you can explicitly set the number of partitions
        by using the overloaded form,
        <code class="literal">groupByKey(int)</code>, if needed.</p></div><div class="book" title="combineValues()"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-755">combineValues()</h4></div></div></div><p class="calibre2">Despite the suggestive <a class="calibre" id="calibre_link-2984"></a>naming, <code class="literal">PGroupedTable</code> is not
        actually a subclass of <code class="literal">PTable</code>, so you can’t
        call methods like <code class="literal">groupByKey()</code> on it. This is
        because there is no reason to group by key on a
        <code class="literal">PTable</code> that was already grouped by key. Another
        way of thinking about <code class="literal">PGroupedTable</code> is as an
        intermediate representation before generating another
        <code class="literal">PTable</code>. After all, the reason to group by key
        is so you can do something to the values for each key. This is the
        basis of the fourth primitive operation,
        <code class="literal">combineValues()</code>.</p><p class="calibre2">In its most general form,
        <code class="literal">combineValues()</code> takes a combining function
        <code class="literal">CombineFn&lt;K, V&gt;</code>, which is a more concise
        name for <code class="literal">DoFn&lt;Pair&lt;K, Iterable&lt;V&gt;&gt;,
        Pair&lt;K, V&gt;&gt;</code>, and returns a
        <code class="literal">PTable&lt;K, V&gt;</code>. To see it in action,
        consider a combining function that concatenates all the string values
        together for a key, using a semicolon as a separator:</p><pre class="screen1"><code class="n">PTable</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;</code> <code class="n">d</code> <code class="o">=</code> <code class="n">c</code><code class="o">.</code><code class="na">combineValues</code><code class="o">(</code><code class="k">new</code> <code class="n">CombineFn</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;()</code> <code class="o">{</code>
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">process</code><code class="o">(</code><code class="n">Pair</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">Iterable</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;&gt;</code> <code class="n">input</code><code class="o">,</code>
      <code class="n">Emitter</code><code class="o">&lt;</code><code class="n">Pair</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;&gt;</code> <code class="n">emitter</code><code class="o">)</code> <code class="o">{</code>
    <code class="n">StringBuilder</code> <code class="n">sb</code> <code class="o">=</code> <code class="k">new</code> <code class="n">StringBuilder</code><code class="o">();</code>
    <code class="k">for</code> <code class="o">(</code><code class="n">Iterator</code> <code class="n">i</code> <code class="o">=</code> <code class="n">input</code><code class="o">.</code><code class="na">second</code><code class="o">().</code><code class="na">iterator</code><code class="o">();</code> <code class="n">i</code><code class="o">.</code><code class="na">hasNext</code><code class="o">();</code> <code class="o">)</code> <code class="o">{</code>
      <code class="n">sb</code><code class="o">.</code><code class="na">append</code><code class="o">(</code><code class="n">i</code><code class="o">.</code><code class="na">next</code><code class="o">());</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">i</code><code class="o">.</code><code class="na">hasNext</code><code class="o">())</code> <code class="o">{</code> <code class="n">sb</code><code class="o">.</code><code class="na">append</code><code class="o">(</code><code class="sb">";"</code><code class="o">);</code> <code class="o">}</code>
    <code class="o">}</code>
    <code class="n">emitter</code><code class="o">.</code><code class="na">emit</code><code class="o">(</code><code class="n">Pair</code><code class="o">.</code><code class="na">of</code><code class="o">(</code><code class="n">input</code><code class="o">.</code><code class="na">first</code><code class="o">(),</code> <code class="n">sb</code><code class="o">.</code><code class="na">toString</code><code class="o">()));</code>
  <code class="o">}</code>
<code class="o">});</code>
<code class="n">assertEquals</code><code class="o">(</code><code class="sb">"{(5,apple),(6,banana;cherry)}"</code><code class="o">,</code> <code class="n">dump</code><code class="o">(</code><code class="n">d</code><code class="o">));</code></pre><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">String concatenation is not commutative, so the result is not
          deterministic. This may or may not be important in your
          application!</p></div><p class="calibre2">The code is cluttered somewhat by the use<a class="calibre" id="calibre_link-2899"></a> of <code class="literal">Pair</code> objects in the
        <code class="literal">process()</code> method signature; they have to be
        unwrapped with calls to <code class="literal">first()</code> and
        <code class="literal">second()</code>, and a new
        <code class="literal">Pair</code> object is created to emit the new
        key-value pair. This combining function does not alter the key, so we
        can use an overloaded form of <code class="literal">combineValues()</code>
        that takes an <code class="literal">Aggregator</code> object for operating
        only on the values and passes the keys through unchanged. Even better,
        we can use a built-in <code class="literal">Aggregator</code> implementation
        for performing string concatenation found in <a class="calibre" id="calibre_link-880"></a>the <code class="literal">Aggregators</code> class. The code
        becomes:</p><pre class="screen1"><code class="n">PTable</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;</code> <code class="n">e</code> <code class="o">=</code> <code class="n">c</code><code class="o">.</code><code class="na">combineValues</code><code class="o">(</code><code class="n">Aggregators</code><code class="o">.</code><code class="na">STRING_CONCAT</code><code class="o">(</code><code class="sb">";"</code><code class="o">,</code>
    <code class="k">false</code><code class="o">));</code>
<code class="n">assertEquals</code><code class="o">(</code><code class="sb">"{(5,apple),(6,banana;cherry)}"</code><code class="o">,</code> <code class="n">dump</code><code class="o">(</code><code class="n">e</code><code class="o">));</code></pre><p class="calibre2">Sometimes you may want to aggregate the values in a
        <code class="literal">PGroupedTable</code> and return a result with a
        different type from the values being grouped. This can be achieved
        using the <code class="literal">mapValues()</code> method with a
        <code class="literal">MapFn</code> <a class="calibre" id="calibre_link-2412"></a>for converting the iterable collection into another
        object. For example, the following calculates the number of values for
        each key:</p><pre class="screen1"><code class="n">PTable</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;</code> <code class="n">f</code> <code class="o">=</code> <code class="n">c</code><code class="o">.</code><code class="na">mapValues</code><code class="o">(</code><code class="k">new</code> <code class="n">MapFn</code><code class="o">&lt;</code><code class="n">Iterable</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;,</code> <code class="n">Integer</code><code class="o">&gt;()</code> <code class="o">{</code>
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="n">Integer</code> <code class="nf">map</code><code class="o">(</code><code class="n">Iterable</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">input</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">return</code> <code class="n">Iterables</code><code class="o">.</code><code class="na">size</code><code class="o">(</code><code class="n">input</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">},</code> <code class="n">ints</code><code class="o">());</code>
<code class="n">assertEquals</code><code class="o">(</code><code class="sb">"{(5,1),(6,2)}"</code><code class="o">,</code> <code class="n">dump</code><code class="o">(</code><code class="n">f</code><code class="o">));</code></pre><p class="calibre2">Notice that the values are strings, but the result of applying
        the map function is an integer, the size of the iterable collection
        computed using Guava’s <code class="literal">Iterables</code> class.</p><p class="calibre2">You might wonder why the
        <code class="literal">combineValues()</code> operation exists at all,
        given that the <code class="literal">mapValues()</code> method is more
        powerful. The reason is that <code class="literal">combineValues()</code>
        can be run as a MapReduce combiner, and therefore it can improve
        performance by being run on the map side, which has the effect of
        reducing the amount of data that has to be transferred in the shuffle
        (see <a class="ulink" href="#calibre_link-539" title="Combiner Functions">Combiner Functions</a>). The
        <code class="literal">mapValues()</code> method is translated into a
        <code class="literal">parallelDo()</code> operation, and in this context
        it can only run on the reduce side, so there is no possibility for
        using a combiner to improve its performance.</p><p class="calibre2">Finally, the other operation on
        <code class="literal">PGroupedTable</code> is
        <code class="literal">ungroup()</code>, which turns a
        <code class="literal">PGrouped</code><code class="literal">Table&lt;K, V&gt;</code>
        back into a <code class="literal">PTable&lt;K, V&gt;</code>—the reverse of
        <code class="literal">groupByKey()</code>. (It’s not a primitive operation
        though, since it is implemented with a
        <code class="literal">parallelDo()</code>.) Calling
        <code class="literal">groupByKey()</code> then
        <code class="literal">ungroup()</code> on a <code class="literal">PTable</code>
        has the effect of performing a partial sort on the table by its keys,
        although it’s normally more convenient to use the
        <code class="literal">Sort</code> library, which implements a total sort
        (which is usually what you want) and also offers <a class="calibre" id="calibre_link-1303"></a><a class="calibre" id="calibre_link-2985"></a>options for ordering.</p></div></div><div class="book" title="Types"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-752">Types</h3></div></div></div><p class="calibre2">Every <code class="literal">PCollection&lt;S&gt;</code> has an
      <a class="calibre" id="calibre_link-3090"></a><a class="calibre" id="calibre_link-2971"></a><a class="calibre" id="calibre_link-1309"></a>associated class, <code class="literal">PType&lt;S&gt;</code>,
      that encapsulates type information about the elements in the
      <code class="literal">PCollection</code>. The
      <code class="literal">PType&lt;S&gt;</code> determines the Java class,
      <code class="literal">S</code>, of the elements in the
      <code class="literal">PCollection</code>, as well as the serialization format
      used to read data from persistent storage into the
      <code class="literal">PCollection</code> and, conversely, write data from the
      <code class="literal">PCollection</code> to persistent storage.</p><p class="calibre2">There are two <code class="literal">PType</code> families in Crunch:
      <a class="calibre" id="calibre_link-3800"></a><a class="calibre" id="calibre_link-937"></a>Hadoop Writables and Avro. The choice of which to use
      broadly corresponds to the file format that you are using in your
      pipeline; Writables for sequence files, and Avro for Avro data files.
      Either family can be used with text files. Pipelines can use a mixture
      of <code class="literal">PType</code>s from different families (since the
      <code class="literal">PType</code> is associated with the
      <code class="literal">PCollection</code>, not the pipeline), but this is
      usually unnecessary unless you are doing something that spans families,
      like file format conversion.</p><p class="calibre2">In general, Crunch strives to hide the differences between
      different serialization formats, so that the types used in code are
      familiar to Java programmers. (Another benefit is that it’s easier to
      write libraries and utilities to work with Crunch collections,
      regardless of the serialization family they belong to.) Lines read from
      a text file, for instance, are presented as regular Java
      <code class="literal">String</code> objects, rather than the Writable
      <code class="literal">Text</code> variant or Avro <code class="literal">Utf8</code>
      objects.</p><p class="calibre2">The <code class="literal">PType</code> used by a
      <code class="literal">PCollection</code> is specified when the
      <code class="literal">PCollection</code> is created, although sometimes it is
      implicit. For example, reading a text file will use Writables by
      default, as this test shows:</p><pre class="screen1"><code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">lines</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">read</code><code class="o">(</code><code class="n">From</code><code class="o">.</code><code class="na">textFile</code><code class="o">(</code><code class="n">inputPath</code><code class="o">));</code>
<code class="n">assertEquals</code><code class="o">(</code><code class="n">WritableTypeFamily</code><code class="o">.</code><code class="na">getInstance</code><code class="o">(),</code> <code class="n">lines</code><code class="o">.</code><code class="na">getPType</code><code class="o">().</code><code class="na">getFamily</code><code class="o">());</code></pre><p class="calibre2">However, it is possible to explicitly use Avro serialization by
      passing the appropriate <code class="literal">PType</code> to the
      <code class="literal">textFile()</code> method. Here we use the static
      factory method on <code class="literal">Avros</code> to create an Avro
      representation of <code class="literal">PType&lt;String&gt;</code>:</p><pre class="screen1"><code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">lines</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">read</code><code class="o">(</code><code class="n">From</code><code class="o">.</code><code class="na">textFile</code><code class="o">(</code><code class="n">inputPath</code><code class="o">,</code>
    <span class="calibre24"><strong class="calibre9"><code class="n1">Avros</code><code class="o1">.</code><code class="na1">strings</code><code class="o1">()</code></strong></span><code class="o">));</code></pre><p class="calibre2">Similarly, operations that create new
      <code class="literal">PCollection</code>s require that the
      <code class="literal">PType</code> is specified and matches the type
      parameters of the <code class="literal">PCollection</code>.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-753" id="calibre_link-775">118</a>]</sup> For instance, in our earlier example the
      <code class="literal">parallelDo()</code> operation
      to extract an integer key from a
      <code class="literal">PCollection&lt;String&gt;</code>, turning it into a
      <code class="literal">PTable&lt;Integer, String&gt;</code>, specified a
        matching <code class="literal">PType</code> of:</p><pre class="screen1"><code class="n">tableOf</code><code class="o">(</code><code class="n">ints</code><code class="o">(),</code> <code class="n">strings</code><code class="o">())</code></pre><p class="calibre2">where all three methods are statically imported from
      <code class="literal">Writables</code>.</p><div class="book" title="Records and tuples"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4562">Records and tuples</h4></div></div></div><p class="calibre2">When it comes to working with complex objects with <a class="calibre" id="calibre_link-1304"></a>multiple fields, you can choose between records or
        tuples in Crunch. A record is a class where fields are accessed by
        name, such as Avro’s <code class="literal">GenericRecord</code>, a plain
        <a class="calibre" id="calibre_link-1791"></a>old Java object (corresponding to Avro Specific or
        Reflect), or a custom <code class="literal">Writable</code>. For a tuple, on the
        other hand, field access is by position, and Crunch provides a
        <code class="literal">Tuple</code> interface as well as a few convenience
        classes for tuples with a small number of elements:
        <code class="literal">Pair&lt;K, V&gt;</code>, <code class="literal">Tuple3&lt;V1, V2,
        V3&gt;</code>, <code class="literal">Tuple4&lt;V1, V2, V3,
        V4&gt;</code>, and <code class="literal">TupleN</code> for
        tuples with an arbitrary but fixed number of values.</p><p class="calibre2">Where possible, you should prefer records over tuples, since the
        resulting Crunch programs are more readable and understandable. If a
        weather record is represented by a
        <code class="literal">WeatherRecord</code> class with year, temperature, and
        station ID fields, then it is easier to work with this type:</p><pre class="screen1"><code class="n">Emitter</code><code class="o">&lt;</code><code class="n">Pair</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">WeatherRecord</code><code class="o">&gt;&gt;</code></pre><p class="calibre2">than this:</p><pre class="screen1"><code class="n">Emitter</code><code class="o">&lt;</code><code class="n">Pair</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">Tuple3</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">Integer</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;&gt;</code></pre><p class="calibre2">The latter does not convey any semantic information through its
        type names, unlike <code class="literal">WeatherRecord</code>, which clearly
        describes what it is.</p><p class="calibre2">As this example hints, it’s is not possible to entirely avoid
        using Crunch <code class="literal">Pair</code> objects, since they are a
        fundamental part of the way Crunch represents table collections
        (recall that a <code class="literal">PTable&lt;K, V&gt;</code> is a
        <code class="literal">PCollection&lt;Pair&lt;K, V&gt;&gt;</code>). However,
        there are opportunities to limit the use of
        <code class="literal">Pair</code> objects in many cases, which will make
        your code more readable. For example, use
        <code class="literal">PCollection</code>’s <code class="literal">by()</code>
        method in favor of <code class="literal">parallelDo()</code> when creating
        a table where the values are the same as the ones in the
        <code class="literal">PCollection</code> (as discussed in <a class="ulink" href="#calibre_link-754" title="parallelDo()">parallelDo()</a>), or use
        <code class="literal">PGroupedTable</code>’s
        <code class="literal">combineValues()</code> with an
        <code class="literal">Aggregator</code> in preference to a
        <code class="literal">CombineFn</code> (see <a class="ulink" href="#calibre_link-755" title="combineValues()">combineValues()</a>).</p><p class="calibre2">The fastest path to using records in a Crunch pipeline is to
        define a Java class that has fields that Avro Reflect can serialize
        and a no-arg constructor, like this
        <code class="literal">WeatherRecord</code> class:</p><pre class="screen1"><code class="k">public</code> <code class="k">class</code> <code class="nc">WeatherRecord</code> <code class="o">{</code>
  <code class="k">private</code> <code class="kt">int</code> <code class="n">year</code><code class="o">;</code>
  <code class="k">private</code> <code class="kt">int</code> <code class="n">temperature</code><code class="o">;</code>
  <code class="k">private</code> <code class="n">String</code> <code class="n">stationId</code><code class="o">;</code>

  <code class="k">public</code> <code class="nf">WeatherRecord</code><code class="o">()</code> <code class="o">{</code>
  <code class="o">}</code>

  <code class="k">public</code> <code class="nf">WeatherRecord</code><code class="o">(</code><code class="kt">int</code> <code class="n">year</code><code class="o">,</code> <code class="kt">int</code> <code class="n">temperature</code><code class="o">,</code> <code class="n">String</code> <code class="n">stationId</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">this</code><code class="o">.</code><code class="na">year</code> <code class="o">=</code> <code class="n">year</code><code class="o">;</code>
    <code class="k">this</code><code class="o">.</code><code class="na">temperature</code> <code class="o">=</code> <code class="n">temperature</code><code class="o">;</code>
    <code class="k">this</code><code class="o">.</code><code class="na">stationId</code> <code class="o">=</code> <code class="n">stationId</code><code class="o">;</code>
  <code class="o">}</code>

  <code class="c2">// ... getters elided</code>
<code class="o">}</code></pre><p class="calibre2">From there, it’s straightforward to generate a
        <code class="literal">PCollection&lt;WeatherRecord&gt;</code> from a
        <code class="literal">PCollection&lt;String&gt;</code>, using
        <code class="literal">parallelDo()</code> to parse each line into a
        <code class="literal">WeatherRecord</code> object:</p><pre class="screen1"><code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">lines</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">read</code><code class="o">(</code><code class="n">From</code><code class="o">.</code><code class="na">textFile</code><code class="o">(</code><code class="n">inputPath</code><code class="o">));</code>
<code class="n">PCollection</code><code class="o">&lt;</code><code class="n">WeatherRecord</code><code class="o">&gt;</code> <code class="n">records</code> <code class="o">=</code> <code class="n">lines</code><code class="o">.</code><code class="na">parallelDo</code><code class="o">(</code>
    <code class="k">new</code> <code class="n">DoFn</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">WeatherRecord</code><code class="o">&gt;()</code> <code class="o">{</code>
  <code class="n">NcdcRecordParser</code> <code class="n">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="n">NcdcRecordParser</code><code class="o">();</code>
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">process</code><code class="o">(</code><code class="n">String</code> <code class="n">input</code><code class="o">,</code> <code class="n">Emitter</code><code class="o">&lt;</code><code class="n">WeatherRecord</code><code class="o">&gt;</code> <code class="n">emitter</code><code class="o">)</code> <code class="o">{</code>
    <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">input</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">isValidTemperature</code><code class="o">())</code> <code class="o">{</code>
      <code class="n">emitter</code><code class="o">.</code><code class="na">emit</code><code class="o">(</code><code class="k">new</code> <code class="n">WeatherRecord</code><code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">getYearInt</code><code class="o">(),</code>
          <code class="n">parser</code><code class="o">.</code><code class="na">getAirTemperature</code><code class="o">(),</code> <code class="n">parser</code><code class="o">.</code><code class="na">getStationId</code><code class="o">()));</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">},</code> <code class="n">Avros</code><code class="o">.</code><code class="na">records</code><code class="o">(</code><code class="n">WeatherRecord</code><code class="o">.</code><code class="na">class</code><code class="o">));</code></pre><p class="calibre2">The <code class="literal">records()</code> factory method returns a
        Crunch <code class="literal">PType</code> for the Avro Reflect data model,
        as we have used it here; but it also supports Avro Specific and
        Generic data models. If you wanted to use Avro Specific instead, then
        you would define your custom type using an Avro schema file, generate
        the Java class for it, and call <code class="literal">records()</code>
        with the generated class. For Avro Generic, you would declare the
        class to be a <code class="literal">GenericRecord</code>.</p><p class="calibre2"><code class="literal">Writables</code> also provides a
        <code class="literal">records()</code> factory method for using custom
        <code class="literal">Writable</code> types; however, they are more cumbersome
        to define since you have to write serialization logic yourself (see
        <a class="ulink" href="#calibre_link-724" title="Implementing a Custom Writable">Implementing a Custom Writable</a>).</p><p class="calibre2">With a collection of records in hand, we can use Crunch
        libraries or our own processing functions to perform computations on
        it. For example, this will perform a total sort of the weather records
        by the fields in the order they are declared (by year, then by
        temperature, then by <a class="calibre" id="calibre_link-1310"></a><a class="calibre" id="calibre_link-3091"></a><a class="calibre" id="calibre_link-2972"></a><a class="calibre" id="calibre_link-1305"></a>station ID):</p><pre class="screen1"><code class="n">PCollection</code><code class="o">&lt;</code><code class="n">WeatherRecord</code><code class="o">&gt;</code> <code class="n">sortedRecords</code> <code class="o">=</code> <code class="n">Sort</code><code class="o">.</code><code class="na">sort</code><code class="o">(</code><code class="n">records</code><code class="o">);</code></pre></div></div><div class="book" title="Sources and Targets"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4563">Sources and Targets</h3></div></div></div><p class="calibre2">This section <a class="calibre" id="calibre_link-1307"></a>covers the different types of sources and targets in
      Crunch, and how to use them.</p><div class="book" title="Reading from a source"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4564">Reading from a source</h4></div></div></div><p class="calibre2">Crunch pipelines start <a class="calibre" id="calibre_link-3158"></a><a class="calibre" id="calibre_link-3446"></a>with one or more <code class="literal">Source&lt;T&gt;</code>
        instances specifying the storage location and
        <code class="literal">PType&lt;T&gt;</code> of the input data. For the
        simple case of reading text files, the
        <code class="literal">readTextFile()</code> method on
        <code class="literal">Pipeline</code> works <a class="calibre" id="calibre_link-3047"></a>well; for other types of source, use the
        <code class="literal">read()</code> method that takes a
        <code class="literal">Source&lt;T&gt;</code> object. In fact, this:</p><pre class="screen1"><code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">lines</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">readTextFile</code><code class="o">(</code><code class="n">inputPath</code><code class="o">);</code></pre><p class="calibre2">is shorthand for:</p><pre class="screen1"><code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">lines</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">read</code><code class="o">(</code><code class="n">From</code><code class="o">.</code><code class="na">textFile</code><code class="o">(</code><code class="n">inputPath</code><code class="o">));</code></pre><p class="calibre2">The <code class="literal">From</code> class (in <a class="calibre" id="calibre_link-2855"></a>the <code class="literal">org.apache.crunch.io</code> package) acts as a
        collection of static factory methods for file sources, of which text
        files are just one example.</p><p class="calibre2">Another common case is reading sequence files of
        <code class="literal">Writable</code> key-value pairs. In this case, the
        <a class="calibre" id="calibre_link-3618"></a>source is a <code class="literal">TableSource&lt;K,
        V&gt;</code>, to accommodate key-value pairs, and it returns a
        <code class="literal">PTable&lt;K, V&gt;</code>. For example, a sequence
        file containing <code class="literal">IntWritable</code> keys and
        <code class="literal">Text</code> values <a class="calibre" id="calibre_link-3086"></a>yields a <code class="literal">PTable&lt;Integer,
        String&gt;</code>:</p><pre class="screen1"><code class="n">TableSource</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;</code> <code class="n">source</code> <code class="o">=</code>
    <code class="n">From</code><code class="o">.</code><code class="na">sequenceFile</code><code class="o">(</code><code class="n">inputPath</code><code class="o">,</code> <code class="n">Writables</code><code class="o">.</code><code class="na">ints</code><code class="o">(),</code> <code class="n">Writables</code><code class="o">.</code><code class="na">strings</code><code class="o">());</code>
<code class="n">PTable</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;</code> <code class="n">table</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">read</code><code class="o">(</code><code class="n">source</code><code class="o">);</code></pre><p class="calibre2">You can also read Avro datafiles into <a class="calibre" id="calibre_link-2970"></a>a <code class="literal">PCollection</code> as follows:</p><pre class="screen1"><code class="n">Source</code><code class="o">&lt;</code><code class="n">WeatherRecord</code><code class="o">&gt;</code> <code class="n">source</code> <code class="o">=</code>
    <code class="n">From</code><code class="o">.</code><code class="na">avroFile</code><code class="o">(</code><code class="n">inputPath</code><code class="o">,</code> <code class="n">Avros</code><code class="o">.</code><code class="na">records</code><code class="o">(</code><code class="n">WeatherRecord</code><code class="o">.</code><code class="na">class</code><code class="o">));</code>
<code class="n">PCollection</code><code class="o">&lt;</code><code class="n">WeatherRecord</code><code class="o">&gt;</code> <code class="n">records</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">read</code><code class="o">(</code><code class="n">source</code><code class="o">);</code></pre><p class="calibre2">Any MapReduce <code class="literal">FileInputFormat</code> (in the
        <a class="calibre" id="calibre_link-1641"></a>new MapReduce API) can be used as a
        <code class="literal">TableSource</code> by means of the
        <code class="literal">formattedFile()</code> method on
        <code class="literal">From</code>, providing Crunch access to the large
        number of different Hadoop-supported file formats. There are also more
        source implementations in Crunch than the ones exposed in the
        <code class="literal">From</code> class, <a class="calibre" id="calibre_link-1729"></a>including:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2"><code class="literal">AvroParquetFileSource</code> for reading
            Parquet files as Avro <code class="literal">PType</code>s.</p></li><li class="listitem"><p class="calibre2"><code class="literal">FromHBase</code>, which has a
            <code class="literal">table()</code> method for reading rows from
            HBase tables into <code class="literal">PTable&lt;ImmutableBytesWritable,
            Result&gt;</code> collections.
            <code class="literal">ImmutableBytesWritable</code> is an HBase class
            for representing a row key as bytes, and
            <code class="literal">Result</code> contains the cells from the row
            scan, which can be configured to return only cells in particular
            columns or column families.</p></li></ul></div></div><div class="book" title="Writing to a target"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4565">Writing to a target</h4></div></div></div><p class="calibre2">Writing a <code class="literal">PCollection</code> to a
        <code class="literal">Target</code> is as <a class="calibre" id="calibre_link-3812"></a><a class="calibre" id="calibre_link-3619"></a><a class="calibre" id="calibre_link-2974"></a>simple as calling <code class="literal">PCollection</code>’s
        <code class="literal">write()</code> method with the desired
        <code class="literal">Target</code>. Most commonly, the target is a file,
        and the file type can be selected with the static factory methods on
        the <code class="literal">To</code> class. For example, the following line
        writes Avro files to a directory called <em class="calibre10">output</em> in the default filesystem:</p><pre class="screen1"><code class="n">collection</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">To</code><code class="o">.</code><code class="na">avroFile</code><code class="o">(</code><code class="sb">"output"</code><code class="o">));</code></pre><p class="calibre2">This is just a slightly more convenient way of saying:</p><pre class="screen1"><code class="n">pipeline</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">collection</code><code class="o">,</code> <code class="n">To</code><code class="o">.</code><code class="na">avroFile</code><code class="o">(</code><code class="sb">"output"</code><code class="o">));</code></pre><p class="calibre2">Since the <code class="literal">PCollection</code> is being written to
        an Avro file, it must have a <code class="literal">PType</code> belonging to
        the Avro family, or the pipeline will fail.</p><p class="calibre2">The <code class="literal">To</code> factory also has methods for
        creating text files, sequence files, and any MapReduce
        <code class="literal">FileOutputFormat</code>. Crunch also has built-in
        <code class="literal">Target</code> implementations for the Parquet file
        format (<code class="literal">AvroParquetFileTarget</code>) and HBase
        (<code class="literal">ToHBase</code>).</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Crunch tries to write the type of collection to the target
          file in the most natural way. For example, a
          <code class="literal">PTable</code> is written to an Avro file using a
          <code class="literal">Pair</code> record schema with key and
          value fields that match the <code class="literal">PTable</code>.
          Similarly, a <code class="literal">PCollection</code>’s values are written
          to a sequence file’s values (the keys are <code class="literal">null</code>),
          and a <code class="literal">PTable</code> is written to a text file with
          tab-separated keys and values.</p></div></div><div class="book" title="Existing outputs"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4566">Existing outputs</h4></div></div></div><p class="calibre2">If a file-based target already exists, Crunch will throw a
        <code class="literal">CrunchRuntimeException</code> when the
        <code class="literal">write()</code> method is called. This preserves the
        behavior of MapReduce, which is to be conservative and not overwrite
        existing outputs unless explicitly directed to by the user (see <a class="ulink" href="#calibre_link-756" title="Java MapReduce">Java MapReduce</a>).</p><p class="calibre2">A flag may be passed to the <code class="literal">write()</code>
        method indicating that outputs should be <a class="calibre" id="calibre_link-2889"></a>overwritten as follows:</p><pre class="screen1"><code class="n">collection</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">To</code><code class="o">.</code><code class="na">avroFile</code><code class="o">(</code><code class="sb">"output"</code><code class="o">),</code> <code class="n">Target</code><code class="o">.</code><code class="na">WriteMode</code><code class="o">.</code><code class="na">OVERWRITE</code><code class="o">);</code></pre><p class="calibre2">If <em class="calibre10">output</em> already exists,
        then it will be deleted before the pipeline runs.</p><p class="calibre2">There is another write mode, <code class="literal">APPEND</code>, which <a class="calibre" id="calibre_link-903"></a>will add new files<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-757" id="calibre_link-776">119</a>]</sup> to the output directory, leaving any existing ones from
        previous runs intact. Crunch takes care to use a unique identifier in
        filenames to avoid the possibility of a new run overwriting files from
        a previous run.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-758" id="calibre_link-777">120</a>]</sup></p><p class="calibre2">The final write mode <a class="calibre" id="calibre_link-1091"></a>is <code class="literal">CHECKPOINT</code>, which
        is for saving work to a file so that a new pipeline can start from
        that point rather than from the beginning of the pipeline. This mode
        is covered in <a class="ulink" href="#calibre_link-759" title="Checkpointing a Pipeline">Checkpointing a Pipeline</a>.</p></div><div class="book" title="Combined sources and targets"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4567">Combined sources and targets</h4></div></div></div><p class="calibre2">Sometimes you want to write to a target and then read from it as
        a source (i.e., in another pipeline in the same program). For this
        case, Crunch provides<a class="calibre" id="calibre_link-3447"></a> the <code class="literal">SourceTarget&lt;T&gt;</code>
        interface, which is both a <code class="literal">Source&lt;T&gt;</code> and
        a <code class="literal">Target</code>. The <code class="literal">At</code> class
        provides static factory methods for creating
        <code class="literal">SourceTarget</code> instances for text files, sequence
        files, and <a class="calibre" id="calibre_link-1308"></a>Avro files.</p></div></div><div class="book" title="Functions"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4568">Functions</h3></div></div></div><p class="calibre2">At the heart of <a class="calibre" id="calibre_link-1294"></a><a class="calibre" id="calibre_link-1763"></a>any Crunch program are the functions (represented by
      <code class="literal">DoFn</code>) that transform one
      <code class="literal">PCollection</code> into another. In this section, we
      examine some of the considerations in writing your own custom
      functions.</p><div class="book" title="Serialization of functions"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4569">Serialization of functions</h4></div></div></div><p class="calibre2">When writing MapReduce <a class="calibre" id="calibre_link-3358"></a>programs, it is up to you to package the code for
        mappers and reducers into a job JAR file so that Hadoop can make the
        user code available on the task classpath (see <a class="ulink" href="#calibre_link-760" title="Packaging a Job">Packaging a Job</a>). Crunch takes a different approach. When a
        pipeline is executed, all the <code class="literal">DoFn</code> instances
        are serialized to a file that is distributed to task nodes using
        Hadoop’s distributed cache mechanism (described in <a class="ulink" href="#calibre_link-54" title="Distributed Cache">Distributed Cache</a>), and then deserialized by the task
        itself so that the <code class="literal">DoFn</code> can be invoked.</p><p class="calibre2">The upshot for you, the user, is that you don’t need to do any
        packaging work; instead, you only need to make sure that your
        <code class="literal">DoFn</code> implementations are serializable according
        to the standard Java serialization mechanism.<sup class="calibre6">[<a class="firstname" href="#calibre_link-761" id="calibre_link-778">121</a>]</sup></p><p class="calibre2">In most cases, no extra work is required, since the
        <code class="literal">DoFn</code> base class is declared as implementing the
        <code class="literal">java.io.Serializable</code> interface. <a class="calibre" id="calibre_link-3352"></a>Thus, if your function is stateless, there are no fields
        to serialize, and it will be serialized without issue.</p><p class="calibre2">There are a couple of problems to watch out for, however. One
        problem occurs if your <code class="literal">DoFn</code> is defined as an
        inner class (also called a nonstatic nested class), such as an
        anonymous class, in an outer class that doesn’t implement
        <code class="literal">Serializable</code>:</p><pre class="screen1"><code class="k">public</code> <code class="k">class</code> <code class="nc">NonSerializableOuterClass</code> <code class="o">{</code>

  <code class="k">public</code> <code class="kt">void</code> <code class="nf">runPipeline</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="c2">// ...</code>
    <code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">lines</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">readTextFile</code><code class="o">(</code><code class="n">inputPath</code><code class="o">);</code>
    <code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">lower</code> <code class="o">=</code> <code class="n">lines</code><code class="o">.</code><code class="na">parallelDo</code><code class="o">(</code><span class="calibre24"><strong class="calibre9"><code class="kc">new</code> <code class="n1">DoFn</code><code class="o1">&lt;</code><code class="n1">String</code><code class="o1">,</code> <code class="n1">String</code><code class="o1">&gt;()</code> <code class="o1">{</code>
      <code class="nd1">@Override</code>
      <code class="kc">public</code> <code class="kt1">void</code> <code class="nf1">process</code><code class="o1">(</code><code class="n1">String</code> <code class="n1">input</code><code class="o1">,</code> <code class="n1">Emitter</code><code class="o1">&lt;</code><code class="n1">String</code><code class="o1">&gt;</code> <code class="n1">emitter</code><code class="o1">)</code> <code class="o1">{</code>
        <code class="n1">emitter</code><code class="o1">.</code><code class="na1">emit</code><code class="o1">(</code><code class="n1">input</code><code class="o1">.</code><code class="na1">toLowerCase</code><code class="o1">());</code>
      <code class="o1">}</code>
    <code class="o1">}</code></strong></span><code class="o">,</code> <code class="n">strings</code><code class="o">());</code>
    <code class="c2">// ...</code>
  <code class="o">}</code>
<code class="o">}</code></pre><p class="calibre2">Since inner classes have an implicit reference to their
        enclosing instance, if the enclosing class is not serializable, then
        the function will not be serializable and the pipeline will fail with
        a <code class="literal">CrunchRuntimeException</code>. You can easily fix
        this by making the function a (named) static nested class or a
        top-level class, or you can make the enclosing class implement
        <code class="literal">Serializable</code>.</p><p class="calibre2">Another problem is when a function depends on nonserializable
        state in the form of an instance variable whose class is not
        <code class="literal">Serializable</code>. In this case, you can mark the
        nonserializable instance variable as <code class="literal">transient</code> so Java doesn’t try to serialize
        it, then set it in the <code class="literal">initialize()</code> method of
        <code class="literal">DoFn</code>. Crunch will call the
        <code class="literal">initialize()</code> method before the
        <code class="literal">process()</code> method is invoked for the first
        time:</p><pre class="screen1"><code class="k">public</code> <code class="k">class</code> <code class="nc">CustomDoFn</code><code class="o">&lt;</code><code class="n">S</code><code class="o">,</code> <code class="n">T</code><code class="o">&gt;</code> <code class="k">extends</code> <code class="n">DoFn</code><code class="o">&lt;</code><code class="n">S</code><code class="o">,</code> <code class="n">T</code><code class="o">&gt;</code> <code class="o">{</code>

  <span class="calibre24"><strong class="calibre9"><code class="kc">transient</code></strong></span> <code class="n">NonSerializableHelper</code> <code class="n">helper</code><code class="o">;</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">initialize</code><code class="o">()</code> <code class="o">{</code>
    <code class="n">helper</code> <code class="o">=</code> <code class="k">new</code> <code class="n">NonSerializableHelper</code><code class="o">();</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">process</code><code class="o">(</code><code class="n">S</code> <code class="n">input</code><code class="o">,</code> <code class="n">Emitter</code><code class="o">&lt;</code><code class="n">T</code><code class="o">&gt;</code> <code class="n">emitter</code><code class="o">)</code> <code class="o">{</code>
    <code class="c2">// use helper here</code>
  <code class="o">}</code>
<code class="o">}</code></pre><p class="calibre2">Although not shown here, it’s possible to pass state to
        initialize the transient instance variable using other, nontransient
        instance variables, such as strings.</p></div><div class="book" title="Object reuse"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4570">Object reuse</h4></div></div></div><p class="calibre2">In MapReduce, the objects in the reducer’s values iterator are
        reused for efficiency (to avoid the overhead of object allocation).
        Crunch has the same behavior for the iterators used in <a class="calibre" id="calibre_link-2986"></a><a class="calibre" id="calibre_link-2987"></a>the <code class="literal">combineValues()</code> and
        <code class="literal">mapValues()</code> methods on
        <code class="literal">PGroupedTable</code>. Therefore, if you retain a
        reference to an object outside the body of the iterator, you should
        make a copy to avoid object identity errors.</p><p class="calibre2">We can see how to go about this by writing a general-purpose
        utility for finding the set of <a class="calibre" id="calibre_link-3083"></a>unique values for each key in a
        <code class="literal">PTable</code>; see <a class="ulink" href="#calibre_link-762" title="Example&nbsp;18-2.&nbsp;Finding the set of unique values for each key in a PTable">Example&nbsp;18-2</a>.</p><div class="example"><a id="calibre_link-762" class="calibre"></a><div class="example-title">Example&nbsp;18-2.&nbsp;Finding the set of unique values for each key in a
          PTable</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">static</code> <code class="o">&lt;</code><code class="n">K</code><code class="o">,</code> <code class="n">V</code><code class="o">&gt;</code> <code class="n">PTable</code><code class="o">&lt;</code><code class="n">K</code><code class="o">,</code> <code class="n">Collection</code><code class="o">&lt;</code><code class="n">V</code><code class="o">&gt;&gt;</code> <code class="n">uniqueValues</code><code class="o">(</code><code class="n">PTable</code><code class="o">&lt;</code><code class="n">K</code><code class="o">,</code> <code class="n">V</code><code class="o">&gt;</code> <code class="n">table</code><code class="o">)</code> <code class="o">{</code>
  <code class="n">PTypeFamily</code> <code class="n">tf</code> <code class="o">=</code> <code class="n">table</code><code class="o">.</code><code class="na">getTypeFamily</code><code class="o">();</code>
  <code class="k">final</code> <code class="n">PType</code><code class="o">&lt;</code><code class="n">V</code><code class="o">&gt;</code> <code class="n">valueType</code> <code class="o">=</code> <code class="n">table</code><code class="o">.</code><code class="na">getValueType</code><code class="o">();</code>
  <code class="k">return</code> <code class="n">table</code><code class="o">.</code><code class="na">groupByKey</code><code class="o">().</code><code class="na">mapValues</code><code class="o">(</code><code class="sb">"unique"</code><code class="o">,</code>
      <code class="k">new</code> <code class="n">MapFn</code><code class="o">&lt;</code><code class="n">Iterable</code><code class="o">&lt;</code><code class="n">V</code><code class="o">&gt;,</code> <code class="n">Collection</code><code class="o">&lt;</code><code class="n">V</code><code class="o">&gt;&gt;()</code> <code class="o">{</code>
        <code class="nd">@Override</code>
        <code class="k">public</code> <code class="kt">void</code> <code class="nf">initialize</code><code class="o">()</code> <code class="o">{</code>
          <code class="n">valueType</code><code class="o">.</code><code class="na">initialize</code><code class="o">(</code><code class="n">getConfiguration</code><code class="o">());</code>
        <code class="o">}</code>

    <code class="nd">@Override</code>
    <code class="k">public</code> <code class="n">Set</code><code class="o">&lt;</code><code class="n">V</code><code class="o">&gt;</code> <code class="nf">map</code><code class="o">(</code><code class="n">Iterable</code><code class="o">&lt;</code><code class="n">V</code><code class="o">&gt;</code> <code class="n">values</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">Set</code><code class="o">&lt;</code><code class="n">V</code><code class="o">&gt;</code> <code class="n">collected</code> <code class="o">=</code> <code class="k">new</code> <code class="n">HashSet</code><code class="o">&lt;</code><code class="n">V</code><code class="o">&gt;();</code>
      <code class="k">for</code> <code class="o">(</code><code class="n">V</code> <code class="n">value</code> <code class="o">:</code> <code class="n">values</code><code class="o">)</code> <code class="o">{</code>
        <code class="n">collected</code><code class="o">.</code><code class="na">add</code><code class="o">(</code><span class="calibre24"><strong class="calibre9"><code class="n1">valueType</code><code class="o1">.</code><code class="na1">getDetachedValue</code><code class="o1">(</code><code class="n1">value</code><code class="o1">)</code></strong></span><code class="o">);</code>
      <code class="o">}</code>
      <code class="k">return</code> <code class="n">collected</code><code class="o">;</code>
    <code class="o">}</code>
  <code class="o">},</code> <code class="n">tf</code><code class="o">.</code><code class="na">collections</code><code class="o">(</code><code class="n">table</code><code class="o">.</code><code class="na">getValueType</code><code class="o">()));</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The idea is to group by key, then iterate over each value
        associated with a key and collect the unique values in a
        <code class="literal">Set</code>, which will automatically remove
        duplicates. Since we want to retain the values outside the iteration,
        we need to make a copy of each value before we put it in the
        set.</p><p class="calibre2">Fortunately, we don’t need to write code that knows how to
        perform the copy for each possible Java class; we can use the
        <code class="literal">getDetachedValue()</code> method that Crunch
        provides for exactly this purpose <a class="calibre" id="calibre_link-3092"></a>on <code class="literal">PType</code>, which we get from the
        table’s value type. Notice that we also have to initialize the
        <code class="literal">PType</code> in the <code class="literal">DoFn</code>’s
        <code class="literal">initialize()</code> method so that the
        <code class="literal">PType</code> can access the configuration in order to
        perform the copying.</p><p class="calibre2">For immutable objects like <code class="literal">String</code>s or
        <code class="literal">Integer</code>s, calling
        <code class="literal">getDetachedValue()</code> is actually a no-op, but
        for mutable Avro or <code class="literal">Writable</code> types, a deep copy of
        each value is <a class="calibre" id="calibre_link-1295"></a><a class="calibre" id="calibre_link-1764"></a>made.</p></div></div><div class="book" title="Materialization"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-766">Materialization</h3></div></div></div><p class="calibre2"><code class="literal">Materialization</code> <a class="calibre" id="calibre_link-1298"></a><a class="calibre" id="calibre_link-2652"></a>is the process of making the values in a
      <code class="literal">PCollection</code> available <a class="calibre" id="calibre_link-2963"></a>so they can be read in your program. For example, you
      might want to read all the values from a (typically small)
      <code class="literal">PCollection</code> and display them, or send them to
      another part of your program, rather than writing them to a Crunch
      target. Another reason to materialize a
      <code class="literal">PCollection</code> is to use the contents as the basis
        for determining further processing steps—for example, to test for
      convergence in an iterative algorithm (see <a class="ulink" href="#calibre_link-763" title="Iterative Algorithms">Iterative Algorithms</a>).</p><p class="calibre2">There are a few ways of materializing a
      <code class="literal">PCollection</code>; the most direct way to accomplish
      this is to call <code class="literal">materialize()</code>, which returns an
      <code class="literal">Iterable</code> collection of its values. If the
      <code class="literal">PCollection</code> has not already been materialized,
      then Crunch will have to run the pipeline to ensure that the objects in
      the <code class="literal">PCollection</code> have been computed and stored in
      a temporary intermediate file so they can be iterated over.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-764" id="calibre_link-779">122</a>]</sup></p><p class="calibre2">Consider the following Crunch program for lowercasing lines in a
      text file:</p><pre class="screen1"><code class="n">Pipeline</code> <code class="n">pipeline</code> <code class="o">=</code> <code class="k">new</code> <code class="n">MRPipeline</code><code class="o">(</code><code class="n">getClass</code><code class="o">());</code>
<code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">lines</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">readTextFile</code><code class="o">(</code><code class="n">inputPath</code><code class="o">);</code>
<code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">lower</code> <code class="o">=</code> <code class="n">lines</code><code class="o">.</code><code class="na">parallelDo</code><code class="o">(</code><code class="k">new</code> <code class="n">ToLowerFn</code><code class="o">(),</code> <code class="n">strings</code><code class="o">());</code>

<code class="n">Iterable</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">materialized</code> <code class="o">=</code> <code class="n">lower</code><code class="o">.</code><code class="na">materialize</code><code class="o">();</code>
<code class="k">for</code> <code class="o">(</code><code class="n">String</code> <code class="n">s</code> <code class="o">:</code> <code class="n">materialized</code><code class="o">)</code> <code class="o">{</code> <code class="c2">// pipeline is run</code>
  <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="n">s</code><code class="o">);</code>
<code class="o">}</code>
<code class="n">pipeline</code><code class="o">.</code><code class="na">done</code><code class="o">();</code></pre><p class="calibre2">The lines from the text file are transformed using <a class="calibre" id="calibre_link-3698"></a>the <code class="literal">ToLowerFn</code> function, which is
      defined separately so we can use it again later:</p><pre class="screen1"><code class="k">public</code> <code class="k">class</code> <code class="nc">ToLowerFn</code> <code class="k">extends</code> <code class="n">DoFn</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;</code> <code class="o">{</code>
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">process</code><code class="o">(</code><code class="n">String</code> <code class="n">input</code><code class="o">,</code> <code class="n">Emitter</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">emitter</code><code class="o">)</code> <code class="o">{</code>
    <code class="n">emitter</code><code class="o">.</code><code class="na">emit</code><code class="o">(</code><code class="n">input</code><code class="o">.</code><code class="na">toLowerCase</code><code class="o">());</code>
  <code class="o">}</code>
<code class="o">}</code></pre><p class="calibre2">The call to <code class="literal">materialize()</code> on the variable
      <code class="literal">lower</code> returns an
      <code class="literal">Iterable&lt;String&gt;</code>, but it is not this method
      call that causes the pipeline to be run. It is only once an
      <code class="literal">Iterator</code> is created from the
      <code class="literal">Iterable</code> (implicitly by the <code class="literal">for
      each</code> loop) that Crunch runs the pipeline. When the pipeline
      has completed, the iteration can proceed over the materialized
      <code class="literal">PCollection</code>, and in this example the lowercase
      lines are printed to the console.</p><p class="calibre2"><code class="literal">PTable</code> has <a class="calibre" id="calibre_link-3085"></a>a <code class="literal">materializeToMap()</code> method,
      which might be expected to behave in a similar way to
      <code class="literal">materialize()</code>. However, there are two important
      differences. First, since it returns a <code class="literal">Map&lt;K,
      V&gt;</code> rather than an iterator, the whole table is loaded
      into memory at once, which should be avoided for large collections.
      Second, although a <code class="literal">PTable</code> is a multi-map, the
      <code class="literal">Map</code> interface does not support multiple values
      for a single key, so if the table has multiple values for the same key,
      all but one will be lost in the returned
      <code class="literal">Map</code>.</p><p class="calibre2">To avoid these limitations, simply call
      <code class="literal">materialize()</code> on the table in order to obtain
      an <code class="literal">Iterable&lt;Pair&lt;K, V&gt;&gt;</code>.</p><div class="book" title="PObject"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4571">PObject</h4></div></div></div><p class="calibre2">Another way to materialize <a class="calibre" id="calibre_link-3055"></a>a <code class="literal">PCollection</code> is to use
        <code class="literal">PObject</code>s. A
        <code class="literal">PObject&lt;T&gt;</code> is a
        <em class="calibre10">future</em>, a computation of a value of type
        <code class="literal">T</code> that may not have been completed
        at the time when the <code class="literal">PObject</code> is created in the
        running program. The computed value can be retrieved by calling
        <code class="literal">getValue()</code> on the
        <code class="literal">PObject</code>, which will block until the computation
        is completed (by running the Crunch pipeline) before returning the
        value.</p><p class="calibre2">Calling <code class="literal">getValue()</code> on a
        <code class="literal">PObject</code> is analogous to calling
        <code class="literal">materialize()</code> on a
        <code class="literal">PCollection</code>, since both calls will trigger
        execution of the pipeline to materialize the necessary collections.
        Indeed, we can rewrite the program to lowercase lines in a text file
        to use a <code class="literal">PObject</code> as follows:</p><pre class="screen1"><code class="n">Pipeline</code> <code class="n">pipeline</code> <code class="o">=</code> <code class="k">new</code> <code class="n">MRPipeline</code><code class="o">(</code><code class="n">getClass</code><code class="o">());</code>
<code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">lines</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">readTextFile</code><code class="o">(</code><code class="n">inputPath</code><code class="o">);</code>
<code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">lower</code> <code class="o">=</code> <code class="n">lines</code><code class="o">.</code><code class="na">parallelDo</code><code class="o">(</code><code class="k">new</code> <code class="n">ToLowerFn</code><code class="o">(),</code> <code class="n">strings</code><code class="o">());</code>

<span class="calibre24"><strong class="calibre9"><code class="n1">PObject</code><code class="o1">&lt;</code><code class="n1">Collection</code><code class="o1">&lt;</code><code class="n1">String</code><code class="o1">&gt;&gt;</code> <code class="n1">po</code> <code class="o1">=</code> <code class="n1">lower</code><code class="o1">.</code><code class="na1">asCollection</code><code class="o1">();</code></strong></span>
<code class="k">for</code> <code class="o">(</code><code class="n">String</code> <code class="n">s</code> <code class="o">:</code> <span class="calibre24"><strong class="calibre9"><code class="n1">po</code><code class="o1">.</code><code class="na1">getValue</code><code class="o1">()</code></strong></span><code class="o">)</code> <code class="o">{</code> <code class="c2">// pipeline is run</code>
  <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="n">s</code><code class="o">);</code>
<code class="o">}</code>
<code class="n">pipeline</code><code class="o">.</code><code class="na">done</code><code class="o">();</code></pre><p class="calibre2">The <code class="literal">asCollection()</code> method <a class="calibre" id="calibre_link-2961"></a>converts a <code class="literal">PCollection&lt;T&gt;</code>
        into a regular <a class="calibre" id="calibre_link-1172"></a>Java
        <code class="literal">Collection&lt;T&gt;</code>.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-765" id="calibre_link-780">123</a>]</sup> This is done by way of a <code class="literal">PObject</code>,
        so that the conversion can be deferred to a later point in the
        program’s execution if necessary. In this case, we call
        <code class="literal">PObject</code>’s <code class="literal">getValue()</code>
        immediately after getting the <code class="literal">PObject</code> so that
        we can iterate over the resulting
        <code class="literal">Collection</code>.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2"><code class="literal">asCollection()</code> will materialize all
          the objects in the <code class="literal">PCollection</code> into memory,
          so you should only call it on small
          <code class="literal">PCollection</code> instances, such as the results of
          a computation that contain only a few objects. There is no such
          restriction on the use of <code class="literal">materialize()</code>,
          which iterates over the collection, rather than holding the entire
          collection in memory at once.</p></div><p class="calibre2">At the time of writing, Crunch does not provide a way to
        evaluate a <code class="literal">PObject</code> during pipeline execution,
        such as from within a <code class="literal">DoFn</code>. A
        <code class="literal">PObject</code> may only be inspected after the
        pipeline execution has <a class="calibre" id="calibre_link-1299"></a><a class="calibre" id="calibre_link-2653"></a><a class="calibre" id="calibre_link-2964"></a>finished.</p></div></div></div><div class="book" title="Pipeline Execution"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4572">Pipeline Execution</h2></div></div></div><p class="calibre2">During <a class="calibre" id="calibre_link-1300"></a><a class="calibre" id="calibre_link-3036"></a>pipeline construction, Crunch builds an internal execution
    plan, which is either run explicitly by the user or implicitly by Crunch
    (as discussed in <a class="ulink" href="#calibre_link-766" title="Materialization">Materialization</a>). An execution
    plan is a <a class="calibre" id="calibre_link-1333"></a><a class="calibre" id="calibre_link-1497"></a>directed acyclic graph of operations <a class="calibre" id="calibre_link-2969"></a>on <code class="literal">PCollection</code>s, where each
    <code class="literal">PCollection</code> in the plan holds a reference to the
    operation that produces it, along with the
    <code class="literal">PCollection</code>s that are arguments to the operation.
    In addition, each <code class="literal">PCollection</code> has an internal state
      that records whether it has been materialized or not.</p><div class="book" title="Running a Pipeline"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4573">Running a Pipeline</h3></div></div></div><p class="calibre2">A pipeline’s operations <a class="calibre" id="calibre_link-3042"></a>can be explicitly executed by calling
      <code class="literal">Pipeline</code>’s <code class="literal">run()</code> method,
      <a class="calibre" id="calibre_link-3049"></a>which performs the following steps.</p><p class="calibre2">First, it optimizes the execution plan as a number of stages. The
      details of the optimization depend on the execution engine—a plan
      optimized for MapReduce will be different from the same plan optimized
      for Spark.</p><p class="calibre2">Second, it executes each stage in the optimized plan (in parallel,
      where possible) to materialize the resulting
      <code class="literal">PCollection</code>. <code class="literal">PCollection</code>s
      that are to be written to a <code class="literal">Target</code> are
      materialized as the target itself—this might be an output file in HDFS
      or a table in HBase. Intermediate <code class="literal">PCollection</code>s
      are materialized by writing the serialized objects in the collection to
      a temporary intermediate file in HDFS.</p><p class="calibre2">Finally, the <code class="literal">run()</code> method <a class="calibre" id="calibre_link-3054"></a>returns a <code class="literal">PipelineResult</code> object to
      the caller, with information about each stage that was run (duration and
      MapReduce counters<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-767" id="calibre_link-781">124</a>]</sup>), as well as whether the pipeline was successful or not
      (via the <code class="literal">succeeded()</code> method).</p><p class="calibre2">The <code class="literal">clean()</code> method removes all of the
      temporary intermediate files that were created to materialize
      <code class="literal">PCollection</code>s. It should be called after the
      pipeline is finished with to free up disk space on HDFS. The method
      takes a Boolean parameter to indicate whether the temporary files should
      be forcibly deleted. If <code class="literal">false</code>, the
      temporary files will only be deleted if all the targets in the pipeline
      have been created.</p><p class="calibre2">Rather than calling <code class="literal">run()</code> followed by
      <code class="literal">clean(false)</code>, it is more convenient to call
      <code class="literal">done()</code>, which has the same effect; it signals
      that the pipeline should be run and then cleaned up since it will not be
      needed any more.</p><div class="book" title="Asynchronous execution"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4574">Asynchronous execution</h4></div></div></div><p class="calibre2">The <code class="literal">run()</code> method is a blocking call
        that waits until the pipeline has completed before returning. There is
        a companion method, <code class="literal">runAsync()</code>, that returns
        <a class="calibre" id="calibre_link-3051"></a>immediately after the pipeline has been started. You can
        think of <code class="literal">run()</code> as being implemented as
        follows:</p><pre class="screen1"><code class="k">public</code> <code class="n">PipelineResult</code> <code class="nf">run</code><code class="o">()</code> <code class="o">{</code>
  <code class="n">PipelineExecution</code> <code class="n">execution</code> <code class="o">=</code> <code class="n">runAsync</code><code class="o">();</code>
  <code class="n">execution</code><code class="o">.</code><code class="na">waitUntilDone</code><code class="o">();</code>
  <code class="k">return</code> <code class="n">execution</code><code class="o">.</code><code class="na">getResult</code><code class="o">();</code>
<code class="o">}</code></pre><p class="calibre2">There are times when you may want to use the
        <code class="literal">runAsync()</code> method directly; most obviously if
        you want to run other code while waiting for the pipeline to complete,
        but also to take advantage of the methods exposed by
        <code class="literal">PipelineExecution</code>, like the ones to inspect the
        execution plan, find the status of the execution, or stop the pipeline
        midway through.</p><p class="calibre2"><code class="literal">PipelineExecution</code> implements
        <code class="literal">Future&lt;PipelineResult&gt;</code> (from <code class="literal">java.util.concurrent</code>), <a class="calibre" id="calibre_link-1784"></a><a class="calibre" id="calibre_link-2194"></a>offering the following simple idiom for performing
        background <a class="calibre" id="calibre_link-3050"></a>work:</p><pre class="screen1"><code class="n">PipelineExecution</code> <code class="n">execution</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">runAsync</code><code class="o">();</code>
<code class="c2">// meanwhile, do other things here</code>
<code class="n">PipelineResult</code> <code class="n">result</code> <code class="o">=</code> <code class="n">execution</code><code class="o">.</code><code class="na">get</code><code class="o">();</code> <code class="c2">// blocks</code></pre></div><div class="book" title="Debugging"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4575">Debugging</h4></div></div></div><p class="calibre2">To get more debug information in the <a class="calibre" id="calibre_link-1403"></a><a class="calibre" id="calibre_link-3046"></a>MapReduce task logs in the event of a failure, you can
        call <code class="literal">enableDebug()</code> on the
        <code class="literal">Pipeline</code> instance.</p><p class="calibre2">Another useful setting is the configuration <a class="calibre" id="calibre_link-1313"></a>property <code class="literal">crunch.log.job.progress</code>,
        which, if set to <code class="literal">true</code>, will log the
        MapReduce job progress of each stage to the <a class="calibre" id="calibre_link-3043"></a>console:</p><pre class="screen1"><code class="n">pipeline</code><code class="o">.</code><code class="na">getConfiguration</code><code class="o">().</code><code class="na">setBoolean</code><code class="o">(</code><code class="sb">"crunch.log.job.progress"</code><code class="o">,</code> <code class="k">true</code><code class="o">);</code></pre></div></div><div class="book" title="Stopping a Pipeline"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4576">Stopping a Pipeline</h3></div></div></div><p class="calibre2">Sometimes you <a class="calibre" id="calibre_link-3044"></a>might need to stop a pipeline before it completes. Perhaps
      only moments after starting a pipeline you realized that there’s a
      programming error in the code, so you’d like to stop the pipeline, fix
      the problem, and then restart.</p><p class="calibre2">If the pipeline was run using <a class="calibre" id="calibre_link-3045"></a>the blocking <code class="literal">run()</code> or
      <code class="literal">done()</code> calls, then using the standard Java
      thread interrupt mechanism will cause the <code class="literal">run()</code>
      or <code class="literal">done()</code> method to return. However, any jobs
      running on the cluster will continue running—they will <span class="calibre">not</span> be killed by Crunch.</p><p class="calibre2">Instead, to stop a pipeline <a class="calibre" id="calibre_link-3052"></a>properly, it needs to be launched asynchronously in order
      to retain a reference to the <code class="literal">PipelineExecution</code>
      object:</p><pre class="screen1"><code class="n">PipelineExecution</code> <code class="n">execution</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">runAsync</code><code class="o">();</code></pre><p class="calibre2">Stopping the pipeline and its jobs is then just a question of
      calling the <code class="literal">kill()</code> method on
      <code class="literal">PipelineExecution</code>, and waiting for the pipeline
      to complete:</p><pre class="screen1"><code class="n">execution</code><code class="o">.</code><code class="na">kill</code><code class="o">();</code>
<code class="n">execution</code><code class="o">.</code><code class="na">waitUntilDone</code><code class="o">();</code></pre><p class="calibre2">At this point, the <code class="literal">PipelineExecution</code>’s
      status will be <code class="literal">PipelineExecution.Status</code><code class="literal">.KILLED</code>, and any previously running
      jobs on the cluster from this pipeline will have been killed. An example
      of where this pattern could be effectively applied is in a Java VM
      shutdown hook to safely stop a currently executing pipeline when the
      Java application is shut down using Ctrl-C.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2"><code class="literal">PipelineExecution</code> implements
        <code class="literal">Future&lt;PipelineResult&gt;</code>, so calling
        <code class="literal">kill()</code> can achieve the same effect as calling
        <code class="literal">cancel(true)</code>.</p></div></div><div class="book" title="Inspecting a Crunch Plan"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4577">Inspecting a Crunch Plan</h3></div></div></div><p class="calibre2">Sometimes it is <a class="calibre" id="calibre_link-3038"></a>useful, or at least enlightening, to inspect the optimized
      execution plan. The following snippet shows how to obtain a DOT file
      representation of the graph of operations in a pipeline as a string, and
      write it to a file (using Guava’s <code class="literal">Files</code> utility
      class). It relies on having access to the
      <code class="literal">PipelineExecution</code> returned from running the
      pipeline asynchronously:</p><pre class="screen1"><code class="n">PipelineExecution</code> <code class="n">execution</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">runAsync</code><code class="o">();</code>
<code class="n">String</code> <code class="n">dot</code> <code class="o">=</code> <code class="n">execution</code><code class="o">.</code><code class="na">getPlanDotFile</code><code class="o">();</code>
<code class="n">Files</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">dot</code><code class="o">,</code> <code class="k">new</code> <code class="n">File</code><code class="o">(</code><code class="sb">"pipeline.dot"</code><code class="o">),</code> <code class="n">Charsets</code><code class="o">.</code><code class="na">UTF_8</code><code class="o">);</code>
<code class="n">execution</code><code class="o">.</code><code class="na">waitUntilDone</code><code class="o">();</code>
<code class="n">pipeline</code><code class="o">.</code><code class="na">done</code><code class="o">();</code></pre><p class="calibre2">The <em class="calibre10">dot</em> command-line tool
      <a class="calibre" id="calibre_link-1540"></a>converts the DOT file into a graphical format, such as
      PNG, for easy inspection. The following invocation converts all DOT
      files in the current directory to PNG format, so <em class="calibre10">pipeline.dot</em> is converted to a file called
      <em class="calibre10">pipeline.dot.png</em>:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9"><code class="n1">dot</code> <code class="o1">-</code><code class="n1">Tpng</code> <code class="o1">-</code><code class="n1">O</code> <code class="o1">*.</code><code class="na1">dot</code></code></strong></pre><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">There is a trick for obtaining the DOT file when you don’t have
        a <code class="literal">PipelineExecution</code> object, such as when the
        pipeline is run synchronously or implicitly (see <a class="ulink" href="#calibre_link-766" title="Materialization">Materialization</a>). Crunch stores the DOT file
        representation in the job configuration, so it can be retrieved after
        the pipeline has finished:</p><pre class="programlisting"><code class="n">PipelineResult</code> <code class="n">result</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">done</code><code class="o">();</code>
<code class="n">String</code> <code class="n">dot</code> <code class="o">=</code> <span class="calibre24"><strong class="calibre9"><code class="n1">pipeline</code><code class="o1">.</code><code class="na1">getConfiguration</code><code class="o1">().</code><code class="na1">get</code><code class="o1">(</code><code class="s">"crunch.planner.dotfile"</code><code class="o1">);</code></strong></span>
<code class="n">Files</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">dot</code><code class="o">,</code> <code class="k">new</code> <code class="n">File</code><code class="o">(</code><code class="sb">"pipeline.dot"</code><code class="o">),</code> <code class="n">Charsets</code><code class="o">.</code><code class="na">UTF_8</code><code class="o">);</code></pre></div><p class="calibre2">Let’s look at a plan for a nontrivial pipeline for calculating a
      histogram of word counts for text files stored in <code class="literal">inputPath</code> (see <a class="ulink" href="#calibre_link-768" title="Example&nbsp;18-3.&nbsp;A Crunch pipeline for calculating a histogram of word counts">Example&nbsp;18-3</a>). Production pipelines can grow
      to be much longer than this one, with dozens of MapReduce jobs, but this
      illustrates some of the characteristics of the Crunch planner.</p><div class="example"><a id="calibre_link-768" class="calibre"></a><div class="example-title">Example&nbsp;18-3.&nbsp;A Crunch pipeline for calculating a histogram of word
        counts</div><div class="book"><pre class="screen"><code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">lines</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">readTextFile</code><code class="o">(</code><code class="n">inputPath</code><code class="o">);</code>
<code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">lower</code> <code class="o">=</code> <code class="n">lines</code><code class="o">.</code><code class="na">parallelDo</code><code class="o">(</code><code class="sb">"lower"</code><code class="o">,</code> <code class="k">new</code> <code class="n">ToLowerFn</code><code class="o">(),</code> <code class="n">strings</code><code class="o">());</code>
<code class="n">PTable</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Long</code><code class="o">&gt;</code> <code class="n">counts</code> <code class="o">=</code> <code class="n">lower</code><code class="o">.</code><code class="na">count</code><code class="o">();</code>
<code class="n">PTable</code><code class="o">&lt;</code><code class="n">Long</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;</code> <code class="n">inverseCounts</code> <code class="o">=</code> <code class="n">counts</code><code class="o">.</code><code class="na">parallelDo</code><code class="o">(</code><code class="sb">"inverse"</code><code class="o">,</code>
    <code class="k">new</code> <code class="n">InversePairFn</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Long</code><code class="o">&gt;(),</code> <code class="n">tableOf</code><code class="o">(</code><code class="n">longs</code><code class="o">(),</code> <code class="n">strings</code><code class="o">()));</code>
<code class="n">PTable</code><code class="o">&lt;</code><code class="n">Long</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;</code> <code class="n">hist</code> <code class="o">=</code> <code class="n">inverseCounts</code>
    <code class="o">.</code><code class="na">groupByKey</code><code class="o">()</code>
    <code class="o">.</code><code class="na">mapValues</code><code class="o">(</code><code class="sb">"count values"</code><code class="o">,</code> <code class="k">new</code> <code class="n">CountValuesFn</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;(),</code> <code class="n">ints</code><code class="o">());</code>
<code class="n">hist</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">To</code><code class="o">.</code><code class="na">textFile</code><code class="o">(</code><code class="n">outputPath</code><code class="o">),</code> <code class="n">Target</code><code class="o">.</code><code class="na">WriteMode</code><code class="o">.</code><code class="na">OVERWRITE</code><code class="o">);</code>
<code class="n">pipeline</code><code class="o">.</code><code class="na">done</code><code class="o">();</code></pre></div></div><p class="calibre2">The plan diagram generated from this pipeline is shown in <a class="ulink" href="#calibre_link-769" title="Figure&nbsp;18-1.&nbsp;Plan diagram for a Crunch pipeline for calculating a histogram of word counts">Figure&nbsp;18-1</a>.</p><p class="calibre2">Sources and targets are rendered as folder icons. The top of the
      diagram shows the input source, and the output target is shown at the
      bottom. We can see that there are two MapReduce jobs (labeled <span class="calibre">Crunch Job 1</span> and <span class="calibre">Crunch
      Job 2</span>), and a temporary sequence file that Crunch generates
      to write the output of one job to so that the other can read it as
      input. The temporary file is deleted when
      <code class="literal">clean()</code> is called at the end of the pipeline
      execution.</p><p class="calibre2">Crunch Job 2 (which is actually the one that runs first; it was
      just produced by the planner second) consists of a map phase and a
      reduce phase, depicted by labeled boxes in the diagram. Each map and
      reduce is decomposed into smaller operations, shown by boxes labeled
      with names that correspond to the names of primitive Crunch operations
      in the code. For example, the <a class="calibre" id="calibre_link-2968"></a>first <code class="literal">parallelDo()</code> operation in
      the map phase is the one labeled <span class="calibre">lower</span>, which simply lowercases each string in a
      <code class="literal">PCollection</code>.</p><div class="note" title="Tip"><h3 class="title3">Tip</h3><p class="calibre2">Use the overloaded methods of <code class="literal">PCollection</code>
        and related classes that take a name to give meaningful names to the
        operations in your pipeline. This makes it easier to follow plan
        diagrams.</p></div><div class="book"><div class="figure"><a id="calibre_link-769" class="calibre"></a><div class="book"><div class="book"><img alt="Plan diagram for a Crunch pipeline for calculating a histogram of word counts" src="images/000005.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;18-1.&nbsp;Plan diagram for a Crunch pipeline for calculating a histogram
          of word counts</div></div></div><p class="calibre2">After the lowercasing operation, the next transformation in the
      program is to produce a <code class="literal">PTable</code> of counts of each
      unique string, using the built-in convenience method
      <code class="literal">count()</code>. This method actually performs three
      primitive Crunch operations: a <code class="literal">parallelDo()</code>
      named <span class="calibre">Aggregate.count</span>, a
      <code class="literal">groupByKey()</code> operation labeled <span class="calibre">GBK</span> in the diagram, and a
      <code class="literal">combineValues()</code> operation labeled <span class="calibre">combine</span>.</p><p class="calibre2">Each <span class="calibre">GBK</span> operation is realized
      as a MapReduce shuffle step, with the
      <code class="literal">groupByKey()</code> and
      <code class="literal">combineValues()</code> operations running in the
      reduce phase. The <span class="calibre">Aggregate.count</span>
      <code class="literal">parallelDo()</code> operation runs in the map phase,
      but notice that it is run in the same map as the <span class="calibre">lower</span> operation: the Crunch planner attempts to
      minimize the number of MapReduce jobs that it needs to run for a
      pipeline. In a similar way, the <span class="calibre">inverse</span> <code class="literal">parallelDo()</code>
      operation is run as a part of the preceding reduce.<sup class="calibre6">[<a class="firstname" href="#calibre_link-770" id="calibre_link-782">125</a>]</sup></p><p class="calibre2">The last transformation is to take the inverted counts
      <code class="literal">PTable</code> and find the frequency of each count. For
      example, if the strings that occur three times are <code class="literal">apple</code> and <code class="literal">orange</code>, then the count of 3 has a frequency of
      2. This transformation is another <span class="calibre">GBK</span>
      operation, which forces a new MapReduce job (Crunch Job 1), followed by
      a <code class="literal">mapValues()</code> operation that we named
      <span class="calibre">count values</span>. The <code class="literal">mapValues()</code> operation is simply a
      <code class="literal">parallelDo()</code> operation that can therefore be
      run in the reduce.</p><p class="calibre2">Notice that the map phase for Crunch Job 1 is omitted from the
      diagram since no primitive Crunch operations <a class="calibre" id="calibre_link-1301"></a><a class="calibre" id="calibre_link-3039"></a>are run in it.</p></div><div class="book" title="Iterative Algorithms"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-763">Iterative Algorithms</h3></div></div></div><p class="calibre2">A common <a class="calibre" id="calibre_link-3040"></a>use of <code class="literal">PObject</code>s is <a class="calibre" id="calibre_link-3056"></a>to check for convergence in an iterative algorithm. The
      classic example of a distributed iterative algorithm is the PageRank
      algorithm for ranking the relative importance of each of a set of linked
      pages, such as the World Wide Web.<sup class="calibre6">[<a class="firstname" href="#calibre_link-771" id="calibre_link-783">126</a>]</sup> The control flow for a Crunch implementation of <a class="calibre" id="calibre_link-2897"></a>PageRank looks like this:</p><pre class="screen1"><code class="n">PTable</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">PageRankData</code><code class="o">&gt;</code> <code class="n">scores</code> <code class="o">=</code> <code class="n">readUrls</code><code class="o">(</code><code class="n">pipeline</code><code class="o">,</code> <code class="n">urlInput</code><code class="o">);</code>
<code class="n">Float</code> <code class="n">delta</code> <code class="o">=</code> <code class="mi">1.0f</code><code class="o">;</code>
<code class="k">while</code> <code class="o">(</code><code class="n">delta</code> <code class="o">&gt;</code> <code class="mi">0.01</code><code class="o">)</code> <code class="o">{</code>
  <code class="n">scores</code> <code class="o">=</code> <code class="n">pageRank</code><code class="o">(</code><code class="n">scores</code><code class="o">,</code> <code class="mi">0.5f</code><code class="o">);</code>
  <code class="n">PObject</code><code class="o">&lt;</code><code class="n">Float</code><code class="o">&gt;</code> <code class="n">pDelta</code> <code class="o">=</code> <code class="n">computeDelta</code><code class="o">(</code><code class="n">scores</code><code class="o">);</code>
  <code class="n">delta</code> <code class="o">=</code> <code class="n">pDelta</code><code class="o">.</code><code class="na">getValue</code><code class="o">();</code>
<code class="o">}</code></pre><p class="calibre2">Without going into detail on the operation of the PageRank
      algorithm itself, we can understand how the higher-level program
      execution works in Crunch.</p><p class="calibre2">The input is a text file with two URLs per line: a page and an
      outbound link from that page. For example, the following file encodes
      the fact that <code class="literal">A</code> has links to <code class="literal">B</code> and
      <code class="literal">C</code>, and <code class="literal">B</code> has a link to
      <code class="literal">D</code>:</p><pre class="screen1">www.A.com	www.B.com
www.A.com	www.C.com
www.B.com	www.D.com</pre><p class="calibre2">Going back to the code, the first line reads the input and
      computes an initial <code class="literal">PageRankData</code> object for each
      unique page. <code class="literal">PageRankData</code> is a simple Java class
      with fields for the score, the previous score (this will be used to
      check for convergence), and a list of outbound links:</p><pre class="screen1"><code class="k">public</code> <code class="k">static</code> <code class="k">class</code> <code class="nc">PageRankData</code> <code class="o">{</code>
  <code class="k">public</code> <code class="kt">float</code> <code class="n">score</code><code class="o">;</code>
  <code class="k">public</code> <code class="kt">float</code> <code class="n">lastScore</code><code class="o">;</code>
  <code class="k">public</code> <code class="n">List</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">urls</code><code class="o">;</code>

  <code class="c2">// ... methods elided</code>
<code class="o">}</code></pre><p class="calibre2">The goal of the algorithm is to compute a score for each page,
      representing its relative importance. All pages start out equal, so the
      initial score is set to be 1 for each page, and the previous score is 0.
      Creating the list of outbound links is achieved using the Crunch
      operations of grouping the input by the first field (page), then
      aggregating the values (outbound links) into a list.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-772" id="calibre_link-784">127</a>]</sup></p><p class="calibre2">The iteration is carried out using a regular Java <code class="literal">while</code> loop. The scores are updated in each
      iteration of the loop by calling the <code class="literal">pageRank()</code>
      method, which encapsulates the PageRank algorithm as a series of Crunch
      operations. <br>If the delta between the last set of scores and the new set
      of scores is below a small enough value (0.01), then the scores have
      converged and the algorithm terminates. The delta is computed by the
      <code class="literal">computeDelta()</code> method, a Crunch aggregation
      that finds the largest absolute difference in page score for all the
      pages in the collection.</p><p class="calibre2">So when is the pipeline run? The answer is each time <code class="literal">pDelta.getValue()</code> is called. The first time
      through the loop, no <code class="literal">PCollection</code>s have been
      materialized yet, so the jobs for <code class="literal">readUrls()</code>,
      <code class="literal">pageRank()</code>, and
      <code class="literal">computeDelta()</code> must be run in order to compute
      <code class="literal">delta</code>. <br>On subsequent iterations only
      the jobs to compute the new scores (<code class="literal">pageRank()</code>)
      and <code class="literal">delta</code>
      (<code class="literal">computeDelta()</code>) need be <a class="calibre" id="calibre_link-3041"></a>run.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">For this pipeline, Crunch’s planner does a better job of
        optimizing the execution plan if <code class="literal">scores.materialize().iterator()</code> is called
        immediately after the <code class="literal">pageRank()</code> call. This
        ensures that the <code class="literal">scores</code> table is
        explicitly materialized, so it is available for the next execution
        plan in the next iteration of the loop. Without the call to
        <code class="literal">materialize()</code>, the program still produces the same
        result, but it’s less efficient: the planner may choose to materialize
        different intermediate results, and so for the next iteration of the
        loop some of the computation must be re-executed to get the <code class="literal">scores</code> to pass to the
        <code class="literal">pageRank()</code> call.</p></div></div><div class="book" title="Checkpointing a Pipeline"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-759">Checkpointing a Pipeline</h3></div></div></div><p class="calibre2">In the previous section, <a class="calibre" id="calibre_link-3037"></a><a class="calibre" id="calibre_link-1094"></a>we saw that Crunch will <a class="calibre" id="calibre_link-2962"></a>reuse any <code class="literal">PCollection</code>s that were
      materialized in any previous runs of the same pipeline. However, if you
      create a new pipeline instance, then it will not automatically share any
      materialized <code class="literal">PCollection</code>s from other pipelines,
      even if the input source is the same. This can make developing a
      pipeline rather time consuming, since even a small change to a
      computation toward the end of the pipeline means Crunch will run the new
      pipeline from the beginning.</p><p class="calibre2">The solution is to <em class="calibre10">checkpoint</em> a
      <code class="literal">PCollection</code> to persistent storage (typically
      HDFS) so that Crunch can start from the checkpoint in the new
      pipeline.</p><p class="calibre2">Consider the Crunch program for calculating a histogram of word
      counts for text files back in <a class="ulink" href="#calibre_link-768" title="Example&nbsp;18-3.&nbsp;A Crunch pipeline for calculating a histogram of word counts">Example&nbsp;18-3</a>. We saw that the Crunch planner
      translates this pipeline into two MapReduce jobs. If the program is run
      for a second time, then Crunch will run the two MapReduce jobs again and
      overwrite the original output, since <code class="literal">WriteMode</code> is
      set to <code class="literal">OVERWRITE</code>.</p><p class="calibre2">If instead we checkpointed <code class="literal">inverseCounts</code>, a subsequent run would only
      launch one MapReduce job (the one for computing <code class="literal">hist</code>, since it is entirely derived from
      <code class="literal">inverseCounts</code>). Checkpointing is
      simply a matter of writing a <code class="literal">PCollection</code> to a
      target with the <code class="literal">WriteMode</code> set <a class="calibre" id="calibre_link-1092"></a>to <code class="literal">CHECKPOINT</code>:</p><pre class="screen1"><code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">lines</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="na">readTextFile</code><code class="o">(</code><code class="n">inputPath</code><code class="o">);</code>
<code class="n">PTable</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Long</code><code class="o">&gt;</code> <code class="n">counts</code> <code class="o">=</code> <code class="n">lines</code><code class="o">.</code><code class="na">count</code><code class="o">();</code>
<code class="n">PTable</code><code class="o">&lt;</code><code class="n">Long</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;</code> <code class="n">inverseCounts</code> <code class="o">=</code> <code class="n">counts</code><code class="o">.</code><code class="na">parallelDo</code><code class="o">(</code>
    <code class="k">new</code> <code class="n">InversePairFn</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Long</code><code class="o">&gt;(),</code> <code class="n">tableOf</code><code class="o">(</code><code class="n">longs</code><code class="o">(),</code> <code class="n">strings</code><code class="o">()));</code><span class="calibre24"><strong class="calibre9">
<code class="n1">inverseCounts</code><code class="o1">.</code><code class="na1">write</code><code class="o1">(</code><code class="n1">To</code><code class="o1">.</code><code class="na1">sequenceFile</code><code class="o1">(</code><code class="n1">checkpointPath</code><code class="o1">),</code>
    <code class="n1">Target</code><code class="o1">.</code><code class="na1">WriteMode</code><code class="o1">.</code><code class="na1">CHECKPOINT</code><code class="o1">);</code></strong></span>
<code class="n">PTable</code><code class="o">&lt;</code><code class="n">Long</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;</code> <code class="n">hist</code> <code class="o">=</code> <code class="n">inverseCounts</code>
    <code class="o">.</code><code class="na">groupByKey</code><code class="o">()</code>
    <code class="o">.</code><code class="na">mapValues</code><code class="o">(</code><code class="k">new</code> <code class="n">CountValuesFn</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;(),</code> <code class="n">ints</code><code class="o">());</code>
<code class="n">hist</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">To</code><code class="o">.</code><code class="na">textFile</code><code class="o">(</code><code class="n">outputPath</code><code class="o">),</code> <code class="n">Target</code><code class="o">.</code><code class="na">WriteMode</code><code class="o">.</code><code class="na">OVERWRITE</code><code class="o">);</code>
<code class="n">pipeline</code><code class="o">.</code><code class="na">done</code><code class="o">();</code></pre><p class="calibre2">Crunch compares the timestamps of the input files with those of
      the checkpoint files; if any inputs have later timestamps than the
      checkpoints, then it will recompute the dependent checkpoints
      automatically, so there is no risk of using out-of-date data in the
      pipeline.</p><p class="calibre2">Since they are persistent between pipeline runs, checkpoints are
      not cleaned up by Crunch, so you will need to delete them once you are
      happy that the code is producing the expected results.</p></div></div><div class="book" title="Crunch Libraries"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4578">Crunch Libraries</h2></div></div></div><p class="calibre2">Crunch comes with a <a class="calibre" id="calibre_link-1296"></a><a class="calibre" id="calibre_link-1765"></a>powerful set of library functions <a class="calibre" id="calibre_link-2856"></a>in the <code class="literal">org.apache.crunch.lib</code> package—they are
    summarized in <a class="ulink" href="#calibre_link-773" title="Table&nbsp;18-1.&nbsp;Crunch libraries">Table&nbsp;18-1</a>.</p><div class="table"><a id="calibre_link-773" class="calibre"></a><div class="table-title">Table&nbsp;18-1.&nbsp;Crunch libraries</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="c6"><col class="c"><col class="c7"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Class</td><td class="calibre20">Method name(s)</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td rowspan="6" class="calibre23"><code class="uri">Aggregate</code></td><td class="calibre23"><code class="uri">length()</code></td><td class="calibre25">Returns the <a class="calibre" id="calibre_link-876"></a>number of elements in a
            <code class="uri">PCollection</code> wrapped in a
            <code class="uri">PObject</code>.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">min()</code></td><td class="calibre25">Returns the smallest value element in a
            <code class="uri">PCollection</code> wrapped in a
            <code class="uri">PObject</code>.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">max()</code></td><td class="calibre25">Returns the largest value element in a
            <code class="uri">PCollection</code> wrapped in a
            <code class="uri">PObject</code>.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">count()</code></td><td class="calibre25">Returns a table of the unique elements of the input
            <code class="uri">PCollection</code> mapped to their counts.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">top()</code></td><td class="calibre25">Returns a table of the top or bottom <span class="calibre">N</span> key-value pairs in a
            <code class="uri">PTable</code>, ordered by value.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">collectValues()</code></td><td class="calibre25">Groups the values for each unique key in a table into a
            Java <code class="uri">Collection</code>, returning a
            <code class="uri">PTable&lt;K,
            Collection&lt;V&gt;&gt;</code>.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">Cartesian</code></td><td class="calibre23"><code class="uri">cross()</code></td><td class="calibre25">Calculates the <a class="calibre" id="calibre_link-1048"></a>cross product of two
            <code class="uri">PCollection</code>s or
            <code class="uri">PTable</code>s.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">Channels</code></td><td class="calibre23"><code class="uri">split()</code></td><td class="calibre25">Splits a <a class="calibre" id="calibre_link-1088"></a>collection of pairs
            (<code class="uri">PCollection&lt;Pair&lt;T, U&gt;&gt;</code>) into
            a pair of collections (<code class="uri">Pair&lt;PCollection&lt;T&gt;,
            PCollection&lt;U&gt;&gt;</code>).</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">Cogroup</code></td><td class="calibre23"><code class="uri">cogroup()</code></td><td class="calibre25">Groups <a class="calibre" id="calibre_link-1166"></a>the elements in two or more
            <code class="uri">PTable</code>s by key.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">Distinct</code></td><td class="calibre23"><code class="uri">distinct()</code></td><td class="calibre25">Creates <a class="calibre" id="calibre_link-1515"></a>a new <code class="uri">PCollection</code> or
            <code class="uri">PTable</code> with duplicate elements
            removed.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">Join</code></td><td class="calibre23"><code class="uri">join()</code></td><td class="calibre25">Performs <a class="calibre" id="calibre_link-2272"></a>an inner join on two <code class="uri">PTable</code>s
            by key. There are also methods for left, right, and full
            joins.</td></tr><tr class="calibre26"><td rowspan="2" class="calibre23"><code class="uri">Mapred</code></td><td class="calibre23"><code class="uri">map()</code></td><td class="calibre25">Runs <a class="calibre" id="calibre_link-2418"></a>a mapper (old API) on a <code class="uri">PTable&lt;K1,
            V1&gt;</code> to produce a <code class="uri">PTable&lt;K2,
            V2&gt;</code>.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">reduce()</code></td><td class="calibre25">Runs a reducer (old API) on a
            <code class="uri">PGroupedTable&lt;K1, V1&gt;</code> to produce a
            <code class="uri">PTable&lt;K2, V2&gt;</code>.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">Mapreduce</code></td><td class="calibre23"><code class="uri">map()</code>,
            <code class="uri">reduce()</code></td><td class="calibre25">Like <code class="uri">Mapred</code>, but for <a class="calibre" id="calibre_link-2521"></a>the new MapReduce API.</td></tr><tr class="calibre19"><td rowspan="5" class="calibre23"><code class="uri">PTables</code></td><td class="calibre23"><code class="uri">asPTable()</code></td><td class="calibre25">Converts <a class="calibre" id="calibre_link-3087"></a>a <code class="uri">PCollection&lt;Pair&lt;K,
            V&gt;&gt;</code> to a <code class="uri">PTable&lt;K,
            V&gt;</code>.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">keys()</code></td><td class="calibre25">Returns a <code class="uri">PTable</code>’s keys as a
            <code class="uri">PCollection</code>.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">values()</code></td><td class="calibre25">Returns a <code class="uri">PTable</code>’s values as a
            <code class="uri">PCollection</code>.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">mapKeys()</code></td><td class="calibre25">Applies a map function to all the keys in a
            <code class="uri">PTable</code>, leaving the values
            unchanged.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">mapValues()</code></td><td class="calibre25">Applies a map function to all the values in a
            <code class="uri">PTable</code> or
            <code class="uri">PGroupedTable</code>, leaving the keys
            unchanged.</td></tr><tr class="calibre26"><td rowspan="2" class="calibre23"><code class="uri">Sample</code></td><td class="calibre23"><code class="uri">sample()</code></td><td class="calibre25">Creates<a class="calibre" id="calibre_link-3261"></a> a sample of a <code class="uri">PCollection</code> by
            choosing each element independently with a specified
            probability.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">reservoirSample()</code></td><td class="calibre25">Creates a sample of a <code class="uri">PCollection</code> of
            a specified size, where each element is equally likely to be
            included.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">SecondarySort</code></td><td class="calibre23"><code class="uri">sortAndApply()</code></td><td class="calibre25">Sorts <a class="calibre" id="calibre_link-3306"></a>a <code class="uri">PTable&lt;K, Pair&lt;V1,
            V2&gt;&gt;</code> by <code class="uri">K</code>
            then <code class="uri">V1</code>, then applies a
            function to give an output <code class="uri">PCollection</code> or
            <code class="uri">PTable</code>.</td></tr><tr class="calibre19"><td rowspan="3" class="calibre23"><code class="uri">Set</code></td><td class="calibre23"><code class="uri">difference()</code></td><td class="calibre25">Returns a <code class="uri">PCollection</code> that is the
            set difference of two <code class="uri">PCollection</code>s.</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">intersection()</code></td><td class="calibre25">Returns<a class="calibre" id="calibre_link-3377"></a> a <code class="uri">PCollection</code> that is the
            set intersection of two
            <code class="uri">PCollection</code>s.</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">comm()</code></td><td class="calibre25">Returns a <code class="uri">PCollection</code> of triples
            that classifies each element from two
            <code class="uri">PCollection</code>s by whether it is only in the
            first collection, only in the second collection, or in both
            collections. (Similar to the Unix <code class="uri">comm</code>
            command.)</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">Shard</code></td><td class="calibre23"><code class="uri">shard()</code></td><td class="calibre25">Creates<a class="calibre" id="calibre_link-3385"></a> a <code class="uri">PCollection</code> that contains
            exactly the same elements as the input
            <code class="uri">PCollection</code>, but is partitioned (sharded)
            across a specified number of files.</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">Sort</code></td><td class="calibre27"><code class="uri">sort()</code></td><td class="calibre28">Performs <a class="calibre" id="calibre_link-3424"></a>a total sort on a <code class="uri">PCollection</code>
            in the natural order of its elements in ascending (the default) or
            descending order. There are also methods to sort <code class="uri">PTable</code>s by
            key, and collections of <code class="uri">Pair</code>s or tuples by
            a subset of their columns in a specified order.</td></tr></tbody></table></div></div><p class="calibre2">One of the most powerful things about Crunch is that if the function
    you need is not provided, then it is simple to write it yourself,
    typically in a few lines of Java. For an example of a general-purpose
    function (for finding the unique values in a
    <code class="literal">PTable</code>), see <a class="ulink" href="#calibre_link-762" title="Example&nbsp;18-2.&nbsp;Finding the set of unique values for each key in a PTable">Example&nbsp;18-2</a>.</p><p class="calibre2">The methods <code class="literal">length()</code>,
    <code class="literal">min()</code>, <code class="literal">max()</code>, and
    <code class="literal">count()</code> from <code class="literal">Aggregate</code>
    have convenience method equivalents on <code class="literal">PCollection</code>.
    Similarly, <code class="literal">top()</code> (as well as the derived method
    <code class="literal">bottom()</code>) and
    <code class="literal">collectValues()</code> from
    <code class="literal">Aggregate</code>, all the methods from
    <code class="literal">PTables</code>, <code class="literal">join()</code> from
    <code class="literal">Join</code>, and <code class="literal">cogroup()</code> from
    <code class="literal">Cogroup</code> are all duplicated on
    <code class="literal">PTable</code>.</p><p class="calibre2">The code in <a class="ulink" href="#calibre_link-774" title="Example&nbsp;18-4.&nbsp;Using the aggregation methods on PCollection and PTable">Example&nbsp;18-4</a> walks through
    the behavior of some of the aggregation <a class="calibre" id="calibre_link-1766"></a><a class="calibre" id="calibre_link-1297"></a>methods.</p><div class="example"><a id="calibre_link-774" class="calibre"></a><div class="example-title">Example&nbsp;18-4.&nbsp;Using the aggregation methods on PCollection and PTable</div><div class="book"><pre class="screen"><code class="n">PCollection</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">a</code> <code class="o">=</code> <code class="n">MemPipeline</code><code class="o">.</code><code class="na">typedCollectionOf</code><code class="o">(</code><code class="n">strings</code><code class="o">(),</code>
    <code class="sb">"cherry"</code><code class="o">,</code> <code class="sb">"apple"</code><code class="o">,</code> <code class="sb">"banana"</code><code class="o">,</code> <code class="sb">"banana"</code><code class="o">);</code>

<code class="n">assertEquals</code><code class="o">((</code><code class="n">Long</code><code class="o">)</code> <code class="mi">4L</code><code class="o">,</code> <code class="n">a</code><code class="o">.</code><code class="na">length</code><code class="o">().</code><code class="na">getValue</code><code class="o">());</code>
<code class="n">assertEquals</code><code class="o">(</code><code class="sb">"apple"</code><code class="o">,</code> <code class="n">a</code><code class="o">.</code><code class="na">min</code><code class="o">().</code><code class="na">getValue</code><code class="o">());</code>
<code class="n">assertEquals</code><code class="o">(</code><code class="sb">"cherry"</code><code class="o">,</code> <code class="n">a</code><code class="o">.</code><code class="na">max</code><code class="o">().</code><code class="na">getValue</code><code class="o">());</code>

<code class="n">PTable</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Long</code><code class="o">&gt;</code> <code class="n">b</code> <code class="o">=</code> <code class="n">a</code><code class="o">.</code><code class="na">count</code><code class="o">();</code>
<code class="n">assertEquals</code><code class="o">(</code><code class="sb">"{(apple,1),(banana,2),(cherry,1)}"</code><code class="o">,</code> <code class="n">dump</code><code class="o">(</code><code class="n">b</code><code class="o">));</code>

<code class="n">PTable</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Long</code><code class="o">&gt;</code> <code class="n">c</code> <code class="o">=</code> <code class="n">b</code><code class="o">.</code><code class="na">top</code><code class="o">(</code><code class="mi">1</code><code class="o">);</code>
<code class="n">assertEquals</code><code class="o">(</code><code class="sb">"{(banana,2)}"</code><code class="o">,</code> <code class="n">dump</code><code class="o">(</code><code class="n">c</code><code class="o">));</code>

<code class="n">PTable</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">Long</code><code class="o">&gt;</code> <code class="n">d</code> <code class="o">=</code> <code class="n">b</code><code class="o">.</code><code class="na">bottom</code><code class="o">(</code><code class="mi">2</code><code class="o">);</code>
<code class="n">assertEquals</code><code class="o">(</code><code class="sb">"{(apple,1),(cherry,1)}"</code><code class="o">,</code> <code class="n">dump</code><code class="o">(</code><code class="n">d</code><code class="o">));</code></pre></div></div></div><div class="book" title="Further Reading"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4579">Further Reading</h2></div></div></div><p class="calibre2">This chapter has given a short introduction to Crunch. To find out
    more, consult the <a class="ulink" href="http://crunch.apache.org/user-guide.html" target="_top">Crunch User
    Guide</a>.</p></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" type="footnote" id="calibre_link-753"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-775">118</a>] </sup>Some operations do not require a <code class="literal">PType</code>,
          since they can infer it from the <code class="literal">PCollection</code>
          they are applied to. For example, <code class="literal">filter()</code>
          returns a <code class="literal">PCollection</code> with the same
          <code class="literal">PType</code> as the original.</p></div><div class="footnote" type="footnote" id="calibre_link-757"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-776">119</a>] </sup>Despite the name, <code class="literal">APPEND</code>
            does not append to existing output files.</p></div><div class="footnote" type="footnote" id="calibre_link-758"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-777">120</a>] </sup><code class="literal">HBaseTarget</code> does not check for
            existing outputs, so it behaves as if <code class="literal">APPEND</code> mode is used.</p></div><div class="footnote" id="calibre_link-761"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-778">121</a>] </sup>See the <a class="ulink" href="http://bit.ly/interface_serializable" target="_top">documentation</a>.</p></div><div class="footnote" type="footnote" id="calibre_link-764"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-779">122</a>] </sup>This is an example of where a pipeline gets executed without
          an explicit call to <code class="literal">run()</code> or
          <code class="literal">done()</code>, but it is still good practice to
          call <code class="literal">done()</code> when the pipeline is finished
          with so that intermediate files are disposed of.</p></div><div class="footnote" type="footnote" id="calibre_link-765"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-780">123</a>] </sup>There is also an <code class="literal">asMap()</code> method on
            <code class="literal">PTable&lt;K, V&gt;</code> that <a class="calibre" id="calibre_link-3081"></a>returns an object of type
            <code class="literal">PObject&lt;Map&lt;K, V&gt;&gt;</code>.</p></div><div class="footnote" type="footnote" id="calibre_link-767"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-781">124</a>] </sup>You can increment your own custom <a class="calibre" id="calibre_link-1265"></a>counters from Crunch <a class="calibre" id="calibre_link-1537"></a>using <code class="literal">DoFn</code>’s
          <code class="literal">increment()</code> method.</p></div><div class="footnote" id="calibre_link-770"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-782">125</a>] </sup>This optimization is called <code class="literal">parallelDo</code>
          <span class="calibre"><em class="calibre10">fusion</em></span>; it explained <a class="calibre" id="calibre_link-2905"></a>in more detail in the <a class="ulink" href="http://bit.ly/data-parallel_pipelines" target="_top">FlumeJava
          paper</a> referenced at the beginning of the chapter, along with
          some of the other optimizations used by Crunch. Note that
          <code class="literal">parallelDo</code> fusion is what allows you to decompose
          pipeline operations into small, logically separate functions without
          any loss of efficiency, since Crunch fuses them into as few
          MapReduce stages as possible.</p></div><div class="footnote" id="calibre_link-771"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-783">126</a>] </sup>For details, see <a class="ulink" href="http://en.wikipedia.org/wiki/PageRank" target="_top">Wikipedia</a>.</p></div><div class="footnote" type="footnote" id="calibre_link-772"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-784">127</a>] </sup>You can find the full source code in the Crunch integration
          tests in a class called <code class="literal">PageRankIT</code>.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-352"><section type="chapter" id="calibre_link-4580" title="Chapter&nbsp;19.&nbsp;Spark"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;19.&nbsp;Spark</h2></div></div></div><p class="calibre2"><a class="ulink" href="https://spark.apache.org/" target="_top">Apache Spark</a> is a cluster <a class="calibre" id="calibre_link-3448"></a><a class="calibre" id="calibre_link-4581"></a>computing framework for large-scale data processing. <br>Unlike
  most of the other processing frameworks discussed in this book, Spark does
  not use MapReduce as an execution engine; <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; instead, it uses its own
  distributed runtime for executing work on a cluster. <br>However, Spark has many
  parallels with MapReduce, in terms of both API and runtime, as we will see
  in this chapter. <br>Spark is closely integrated with Hadoop: it can run on YARN
  and works with Hadoop file formats and storage backends like HDFS.</p><p class="calibre2">Spark is best known for its ability to keep large working datasets in <a class="calibre" id="calibre_link-2668"></a>memory <span class="calibre"><em class="calibre10">between jobs</em></span>. <br>This capability allows Spark to outperform
    the equivalent MapReduce workflow (by an order of magnitude or more in some cases<sup class="calibre6">[<a class="firstname" href="#calibre_link-828" id="calibre_link-849">128</a>]</sup>), where datasets are always loaded from disk. <br>Two styles of application that benefit
    greatly from Spark’s <u style="
    text-decoration: underline 0.14em green;
">processing model</u> are iterative algorithms (where a function is applied to a
    dataset repeatedly until an exit condition is met) and interactive analysis (where a user issues
    a series of ad hoc exploratory queries on a dataset).</p><p class="calibre2">Even if you don’t need in-memory caching, Spark is very attractive for
  a couple of other reasons: its DAG engine and its user experience. <br>Unlike
  MapReduce, Spark’s DAG engine can process arbitrary pipelines of operators
  and translate them into a single job for the user.</p><p class="calibre2">Spark’s user experience is also second to none, with a rich set of APIs for performing many
    common data processing tasks, such as joins. <br>At the time of writing, Spark provides APIs in
    three languages: Scala, Java, and Python. <br>We’ll use the Scala API for most of the examples in
    this chapter, but they should be easy to translate to the other languages. <br>Spark also comes with
    a REPL (read—eval—print loop) for both Scala and Python, which makes it quick and easy to
    explore datasets.</p><p class="calibre2">Spark is proving to be a good platform on which to build analytics tools, too, and to this
    end the Apache Spark project includes modules for machine learning (MLlib), graph processing
    (GraphX), stream processing (Spark Streaming), and SQL (Spark SQL). <br>These modules are not
    covered in this chapter; the interested reader should refer to the <a class="ulink" href="http://spark.apache.org/" target="_top">Apache Spark website</a>.</p><div class="book" title="Installing Spark"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4582">Installing Spark</h2></div></div></div><p class="calibre2">Download a <a class="calibre" id="calibre_link-3457"></a>stable release of the Spark binary distribution from the
    <a class="ulink" href="https://spark.apache.org/downloads.html" target="_top">downloads
    page</a> (choose the one that matches the Hadoop distribution you are
    using), and unpack the tarball in a suitable location:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">tar xzf spark-<em class="replaceable1"><code class="calibre46">x.y.z</code></em>-bin-<em class="replaceable1"><code class="calibre46">distro</code></em>.tgz</code></strong></pre><p class="calibre2">It’s convenient to put the Spark binaries on your path as
    follows:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">export SPARK_HOME=~/sw/spark-<em class="replaceable1"><code class="calibre46">x.y.z</code></em>-bin-<em class="replaceable1"><code class="calibre46">distro</code></em></code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">export PATH=$PATH:$SPARK_HOME/bin</code></strong></pre><p class="calibre2">We’re now ready to run an example in Spark.</p></div><div class="book" title="An Example"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-837">An Example</h2></div></div></div><p class="calibre2">To introduce <a class="calibre" id="calibre_link-3453"></a>Spark, let’s run an interactive session <a class="calibre" id="calibre_link-3467"></a>using <em class="calibre10">spark-shell</em>, which
    is a Scala REPL with a few Spark additions. <br>Start up the shell with the
    following:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">spark-shell</code></strong>
Spark context available as sc.

<code class="literal">scala&gt;</code></pre><p class="calibre2">From the console output, we can see that the shell has created a Scala variable, <code class="literal">sc</code>, to store <a class="calibre" id="calibre_link-3472"></a>the <code class="literal">SparkContext</code> instance. <br>This is our entry point to
      Spark, and allows us to load a text file as follows:</p><pre class="screen1"><code class="literal">scala&gt;</code> <strong class="userinput"><code class="calibre9">val lines = sc.textFile("input/ncdc/micro-tab/sample.txt")</code></strong>
lines: org.apache.spark.rdd.RDD[String] = MappedRDD[1] at textFile at
 &lt;console&gt;:12</pre><p class="calibre2">The <code class="literal">lines</code> variable is a reference to <a class="calibre" id="calibre_link-4583"></a><a class="calibre" id="calibre_link-3147"></a>a <em class="calibre10">Resilient Distributed Dataset</em>, abbreviated to
        <em class="calibre10">RDD</em>, which is the central abstraction in Spark: a read-only
      collection of objects that is partitioned across multiple machines in a cluster. In a typical
      Spark program, one or more RDDs are loaded as input and through a series of transformations
      are turned into a set of target RDDs, which have an action performed on them (such as
      computing a result or writing them to persistent storage). The term “resilient” in “Resilient
      Distributed Dataset” refers to the fact that Spark can automatically reconstruct a lost
      partition by recomputing it from the RDDs that it was computed from.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Loading an RDD or performing a transformation on one does not trigger any data
        processing; it merely creates a plan for performing a computation. The computation is only
        triggered when an action (like <code class="literal">foreach()</code>) is performed on an
        RDD.</p></div><p class="calibre2">Let’s continue with the program. The first transformation we want to
    perform is to split the lines into fields:</p><pre class="screen1"><code class="literal">scala&gt;</code> <strong class="userinput"><code class="calibre9">val records = lines.map(_.split("\t"))</code></strong>
records: org.apache.spark.rdd.RDD[Array[String]] = MappedRDD[2] at map at
 &lt;console&gt;:14</pre><p class="calibre2">This uses the <code class="literal">map()</code> method <a class="calibre" id="calibre_link-3146"></a>on <code class="literal">RDD</code> to apply a function to every
    element in the RDD. In this case, we split each line (a
    <code class="literal">String</code>) into a Scala <code class="literal">Array</code>
    of <code class="literal">String</code>s.</p><p class="calibre2">Next, we apply a filter to remove any bad records:</p><pre class="screen1"><code class="literal">scala&gt;</code> <strong class="userinput"><code class="calibre9">val filtered = records.filter(rec =&gt; (rec(1) != "9999"
  &amp;&amp; rec(2).matches("[01459]")))</code></strong>
filtered: org.apache.spark.rdd.RDD[Array[String]] = FilteredRDD[3] at filter at
 &lt;console&gt;:16</pre><p class="calibre2">The <code class="literal">filter()</code> method on <code class="literal">RDD</code> takes a <a class="calibre" id="calibre_link-3145"></a>predicate, a function that returns a <code class="literal">Boolean</code>. This one
      tests for records that don’t have a missing temperature (indicated by 9999) or a bad quality
      reading.</p><p class="calibre2">To find the maximum temperatures for each year, we need to perform a grouping operation on
      the year field so we can process all the temperature values for each year. Spark provides a
        <code class="literal">reduceByKey()</code> method to do this, but it needs an RDD of key-value
      pairs, represented by a Scala <code class="literal">Tuple2</code>. So, first we need to transform
      our RDD into the correct form using another map:</p><pre class="screen1"><code class="literal">scala&gt;</code> <strong class="userinput"><code class="calibre9">val tuples = filtered.map(rec =&gt; (rec(0).toInt, rec(1).toInt))</code></strong>
tuples: org.apache.spark.rdd.RDD[(Int, Int)] = MappedRDD[4] at map at
 &lt;console&gt;:18</pre><p class="calibre2">Then we can perform the aggregation. The
    <code class="literal">reduceByKey()</code> method’s argument is a function
    that takes a pair of values and combines them into a single value; in this
    case, we use Java’s <code class="literal">Math.max</code>
    function:</p><pre class="screen1"><code class="literal">scala&gt;</code> <strong class="userinput"><code class="calibre9">val maxTemps = tuples.reduceByKey((a, b) =&gt; Math.max(a, b))</code></strong>
maxTemps: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[7] at
 reduceByKey at &lt;console&gt;:21</pre><p class="calibre2">We can display the contents of <code class="literal">maxTemps</code> by invoking the
    <code class="literal">foreach()</code> method and passing
    <code class="literal">println()</code> to print each element to the
    console:</p><pre class="screen1"><code class="literal">scala&gt;</code> <strong class="userinput"><code class="calibre9">maxTemps.foreach(println(_))</code></strong>
(1950,22)
(1949,111)</pre><p class="calibre2">The <code class="literal">foreach()</code> method is the same as the equivalent on standard
      Scala collections, like <code class="literal">List</code>, and applies a function (that has some
      side effect) to each element in the RDD. It is this operation that causes Spark to run a job
      to compute the values in the RDD, so they can be run through the
        <code class="literal">println()</code> method.</p><p class="calibre2">Alternatively, we can save the RDD to the filesystem with:</p><pre class="screen1"><code class="literal">scala&gt;</code> <strong class="userinput"><code class="calibre9">maxTemps.saveAsTextFile("output")</code></strong></pre><p class="calibre2">which creates a directory called <em class="calibre10">output</em> containing
      the partition files:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">cat output/part-*</code></strong>
(1950,22)
(1949,111)</pre><p class="calibre2">The <code class="literal">saveAsTextFile()</code> method also triggers a
    Spark job. The main difference is that no value is returned, and instead
    the RDD is computed and its partitions are written to files in the
    <em class="calibre10">output</em> directory.</p><div class="book" title="Spark Applications, Jobs, Stages, and Tasks"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4584">Spark Applications, Jobs, Stages, and Tasks</h3></div></div></div><p class="calibre2">As we’ve seen in the example, like MapReduce, Spark has the
      concept of <a class="calibre" id="calibre_link-2260"></a>a <em class="calibre10">job</em>. A Spark job is more general
      than a MapReduce job, though, since it is made up of an arbitrary
      directed acyclic graph (DAG) of <em class="calibre10">stages</em>, each of
      which is roughly equivalent to a map or reduce phase in
      MapReduce.</p><p class="calibre2">Stages are split into <em class="calibre10">tasks</em> by the
      <a class="calibre" id="calibre_link-3638"></a>Spark runtime and are run in parallel on partitions of an
      RDD spread across the cluster—just like tasks in MapReduce.</p><p class="calibre2">A job always runs in the context of an
      <em class="calibre10">application</em> (represented by a
      <code class="literal">SparkContext</code> instance) that serves to group RDDs
      and shared variables. An application can run more than one job, in
      series or in parallel, and provides the mechanism for a job to access an
      RDD that was cached by a previous job in the same application. (We will
      see how to cache RDDs in <a class="ulink" href="#calibre_link-829" title="Persistence">Persistence</a>.) An
      interactive Spark session, such as a <em class="calibre10">spark-shell</em> session, is just an instance of an
      application.</p></div><div class="book" title="A Scala Standalone Application"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4585">A Scala Standalone Application</h3></div></div></div><p class="calibre2">After working with the <a class="calibre" id="calibre_link-3263"></a>Spark shell to refine a program, you may want to package
      it into a self-contained application that can be run more than once. The
      Scala program in <a class="ulink" href="#calibre_link-830" title="Example&nbsp;19-1.&nbsp;Scala application to find the maximum temperature, using Spark">Example&nbsp;19-1</a> shows how
      to do this.</p><div class="example"><a id="calibre_link-830" class="calibre"></a><div class="example-title">Example&nbsp;19-1.&nbsp;Scala application to find the maximum temperature, using
        Spark</div><div class="book"><pre class="screen"><code class="k">import</code> <code class="nn">org.apache.spark.SparkContext._</code>
<code class="k">import</code> <code class="nn">org.apache.spark.</code><code class="o">{</code><code class="nc">SparkConf</code><code class="o">,</code> <code class="nc">SparkContext</code><code class="o">}</code>

<code class="k">object</code> <code class="nc">MaxTemperature</code> <code class="o">{</code>
  <code class="k">def</code> <code class="n">main</code><code class="o">(</code><code class="n">args</code><code class="k">:</code> <code class="kt">Array</code><code class="o">[</code><code class="kt">String</code><code class="o">])</code> <code class="o">{</code>
    <code class="k">val</code> <code class="n">conf</code> <code class="k">=</code> <code class="k">new</code> <code class="nc">SparkConf</code><code class="o">().</code><code class="n">setAppName</code><code class="o">(</code><code class="sb">"Max Temperature"</code><code class="o">)</code>
    <code class="k">val</code> <code class="n">sc</code> <code class="k">=</code> <code class="k">new</code> <code class="nc">SparkContext</code><code class="o">(</code><code class="n">conf</code><code class="o">)</code>

    <code class="n">sc</code><code class="o">.</code><code class="n">textFile</code><code class="o">(</code><code class="n">args</code><code class="o">(</code><code class="mi">0</code><code class="o">))</code>
      <code class="o">.</code><code class="n">map</code><code class="o">(</code><code class="k">_</code><code class="o">.</code><code class="n">split</code><code class="o">(</code><code class="sb">"\t"</code><code class="o">))</code>
      <code class="o">.</code><code class="n">filter</code><code class="o">(</code><code class="n">rec</code> <code class="k">=&gt;</code> <code class="o">(</code><code class="n">rec</code><code class="o">(</code><code class="mi">1</code><code class="o">)</code> <code class="o">!=</code> <code class="sb">"9999"</code> <code class="o">&amp;&amp;</code> <code class="n">rec</code><code class="o">(</code><code class="mi">2</code><code class="o">).</code><code class="n">matches</code><code class="o">(</code><code class="sb">"[01459]"</code><code class="o">)))</code>
      <code class="o">.</code><code class="n">map</code><code class="o">(</code><code class="n">rec</code> <code class="k">=&gt;</code> <code class="o">(</code><code class="n">rec</code><code class="o">(</code><code class="mi">0</code><code class="o">).</code><code class="n">toInt</code><code class="o">,</code> <code class="n">rec</code><code class="o">(</code><code class="mi">1</code><code class="o">).</code><code class="n">toInt</code><code class="o">))</code>
      <code class="o">.</code><code class="n">reduceByKey</code><code class="o">((</code><code class="n">a</code><code class="o">,</code> <code class="n">b</code><code class="o">)</code> <code class="k">=&gt;</code> <code class="nc">Math</code><code class="o">.</code><code class="n">max</code><code class="o">(</code><code class="n">a</code><code class="o">,</code> <code class="n">b</code><code class="o">))</code>
      <code class="o">.</code><code class="n">saveAsTextFile</code><code class="o">(</code><code class="n">args</code><code class="o">(</code><code class="mi">1</code><code class="o">))</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">When running a standalone program, we need to create the
      <code class="literal">SparkContext</code> since there is no shell to provide
      it. We create a new instance <a class="calibre" id="calibre_link-3471"></a>with a <code class="literal">SparkConf</code>, which allows us
      to pass various Spark properties to the application; here we just set
      the application name.</p><p class="calibre2">There are a couple of other minor changes. The first is that we’ve used the command-line
        arguments to specify the input and output paths. We’ve also used method chaining to avoid
        having to create intermediate variables for each RDD. This makes the program more compact,
        and we can still view the type information for each transformation in the Scala IDE if
        needed.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Not all the transformations that Spark defines are available on
        the <code class="literal">RDD</code> class itself. In this case,
        <code class="literal">reducebyKey()</code> (which acts only on RDDs of
        key-value pairs) is actually defined in the
        <code class="literal">PairRDDFunctions</code> class, but we can get Scala to
        implicitly convert <code class="literal">RDD[(Int, Int)]</code> to
        <code class="literal">PairRDDFunctions</code> with <a class="calibre" id="calibre_link-2900"></a>the following import:</p><pre class="programlisting"><code class="k">import</code> <code class="nn">org.apache.spark.SparkContext._</code></pre><p class="calibre2">This imports various implicit conversion functions used in
        Spark, so it is worth including in programs as a matter of
        course.</p></div><p class="calibre2">This time we use <em class="calibre10">spark-submit</em>
      to <a class="calibre" id="calibre_link-3468"></a>run the program, passing as arguments the application JAR
      containing the compiled Scala program, followed by our program’s
      command-line arguments (the input and output paths):</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">spark-submit --class MaxTemperature --master local \
  spark-examples.jar input/ncdc/micro-tab/sample.txt output</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">cat output/part-*</code></strong>
(1950,22)
(1949,111)</pre><p class="calibre2">We also specified two options: <code class="literal">--class</code> to tell Spark the name of the
      application class, and <code class="literal">--master</code> to
      specify where the job should run. The value <code class="literal">local</code> tells Spark to run everything in a
      single JVM on the local machine. We’ll learn about the options for
      running on a cluster in <a class="ulink" href="#calibre_link-831" title="Executors and Cluster Managers">Executors and Cluster Managers</a>.
      Next, let’s see how to use other languages with Spark, starting with
      <a class="calibre" id="calibre_link-3264"></a>Java.</p></div><div class="book" title="A Java Example"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4586">A Java Example</h3></div></div></div><p class="calibre2">Spark is implemented in <a class="calibre" id="calibre_link-2175"></a>Scala, which as a JVM-based language has excellent
      integration with Java. It is straightforward—albeit verbose—to express
      the same example in Java (see <a class="ulink" href="#calibre_link-832" title="Example&nbsp;19-2.&nbsp;Java application to find the maximum temperature, using Spark">Example&nbsp;19-2</a>).<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-833" id="calibre_link-850">129</a>]</sup></p><div class="example"><a id="calibre_link-832" class="calibre"></a><div class="example-title">Example&nbsp;19-2.&nbsp;Java application to find the maximum temperature, using
        Spark</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">MaxTemperatureSpark</code> <code class="o">{</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="na">length</code> <code class="o">!=</code> <code class="mi">2</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="sb">"Usage: MaxTemperatureSpark &lt;input path&gt; &lt;output path&gt;"</code><code class="o">);</code>
      <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(-</code><code class="mi">1</code><code class="o">);</code>
    <code class="o">}</code>

    <code class="n">SparkConf</code> <code class="n">conf</code> <code class="o">=</code> <code class="k">new</code> <code class="n">SparkConf</code><code class="o">();</code>
    <code class="n">JavaSparkContext</code> <code class="n">sc</code> <code class="o">=</code> <code class="k">new</code> <code class="n">JavaSparkContext</code><code class="o">(</code><code class="sb">"local"</code><code class="o">,</code> <code class="sb">"MaxTemperatureSpark"</code><code class="o">,</code> <code class="n">conf</code><code class="o">);</code>
    <code class="n">JavaRDD</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">lines</code> <code class="o">=</code> <code class="n">sc</code><code class="o">.</code><code class="na">textFile</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]);</code>
    <code class="n">JavaRDD</code><code class="o">&lt;</code><code class="n">String</code><code class="o">[]&gt;</code> <code class="n">records</code> <code class="o">=</code> <code class="n">lines</code><code class="o">.</code><code class="na">map</code><code class="o">(</code><code class="k">new</code> <code class="n">Function</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">String</code><code class="o">[]&gt;()</code> <code class="o">{</code>
      <code class="nd">@Override</code> <code class="k">public</code> <code class="n">String</code><code class="o">[]</code> <code class="n">call</code><code class="o">(</code><code class="n">String</code> <code class="n">s</code><code class="o">)</code> <code class="o">{</code>
        <code class="k">return</code> <code class="n">s</code><code class="o">.</code><code class="na">split</code><code class="o">(</code><code class="sb">"\t"</code><code class="o">);</code>
      <code class="o">}</code>
    <code class="o">});</code>
    <code class="n">JavaRDD</code><code class="o">&lt;</code><code class="n">String</code><code class="o">[]&gt;</code> <code class="n">filtered</code> <code class="o">=</code> <code class="n">records</code><code class="o">.</code><code class="na">filter</code><code class="o">(</code><code class="k">new</code> <code class="n">Function</code><code class="o">&lt;</code><code class="n">String</code><code class="o">[],</code> <code class="n">Boolean</code><code class="o">&gt;()</code> <code class="o">{</code>
      <code class="nd">@Override</code> <code class="k">public</code> <code class="n">Boolean</code> <code class="n">call</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">rec</code><code class="o">)</code> <code class="o">{</code>
        <code class="k">return</code> <code class="n">rec</code><code class="o">[</code><code class="mi">1</code><code class="o">]</code> <code class="o">!=</code> <code class="sb">"9999"</code> <code class="o">&amp;&amp;</code> <code class="n">rec</code><code class="o">[</code><code class="mi">2</code><code class="o">].</code><code class="na">matches</code><code class="o">(</code><code class="sb">"[01459]"</code><code class="o">);</code>
      <code class="o">}</code>
    <code class="o">});</code>
    <code class="n">JavaPairRDD</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;</code> <code class="n">tuples</code> <code class="o">=</code> <code class="n">filtered</code><code class="o">.</code><code class="na">mapToPair</code><code class="o">(</code>
      <code class="k">new</code> <code class="n">PairFunction</code><code class="o">&lt;</code><code class="n">String</code><code class="o">[],</code> <code class="n">Integer</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;()</code> <code class="o">{</code>
        <code class="nd">@Override</code> <code class="k">public</code> <code class="n">Tuple2</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;</code> <code class="n">call</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">rec</code><code class="o">)</code> <code class="o">{</code>
          <code class="k">return</code> <code class="k">new</code> <code class="n">Tuple2</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;(</code>
              <code class="n">Integer</code><code class="o">.</code><code class="na">parseInt</code><code class="o">(</code><code class="n">rec</code><code class="o">[</code><code class="mi">0</code><code class="o">]),</code> <code class="n">Integer</code><code class="o">.</code><code class="na">parseInt</code><code class="o">(</code><code class="n">rec</code><code class="o">[</code><code class="mi">1</code><code class="o">]));</code>
        <code class="o">}</code>
      <code class="o">}</code>
    <code class="o">);</code>
    <code class="n">JavaPairRDD</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;</code> <code class="n">maxTemps</code> <code class="o">=</code> <code class="n">tuples</code><code class="o">.</code><code class="na">reduceByKey</code><code class="o">(</code>
      <code class="k">new</code> <code class="n">Function2</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">,</code> <code class="n">Integer</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;()</code> <code class="o">{</code>
        <code class="nd">@Override</code> <code class="k">public</code> <code class="n">Integer</code> <code class="n">call</code><code class="o">(</code><code class="n">Integer</code> <code class="n">i1</code><code class="o">,</code> <code class="n">Integer</code> <code class="n">i2</code><code class="o">)</code> <code class="o">{</code>
          <code class="k">return</code> <code class="n">Math</code><code class="o">.</code><code class="na">max</code><code class="o">(</code><code class="n">i1</code><code class="o">,</code> <code class="n">i2</code><code class="o">);</code>
        <code class="o">}</code>
      <code class="o">}</code>
    <code class="o">);</code>
    <code class="n">maxTemps</code><code class="o">.</code><code class="na">saveAsTextFile</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">]);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">In Spark’s Java API, an <a class="calibre" id="calibre_link-3150"></a>RDD is <a class="calibre" id="calibre_link-2196"></a><a class="calibre" id="calibre_link-2195"></a>represented by an instance of <code class="literal">JavaRDD</code>, or
          <code class="literal">JavaPairRDD</code> for the special case of an RDD of key-value pairs. Both
        of these classes implement <a class="calibre" id="calibre_link-2197"></a>the <code class="literal">JavaRDDLike</code> interface, where most of the methods for
        working with RDDs can be found (when viewing class documentation, for example).</p><p class="calibre2">Running the program is identical to running the Scala version,
      except the classname is
      <code class="literal">MaxTemperatureSpark</code>.</p></div><div class="book" title="A Python Example"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4587">A Python Example</h3></div></div></div><p class="calibre2">Spark also has <a class="calibre" id="calibre_link-3100"></a>language support for Python, in an API <a class="calibre" id="calibre_link-3095"></a>called <em class="calibre10">PySpark</em>. By taking advantage
      of Python’s lambda expressions, we can rewrite the example program in a
      way that closely mirrors the Scala equivalent, as shown in <a class="ulink" href="#calibre_link-834" title="Example&nbsp;19-3.&nbsp;Python application to find the maximum temperature, using PySpark">Example&nbsp;19-3</a>.</p><div class="example"><a id="calibre_link-834" class="calibre"></a><div class="example-title">Example&nbsp;19-3.&nbsp;Python application to find the maximum temperature, using
        PySpark</div><div class="book"><pre class="screen"><code class="k">from</code> <code class="nn">pyspark</code> <code class="k">import</code> <code class="n">SparkContext</code>
<code class="k">import</code> <code class="nn">re</code><code class="o">,</code> <code class="nn">sys</code>

<code class="n">sc</code> <code class="o">=</code> <code class="n">SparkContext</code><code class="p">(</code><code class="sb">"local"</code><code class="p">,</code> <code class="sb">"Max Temperature"</code><code class="p">)</code>
<code class="n">sc</code><code class="o">.</code><code class="n">textFile</code><code class="p">(</code><code class="n">sys</code><code class="o">.</code><code class="n">argv</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code> \
  <code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="k">lambda</code> <code class="n">s</code><code class="p">:</code> <code class="n">s</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="sb">"</code><code class="se">\t</code><code class="sb">"</code><code class="p">))</code> \
  <code class="o">.</code><code class="n">filter</code><code class="p">(</code><code class="k">lambda</code> <code class="n">rec</code><code class="p">:</code> <code class="p">(</code><code class="n">rec</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="o">!=</code> <code class="sb">"9999"</code> <code class="ow">and</code> <code class="n">re</code><code class="o">.</code><code class="n">match</code><code class="p">(</code><code class="sb">"[01459]"</code><code class="p">,</code> <code class="n">rec</code><code class="p">[</code><code class="mi">2</code><code class="p">])))</code> \
  <code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="k">lambda</code> <code class="n">rec</code><code class="p">:</code> <code class="p">(</code><code class="nb">int</code><code class="p">(</code><code class="n">rec</code><code class="p">[</code><code class="mi">0</code><code class="p">]),</code> <code class="nb">int</code><code class="p">(</code><code class="n">rec</code><code class="p">[</code><code class="mi">1</code><code class="p">])))</code> \
  <code class="o">.</code><code class="n">reduceByKey</code><code class="p">(</code><code class="nb">max</code><code class="p">)</code> \
  <code class="o">.</code><code class="n">saveAsTextFile</code><code class="p">(</code><code class="n">sys</code><code class="o">.</code><code class="n">argv</code><code class="p">[</code><code class="mi">2</code><code class="p">])</code></pre></div></div><p class="calibre2">Notice that for the <code class="literal">reduceByKey()</code>
      transformation we can use Python’s built-in <code class="literal">max</code> function.</p><p class="calibre2">The important thing to note is that this program is written in
      regular CPython. Spark will fork Python subprocesses to run the user’s
      Python code (both in the launcher program and on
      <em class="calibre10">executors</em> that run user tasks in the cluster),
      and uses a socket to connect the two processes so the parent can pass
      RDD partition data to be processed by the Python code.</p><p class="calibre2">To run, we specify the Python file rather than the application
      JAR:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">spark-submit --master local \
  ch19-spark/src/main/python/MaxTemperature.py \
  input/ncdc/micro-tab/sample.txt output</code></strong></pre><p class="calibre2">Spark can also be run with Python in interactive mode using
      <a class="calibre" id="calibre_link-3096"></a><a class="calibre" id="calibre_link-3454"></a>the <code class="literal">pyspark</code> command.</p></div></div><div class="book" title="Resilient Distributed Datasets"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4588">Resilient Distributed Datasets</h2></div></div></div><p class="calibre2">RDDs are at the heart of every <a class="calibre" id="calibre_link-3148"></a><a class="calibre" id="calibre_link-3459"></a>Spark program, so in this section we look at how to work
    with them in more detail.</p><div class="book" title="Creation"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4589">Creation</h3></div></div></div><p class="calibre2">There are three ways of <a class="calibre" id="calibre_link-3149"></a>creating RDDs: from an in-memory collection of objects
      (known as <em class="calibre10">parallelizing</em> a collection), using a
      dataset from external storage (such as HDFS), or transforming an
      existing RDD. The first way is useful for doing CPU-intensive
      computations on small amounts of input data in parallel. For example,
      the following runs separate computations on the numbers from 1 to
      10:<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-835" id="calibre_link-851">130</a>]</sup></p><pre class="screen1"><code class="k">val</code> <code class="n">params</code> <code class="k">=</code> <code class="n">sc</code><code class="o">.</code><code class="n">parallelize</code><code class="o">(</code><code class="mi">1</code> <code class="n">to</code> <code class="mi">10</code><code class="o">)</code>
<code class="k">val</code> <code class="n">result</code> <code class="k">=</code> <code class="n">params</code><code class="o">.</code><code class="n">map</code><code class="o">(</code><code class="n">performExpensiveComputation</code><code class="o">)</code></pre><p class="calibre2">The <code class="literal">performExpensiveComputation</code>
      function is run on input values in parallel. The level of parallelism is
      determined from the <code class="literal">spark.default.parallelism</code> property, which has
      a default value that depends on where the Spark job is running. When
      running locally it is the number of cores on the machine, while for a
      cluster it is the total number of cores on all executor nodes in the
      cluster.</p><p class="calibre2">You can also override the level of parallelism for a particular
      computation by passing it as the second argument to
      <code class="literal">parallelize()</code>:</p><pre class="screen1"><code class="n">sc</code><code class="o">.</code><code class="n">parallelize</code><code class="o">(</code><code class="mi">1</code> <code class="n">to</code> <code class="mi">10</code><code class="o">,</code> <code class="mi">10</code><code class="o">)</code></pre><p class="calibre2">The second way to create an RDD is by creating a reference to an
      external dataset. We have already seen how to create an RDD of
      <code class="literal">String</code> objects for a text file:</p><pre class="screen1"><code class="k">val</code> <code class="n">text</code><code class="k">:</code> <code class="kt">RDD</code><code class="o">[</code><code class="kt">String</code><code class="o">]</code> <code class="k">=</code> <code class="n">sc</code><code class="o">.</code><code class="n">textFile</code><code class="o">(</code><code class="n">inputPath</code><code class="o">)</code></pre><p class="calibre2">The path may be any Hadoop filesystem path, such as a file on the local filesystem or on
        HDFS. Internally, Spark uses <code class="literal">TextInputFormat</code> from the old MapReduce
        API to read the file (see <a class="ulink" href="#calibre_link-836" title="TextInputFormat">TextInputFormat</a>). This means that the
        file-splitting behavior is the same as in Hadoop itself, so in the case of HDFS there is one
        Spark partition per HDFS block. The default can be changed by passing a second argument to
        request a particular number of splits:</p><pre class="screen1"><code class="n">sc</code><code class="o">.</code><code class="n">textFile</code><code class="o">(</code><code class="n">inputPath</code><code class="o">,</code> <code class="mi">10</code><code class="o">)</code></pre><p class="calibre2">Another variant permits text files to be processed as whole files (similar to <a class="ulink" href="#calibre_link-265" title="Processing a whole file as a record">Processing a whole file as a record</a>) by returning an RDD of string pairs, where the first
        string is the file path and the second is the file contents. Since each file is loaded into
        memory, this is only suitable for small files:</p><pre class="screen1"><code class="k">val</code> <code class="n">files</code><code class="k">:</code> <code class="kt">RDD</code><code class="o">[(</code><code class="kt">String</code>, <code class="kt">String</code><code class="o">)]</code> <code class="k">=</code> <code class="n">sc</code><code class="o">.</code><code class="n">wholeTextFiles</code><code class="o">(</code><code class="n">inputPath</code><code class="o">)</code></pre><p class="calibre2">Spark can work with other file formats besides text. For example,
      sequence files can be read with:</p><pre class="screen1"><code class="n">sc</code><code class="o">.</code><code class="n">sequenceFile</code><code class="o">[</code><code class="kt">IntWritable</code>, <code class="kt">Text</code><code class="o">](</code><code class="n">inputPath</code><code class="o">)</code></pre><p class="calibre2">Notice how the sequence file’s key and value <code class="literal">Writable</code> types have
      been specified. For common <code class="literal">Writable</code> types, Spark can map them to the
      Java equivalents, so we could use the equivalent form:</p><pre class="screen1"><code class="n">sc</code><code class="o">.</code><code class="n">sequenceFile</code><code class="o">[</code><code class="kt">Int</code>, <code class="kt">String</code><code class="o">](</code><code class="n">inputPath</code><code class="o">)</code></pre><p class="calibre2">There are two methods for creating RDDs from an arbitrary Hadoop
      <code class="literal">InputFormat</code>:
      <code class="literal">hadoopFile()</code> for file-based formats that expect
      a path, and <code class="literal">hadoopRDD()</code> for those that don’t,
      such as HBase’s <code class="literal">TableInputFormat</code>. These methods
      are for the old MapReduce API; for the new one, use
      <code class="literal">newAPIHadoopFile()</code> and
      <code class="literal">newAPIHadoopRDD()</code>. Here is an example of
      reading an Avro datafile using the Specific API with a
      <code class="literal">WeatherRecord</code> class:</p><pre class="screen1"><code class="k">val</code> <code class="n">job</code> <code class="k">=</code> <code class="k">new</code> <code class="nc">Job</code><code class="o">()</code>
<code class="nc">AvroJob</code><code class="o">.</code><code class="n">setInputKeySchema</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="nc">WeatherRecord</code><code class="o">.</code><code class="n">getClassSchema</code><code class="o">)</code>
<code class="k">val</code> <code class="n">data</code> <code class="k">=</code> <code class="n">sc</code><code class="o">.</code><code class="n">newAPIHadoopFile</code><code class="o">(</code><code class="n">inputPath</code><code class="o">,</code>
    <code class="n">classOf</code><code class="o">[</code><code class="kt">AvroKeyInputFormat</code><code class="o">[</code><code class="kt">WeatherRecord</code><code class="o">]],</code>
    <code class="n">classOf</code><code class="o">[</code><code class="kt">AvroKey</code><code class="o">[</code><code class="kt">WeatherRecord</code><code class="o">]],</code> <code class="n">classOf</code><code class="o">[</code><code class="kt">NullWritable</code><code class="o">],</code>
    <code class="n">job</code><code class="o">.</code><code class="n">getConfiguration</code><code class="o">)</code></pre><p class="calibre2">In addition to the path, the
      <code class="literal">newAPIHadoopFile()</code> method expects the
      <code class="literal">InputFormat</code> type, the key type, and the value
      type, plus the Hadoop configuration. The configuration carries the Avro
      schema, which we set in the second line using the
      <code class="literal">AvroJob</code> helper class.</p><p class="calibre2">The third way of creating an RDD is by transforming an existing
      RDD. We look at transformations next.</p></div><div class="book" title="Transformations and Actions"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4590">Transformations and Actions</h3></div></div></div><p class="calibre2">Spark provides two<a class="calibre" id="calibre_link-3151"></a> categories of operations on RDDs: <em class="calibre10">transformations</em> and
          <em class="calibre10">actions</em>. A transformation <a class="calibre" id="calibre_link-3715"></a><a class="calibre" id="calibre_link-866"></a>generates a new RDD from an existing one, while an action triggers a computation
        on an RDD and does something with the results—either returning them to the user, or saving
        them to external storage.</p><p class="calibre2">Actions have an immediate effect, but transformations do not—they
      are lazy, in the sense that they don’t perform any work until an action
      is performed on the transformed RDD. For example, the following
      lowercases lines in a text file:</p><pre class="screen1"><code class="k">val</code> <code class="n">text</code> <code class="k">=</code> <code class="n">sc</code><code class="o">.</code><code class="n">textFile</code><code class="o">(</code><code class="n">inputPath</code><code class="o">)</code>
<code class="k">val</code> <code class="n">lower</code><code class="k">:</code> <code class="kt">RDD</code><code class="o">[</code><code class="kt">String</code><code class="o">]</code> <code class="k">=</code> <code class="n">text</code><code class="o">.</code><code class="n">map</code><code class="o">(</code><code class="k">_</code><code class="o">.</code><code class="n">toLowerCase</code><code class="o">())</code>
<code class="n">lower</code><code class="o">.</code><code class="n">foreach</code><code class="o">(</code><code class="n">println</code><code class="o">(</code><code class="k">_</code><code class="o">))</code></pre><p class="calibre2">The <code class="literal">map()</code> method is a transformation, which Spark represents
        internally as a function (<code class="literal">toLowerCase()</code>) to be called at some later
        time on each element in the input RDD (<code class="literal">text</code>). The
        function is not actually called until the <code class="literal">foreach()</code> method (which
        is an action) is invoked and Spark runs a job to read the input file and call
          <code class="literal">toLowerCase()</code> on each line in it, before writing the result to
        the console.</p><p class="calibre2">One way of telling if an operation is a transformation or an
      action is by looking at its return type: if the return type is
      <code class="literal">RDD</code>, then it’s a transformation; otherwise, it’s
      an action. It’s useful to know this when looking at the documentation for
      <code class="literal">RDD</code> (in <a class="calibre" id="calibre_link-2871"></a>the <code class="literal">org.apache.spark.rdd</code> package),
      where most of the operations that can be performed on RDDs can be found.
      More operations can be found in <code class="literal">PairRDDFunctions</code>,
      which contains transformations and actions for working with RDDs of
      key-value pairs.</p><p class="calibre2">Spark’s library contains a rich set of operators, including transformations for mapping,
        grouping, aggregating, repartitioning, sampling, and joining RDDs, and for treating RDDs as
        sets. There are also actions for materializing RDDs as collections, computing statistics on
        RDDs, sampling a fixed number of elements from an RDD, and saving RDDs to external storage.
        For details, consult the class documentation.</p><div class="sidebar"><a id="calibre_link-4591" class="calibre"></a><div class="sidebar-title">MapReduce in Spark</div><p class="calibre2">Despite the suggestive <a class="calibre" id="calibre_link-2505"></a><a class="calibre" id="calibre_link-3458"></a>naming, Spark’s<a class="calibre" id="calibre_link-1781"></a> <code class="literal">map()</code> and
        <code class="literal">reduce()</code> operations do not directly
        correspond to the functions of the same name in Hadoop MapReduce. The
        general form of map and reduce in Hadoop MapReduce is (from <a class="ulink" href="#calibre_link-568" title="Chapter&nbsp;8.&nbsp;MapReduce Types and Formats">Chapter&nbsp;8</a>):</p><pre class="screen2">map: (K1, V1) → list(K2, V2)
reduce: (K2, list(V2)) → list(K3, V3)</pre><p class="calibre2">Notice that both functions can return multiple output pairs,
        indicated by the <code class="literal">list</code> notation.
        This is implemented by the <code class="literal">flatMap()</code>
        operation in Spark (and Scala in general), which is like
        <code class="literal">map()</code>, but removes a layer of nesting:</p><pre class="screen2"><code class="literal">scala&gt;</code> <strong class="userinput"><code class="calibre9">val l = List(1, 2, 3)</code></strong>
l: List[Int] = List(1, 2, 3)

<code class="literal">scala&gt;</code> <strong class="userinput"><code class="calibre9">l.map(a =&gt; List(a))</code></strong>
res0: List[List[Int]] = List(List(1), List(2), List(3))

<code class="literal">scala&gt;</code> <strong class="userinput"><code class="calibre9">l.flatMap(a =&gt; List(a))</code></strong>
res1: List[Int] = List(1, 2, 3)</pre><p class="calibre2">One naive way to try to emulate Hadoop MapReduce in Spark is with two
            <code class="literal">flatMap()</code> operations, separated by a
            <code class="literal">groupByKey()</code> and a <code class="literal">sortByKey()</code> to
          perform a MapReduce shuffle and sort:</p><pre class="screen2"><code class="k">val</code> <code class="n">input</code><code class="k">:</code> <code class="kt">RDD</code><code class="o">[(</code><code class="kt">K1</code>, <code class="kt">V1</code><code class="o">)]</code> <code class="k">=</code> <code class="o">...</code>
<code class="k">val</code> <code class="n">mapOutput</code><code class="k">:</code> <code class="kt">RDD</code><code class="o">[(</code><code class="kt">K2</code>, <code class="kt">V2</code><code class="o">)]</code> <code class="k">=</code> <code class="n">input</code><code class="o">.</code><code class="n">flatMap</code><code class="o">(</code><code class="n">mapFn</code><code class="o">)</code>
<code class="k">val</code> <code class="n">shuffled</code><code class="k">:</code> <code class="kt">RDD</code><code class="o">[(</code><code class="kt">K2</code>, <code class="kt">Iterable</code><code class="o">[</code><code class="kt">V2</code><code class="o">])]</code> <code class="k">=</code> <code class="n">mapOutput</code><code class="o">.</code><code class="n">groupByKey</code><code class="o">().</code><code class="n">sortByKey</code><code class="o">()</code>
<code class="k">val</code> <code class="n">output</code><code class="k">:</code> <code class="kt">RDD</code><code class="o">[(</code><code class="kt">K3</code>, <code class="kt">V3</code><code class="o">)]</code> <code class="k">=</code> <code class="n">shuffled</code><code class="o">.</code><code class="n">flatMap</code><code class="o">(</code><code class="n">reduceFn</code><code class="o">)</code></pre><p class="calibre2">Here the key type <code class="literal">K2</code> needs to
        inherit from Scala’s <code class="literal">Ordering</code> type to satisfy
        <code class="literal">sortByKey()</code>.</p><p class="calibre2">This example may be useful as a way to help understand the relationship between
          MapReduce and Spark, but it should not be applied blindly. For one thing, the semantics
          are slightly different from Hadoop’s MapReduce, since <code class="literal">sortByKey()</code>
          performs a total sort. This issue can be avoided by using
            <code class="literal">repartitionAndSortWithinPartitions()</code> to perform a partial sort.
          However, even this isn’t as efficient, since Spark uses two shuffles (one for the
            <code class="literal">groupByKey()</code> and one for the
          sort).</p><p class="calibre2">Rather than trying to reproduce MapReduce, it is better to use
        only the operations that you actually need. For example, if you don’t
        need keys to be sorted, you can omit the
        <code class="literal">sortByKey()</code> call (something that is not
        possible in regular Hadoop MapReduce).</p><p class="calibre2">Similarly, <code class="literal">groupByKey()</code> is too general in most cases. Usually
          you only need the shuffle to aggregate values, so you should use
            <code class="literal">reduceByKey()</code>, <code class="literal">foldByKey()</code>, or
            <code class="literal">aggregateByKey()</code> (covered in the next section), which are more
          efficient than <code class="literal">groupByKey()</code> since they can also run as combiners
          in the map task. Finally, <code class="literal">flatMap()</code> may not always be needed
          either, with <code class="literal">map()</code> being preferred if there is always one return
          value, and <code class="literal">filter()</code> if
          there is zero or one.</p></div><div class="book" title="Aggregation transformations"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4592">Aggregation transformations</h4></div></div></div><p class="calibre2">The three main transformations for aggregating RDDs of pairs by
        their keys are <code class="literal">reduceByKey()</code>,
        <code class="literal">foldByKey()</code>, and
        <code class="literal">aggregateByKey()</code>. They work in slightly
        different ways, but they all aggregate the values for a given key to
        produce a single value for each key. (The equivalent actions are
        <code class="literal">reduce()</code>, <code class="literal">fold()</code>,
        and <code class="literal">aggregate()</code>, which operate in an
        analogous way, resulting in a single value for the whole RDD.)</p><p class="calibre2">The simplest is <code class="literal">reduceByKey()</code>, which
        repeatedly applies a binary function to values in pairs until a single
        value is produced. For example:</p><pre class="screen1"><code class="k">val</code> <code class="n">pairs</code><code class="k">:</code> <code class="kt">RDD</code><code class="o">[(</code><code class="kt">String</code>, <code class="kt">Int</code><code class="o">)]</code> <code class="k">=</code>
    <code class="n">sc</code><code class="o">.</code><code class="n">parallelize</code><code class="o">(</code><code class="nc">Array</code><code class="o">((</code><code class="sb">"a"</code><code class="o">,</code> <code class="mi">3</code><code class="o">),</code> <code class="o">(</code><code class="sb">"a"</code><code class="o">,</code> <code class="mi">1</code><code class="o">),</code> <code class="o">(</code><code class="sb">"b"</code><code class="o">,</code> <code class="mi">7</code><code class="o">),</code> <code class="o">(</code><code class="sb">"a"</code><code class="o">,</code> <code class="mi">5</code><code class="o">)))</code>
<code class="k">val</code> <code class="n">sums</code><code class="k">:</code> <code class="kt">RDD</code><code class="o">[(</code><code class="kt">String</code>, <code class="kt">Int</code><code class="o">)]</code> <code class="k">=</code> <code class="n">pairs</code><code class="o">.</code><code class="n">reduceByKey</code><code class="o">(</code><code class="k">_</code><code class="o">+</code><code class="k">_</code><code class="o">)</code>
<code class="n">assert</code><code class="o">(</code><code class="n">sums</code><code class="o">.</code><code class="n">collect</code><code class="o">().</code><code class="n">toSet</code> <code class="o">===</code> <code class="nc">Set</code><code class="o">((</code><code class="sb">"a"</code><code class="o">,</code> <code class="mi">9</code><code class="o">),</code> <code class="o">(</code><code class="sb">"b"</code><code class="o">,</code> <code class="mi">7</code><code class="o">)))</code></pre><p class="calibre2">The values for key <code class="literal">a</code> are aggregated using the
          addition function (<code class="literal">_+_</code>) as <span class="calibre"><em class="calibre10">(3 + 1) + 5 =
            9</em></span>, while there is only one value for key <code class="literal">b</code>, so no aggregation is needed. Since in general the operations are distributed
          and performed in different tasks for different partitions of the RDD, the function should
          be commutative and associative. In other words, the order and grouping of the operations
          should not matter; in this case, the aggregation could be <span class="calibre"><em class="calibre10">5 + (3 +
          1)</em></span>, or <span class="calibre"><em class="calibre10">3 + (1 + 5)</em></span>, which both return the same
          result.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">The triple equals operator (<code class="literal">===</code>) used in the <code class="literal">assert</code> statement is from ScalaTest, and
          provides more informative failure messages than using the regular
          <code class="literal">==</code> operator.</p></div><p class="calibre2">Here’s how we would perform the same operation using
        <code class="literal">foldByKey()</code>:</p><pre class="screen1"><code class="k">val</code> <code class="n">sums</code><code class="k">:</code> <code class="kt">RDD</code><code class="o">[(</code><code class="kt">String</code>, <code class="kt">Int</code><code class="o">)]</code> <code class="k">=</code> <code class="n">pairs</code><code class="o">.</code><code class="n">foldByKey</code><code class="o">(</code><code class="mi">0</code><code class="o">)(</code><code class="k">_</code><code class="o">+</code><code class="k">_</code><code class="o">)</code>
<code class="n">assert</code><code class="o">(</code><code class="n">sums</code><code class="o">.</code><code class="n">collect</code><code class="o">().</code><code class="n">toSet</code> <code class="o">===</code> <code class="nc">Set</code><code class="o">((</code><code class="sb">"a"</code><code class="o">,</code> <code class="mi">9</code><code class="o">),</code> <code class="o">(</code><code class="sb">"b"</code><code class="o">,</code> <code class="mi">7</code><code class="o">)))</code></pre><p class="calibre2">Notice that this time we had to supply a <span class="calibre"><em class="calibre10">zero
        value</em></span>, which is just 0 when adding integers, but would be
        something different for other types and operations. This time, values
        for <code class="literal">a</code> are aggregated as
        <span class="calibre"><em class="calibre10">((0 + 3) + 1) + 5) = 9</em></span> (or possibly some other
        order, although adding to 0 is always the first operation). For
        <code class="literal">b</code> it is <span class="calibre"><em class="calibre10">0 + 7 =
        7</em></span>.</p><p class="calibre2">Using <code class="literal">foldByKey()</code> is no more or less powerful than using
            <code class="literal">reduceByKey()</code>. In particular, neither can change the type of
          the value that is the result of the aggregation. For that we need
            <code class="literal">aggregateByKey()</code>. For example, we can aggregate the integer
          values into a set:</p><pre class="screen1"><code class="k">val</code> <code class="n">sets</code><code class="k">:</code> <code class="kt">RDD</code><code class="o">[(</code><code class="kt">String</code>, <code class="kt">HashSet</code><code class="o">[</code><code class="kt">Int</code><code class="o">])]</code> <code class="k">=</code>
    <code class="n">pairs</code><code class="o">.</code><code class="n">aggregateByKey</code><code class="o">(</code><code class="k">new</code> <code class="nc">HashSet</code><code class="o">[</code><code class="kt">Int</code><code class="o">])(</code><code class="k">_</code><code class="o">+=</code><code class="k">_</code><code class="o">,</code> <code class="k">_</code><code class="o">++=</code><code class="k">_</code><code class="o">)</code>
<code class="n">assert</code><code class="o">(</code><code class="n">sets</code><code class="o">.</code><code class="n">collect</code><code class="o">().</code><code class="n">toSet</code> <code class="o">===</code> <code class="nc">Set</code><code class="o">((</code><code class="sb">"a"</code><code class="o">,</code> <code class="nc">Set</code><code class="o">(</code><code class="mi">1</code><code class="o">,</code> <code class="mi">3</code><code class="o">,</code> <code class="mi">5</code><code class="o">)),</code> <code class="o">(</code><code class="sb">"b"</code><code class="o">,</code> <code class="nc">Set</code><code class="o">(</code><code class="mi">7</code><code class="o">))))</code></pre><p class="calibre2">For set addition, the zero value is the empty set, so we create
        a new mutable set with <code class="literal">new
        HashSet[Int]</code>. We have to supply two functions to
        <code class="literal">aggregateByKey()</code>. The first controls how an
        <code class="literal">Int</code> is combined with a
        <code class="literal">HashSet[Int]</code>, and in this case we use the
        addition and assignment function <code class="literal">_+=_</code> to add the integer to the set (<code class="literal">_+_</code> would return a new set and leave the
        first set unchanged).</p><p class="calibre2">The second function controls how two <code class="literal">HashSet[Int]</code> values are
          combined (this happens after the combiner runs in the map task, while the two partitions
          are being aggregated in the reduce task), and here we use <code class="literal">_++=_</code> to add all the elements of the second set to the first.</p><p class="calibre2">For key <code class="literal">a</code>, the sequence of operations might
          be:</p><table class="simplelist"><tbody><tr class="calibre11"><td class="calibre12"><span class="calibre"><em class="calibre10">((∅ + 3) + 1) + 5) = (1, 3, 5)</em></span></td></tr></tbody></table><p class="calibre2">or:</p><table class="simplelist"><tbody><tr class="calibre11"><td class="calibre12"><span class="calibre"><em class="calibre10">(∅ + 3) + 1) ++ (∅ + 5) = (1, 3) ++ (5) = (1, 3,
          5)</em></span></td></tr></tbody></table><p class="calibre2">if Spark uses a combiner.</p><p class="calibre2">A transformed RDD can be persisted in memory so that subsequent
        operations on it are more efficient. We look at that <a class="calibre" id="calibre_link-3152"></a><a class="calibre" id="calibre_link-3716"></a>next.</p></div></div><div class="book" title="Persistence"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-829">Persistence</h3></div></div></div><p class="calibre2">Going back to the introductory <a class="calibre" id="calibre_link-3153"></a><a class="calibre" id="calibre_link-2978"></a>example in <a class="ulink" href="#calibre_link-837" title="An Example">An Example</a>, we can cache
      the intermediate dataset of year-temperature pairs in memory with the
      following:</p><pre class="screen1"><code class="literal">scala&gt;</code> <strong class="userinput"><code class="calibre9">tuples.cache()</code></strong>
res1: tuples.type = MappedRDD[4] at map at &lt;console&gt;:18</pre><p class="calibre2">Calling <code class="literal">cache()</code> does not cache the RDD in
      memory straightaway. Instead, it marks the RDD with a flag indicating it
      should be cached when the Spark job is run. So let’s first force a job
      run:</p><pre class="screen1"><code class="literal">scala&gt;</code> <strong class="userinput"><code class="calibre9">tuples.reduceByKey((a, b) =&gt; Math.max(a, b)).foreach(println(_))</code></strong>
INFO BlockManagerInfo: Added rdd_4_0 in memory on 192.168.1.90:64640
INFO BlockManagerInfo: Added rdd_4_1 in memory on 192.168.1.90:64640
(1950,22)
(1949,111)</pre><p class="calibre2">The log lines for <code class="literal">BlockManagerInfo</code> show
      that the RDD’s partitions have been kept in memory as a part of the job
      run. The log shows that the RDD’s number is 4 (this was shown in the
      console after calling the <code class="literal">cache()</code> method), and
      it has two partitions labeled 0 and 1. If we run another job on the
      cached dataset, we’ll see that the RDD is loaded from memory. This time
      we’ll compute minimum temperatures:</p><pre class="screen1"><code class="literal">scala&gt;</code> <strong class="userinput"><code class="calibre9">tuples.reduceByKey((a, b) =&gt; Math.min(a, b)).foreach(println(_))</code></strong>
INFO BlockManager: Found block rdd_4_0 locally
INFO BlockManager: Found block rdd_4_1 locally
(1949,78)
(1950,-11)</pre><p class="calibre2">This is a simple example on a tiny dataset, but for larger jobs
      the time savings can be impressive. Compare this to MapReduce, where to
      perform another calculation the input dataset has to be loaded from disk
      again. Even if an intermediate dataset can be used as input (such as a
      cleaned-up dataset with invalid rows and unnecessary fields removed),
      there is no getting away from the fact that it must be loaded from disk,
      which is slow. Spark will cache datasets in a cross-cluster in-memory
      cache, which means that any computation performed on those datasets will
      be very fast.</p><p class="calibre2">This turns out to be tremendously useful for interactive
      exploration of data. It’s also a natural fit for certain styles of
      algorithm, such as iterative algorithms where a result computed in one
      iteration can be cached in memory and used as input for the next
      iteration. These algorithms can be expressed in MapReduce, but each
      iteration runs as a single MapReduce job, so the result from each
      iteration must be written to disk and then read back in the next
      iteration.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Cached RDDs can be retrieved only by jobs in the same application. To share datasets
          between applications, they must be written to external storage using one of the
            <code class="literal">saveAs*()</code> methods (<code class="literal">saveAsTextFile()</code>,
            <code class="literal">saveAsHadoopFile()</code>, etc.) in the first application, then loaded
          using the corresponding method in <code class="literal">SparkContext</code>
            (<code class="literal">textFile()</code>, <code class="literal">hadoopFile()</code>, etc.) in
          the second application. Likewise, when the application terminates, all its cached RDDs are
          destroyed and cannot be accessed again unless they have been explicitly saved.</p></div><div class="book" title="Persistence levels"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-838">Persistence levels</h4></div></div></div><p class="calibre2">Calling <code class="literal">cache()</code> will persist each
        partition of the RDD in the executor’s memory. If an executor does not
        have enough memory to store the RDD partition, the computation will
        not fail, but instead the partition will be recomputed as needed. For
        complex programs with lots of transformations, this may be expensive,
        so Spark offers different types of persistence behavior that may be
        selected by calling <code class="literal">persist()</code> with an
        argument to specify the <code class="literal">StorageLevel</code>.</p><p class="calibre2">By default, the level is <code class="literal">MEMORY_ONLY</code>, which uses
          the regular in-memory representation of objects. A more compact representation can be used
          by serializing the elements in a partition as a byte array. This level is <code class="literal">MEMORY_ONLY_SER</code>; it incurs CPU overhead compared to <code class="literal">MEMORY_ONLY</code>, but is worth it if the resulting serialized RDD
          partition fits in memory when the regular in-memory representation doesn’t. <code class="literal">MEMORY_ONLY_SER</code> also reduces garbage collection pressure,
          since each RDD is stored as one byte array rather than lots of objects.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">You can see if an RDD partition doesn’t fit in memory by
          inspecting the driver logfile for messages from the
          <code class="literal">BlockManager</code>. Also, every driver’s
          <code class="literal">SparkContext</code> runs an HTTP server (on port
          4040) that provides useful information about its environment and the
          jobs it is running, including information about cached RDD
          partitions.</p></div><p class="calibre2">By default, regular Java serialization is used to serialize RDD partitions, but Kryo
          serialization (covered in the next section) is normally a better choice, both in terms of
          size and speed. Further space savings can be achieved (again at the expense of CPU) by
          compressing the serialized partitions by setting the <code class="literal">spark.rdd.compress</code> property to <code class="literal">true</code>, and
          optionally setting <code class="literal">spark.io.compression.codec</code>.</p><p class="calibre2">If recomputing a dataset is expensive, then either <code class="literal">MEMORY_AND_DISK</code> (spill to disk if the
        dataset doesn’t fit in memory) or <code class="literal">MEMORY_AND_DISK_SER</code> (spill to disk if the
        serialized dataset doesn’t fit in memory) is appropriate.</p><p class="calibre2">There are also some more advanced and experimental persistence
        levels for replicating partitions on more than one node in the
        cluster, or using off-heap memory—see the Spark documentation for
        <a class="calibre" id="calibre_link-2979"></a><a class="calibre" id="calibre_link-3154"></a>details.</p></div></div><div class="book" title="Serialization"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4593">Serialization</h3></div></div></div><p class="calibre2">There are two <a class="calibre" id="calibre_link-3155"></a><a class="calibre" id="calibre_link-3364"></a>aspects of serialization to consider in Spark:
      serialization of data and serialization of functions (or
      closures).</p><div class="book" title="Data"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4594">Data</h4></div></div></div><p class="calibre2">Let’s look at data serialization first. By default, Spark will
        use Java serialization to send data over the network from one executor
        to another, or when caching (persisting) data in serialized form as
        described in <a class="ulink" href="#calibre_link-838" title="Persistence levels">Persistence levels</a>. Java serialization
        is well understood by programmers (you make sure the class you are
        using implements <code class="literal">java.io.Serializable</code> or
        <code class="literal">java.io.Externalizable</code>), but it is not
        particularly efficient from a performance or size perspective.</p><p class="calibre2">A better choice for most Spark programs is <a class="ulink" href="https://github.com/EsotericSoftware/kryo" target="_top">Kryo serialization</a>. Kryo is a
          more efficient general-purpose serialization library for Java. In order to use Kryo
          serialization, set the <code class="literal">spark.serializer</code> as follows on
          the <code class="literal">SparkConf</code> in your driver program:</p><pre class="screen1"><code class="n">conf</code><code class="o">.</code><code class="n">set</code><code class="o">(</code><code class="sb">"spark.serializer"</code><code class="o">,</code> <code class="sb">"org.apache.spark.serializer.KryoSerializer"</code><code class="o">)</code></pre><p class="calibre2">Kryo does not require that a class implement a particular interface (like
            <code class="literal">java.io.Serializable</code>) to be serialized, so plain old Java objects
          can be used in RDDs without any further work beyond enabling Kryo serialization. Having
          said that, it is much more efficient to register classes with Kryo before using them. This
          is because Kryo writes a reference to the class of the object being serialized (one
          reference is written for every object written), which is just an integer identifier if the
          class has been registered but is the full classname otherwise. This guidance only applies
          to your own classes; Spark registers Scala classes and many other framework classes (like
          Avro Generic or Thrift classes) on your
          behalf.</p><p class="calibre2">Registering classes with Kryo is straightforward. Create a
        subclass of <code class="literal">KryoRegistrator</code>, and override the
        <code class="literal">registerClasses()</code> method:</p><pre class="screen1"><code class="k">class</code> <code class="nc">CustomKryoRegistrator</code> <code class="k">extends</code> <code class="nc">KryoRegistrator</code> <code class="o">{</code>
  <code class="k">override</code> <code class="k">def</code> <code class="n">registerClasses</code><code class="o">(</code><code class="n">kryo</code><code class="k">:</code> <code class="kt">Kryo</code><code class="o">)</code> <code class="o">{</code>
    <code class="n">kryo</code><code class="o">.</code><code class="n">register</code><code class="o">(</code><code class="n">classOf</code><code class="o">[</code><code class="kt">WeatherRecord</code><code class="o">])</code>
  <code class="o">}</code>
<code class="o">}</code></pre><p class="calibre2">Finally, in your driver program, set <a class="calibre" id="calibre_link-3470"></a>the <code class="literal">spark.kryo.registrator</code> property to the fully
        qualified classname of your <code class="literal">KryoRegistrator</code>
        implementation:</p><pre class="screen1"><code class="n">conf</code><code class="o">.</code><code class="n">set</code><code class="o">(</code><code class="sb">"spark.kryo.registrator"</code><code class="o">,</code> <code class="sb">"CustomKryoRegistrator"</code><code class="o">)</code></pre></div><div class="book" title="Functions"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4595">Functions</h4></div></div></div><p class="calibre2">Generally, serialization of <a class="calibre" id="calibre_link-1782"></a>functions will “just work”: in Scala, functions are serializable using the
          standard Java serialization mechanism, which is what Spark uses to send functions to
          remote executor nodes. Spark will serialize functions even when running in local mode, so
          if you inadvertently introduce a function that is not serializable (such as one converted
          from a method on a nonserializable class), you will catch it early on in the development
            <a class="calibre" id="calibre_link-3460"></a>process.</p></div></div></div><div class="book" title="Shared Variables"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4596">Shared Variables</h2></div></div></div><p class="calibre2">Spark programs <a class="calibre" id="calibre_link-3462"></a>often need to access data that is not part of an RDD. For example, this program
      uses a lookup table in a <code class="literal">map()</code> operation:</p><pre class="screen1"><code class="k">val</code> <code class="n">lookup</code> <code class="k">=</code> <code class="nc">Map</code><code class="o">(</code><code class="mi">1</code> <code class="o">-&gt;</code> <code class="sb">"a"</code><code class="o">,</code> <code class="mi">2</code> <code class="o">-&gt;</code> <code class="sb">"e"</code><code class="o">,</code> <code class="mi">3</code> <code class="o">-&gt;</code> <code class="sb">"i"</code><code class="o">,</code> <code class="mi">4</code> <code class="o">-&gt;</code> <code class="sb">"o"</code><code class="o">,</code> <code class="mi">5</code> <code class="o">-&gt;</code> <code class="sb">"u"</code><code class="o">)</code>
<code class="k">val</code> <code class="n">result</code> <code class="k">=</code> <code class="n">sc</code><code class="o">.</code><code class="n">parallelize</code><code class="o">(</code><code class="nc">Array</code><code class="o">(</code><code class="mi">2</code><code class="o">,</code> <code class="mi">1</code><code class="o">,</code> <code class="mi">3</code><code class="o">)).</code><code class="n">map</code><code class="o">(</code><code class="n">lookup</code><code class="o">(</code><code class="k">_</code><code class="o">))</code>
<code class="n">assert</code><code class="o">(</code><code class="n">result</code><code class="o">.</code><code class="n">collect</code><code class="o">().</code><code class="n">toSet</code> <code class="o">===</code> <code class="nc">Set</code><code class="o">(</code><code class="sb">"a"</code><code class="o">,</code> <code class="sb">"e"</code><code class="o">,</code> <code class="sb">"i"</code><code class="o">))</code></pre><p class="calibre2">While it works correctly (the variable <code class="literal">lookup</code> is serialized as a part of the closure
    passed to <code class="literal">map()</code>), there is a more efficient way
    to achieve the same thing using <em class="calibre10">broadcast
      variables</em>.</p><div class="book" title="Broadcast Variables"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4597">Broadcast Variables</h3></div></div></div><p class="calibre2">A broadcast variable<a class="calibre" id="calibre_link-1022"></a> is serialized and sent to each executor, where it is cached so that later tasks
        can access it if needed. This is unlike a regular variable that is serialized as part of the
        closure, which is transmitted over the network once per task. Broadcast variables play a
        similar role to the distributed cache in MapReduce (see <a class="ulink" href="#calibre_link-54" title="Distributed Cache">Distributed Cache</a>),
        although the implementation in Spark stores the data in memory, only spilling to disk when
        memory is exhausted.</p><p class="calibre2">A broadcast variable is created by passing the variable to be
      broadcast to the <code class="literal">broadcast()</code> method on
      <code class="literal">SparkContext</code>. It returns a
      <code class="literal">Broadcast[T]</code> wrapper around the variable of type
      <code class="literal">T</code>:</p><pre class="screen1"><code class="k">val</code> <code class="n">lookup</code><code class="k">:</code> <span class="calibre24"><strong class="calibre9"><code class="nc1">Broadcast</code><code class="o1">[</code><code class="kt1">Map</code><code class="o1">[</code><code class="kt1">Int</code>, <code class="kt1">String</code><code class="o1">]]</code></strong></span> <code class="k">=</code>
    <span class="calibre24"><strong class="calibre9"><code class="n1">sc</code><code class="o1">.</code><code class="n1">broadcast</code><code class="o1">(</code></strong></span><code class="nc">Map</code><code class="o">(</code><code class="mi">1</code> <code class="o">-&gt;</code> <code class="sb">"a"</code><code class="o">,</code> <code class="mi">2</code> <code class="o">-&gt;</code> <code class="sb">"e"</code><code class="o">,</code> <code class="mi">3</code> <code class="o">-&gt;</code> <code class="sb">"i"</code><code class="o">,</code> <code class="mi">4</code> <code class="o">-&gt;</code> <code class="sb">"o"</code><code class="o">,</code> <code class="mi">5</code> <code class="o">-&gt;</code> <code class="sb">"u"</code><code class="o">)</code><span class="calibre24"><strong class="calibre9"><code class="o1">)</code></strong></span>
<code class="k">val</code> <code class="n">result</code> <code class="k">=</code> <code class="n">sc</code><code class="o">.</code><code class="n">parallelize</code><code class="o">(</code><code class="nc">Array</code><code class="o">(</code><code class="mi">2</code><code class="o">,</code> <code class="mi">1</code><code class="o">,</code> <code class="mi">3</code><code class="o">)).</code><code class="n">map</code><code class="o">(</code><code class="n">lookup</code><span class="calibre24"><strong class="calibre9"><code class="o1">.</code><code class="n1">value</code></strong></span><code class="o">(</code><code class="k">_</code><code class="o">))</code>
<code class="n">assert</code><code class="o">(</code><code class="n">result</code><code class="o">.</code><code class="n">collect</code><code class="o">().</code><code class="n">toSet</code> <code class="o">===</code> <code class="nc">Set</code><code class="o">(</code><code class="sb">"a"</code><code class="o">,</code> <code class="sb">"e"</code><code class="o">,</code> <code class="sb">"i"</code><code class="o">))</code></pre><p class="calibre2">Notice that the variable is accessed in the RDD
      <code class="literal">map()</code> operation by calling
      <code class="literal">value</code> on the broadcast variable.</p><p class="calibre2">As the name suggests, broadcast variables are sent one way, from driver to task—there is
        no way to update a broadcast variable and have the update propagate back to the driver. For
        that, we need an <span class="calibre">accumulator</span>.</p></div><div class="book" title="Accumulators"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4598">Accumulators</h3></div></div></div><p class="calibre2">An accumulator is a <a class="calibre" id="calibre_link-861"></a>shared variable that tasks can only add to, like counters
      in MapReduce (see <a class="ulink" href="#calibre_link-12" title="Counters">Counters</a>). After a job has
      completed, the accumulator’s final value can be retrieved from the
      driver program. Here is an example that counts the number of elements in
      an RDD of integers using an accumulator, while at the same time summing
      the values in the RDD using a <code class="literal">reduce()</code>
      action:</p><pre class="screen1"><code class="k">val</code> <code class="n">count</code><code class="k">:</code> <code class="kt">Accumulator</code><code class="o">[</code><code class="kt">Int</code><code class="o">]</code> <code class="k">=</code> <code class="n">sc</code><code class="o">.</code><code class="n">accumulator</code><code class="o">(</code><code class="mi">0</code><code class="o">)</code>
<code class="k">val</code> <code class="n">result</code> <code class="k">=</code> <code class="n">sc</code><code class="o">.</code><code class="n">parallelize</code><code class="o">(</code><code class="nc">Array</code><code class="o">(</code><code class="mi">1</code><code class="o">,</code> <code class="mi">2</code><code class="o">,</code> <code class="mi">3</code><code class="o">))</code>
  <code class="o">.</code><code class="n">map</code><code class="o">(</code><code class="n">i</code> <code class="k">=&gt;</code> <code class="o">{</code> <code class="n">count</code> <code class="o">+=</code> <code class="mi">1</code><code class="o">;</code> <code class="n">i</code> <code class="o">})</code>
  <code class="o">.</code><code class="n">reduce</code><code class="o">((</code><code class="n">x</code><code class="o">,</code> <code class="n">y</code><code class="o">)</code> <code class="k">=&gt;</code> <code class="n">x</code> <code class="o">+</code> <code class="n">y</code><code class="o">)</code>
<code class="n">assert</code><code class="o">(</code><code class="n">count</code><code class="o">.</code><code class="n">value</code> <code class="o">===</code> <code class="mi">3</code><code class="o">)</code>
<code class="n">assert</code><code class="o">(</code><code class="n">result</code> <code class="o">===</code> <code class="mi">6</code><code class="o">)</code></pre><p class="calibre2">An accumulator variable, <code class="literal">count</code>,
      is created in the first line using the
      <code class="literal">accumulator()</code> method on
      <code class="literal">SparkContext</code>. The <code class="literal">map()</code>
      operation is an identity function with a side effect that increments
      <code class="literal">count</code>. When the result of the Spark
      job has been computed, the value of the accumulator is accessed by
      calling <code class="literal">value</code> on it.</p><p class="calibre2">In this example, we used an <code class="literal">Int</code> for the accumulator, but any
        numeric value type can be used. Spark also provides a way to use accumulators whose result
        type is different to the type being added (see the <code class="literal">accumulable()</code>
        method on <code class="literal">SparkContext</code>), and a way to accumulate values in mutable
          <a class="calibre" id="calibre_link-3463"></a>collections (via <code class="literal">accumulableCollection()</code>).</p></div></div><div class="book" title="Anatomy of a Spark Job Run"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4599">Anatomy of a Spark Job Run</h2></div></div></div><p class="calibre2">Let’s walk through what <a class="calibre" id="calibre_link-2233"></a><a class="calibre" id="calibre_link-3450"></a>happens when we run a Spark job. At the highest level, there are two independent
      entities: the <em class="calibre10">driver</em>, which hosts the application
        (<code class="literal">SparkContext</code>) and schedules tasks for a job; and the
        <em class="calibre10">executors</em>, which are exclusive to the application, run for the
      duration of the application, and execute the application’s tasks. Usually the driver runs as a
      client that is not managed by the cluster manager and the executors run on machines in the
      cluster, but this isn’t always the case (as we’ll see in <a class="ulink" href="#calibre_link-831" title="Executors and Cluster Managers">Executors and Cluster Managers</a>). For the remainder of this section, we assume that
      the application’s executors are already running.</p><div class="book" title="Job Submission"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4600">Job Submission</h3></div></div></div><p class="calibre2"><a class="ulink" href="#calibre_link-839" title="Figure&nbsp;19-1.&nbsp;How Spark runs a job">Figure&nbsp;19-1</a> illustrates how Spark runs a job. A Spark
        job is <a class="calibre" id="calibre_link-2262"></a>submitted automatically when an action (such as
        <code class="literal">count()</code>) is performed on an RDD. Internally, this causes
          <code class="literal">runJob()</code> to be called on the <code class="literal">SparkContext</code>
        (step 1 in <a class="ulink" href="#calibre_link-839" title="Figure&nbsp;19-1.&nbsp;How Spark runs a job">Figure&nbsp;19-1</a>), which passes the call on to the
        scheduler that runs as a part of the driver (step 2). The scheduler is made up of two parts:
        a DAG scheduler that breaks down the job into a DAG of stages, and a task scheduler that is
        responsible for submitting the tasks from each stage to the cluster.</p><div class="book"><div class="figure"><a id="calibre_link-839" class="calibre"></a><div class="book"><div class="book"><img alt="How Spark runs a job" src="images/000014.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;19-1.&nbsp;How Spark runs a job</div></div></div><p class="calibre2">Next, let’s take a look at how the DAG scheduler constructs a
      DAG.</p></div><div class="book" title="DAG Construction"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4601">DAG Construction</h3></div></div></div><p class="calibre2">To understand how <a class="calibre" id="calibre_link-2236"></a><a class="calibre" id="calibre_link-1334"></a><a class="calibre" id="calibre_link-1498"></a>a job is broken up into stages, we need to look at the type of tasks that can
        run in a stage. There are two types: <em class="calibre10">shuffle map tasks</em> and
          <em class="calibre10">result tasks</em>. The name of the task type indicates what Spark does
        with the task’s output:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Shuffle map tasks</span></dt><dd class="calibre8"><p class="calibre2">As the name <a class="calibre" id="calibre_link-2400"></a>suggests, shuffle map tasks are like the map-side part of the shuffle in
              MapReduce. Each shuffle map task runs a computation on one RDD partition and, based on
              a partitioning function, writes its output to a new set of partitions, which are then
              fetched in a later stage (which could be composed of either shuffle map tasks or
              result tasks). Shuffle map tasks run in all stages except the final stage.</p></dd><dt class="calibre7"><span class="term">Result tasks</span></dt><dd class="calibre8"><p class="calibre2">Result tasks run in <a class="calibre" id="calibre_link-3188"></a>the final stage that returns the result to the
            user’s program (such as the result of a
            <code class="literal">count()</code>). Each result task runs a
            computation on its RDD partition, then sends the result back to
            the driver, and the driver assembles the results from each
            partition into a final result (which may be
            <code class="literal">Unit</code>, in the case of actions like
            <code class="literal">saveAsTextFile()</code>).</p></dd></dl></div><p class="calibre2">The simplest Spark job is one that does not need a shuffle and therefore has just a
        single stage composed of result tasks. This is like a map-only job in MapReduce.</p><p class="calibre2">More complex jobs involve grouping operations and require one or
      more shuffle stages. For example, consider the following job for
      calculating a histogram of word counts for text files stored in <code class="literal">inputPath</code> (one word per line):</p><pre class="screen1"><code class="k">val</code> <code class="n">hist</code><code class="k">:</code> <code class="kt">Map</code><code class="o">[</code><code class="kt">Int</code>, <code class="kt">Long</code><code class="o">]</code> <code class="k">=</code> <code class="n">sc</code><code class="o">.</code><code class="n">textFile</code><code class="o">(</code><code class="n">inputPath</code><code class="o">)</code>
  <code class="o">.</code><code class="n">map</code><code class="o">(</code><code class="n">word</code> <code class="k">=&gt;</code> <code class="o">(</code><code class="n">word</code><code class="o">.</code><code class="n">toLowerCase</code><code class="o">(),</code> <code class="mi">1</code><code class="o">))</code>
  <code class="o">.</code><code class="n">reduceByKey</code><code class="o">((</code><code class="n">a</code><code class="o">,</code> <code class="n">b</code><code class="o">)</code> <code class="k">=&gt;</code> <code class="n">a</code> <code class="o">+</code> <code class="n">b</code><code class="o">)</code>
  <code class="o">.</code><code class="n">map</code><code class="o">(</code><code class="k">_</code><code class="o">.</code><code class="n">swap</code><code class="o">)</code>
  <code class="o">.</code><code class="n">countByKey</code><code class="o">()</code></pre><p class="calibre2">The first two transformations, <code class="literal">map()</code> and
      <code class="literal">reduceByKey()</code>, perform a word count. The third
      transformation is a <code class="literal">map()</code> that swaps the key
      and value in each pair, to give <span class="calibre">(count,
      word)</span> pairs, and the final operation is the
      <code class="literal">countByKey()</code> action, which returns the number
      of words with each count (i.e., a frequency distribution of word
      counts).</p><p class="calibre2">Spark’s DAG scheduler turns this job into two stages since the
      <code class="literal">reduceByKey()</code> operation forces a shuffle
      stage.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-840" id="calibre_link-852">131</a>]</sup> The resulting DAG is illustrated in <a class="ulink" href="#calibre_link-841" title="Figure&nbsp;19-2.&nbsp;The stages and RDDs in a Spark job for calculating a histogram of word counts">Figure&nbsp;19-2</a>.</p><p class="calibre2">The RDDs within each stage are also, in general, arranged in a
      DAG. The diagram shows the type of the RDD and the operation that
      created it. <code class="literal">RDD[String]</code> was created by
      <code class="literal">textFile()</code>, for instance. To simplify the
      diagram, some intermediate RDDs generated internally by Spark have been
      omitted. For example, the RDD returned by
      <code class="literal">textFile()</code> is actually a
      <code class="literal">MappedRDD[String]</code> whose parent is a
      <code class="literal">HadoopRDD[LongWritable, Text]</code>.</p><p class="calibre2">Notice that the <code class="literal">reduceByKey()</code>
      transformation spans two stages; this is because it is implemented using
      a shuffle, and the reduce function runs as a combiner on the map side
      (stage 1) and as a reducer on the reduce side (stage 2)—just like in
      MapReduce. Also like MapReduce, Spark’s shuffle implementation writes
      its output to partitioned files on local disk (even for in-memory RDDs),
      and the files are fetched by the RDD in the next stage.<sup class="calibre6">[<a class="firstname" href="#calibre_link-842" id="calibre_link-853">132</a>]</sup></p><div class="book"><div class="figure"><a id="calibre_link-841" class="calibre"></a><div class="book"><div class="book"><img alt="The stages and RDDs in a Spark job for calculating a histogram of word counts" src="images/000022.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;19-2.&nbsp;The stages and RDDs in a Spark job for calculating a histogram
          of word counts</div></div></div><p class="calibre2">If an RDD has been persisted from a previous job in the same
      application (<code class="literal">SparkContext</code>), then the DAG
      scheduler will save work and not create stages for recomputing it (or
      the RDDs it was derived from).</p><p class="calibre2">The DAG scheduler is responsible for splitting a stage into tasks
      for submission to the task scheduler. In this example, in the first
      stage one shuffle map task is run for each partition of the input file.
      The level of parallelism for a <code class="literal">reduceByKey()</code>
      operation can be set explicitly by passing it as the second parameter.
      If not set, it will be determined from the parent RDD, which in this
      case is the number of partitions in the input data.</p><p class="calibre2">Each task is given a placement preference by the DAG scheduler to
      allow the task scheduler to take advantage of data locality. A task that
      processes a partition of an input RDD stored on HDFS, for example, will
      have a placement preference for the datanode hosting the partition’s
      block (known as <em class="calibre10">node local</em>), while a task that
      processes a partition of an RDD that is cached in memory will prefer the
      executor storing the RDD partition (<em class="calibre10">process
      local</em>).</p><p class="calibre2">Going back to <a class="ulink" href="#calibre_link-839" title="Figure&nbsp;19-1.&nbsp;How Spark runs a job">Figure&nbsp;19-1</a>, once
      the DAG scheduler has constructed the complete DAG of stages, it submits
      each stage’s set of tasks to the task scheduler (step 3). Child stages
      are only submitted once their parents have completed <a class="calibre" id="calibre_link-1335"></a><a class="calibre" id="calibre_link-1499"></a><a class="calibre" id="calibre_link-2237"></a>successfully.</p></div><div class="book" title="Task Scheduling"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-846">Task Scheduling</h3></div></div></div><p class="calibre2">When the <a class="calibre" id="calibre_link-3281"></a><a class="calibre" id="calibre_link-3637"></a><a class="calibre" id="calibre_link-2259"></a>task scheduler is sent a set of tasks, it uses its list of executors that are
        running for the application and constructs a mapping of tasks to executors that takes
        placement preferences into account. Next, the task scheduler assigns tasks to executors that
        have free cores (this may not be the complete set if another job in the same application is
        running), and it continues to assign more tasks as executors finish running tasks, until the
        task set is complete. Each task is allocated one core by default, although this can be
        changed by setting <code class="literal">spark.task.cpus</code>.</p><p class="calibre2">Note that for a given executor the scheduler will first assign process-local tasks, then
        node-local tasks, then rack-local tasks, before assigning an arbitrary (nonlocal) task, or a
        speculative task if there are no other candidates.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-843" id="calibre_link-854">133</a>]</sup></p><p class="calibre2">Assigned tasks are launched through a scheduler backend (step 4 in
      <a class="ulink" href="#calibre_link-839" title="Figure&nbsp;19-1.&nbsp;How Spark runs a job">Figure&nbsp;19-1</a>), which sends a remote
      launch task message (step 5) to the executor backend to tell the
      executor to run the task (step 6).</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Rather than using Hadoop RPC for remote calls, Spark uses <a class="ulink" href="http://akka.io/" target="_top">Akka</a>, an actor-based platform for
        building highly scalable, event-driven distributed
        applications.</p></div><p class="calibre2">Executors also send status update messages to the driver when a task has finished or if
        a task fails. In the latter case, the task scheduler will resubmit the task on another
        executor. It will also launch speculative tasks for tasks that are running slowly, if this
        is enabled (it is not by default).</p></div><div class="book" title="Task Execution"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4602">Task Execution</h3></div></div></div><p class="calibre2">An executor runs <a class="calibre" id="calibre_link-3632"></a><a class="calibre" id="calibre_link-2263"></a>a task as follows (step 7). First, it makes sure that the
      JAR and file dependencies for the task are up to date. The executor
      keeps a local cache of all the dependencies that previous tasks have
      used, so that it only downloads them when they have changed. Second, it
      deserializes the task code (which includes the user’s functions) from
      the serialized bytes that were sent as a part of the launch task
      message. Third, the task code is executed. Note that tasks are run in
      the same JVM as the executor, so there is no process overhead for task
      launch.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-844" id="calibre_link-855">134</a>]</sup></p><p class="calibre2">Tasks can return a result to the driver. The result is serialized
      and sent to the executor backend, and then back to the driver as a
      status update message. A shuffle map task returns information that
      allows the next stage to retrieve the output partitions, while a result
      task returns the value of the result for the partition it ran on, which
      the driver assembles into a final result to return to the user’s
      <a class="calibre" id="calibre_link-2234"></a><a class="calibre" id="calibre_link-3451"></a>program.</p></div></div><div class="book" title="Executors and Cluster Managers"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-831">Executors and Cluster Managers</h2></div></div></div><p class="calibre2">We have seen how <a class="calibre" id="calibre_link-3455"></a><a class="calibre" id="calibre_link-3452"></a><a class="calibre" id="calibre_link-1105"></a>Spark relies on executors to run the tasks that make up a
    Spark job, but we glossed over how the executors actually get started.
    Managing the lifecycle of executors is the responsibility of the
    <em class="calibre10">cluster manager</em>, and Spark provides a variety of
    cluster managers with different characteristics:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Local</span></dt><dd class="calibre8"><p class="calibre2">In local mode<a class="calibre" id="calibre_link-2345"></a> there is a single executor running in the same JVM as
          the driver. This mode is useful for testing or running small jobs.
          The master URL for this mode is <code class="literal">local</code> (use one thread),
            <code class="literal">local[</code><em class="replaceable"><code class="replaceable">n</code></em><code class="literal">]</code> (<span class="calibre"><em class="calibre10">n</em></span> threads), or
          <code class="literal">local(*)</code> (one thread per core on the machine).</p></dd><dt class="calibre7"><span class="term">Standalone</span></dt><dd class="calibre8"><p class="calibre2">The standalone cluster manager is a simple distributed
          implementation that runs a single Spark master and one or more
          workers. When a Spark application starts, the master will ask the
          workers to spawn executor processes on behalf of the
          application.
            The master URL is <code class="literal">spark://</code><em class="replaceable"><code class="replaceable">host</code></em><code class="literal">:</code><em class="replaceable"><code class="replaceable">port</code></em>.</p></dd><dt class="calibre7"><span class="term">Mesos</span></dt><dd class="calibre8"><p class="calibre2">Apache Mesos is a general-purpose <a class="calibre" id="calibre_link-890"></a>cluster resource manager that allows fine-grained
          sharing of resources across different applications according to an
          organizational policy. By default (fine-grained mode), each Spark
          task is run as a Mesos task. This uses the cluster resources more
          efficiently, but at the cost of additional process launch overhead.
          In coarse-grained mode, executors run their tasks in-process, so the
          cluster resources are held by the executor processes for the
          duration of the Spark application. The master URL is
            <code class="literal">mesos://</code><em class="replaceable"><code class="replaceable">host</code></em>:<em class="replaceable"><code class="replaceable">port</code></em>.</p></dd><dt class="calibre7"><span class="term">YARN</span></dt><dd class="calibre8"><p class="calibre2">YARN is the resource manager used in Hadoop (see <a class="ulink" href="#calibre_link-318" title="Chapter&nbsp;4.&nbsp;YARN">Chapter&nbsp;4</a>). Each running Spark application corresponds
          to an instance of a YARN application, and each executor runs in its
          own YARN container. The master URL is <code class="literal">yarn-client</code> or
          <code class="literal">yarn-cluster</code>.</p></dd></dl></div><p class="calibre2">The Mesos and YARN cluster managers are superior to the standalone
    manager since they take into account the resource needs of other
    applications running on the cluster (MapReduce jobs, for example) and
    enforce a scheduling policy across all of them. The standalone cluster
    manager uses a static allocation of resources from the cluster, and
    therefore is not able to adapt to the varying needs of other applications
    over time. Also, YARN is the only cluster manager that is integrated with
    Hadoop’s Kerberos security mechanisms (see <a class="ulink" href="#calibre_link-169" title="Security">Security</a>).</p><div class="book" title="Spark on YARN"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-324">Spark on YARN</h3></div></div></div><p class="calibre2">Running Spark on <a class="calibre" id="calibre_link-3844"></a><a class="calibre" id="calibre_link-3465"></a>YARN provides the tightest integration with other Hadoop
      components and is the most convenient way to use Spark when you have an
      existing Hadoop cluster. Spark offers two deploy modes for running on
      YARN: <em class="calibre10">YARN client</em> mode, where the driver runs in
      the client, and <em class="calibre10">YARN cluster</em> mode, where the
      driver runs on the cluster in the YARN application master.</p><p class="calibre2">YARN client mode<a class="calibre" id="calibre_link-3847"></a> is required for programs that have any interactive
      component, such as <em class="calibre10">spark-shell</em> or
      <em class="calibre10">pyspark</em>. Client mode is also useful
      when building Spark programs, since any debugging output is immediately
      visible.</p><p class="calibre2">YARN cluster mode, on the other hand, is appropriate for
      production jobs, since the entire application runs on the cluster, which
      makes it much easier to retain logfiles (including those from the driver
      program) for later inspection. YARN will also retry the application if
      the application master fails (see <a class="ulink" href="#calibre_link-503" title="Application Master Failure">Application Master Failure</a>).</p><div class="book" title="YARN client mode"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4603">YARN client mode</h4></div></div></div><p class="calibre2">In YARN client mode, the interaction with YARN starts when a <a class="calibre" id="calibre_link-3473"></a>new <code class="literal">SparkContext</code> instance is
        constructed by the driver program (step 1 in <a class="ulink" href="#calibre_link-845" title="Figure&nbsp;19-3.&nbsp;How Spark executors are started in YARN client mode">Figure&nbsp;19-3</a>).
        The context submits a YARN application to the YARN resource manager
        (step 2), which starts a YARN
        container on a node manager in the cluster and runs a Spark
        <code class="literal">ExecutorLauncher</code> application master in it (step
        3). The job of the <code class="literal">ExecutorLauncher</code> is to start
        executors in YARN containers, which it does by requesting resources
        from the resource manager (step 4),
        then launching <code class="literal">ExecutorBackend</code> processes as the
        containers are allocated to it (step 5).</p><div class="book"><div class="figure"><a id="calibre_link-845" class="calibre"></a><div class="book"><div class="book"><img alt="How Spark executors are started in YARN client mode" src="images/000030.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;19-3.&nbsp;How Spark executors are started in YARN client mode</div></div></div><p class="calibre2">As each executor starts, it connects back to the
        <code class="literal">SparkContext</code> and registers itself.
        This gives the <code class="literal">SparkContext</code> information about
        the number of executors available for running tasks and their
        locations, which is used for making task placement decisions (described
        in <a class="ulink" href="#calibre_link-846" title="Task Scheduling">Task Scheduling</a>).</p><p class="calibre2">The number of executors that are launched is set in <em class="calibre10">spark-shell</em>, <em class="calibre10">spark-submit</em>, or <em class="calibre10">pyspark</em> (if not set, it defaults to two),
        along with the number of cores that each executor uses (the default is
        one) and the amount of memory (the default is 1,024 MB).
        Here’s an example showing how to run <em class="calibre10">spark-shell</em> on YARN with four executors,
        each using one core and 2 GB of memory:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">spark-shell --master yarn-client \
  --num-executors 4 \
  --executor-cores 1 \
  --executor-memory 2g</code></strong></pre><p class="calibre2">The YARN resource manager address is not specified in the master
        URL (unlike when using the standalone or Mesos cluster managers), but is
        picked up from Hadoop configuration in the directory specified by the
        <code class="literal">HADOOP_CONF_DIR</code> environment
        variable.</p></div><div class="book" title="YARN cluster mode"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4604">YARN cluster mode</h4></div></div></div><p class="calibre2">In YARN cluster<a class="calibre" id="calibre_link-3848"></a> mode, the user’s driver program runs in a YARN
        application master process. The <code class="literal">spark-submit</code>
        command <a class="calibre" id="calibre_link-3469"></a>is used with a master URL of <code class="literal">yarn-cluster</code>:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">spark-submit --master yarn-cluster ...</code></strong></pre><p class="calibre2">All other parameters, like <code class="literal">--num-executors</code> and the application JAR (or
          Python file), are the same as for YARN client mode (use <code class="literal">spark-submit --help</code> for usage).</p><p class="calibre2">The <em class="calibre10">spark-submit</em> client will
        launch the YARN application (step 1 in <a class="ulink" href="#calibre_link-847" title="Figure&nbsp;19-4.&nbsp;How Spark executors are started in YARN cluster mode">Figure&nbsp;19-4</a>), but it doesn’t run any
        user code. The rest of the process is the same as client mode, except
        the application master starts the driver program (step 3b) before
        allocating resources for executors (step 4).</p><div class="book"><div class="figure"><a id="calibre_link-847" class="calibre"></a><div class="book"><div class="book"><img alt="How Spark executors are started in YARN cluster mode" src="images/000040.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;19-4.&nbsp;How Spark executors are started in YARN cluster mode</div></div></div><p class="calibre2">In both YARN modes, the executors are launched before there is
        any data locality information available, so it could be that they end
        up not being co-located on the datanodes hosting the files that the
        jobs access. For interactive sessions, this may be acceptable,
        particularly as it may not be known which datasets are going to be
        accessed before the session starts. This is less true of production
        jobs, however, so Spark provides a way to give placement hints to
        improve data locality when running in YARN cluster mode.</p><p class="calibre2">The <code class="literal">SparkContext</code> constructor can take a
        second argument of preferred locations, computed from the input format
        and path using the <code class="literal">InputFormatInfo</code> helper
        class. For example, for text files, we use
        <code class="literal">TextInputFormat</code>:</p><pre class="screen1"><code class="k">val</code> <code class="n">preferredLocations</code> <code class="k">=</code> <code class="nc">InputFormatInfo</code><code class="o">.</code><code class="n">computePreferredLocations</code><code class="o">(</code>
    <code class="nc">Seq</code><code class="o">(</code><code class="k">new</code> <code class="nc">InputFormatInfo</code><code class="o">(</code><code class="k">new</code> <code class="nc">Configuration</code><code class="o">(),</code> <code class="n">classOf</code><code class="o">[</code><code class="kt">TextInputFormat</code><code class="o">],</code>
    <code class="n">inputPath</code><code class="o">)))</code>
<code class="k">val</code> <code class="n">sc</code> <code class="k">=</code> <code class="k">new</code> <code class="nc">SparkContext</code><code class="o">(</code><code class="n">conf</code><code class="o">,</code> <span class="calibre24"><strong class="calibre9"><code class="n1">preferredLocations</code></strong></span><code class="o">)</code></pre><p class="calibre2">The preferred locations are used by the application master when
        making allocation requests to the resource <a class="calibre" id="calibre_link-3466"></a><a class="calibre" id="calibre_link-3845"></a><a class="calibre" id="calibre_link-3849"></a><a class="calibre" id="calibre_link-3474"></a>manager (step 4).<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-848" id="calibre_link-856">135</a>]</sup></p></div></div></div><div class="book" title="Further Reading"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4605">Further Reading</h2></div></div></div><p class="calibre2">This chapter only <a class="calibre" id="calibre_link-3449"></a>covered the basics of Spark. For more detail, see <span class="calibre"><a class="ulink" href="http://shop.oreilly.com/product/0636920028512.do" target="_top">Learning
    Spark</a></span> by Holden Karau, Andy Konwinski, Patrick Wendell,
    and Matei Zaharia (O’Reilly, 2014). The <a class="ulink" href="http://spark.apache.org/" target="_top">Apache Spark website</a> also has
    up-to-date documentation about the latest Spark release.</p></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" id="calibre_link-828"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-849">128</a>] </sup>See Matei
        Zaharia et al., <a class="ulink" href="http://bit.ly/resilient_dist_datasets" target="_top">“Resilient Distributed Datasets: A Fault-Tolerant Abstraction for
          In-Memory Cluster Computing,”</a> <span class="calibre"><em class="calibre10">NSDI ’12 Proceedings of the 9th USENIX
        Conference on Networked Systems Design and Implementation</em></span>, 2012.</p></div><div class="footnote" type="footnote" id="calibre_link-833"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-850">129</a>] </sup>The Java version is much more compact when written using Java
          8 lambda expressions.</p></div><div class="footnote" type="footnote" id="calibre_link-835"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-851">130</a>] </sup>This is like performing a parameter sweep using
              <code class="literal">NLineInputFormat</code> in MapReduce, as described in <a class="ulink" href="#calibre_link-473" title="NLineInputFormat">NLineInputFormat</a>.</p></div><div class="footnote" type="footnote" id="calibre_link-840"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-852">131</a>] </sup>Note that <code class="literal">countByKey()</code> performs its final aggregation
            locally on the driver rather than using a second shuffle step. This is unlike the
            equivalent Crunch program in <a class="ulink" href="#calibre_link-768" title="Example&nbsp;18-3.&nbsp;A Crunch pipeline for calculating a histogram of word counts">Example&nbsp;18-3</a>, which uses a
            second MapReduce job for the count.</p></div><div class="footnote" id="calibre_link-842"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-853">132</a>] </sup>There is scope for tuning the performance of the shuffle
            through <a class="ulink" href="http://bit.ly/shuffle_behavior" target="_top">configuration</a>.
          Note also that Spark uses its own custom implementation for the
          shuffle, and does not share any code with the MapReduce shuffle
          implementation.</p></div><div class="footnote" type="footnote" id="calibre_link-843"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-854">133</a>] </sup>Speculative tasks are duplicates of existing tasks, which the scheduler may run as a
            backup if a task is running more slowly than expected. See <a class="ulink" href="#calibre_link-497" title="Speculative Execution">Speculative Execution</a>.</p></div><div class="footnote" type="footnote" id="calibre_link-844"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-855">134</a>] </sup>This is not true for Mesos fine-grained mode, where each task runs as a separate
            process. See the following section for details.</p></div><div class="footnote" type="footnote" id="calibre_link-848"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-856">135</a>] </sup>The preferred locations API is not stable (in Spark 1.2.0,
            the latest release as of this writing) and may change in a later
            release.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-68"><section type="chapter" id="calibre_link-4606" title="Chapter&nbsp;20.&nbsp;HBase"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;20.&nbsp;HBase</h2></div><div class="book"><div class="book"><div class="author2"><h3 class="author1"><span class="firstname">Jonathan</span> <span class="firstname">Gray</span></h3></div><div class="author2"><h3 class="author1"><span class="firstname">Michael</span> <span class="firstname">Stack</span></h3></div></div></div></div></div><div class="book" title="HBasics"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4607">HBasics</h2></div></div></div><p class="calibre2">HBase is a distributed <a class="calibre" id="calibre_link-3516"></a><a class="calibre" id="calibre_link-1806"></a><a class="calibre" id="calibre_link-1891"></a>column-oriented database built on top of HDFS. HBase is the
    Hadoop application to use when you require real-time read/write random
    access to very large datasets.</p><p class="calibre2">Although there are countless strategies and implementations for database storage and
      retrieval, most solutions—especially those of the relational variety—are not built with very
      large scale and distribution in mind. Many vendors offer replication and partitioning
      solutions to grow the database beyond the confines of a single node, but these add-ons are
      generally an afterthought and are complicated to install and maintain. They also severely
      compromise the RDBMS feature set. Joins, complex queries, triggers, views, and foreign-key
      constraints become prohibitively expensive to run on a scaled RDBMS, or do not work at
      all.</p><p class="calibre2">HBase approaches the scaling problem from the opposite direction. It
    is built from the ground up to scale linearly just by adding nodes. HBase
    is not relational and does not support SQL,<sup class="calibre6">[<a class="firstname" href="#calibre_link-69" id="calibre_link-82">136</a>]</sup> but given the proper problem space, it is able to do what an
    RDBMS cannot: host very large, sparsely populated tables on clusters made
    from commodity hardware.</p><p class="calibre2">The canonical HBase use case is <a class="calibre" id="calibre_link-3782"></a>the <span class="calibre"><em class="calibre10">webtable</em></span>, a table of crawled web
    pages and their attributes (such as language and MIME type) keyed by the
    web page URL. The webtable is large, with row counts that run into the
    billions. Batch analytic and parsing MapReduce jobs are continuously run against
    the webtable, deriving statistics and adding new columns of verified
    MIME-type and parsed-text content for later indexing by a search engine.
    Concurrently, the table is randomly accessed by crawlers running at
    various rates and updating random rows while random web pages are served
    in real time as users click on a website’s cached-page feature.</p><div class="book" title="Backdrop"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4608">Backdrop</h3></div></div></div><p class="calibre2">The HBase project was started toward the end of 2006 by <a class="calibre" id="calibre_link-3771"></a><a class="calibre" id="calibre_link-2300"></a>Chad Walters and Jim Kellerman at Powerset. It was modeled
      after Google’s Bigtable,
      which had just been published.<sup class="calibre6">[<a class="firstname" href="#calibre_link-70" id="calibre_link-84">137</a>]</sup> In February 2007, Mike
      Cafarella<a class="calibre" id="calibre_link-1045"></a> made a code drop of a mostly working system that Jim
      Kellerman then carried forward.</p><p class="calibre2">The first HBase release was bundled as part of Hadoop 0.15.0 in
      October 2007. In May 2010, HBase graduated from a Hadoop subproject to
      become an Apache Top Level Project. Today, HBase is a mature technology
      used in production across a wide range of industries.</p></div></div><div class="book" title="Concepts"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4609">Concepts</h2></div></div></div><p class="calibre2">In this section, we provide a quick overview of core HBase concepts.
    At a minimum, a passing familiarity will ease the digestion of all that
    follows.</p><div class="book" title="Whirlwind Tour of the Data Model"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4610">Whirlwind Tour of the Data Model</h3></div></div></div><p class="calibre2">Applications store data <a class="calibre" id="calibre_link-3592"></a><a class="calibre" id="calibre_link-1904"></a>in labeled tables. Tables are made of rows and columns.
      Table cells—the intersection of row and column coordinates—are
      versioned. By default, their version is a timestamp auto-assigned by
      HBase at the time of cell insertion. A cell’s content is an
      uninterpreted array of bytes. An example HBase table for storing photos
      is shown in <a class="ulink" href="#calibre_link-71" title="Figure&nbsp;20-1.&nbsp;The HBase data model, illustrated for a table storing photos">Figure&nbsp;20-1</a>.</p><div class="book"><div class="figure"><a id="calibre_link-71" class="calibre"></a><div class="book"><div class="book"><img alt="The HBase data model, illustrated for a table storing photos" src="images/000048.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;20-1.&nbsp;The HBase data model, illustrated for a table storing
        photos</div></div></div><p class="calibre2">Table row keys are also byte arrays, so theoretically anything can
      serve as a row key, from strings to binary representations of long or
      even serialized data structures. Table rows are sorted by row key, aka the
      table’s primary key. The sort is byte-ordered. All table accesses are
      via the primary key.<sup class="calibre6">[<a class="firstname" href="#calibre_link-72" id="calibre_link-85">138</a>]</sup></p><p class="calibre2">Row columns are grouped into <em class="calibre10">column
      families</em>. All column family members have a common prefix,
      so, for example, the columns <code class="literal">info:format</code> and <code class="literal">info:geo</code> are both members of the <code class="literal">info</code> column family, whereas <code class="literal">contents:image</code> belongs to
      the <code class="literal">contents</code> family. The column family
      prefix must be composed of <span class="calibre"><em class="calibre10">printable</em></span> characters.
      The qualifying tail, the column family <em class="calibre10">qualifier</em>,
      can be made of any arbitrary bytes. The column family and the qualifier
      are always separated by a colon character (<code class="literal">:</code>).</p><p class="calibre2">A table’s column families must be specified up front as part of
      the table schema definition, but new column family members can be added
      on demand. For example, a new column <code class="literal">info:camera</code> can be offered by a client as part
      of an update, and its value persisted, as long as the column family
      <code class="literal">info</code> already exists on the
      table.</p><p class="calibre2">Physically, all column family members are stored together on the
      filesystem. So although earlier we described HBase as a column-oriented
      store, it would be more accurate
      if it were described as a column<span class="calibre"><em class="calibre10">-family</em></span>-oriented
      store. Because tuning and storage specifications are done at the
      column family level, it is
      advised that all column family members have the same general <u style="
    text-decoration: underline 0.14em;
">access
      pattern</u> and size characteristics. For the photos table, the image data,
      which is large (megabytes), is stored in a separate column family from the
      metadata, which is much smaller in size (kilobytes).</p><p class="calibre2">In synopsis, HBase tables are like those in an RDBMS, only cells
      are versioned, rows are sorted, and columns can be added on the fly by
      the client as long as the column family they belong to <a class="calibre" id="calibre_link-3593"></a>preexists.</p><div class="book" title="Regions"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4611">Regions</h4></div></div></div><p class="calibre2">Tables are <a class="calibre" id="calibre_link-3597"></a>automatically partitioned horizontally by HBase into
        <em class="calibre10">regions</em>. Each region comprises a subset of a
        table’s rows. A region is denoted by the table it belongs to, its
        first row (inclusive), and its last row (exclusive). Initially, a table
        comprises a single region, but as the region grows it eventually crosses a configurable size threshold, at which point it splits at a row boundary
        into two new regions of approximately equal size. Until this first
        split happens, all loading will be against the single server hosting
        the original region. As the table grows, the number of its regions
        grows. Regions are the units that get distributed over an HBase
        cluster. In this way, a table that is too big for any one server can
        be carried by a cluster of servers, with each node hosting a subset of
        the table’s total regions. This is also the means by which the loading
        on a table gets distributed. The online set of sorted regions
        comprises the table’s total content.</p></div><div class="book" title="Locking"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4612">Locking</h4></div></div></div><p class="calibre2">Row updates are <a class="calibre" id="calibre_link-3596"></a><a class="calibre" id="calibre_link-2351"></a>atomic, no matter how many row columns constitute the
        row-level transaction. This keeps the locking model<a class="calibre" id="calibre_link-1905"></a> simple.</p></div></div><div class="book" title="Implementation"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4613">Implementation</h3></div></div></div><p class="calibre2">Just as <a class="calibre" id="calibre_link-1908"></a>HDFS and YARN are built of clients, workers, and a coordinating master—the
          <span class="calibre"><em class="calibre10">namenode</em></span> and <span class="calibre"><em class="calibre10">datanodes</em></span> in HDFS and
        <span class="calibre"><em class="calibre10">resource manager</em></span> and <span class="calibre"><em class="calibre10">node managers</em></span> in YARN—so is <a class="calibre" id="calibre_link-2650"></a>HBase made up of an HBase <em class="calibre10">master</em> node orchestrating a
        cluster of one or <a class="calibre" id="calibre_link-3208"></a>more <em class="calibre10">regionserver</em> workers (see <a class="ulink" href="#calibre_link-73" title="Figure&nbsp;20-2.&nbsp;HBase cluster members">Figure&nbsp;20-2</a>). The HBase master is responsible for bootstrapping a virgin
        install, for assigning regions to registered regionservers, and for recovering regionserver
        failures. The master node is lightly loaded. The regionservers carry zero or more regions
        and field client read/write requests. They also manage region splits, informing the HBase
        master about the new daughter regions so it can manage the offlining of parent regions and
        assignment of the replacement daughters.</p><div class="book"><div class="figure"><a id="calibre_link-73" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4614" class="calibre"></a><img alt="HBase cluster members" src="images/000056.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;20-2.&nbsp;HBase cluster members</div></div></div><p class="calibre2">HBase depends on <a class="calibre" id="calibre_link-3942"></a>ZooKeeper (<a class="ulink" href="#calibre_link-74" title="Chapter&nbsp;21.&nbsp;ZooKeeper">Chapter&nbsp;21</a>), and by default it manages a ZooKeeper
        instance as the authority on cluster state, although it can be configured to use an
        existing ZooKeeper cluster instead. The ZooKeeper ensemble hosts vitals such as the location
        of the <code class="literal">hbase:meta</code>
        catalog table and the address of the current cluster master. Assignment of regions is
        mediated via ZooKeeper in case participating servers crash mid-assignment. Hosting the
        assignment transaction state in ZooKeeper makes it so recovery can pick up on the assignment
        where the crashed server left off. At a minimum, when bootstrapping a client connection to
        an HBase cluster, the client must be passed the location of the ZooKeeper ensemble.
        Thereafter, the client navigates the ZooKeeper hierarchy to learn cluster attributes such as
        server locations.</p><p class="calibre2">Regionserver worker nodes are listed in the HBase <em class="calibre10">conf/regionservers</em> file, as you would list
      datanodes and node managers in the Hadoop <em class="calibre10">etc/hadoop/slaves</em> file. Start and stop
      scripts are like those in Hadoop and use the same SSH-based mechanism
      for running remote commands. A cluster’s site-specific configuration is
      done in the HBase <em class="calibre10">conf/hbase-site.xml</em> and <em class="calibre10">conf/hbase-env.sh</em> files, which have the same
      format as their equivalents in the Hadoop parent project (see
      <a class="ulink" href="#calibre_link-1" title="Chapter&nbsp;10.&nbsp;Setting Up a Hadoop Cluster">Chapter&nbsp;10</a>).</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Where there is commonality to be found, whether in a service or type, HBase typically
          directly uses or subclasses the parent Hadoop implementation. When this is not possible, HBase will follow the Hadoop model
          where it can. For example, HBase uses the Hadoop configuration system, so configuration
          files have the same format. What this means for you, the user, is that you can leverage
          any Hadoop familiarity in your exploration of HBase.
          HBase deviates from this rule only when adding its specializations.</p></div><p class="calibre2">HBase persists data via the Hadoop filesystem API. Most people
      using HBase run it on HDFS for storage, though by default, and unless
      told otherwise, HBase writes to the local filesystem. The local
      filesystem is fine for experimenting with your initial HBase install,
      but thereafter, the first configuration made in an HBase cluster usually
      involves pointing HBase at the HDFS cluster it should use.</p><div class="book" title="HBase in operation"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4615">HBase in operation</h4></div></div></div><p class="calibre2">Internally, HBase keeps a special catalog <a class="calibre" id="calibre_link-1918"></a>table named <code class="literal">hbase:meta</code>, within which it maintains the current list, state, and
          locations of all user-space regions afloat on the cluster. Entries in <code class="literal">hbase:meta</code> are keyed by region name, where a region name is
          made up of the name of the table the region belongs to, the region’s start row, its time
          of creation, and finally, an MD5 hash of all of these (i.e., a hash of table name, start
          row, and creation timestamp). Here is an example region name for a region in the table
            <code class="literal">TestTable</code> whose start row is <code class="literal">xyz</code>:</p><pre class="screen1">TestTable,xyz,1279729913622.1b6e176fb8d8aa88fd4ab6bc80247ece.</pre><p class="calibre2">Commas delimit the table name, start row, and timestamp. The MD5
        hash is surrounded by a leading and trailing period.</p><p class="calibre2">As noted previously, row keys are sorted, so finding the region that hosts a
          particular row is a matter of a lookup to find the largest entry whose key is less than or
          equal to that of the requested row key. As regions transition—are split, disabled, enabled, deleted, or redeployed by the region
          load balancer, or redeployed due to a regionserver
          crash—the catalog table is updated so the state of all regions on the cluster is kept
          current.</p><p class="calibre2">Fresh clients connect to the ZooKeeper cluster first to learn
        the location of <code class="literal">hbase:meta</code>. The client then does a
        lookup against the appropriate <code class="literal">hbase:meta</code> region to
        figure out the hosting user-space region and its location. Thereafter,
        the client interacts directly with the hosting regionserver.</p><p class="calibre2">To save on having to make three round-trips per row operation, clients cache all they
          learn while doing lookups for <code class="literal">hbase:meta</code>. They cache locations as well
          as user-space region start and stop rows, so they can figure out hosting regions
          themselves without having to go back to the <code class="literal">hbase:meta</code> table. Clients
          continue to use the cached entries as they work, until there is a fault. When this
          happens—i.e., when the region has moved—the client consults the
            <code class="literal">hbase:meta</code> table again to learn the new location. If the consulted
            <code class="literal">hbase:meta</code> region has moved, then ZooKeeper is reconsulted.</p><p class="calibre2">Writes arriving at a regionserver are first appended to a commit log and then added to
          an in-memory <span class="calibre"><em class="calibre10">memstore</em></span>. When a memstore fills, its content is flushed
          to the filesystem.</p><p class="calibre2">The commit log is hosted on HDFS, so it remains available through a regionserver
          crash. When the master notices that a regionserver is no longer reachable, usually because
          the server’s znode has expired in ZooKeeper, it splits the dead regionserver’s commit log
          by region. On reassignment and before they reopen for business, regions that were on the
          dead regionserver will pick up their just-split files of not-yet-persisted edits and
          replay them to bring themselves up to date with the state they had just before the
          failure.</p><p class="calibre2">When reading, the region’s memstore is consulted first. If
        sufficient versions are found reading memstore alone, the query
        completes there. Otherwise, flush files are consulted in order, from
        newest to oldest, either until versions sufficient to satisfy the
        query are found or until we run out of flush files.</p><p class="calibre2">A background process compacts flush files once their number has exceeded a threshold,
          rewriting many files as one, because the fewer files a read consults, the more performant
          it will be. On compaction, the process cleans out versions beyond the schema-configured
          maximum and removes deleted and expired cells. A separate process running in the
          regionserver monitors flush file sizes, splitting the region when they grow in excess of
          the configured <a class="calibre" id="calibre_link-1909"></a>maximum.</p></div></div></div><div class="book" title="Installation"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4616">Installation</h2></div></div></div><p class="calibre2">Download a <a class="calibre" id="calibre_link-1910"></a>stable release from an <a class="ulink" href="http://www.apache.org/dyn/closer.cgi/hbase/" target="_top">Apache Download
    Mirror</a> and unpack it on your local filesystem. For example:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">tar xzf hbase-<em class="replaceable1"><code class="calibre46">x.y.z</code></em>.tar.gz</code></strong></pre><p class="calibre2">As with Hadoop, you first need to tell HBase where Java is located on your system. If you
      have the <code class="literal">JAVA_HOME</code> environment variable set to point to a suitable Java
      installation, then that will be used, and you don’t have to configure anything further.
      Otherwise, you can set the Java installation that HBase uses by editing HBase’s <em class="calibre10">conf/hbase-env.sh</em> file and specifying the
        <code class="literal">JAVA_HOME</code> variable (see <a class="ulink" href="#calibre_link-28" title="Appendix&nbsp;A.&nbsp;Installing Apache Hadoop">Appendix&nbsp;A</a> for some examples).</p><p class="calibre2">For convenience, add the HBase binary directory to your command-line
    path. For example:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">export HBASE_HOME=~/sw/hbase-<em class="replaceable1"><code class="calibre46">x.y.z</code></em></code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">export PATH=$PATH:$HBASE_HOME/bin</code></strong></pre><p class="calibre2">To get the list of HBase options, use the following:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hbase</code></strong>
Options:
  --config DIR    Configuration direction to use. Default: ./conf
  --hosts HOSTS   Override the list in 'regionservers' file

Commands:
Some commands take arguments. Pass no args or -h for usage.
  shell           Run the HBase shell
  hbck            Run the hbase 'fsck' tool
  hlog            Write-ahead-log analyzer
  hfile           Store file analyzer
  zkcli           Run the ZooKeeper shell
  upgrade         Upgrade hbase
  master          Run an HBase HMaster node
  regionserver    Run an HBase HRegionServer node
  zookeeper       Run a Zookeeper server
  rest            Run an HBase REST server
  thrift          Run the HBase Thrift server
  thrift2         Run the HBase Thrift2 server
  clean           Run the HBase clean up script
  classpath       Dump hbase CLASSPATH
  mapredcp        Dump CLASSPATH entries required by mapreduce
  pe              Run PerformanceEvaluation
  ltt             Run LoadTestTool
  version         Print the version
  CLASSNAME       Run the class named CLASSNAME</pre><div class="book" title="Test Drive"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4617">Test Drive</h3></div></div></div><p class="calibre2">To start a <a class="calibre" id="calibre_link-3649"></a><a class="calibre" id="calibre_link-1914"></a>standalone instance of HBase that uses a temporary
      directory on the local filesystem for persistence, use this:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">start-hbase.sh</code></strong></pre><p class="calibre2">By default, HBase writes to <em class="calibre10">/${java.io.tmpdir}/hbase-${user.name}</em>. <code class="literal">${java.io.tmpdir}</code> usually maps to <em class="calibre10">/tmp</em>, but you should configure HBase to use a more permanent
        location by setting <code class="literal">hbase.tmp.dir</code> in <em class="calibre10">hbase-site.xml</em>. In standalone mode, the HBase master, the
        regionserver, and a <code class="literal">ZooKeeper</code> instance are all run in the same
        JVM.</p><p class="calibre2">To administer your HBase instance, launch the HBase shell as
      follows:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hbase shell</code></strong>
HBase Shell; enter 'help&lt;RETURN&gt;' for list of supported commands.
Type "exit&lt;RETURN&gt;" to leave the HBase Shell
Version 0.98.7-hadoop2, r800c23e2207aa3f9bddb7e9514d8340bcfb89277, Wed Oct  8
15:58:11 PDT 2014

hbase(main):001:0&gt;</pre><p class="calibre2">This will bring up a JRuby IRB interpreter that has had some
      HBase-specific commands added to it. Type <code class="literal">help</code> and then press Return to see the list of
      shell commands grouped into categories. Type <code class="literal">help
      "<em class="replaceable"><code class="replaceable">COMMAND_GROUP</code></em>"</code> for help by
      category or <code class="literal">help
      "<em class="replaceable"><code class="replaceable">COMMAND</code></em>"</code> for help on a specific
      command and example usage. Commands use Ruby formatting to specify lists
      and dictionaries. See the end of the main help screen for a quick
      tutorial.</p><p class="calibre2">Now let’s <a class="calibre" id="calibre_link-3594"></a>create a simple table, add some data, and then clean
      up.</p><p class="calibre2">To create a table, you must name your table and define its schema.
      A table’s schema comprises table attributes and the list of table column
      families. Column families themselves have attributes that you in
      turn set at schema definition time. Examples of column family attributes
      include whether the family content should be compressed on the filesystem and how many versions of a cell
      to keep. Schemas can be edited later by offlining the table using the
      shell <code class="literal">disable</code> command, making the
      necessary alterations using <code class="literal">alter</code>,
      then putting the table back online with <code class="literal">enable</code>.</p><p class="calibre2">To create a table named <code class="literal">test</code>
      with a single column family named <code class="literal">data</code> using defaults for table and column
      family attributes, enter:</p><pre class="screen1"><code class="literal">hbase(main):001:0&gt;</code> <strong class="userinput"><code class="calibre9">create 'test', 'data'</code></strong>
0 row(s) in 0.9810 seconds</pre><div class="note" title="Tip"><h3 class="title3">Tip</h3><p class="calibre2">If the previous command does not complete successfully, and the
        shell displays an error and a stack trace, your install was not
        successful. Check the master logs under the HBase <em class="calibre10">logs</em> directory—the default location for the
        logs directory is <em class="calibre10">${HBASE_HOME}/logs</em>—for a clue as to where
        things went awry.</p></div><p class="calibre2">See the <code class="literal">help</code> output for
      examples of adding table and column family attributes when specifying a
      schema.</p><p class="calibre2">To prove the new table was created successfully, <a class="calibre" id="calibre_link-2329"></a>run the <code class="literal">list</code> command.
      This will output all tables in user space:</p><pre class="screen1"><code class="literal">hbase(main):002:0&gt;</code> <strong class="userinput"><code class="calibre9">list</code></strong>
TABLE                                                                                                                                                                                                                                 
test                                                                                                                                                                                                                                  
1 row(s) in 0.0260 seconds</pre><p class="calibre2">To insert data <a class="calibre" id="calibre_link-3595"></a>into three different rows and columns in the <code class="literal">data</code> column family, get the first row, and
      then list the table content, do the following:</p><pre class="screen1"><code class="literal">hbase(main):003:0&gt;</code> <strong class="userinput"><code class="calibre9">put 'test', 'row1', 'data:1', 'value1'</code></strong>
<code class="literal">hbase(main):004:0&gt;</code> <strong class="userinput"><code class="calibre9">put 'test', 'row2', 'data:2', 'value2'</code></strong>
<code class="literal">hbase(main):005:0&gt;</code> <strong class="userinput"><code class="calibre9">put 'test', 'row3', 'data:3', 'value3'</code></strong>
<code class="literal">hbase(main):006:0&gt;</code> <strong class="userinput"><code class="calibre9">get 'test', 'row1'</code></strong>
COLUMN                       CELL                                                                                                                                                                       
 data:1                      timestamp=1414927084811, value=value1                                                                                                                                      
1 row(s) in 0.0240 seconds
<code class="literal">hbase(main):007:0&gt;</code> <strong class="userinput"><code class="calibre9">scan 'test'</code></strong>
ROW                          COLUMN+CELL                                                                                                                                                                
 row1                        column=data:1, timestamp=1414927084811, value=value1                                                                                                                       
 row2                        column=data:2, timestamp=1414927125174, value=value2                                                                                                                       
 row3                        column=data:3, timestamp=1414927131931, value=value3                                                                                                                       
3 row(s) in 0.0240 seconds</pre><p class="calibre2">Notice how we added three new columns without changing the
      schema.</p><p class="calibre2">To remove the <a class="calibre" id="calibre_link-3598"></a>table, you must first disable it before dropping
      it:</p><pre class="screen1"><code class="literal">hbase(main):009:0&gt;</code> <strong class="userinput"><code class="calibre9">disable 'test'</code></strong>
0 row(s) in 5.8420 seconds
<code class="literal">hbase(main):010:0&gt;</code> <strong class="userinput"><code class="calibre9">drop 'test'</code></strong>
0 row(s) in 5.2560 seconds
<code class="literal">hbase(main):011:0&gt;</code> <strong class="userinput"><code class="calibre9">list</code></strong>
TABLE                                                                                                                                                                                                                                 
0 row(s) in 0.0200 seconds</pre><p class="calibre2">Shut down your HBase instance by running:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">stop-hbase.sh</code></strong></pre><p class="calibre2">To learn how to set up a distributed HBase cluster and point it at a running HDFS,
          <a class="calibre" id="calibre_link-1911"></a><a class="calibre" id="calibre_link-3650"></a><a class="calibre" id="calibre_link-1915"></a>see the <a class="ulink" href="http://hbase.apache.org/book/configuration.html" target="_top">configuration section of the HBase documentation</a>.</p></div></div><div class="book" title="Clients"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4618">Clients</h2></div></div></div><p class="calibre2">There are a <a class="calibre" id="calibre_link-1900"></a>number of client options for interacting with an HBase
    cluster.</p><div class="book" title="Java"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4619">Java</h3></div></div></div><p class="calibre2">HBase, like <a class="calibre" id="calibre_link-2164"></a>Hadoop, is written in Java. <a class="ulink" href="#calibre_link-75" title="Example&nbsp;20-1.&nbsp;Basic table administration and access">Example&nbsp;20-1</a> shows the Java version of how you would
        do the shell operations listed in the previous section.</p><div class="example"><a id="calibre_link-75" class="calibre"></a><div class="example-title">Example&nbsp;20-1.&nbsp;Basic table administration and
        access</div><div class="book"><a id="calibre_link-4620" class="calibre"></a><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">ExampleClient</code> <code class="o">{</code>

  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="n">Configuration</code> <code class="n">config</code> <code class="o">=</code> <code class="n">HBaseConfiguration</code><code class="o">.</code><code class="na">create</code><code class="o">();</code>
    <code class="c2">// Create table</code>
    <code class="n">HBaseAdmin</code> <code class="n">admin</code> <code class="o">=</code> <code class="k">new</code> <code class="n">HBaseAdmin</code><code class="o">(</code><code class="n">config</code><code class="o">);</code>
    <code class="k">try</code> <code class="o">{</code>
      <code class="n">TableName</code> <code class="n">tableName</code> <code class="o">=</code> <code class="n">TableName</code><code class="o">.</code><code class="na">valueOf</code><code class="o">(</code><code class="sb">"test"</code><code class="o">);</code>
      <code class="n">HTableDescriptor</code> <code class="n">htd</code> <code class="o">=</code> <code class="k">new</code> <code class="n">HTableDescriptor</code><code class="o">(</code><code class="n">tableName</code><code class="o">);</code>
      <code class="n">HColumnDescriptor</code> <code class="n">hcd</code> <code class="o">=</code> <code class="k">new</code> <code class="n">HColumnDescriptor</code><code class="o">(</code><code class="sb">"data"</code><code class="o">);</code>
      <code class="n">htd</code><code class="o">.</code><code class="na">addFamily</code><code class="o">(</code><code class="n">hcd</code><code class="o">);</code>
      <code class="n">admin</code><code class="o">.</code><code class="na">createTable</code><code class="o">(</code><code class="n">htd</code><code class="o">);</code>
      <code class="n">HTableDescriptor</code><code class="o">[]</code> <code class="n">tables</code> <code class="o">=</code> <code class="n">admin</code><code class="o">.</code><code class="na">listTables</code><code class="o">();</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">tables</code><code class="o">.</code><code class="na">length</code> <code class="o">!=</code> <code class="mi">1</code> <code class="o">&amp;&amp;</code>
          <code class="n">Bytes</code><code class="o">.</code><code class="na">equals</code><code class="o">(</code><code class="n">tableName</code><code class="o">.</code><code class="na">getName</code><code class="o">(),</code> <code class="n">tables</code><code class="o">[</code><code class="mi">0</code><code class="o">].</code><code class="na">getTableName</code><code class="o">().</code><code class="na">getName</code><code class="o">()))</code> <code class="o">{</code>
        <code class="k">throw</code> <code class="k">new</code> <code class="nf">IOException</code><code class="o">(</code><code class="sb">"Failed create of table"</code><code class="o">);</code>
      <code class="o">}</code>
      <code class="c2">// Run some operations -- three puts, a get, and a scan -- against the table.</code>
      <code class="n">HTable</code> <code class="n">table</code> <code class="o">=</code> <code class="k">new</code> <code class="n">HTable</code><code class="o">(</code><code class="n">config</code><code class="o">,</code> <code class="n">tableName</code><code class="o">);</code>
      <code class="k">try</code> <code class="o">{</code>
        <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">1</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;=</code> <code class="mi">3</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>
          <code class="kt">byte</code><code class="o">[]</code> <code class="n">row</code> <code class="o">=</code> <code class="n">Bytes</code><code class="o">.</code><code class="na">toBytes</code><code class="o">(</code><code class="sb">"row"</code> <code class="o">+</code> <code class="n">i</code><code class="o">);</code>
          <code class="n">Put</code> <code class="n">put</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Put</code><code class="o">(</code><code class="n">row</code><code class="o">);</code>
          <code class="kt">byte</code><code class="o">[]</code> <code class="n">columnFamily</code> <code class="o">=</code> <code class="n">Bytes</code><code class="o">.</code><code class="na">toBytes</code><code class="o">(</code><code class="sb">"data"</code><code class="o">);</code>
          <code class="kt">byte</code><code class="o">[]</code> <code class="n">qualifier</code> <code class="o">=</code> <code class="n">Bytes</code><code class="o">.</code><code class="na">toBytes</code><code class="o">(</code><code class="n">String</code><code class="o">.</code><code class="na">valueOf</code><code class="o">(</code><code class="n">i</code><code class="o">));</code>
          <code class="kt">byte</code><code class="o">[]</code> <code class="n">value</code> <code class="o">=</code> <code class="n">Bytes</code><code class="o">.</code><code class="na">toBytes</code><code class="o">(</code><code class="sb">"value"</code> <code class="o">+</code> <code class="n">i</code><code class="o">);</code>
          <code class="n">put</code><code class="o">.</code><code class="na">add</code><code class="o">(</code><code class="n">columnFamily</code><code class="o">,</code> <code class="n">qualifier</code><code class="o">,</code> <code class="n">value</code><code class="o">);</code>
          <code class="n">table</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="n">put</code><code class="o">);</code>
        <code class="o">}</code>
        <code class="n">Get</code> <code class="n">get</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Get</code><code class="o">(</code><code class="n">Bytes</code><code class="o">.</code><code class="na">toBytes</code><code class="o">(</code><code class="sb">"row1"</code><code class="o">));</code>
        <code class="n">Result</code> <code class="n">result</code> <code class="o">=</code> <code class="n">table</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="n">get</code><code class="o">);</code>
        <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="sb">"Get: "</code> <code class="o">+</code> <code class="n">result</code><code class="o">);</code>
        <code class="n">Scan</code> <code class="n">scan</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Scan</code><code class="o">();</code>
        <code class="n">ResultScanner</code> <code class="n">scanner</code> <code class="o">=</code> <code class="n">table</code><code class="o">.</code><code class="na">getScanner</code><code class="o">(</code><code class="n">scan</code><code class="o">);</code>
        <code class="k">try</code> <code class="o">{</code>
          <code class="k">for</code> <code class="o">(</code><code class="n">Result</code> <code class="n">scannerResult</code> <code class="o">:</code> <code class="n">scanner</code><code class="o">)</code> <code class="o">{</code>
            <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="sb">"Scan: "</code> <code class="o">+</code> <code class="n">scannerResult</code><code class="o">);</code>
          <code class="o">}</code>
        <code class="o">}</code> <code class="k">finally</code> <code class="o">{</code>
          <code class="n">scanner</code><code class="o">.</code><code class="na">close</code><code class="o">();</code>
        <code class="o">}</code>
        <code class="c2">// Disable then drop the table</code>
        <code class="n">admin</code><code class="o">.</code><code class="na">disableTable</code><code class="o">(</code><code class="n">tableName</code><code class="o">);</code>
        <code class="n">admin</code><code class="o">.</code><code class="na">deleteTable</code><code class="o">(</code><code class="n">tableName</code><code class="o">);</code>
      <code class="o">}</code> <code class="k">finally</code> <code class="o">{</code>
        <code class="n">table</code><code class="o">.</code><code class="na">close</code><code class="o">();</code>
      <code class="o">}</code>
    <code class="o">}</code> <code class="k">finally</code> <code class="o">{</code>
      <code class="n">admin</code><code class="o">.</code><code class="na">close</code><code class="o">();</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">This class has a <code class="literal">main()</code> method only. For the sake of brevity,
        we do not include the package name, nor imports. Most
        of the HBase classes are found in<a class="calibre" id="calibre_link-2860"></a><a class="calibre" id="calibre_link-3241"></a> the <code class="literal">org.apache.hadoop.hbase</code> and <code class="literal">org.apache.hadoop.hbase.client</code> packages.</p><p class="calibre2">In this class, we first ask the
      <code class="literal">HBaseConfiguration</code> class <a class="calibre" id="calibre_link-1919"></a>to create a <code class="literal">Configuration</code> object. It will return a
      <code class="literal">Configuration</code> that has read the HBase
      configuration from the <em class="calibre10">hbase-site.xml</em> and <em class="calibre10">hbase-default.xml</em> files found on the program’s classpath. This
      <code class="literal">Configuration</code> is subsequently used to create
      instances of <code class="literal">HBaseAdmin</code> and
      <code class="literal">HTable</code>. <code class="literal">HBaseAdmin</code> is used
      for administering your HBase cluster, specifically for adding and
      dropping tables. <code class="literal">HTable</code> is used to access a
      specific table. The <code class="literal">Configuration</code> instance points
      these classes at the cluster the code is to work against.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">From HBase 1.0, there is a new client API that is cleaner and more intuitive. The
          constructors of <code class="literal">HBaseAdmin</code> and <code class="literal">HTable</code> have
          been deprecated, and clients are discouraged from making explicit reference to these old
          classes. In their place, clients should use the <a class="calibre" id="calibre_link-1241"></a>new <code class="literal">ConnectionFactory</code> class to create a
            <code class="literal">Connection</code> object, <a class="calibre" id="calibre_link-1239"></a>then call <code class="literal">getAdmin()</code> or
            <code class="literal">getTable()</code> to retrieve an <code class="literal">Admin</code> or
            <code class="literal">Table</code> instance, as appropriate. Connection management was
          previously done for the user under the covers, but is now the responsibility of the
          client. You can find versions of the examples in this chapter updated to use the new API
          on this book’s accompanying website.</p></div><p class="calibre2">To create a table, we need to create an instance of
      <code class="literal">HBaseAdmin</code> and then ask it to create the table
      named <code class="literal">test</code> with a single column family named
      <code class="literal">data</code>. In our example, our table schema is the default. We
      could use <a class="calibre" id="calibre_link-2058"></a>methods on <code class="literal">HTableDescriptor</code> and
      <code class="literal">HColumnDescriptor</code> to change the table schema.
      Next, the code asserts the table was actually created, and throws an
      exception if it wasn’t.</p><p class="calibre2">To operate on a table, we will need an instance of
      <code class="literal">HTable</code>, which we construct by passing it our
      <code class="literal">Configuration</code> instance and the name of the table.
      We then create <code class="literal">Put</code> objects in a loop to insert
      data into the table. Each <code class="literal">Put</code> puts a single cell
      value of <code class="literal">value<em class="replaceable"><code class="replaceable">n</code></em></code> into a row named
      <code class="literal">row<em class="replaceable"><code class="replaceable">n</code></em></code> on the column named
      <code class="literal">data:<em class="replaceable"><code class="replaceable">n</code></em></code>, where
      <code class="literal"><em class="replaceable"><code class="replaceable">n</code></em></code> is from 1 to 3. The column
      name is specified in two parts: the column family name, and the column
      family qualifier. The code makes liberal use of HBase’s
      <code class="literal">Bytes</code> utility class (found in the <code class="literal">org.apache.hadoop.hbase.util</code> package)
      <a class="calibre" id="calibre_link-2862"></a>to convert identifiers and values to the byte arrays that
      HBase requires.</p><p class="calibre2">Next, we create a <code class="literal">Get</code> object <a class="calibre" id="calibre_link-1795"></a>to retrieve and print the first row that we added. Then we
      use <a class="calibre" id="calibre_link-3271"></a>a <code class="literal">Scan</code> object to scan over the
      table, printing out what we find.</p><p class="calibre2">At the end of the program, we clean up by first disabling the table and then deleting it
        (recall that a table must be disabled before it can be dropped).</p><div class="sidebar"><div class="sidebar-title">Scanners</div><p class="calibre2">HBase scanners are like cursors in a traditional database or Java iterators,
          except—unlike the latter—they have to be closed after use. Scanners return rows in order.
          Users obtain a scanner on a <code class="literal">Table</code> object by calling <code class="literal">getScanner()</code>,
          passing a configured instance of a <code class="literal">Scan</code> object as a parameter. In
          the <code class="literal">Scan</code> instance, you can pass the row at which to start and stop
          the scan, which columns in a row to return in the row result, and a filter to run on the
          server side. The <code class="literal">ResultScanner</code> interface, <a class="calibre" id="calibre_link-3239"></a>which is returned when you call <code class="literal">get</code><code class="literal">Scanner()</code>, is as follows:</p><a id="calibre_link-4621" class="calibre"></a><pre class="screen2"><code class="k">public</code> <code class="k">interface</code> <code class="nc">ResultScanner</code> <code class="k">extends</code> <code class="n">Closeable</code><code class="o">,</code> <code class="n">Iterable</code><code class="o">&lt;</code><code class="n">Result</code><code class="o">&gt;</code> <code class="o">{</code> 
  <code class="k">public</code> <code class="n">Result</code> <code class="nf">next</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">;</code>
  <code class="k">public</code> <code class="n">Result</code><code class="o">[]</code> <code class="nf">next</code><code class="o">(</code><code class="kt">int</code> <code class="n">nbRows</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">;</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">close</code><code class="o">();</code>  
<code class="o">}</code></pre><p class="calibre2">You can ask for the next row’s results, or a number of rows. Scanners will, under the
          covers, fetch batches of 100 rows at a time, bringing them client-side and returning to
          the server to fetch the next batch only after the current batch has been exhausted. The
          number of rows to fetch and cache in this way is determined by <a class="calibre" id="calibre_link-1916"></a>the <code class="literal">hbase.client.</code><code class="literal">scanner.caching</code>
          configuration option. Alternatively, you can set how many rows to cache on the
            <code class="literal">Scan</code> instance itself via the
            <code class="literal">setCaching()</code> method.</p><p class="calibre2">Higher caching values will enable faster scanning but will eat
        up more memory in the client. Also, avoid setting the caching so high
        that the time spent processing the batch client-side exceeds the
        scanner timeout period. If a client fails to check back with the
        server before the scanner timeout expires, the server will go ahead
        and garbage collect resources consumed by the scanner server-side. The
        default scanner timeout is 60 seconds, and can be changed by
        <a class="calibre" id="calibre_link-1917"></a>setting <code class="literal">hbase.client.scanner.timeout.period</code>. Clients
        will see an <code class="literal">UnknownScannerException</code>
        if the scanner timeout has expired.</p></div><p class="calibre2">The simplest way to compile the program is to use the Maven POM
      that comes with the book’s example code. Then we can use the
      <code class="literal">hbase</code> command followed by the classname to run the
      program. Here’s a sample run:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">mvn package</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">export HBASE_CLASSPATH=hbase-examples.jar</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">hbase ExampleClient</code></strong>
Get: keyvalues={row1/data:1/1414932826551/Put/vlen=6/mvcc=0}
Scan: keyvalues={row1/data:1/1414932826551/Put/vlen=6/mvcc=0}
Scan: keyvalues={row2/data:2/1414932826564/Put/vlen=6/mvcc=0}
Scan: keyvalues={row3/data:3/1414932826566/Put/vlen=6/mvcc=0}</pre><p class="calibre2">Each line of output shows an HBase row, rendered using the
      <code class="literal">toString()</code> method from
      <code class="literal">Result</code>. The fields are separated by a slash
      character, and are as follows: the row name, the column name, the cell
      timestamp, the cell type, the length of the value’s byte array (<code class="literal">vlen</code>), and an internal HBase field (<code class="literal">mvcc</code>). We’ll see later how to get the value
      <a class="calibre" id="calibre_link-3238"></a>from a <code class="literal">Result</code> object <a class="calibre" id="calibre_link-2165"></a>using its <code class="literal">getValue()</code>
      method.</p></div><div class="book" title="MapReduce"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4622">MapReduce</h3></div></div></div><p class="calibre2">HBase classes <a class="calibre" id="calibre_link-2478"></a><a class="calibre" id="calibre_link-2861"></a>and utilities in the
      <code class="literal">org.apache.hadoop.hbase.mapreduce</code> package
      facilitate using HBase as a source and/or sink in MapReduce jobs. The
      <code class="literal">TableInputFormat</code> class <a class="calibre" id="calibre_link-3587"></a>makes splits on region boundaries so maps are handed a
      single region to work on. The <code class="literal">TableOutputFormat</code>
      will write the <a class="calibre" id="calibre_link-3591"></a>result of the reduce into HBase.</p><p class="calibre2">The <code class="literal">SimpleRowCounter</code> class in <a class="ulink" href="#calibre_link-76" title="Example&nbsp;20-2.&nbsp;A MapReduce application to count the number of rows in an HBase table">Example&nbsp;20-2</a> (which
        is a simplified <a class="calibre" id="calibre_link-3247"></a>version of <code class="literal">RowCounter</code> in the HBase
          <code class="literal">mapreduce</code> package) runs a map task to count rows using <code class="literal">TableInputFormat</code>.</p><div class="example"><a id="calibre_link-76" class="calibre"></a><div class="example-title">Example&nbsp;20-2.&nbsp;A MapReduce application to count the number of rows in an HBase
        table</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">SimpleRowCounter</code> <code class="k">extends</code> <code class="n">Configured</code> <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>

  <code class="k">static</code> <code class="k">class</code> <code class="nc">RowCounterMapper</code> <code class="k">extends</code> <code class="n">TableMapper</code><code class="o">&lt;</code><code class="n">ImmutableBytesWritable</code><code class="o">,</code> <code class="n">Result</code><code class="o">&gt;</code> <code class="o">{</code>
    <code class="k">public</code> <code class="k">static</code> <code class="k">enum</code> <code class="n">Counters</code> <code class="o">{</code> <code class="n">ROWS</code> <code class="o">}</code>

    <code class="nd">@Override</code>
    <code class="k">public</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">ImmutableBytesWritable</code> <code class="n">row</code><code class="o">,</code> <code class="n">Result</code> <code class="n">value</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">context</code><code class="o">.</code><code class="na">getCounter</code><code class="o">(</code><code class="n">Counters</code><code class="o">.</code><code class="na">ROWS</code><code class="o">).</code><code class="na">increment</code><code class="o">(</code><code class="mi">1</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="na">length</code> <code class="o">!=</code> <code class="mi">1</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="sb">"Usage: SimpleRowCounter &lt;tablename&gt;"</code><code class="o">);</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>
    <code class="n">String</code> <code class="n">tableName</code> <code class="o">=</code> <code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">];</code>
    <code class="n">Scan</code> <code class="n">scan</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Scan</code><code class="o">();</code>
    <code class="n">scan</code><code class="o">.</code><code class="na">setFilter</code><code class="o">(</code><code class="k">new</code> <code class="n">FirstKeyOnlyFilter</code><code class="o">());</code>

    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Job</code><code class="o">(</code><code class="n">getConf</code><code class="o">(),</code> <code class="n">getClass</code><code class="o">().</code><code class="na">getSimpleName</code><code class="o">());</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setJarByClass</code><code class="o">(</code><code class="n">getClass</code><code class="o">());</code>
    <code class="n">TableMapReduceUtil</code><code class="o">.</code><code class="na">initTableMapperJob</code><code class="o">(</code><code class="n">tableName</code><code class="o">,</code> <code class="n">scan</code><code class="o">,</code>
        <code class="n">RowCounterMapper</code><code class="o">.</code><code class="na">class</code><code class="o">,</code> <code class="n">ImmutableBytesWritable</code><code class="o">.</code><code class="na">class</code><code class="o">,</code> <code class="n">Result</code><code class="o">.</code><code class="na">class</code><code class="o">,</code> <code class="n">job</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setNumReduceTasks</code><code class="o">(</code><code class="mi">0</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputFormatClass</code><code class="o">(</code><code class="n">NullOutputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="k">return</code> <code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">;</code>
  <code class="o">}</code>

  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="n">HBaseConfiguration</code><code class="o">.</code><code class="na">create</code><code class="o">(),</code>
        <code class="k">new</code> <code class="nf">SimpleRowCounter</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The <code class="literal">RowCounterMapper</code> nested class is a subclass of the HBase
          <code class="literal">TableMapper</code> abstract <a class="calibre" id="calibre_link-3588"></a>class, a specialization of
          <code class="literal">org.apache.hadoop.mapreduce.Mapper</code> that sets the map input types
        passed by <code class="literal">TableInputFormat</code>. Input keys are
          <code class="literal">ImmutableBytesWritable</code> objects <a class="calibre" id="calibre_link-2077"></a>(row keys), and values are <code class="literal">Result</code> objects (row results
        from a scan). Since this job counts rows and does not emit any output from the map, we just<a class="calibre" id="calibre_link-1275"></a> increment <code class="literal">Counters.ROWS</code> by 1 for every row we see.</p><p class="calibre2">In the <code class="literal">run()</code> method, we <a class="calibre" id="calibre_link-3589"></a>create a scan object that is used to configure the job by invoking the
          <code class="literal">TableMapReduceUtil.initTableMapJob()</code> utility method, which, among
        other things (such as setting the map class to use), sets the input format to
          <code class="literal">TableInputFormat</code>.</p><p class="calibre2">Notice how we set a filter, an <a class="calibre" id="calibre_link-1698"></a>instance of <code class="literal">FirstKeyOnlyFilter</code>, on the scan. This filter
        instructs the server to short-circuit when running server-side, populating the
          <code class="literal">Result</code> object in the mapper with only the first cell in each row.
        Since the mapper ignores the cell values, this is a useful <a class="calibre" id="calibre_link-2479"></a>optimization.</p><div class="note" title="Tip"><h3 class="title3">Tip</h3><p class="calibre2">You can also find the number of rows in a table by typing <span class="calibre"><code class="literal">count '<em class="replaceable"><code class="replaceable">tablename</code></em>'</code></span> in the
          HBase shell. It’s not distributed, though, so for large tables the MapReduce program is
          preferable.</p></div></div><div class="book" title="REST and Thrift"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4623">REST and Thrift</h3></div></div></div><p class="calibre2">HBase ships with <a class="calibre" id="calibre_link-3685"></a><a class="calibre" id="calibre_link-3237"></a>REST and Thrift interfaces. These are useful when the interacting application is
        written in a language other than Java. In both cases, a Java server hosts an instance of the
        HBase client brokering REST and Thrift application requests into and out of the HBase
        cluster. Consult the <a class="ulink" href="http://hbase.apache.org/book.html" target="_top">Reference Guide</a> for information on running the services, and the client
          <a class="calibre" id="calibre_link-1901"></a>interfaces.</p></div></div><div class="book" title="Building an Online Query Application"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4624">Building an Online Query Application</h2></div></div></div><p class="calibre2">Although HDFS and MapReduce <a class="calibre" id="calibre_link-1894"></a><a class="calibre" id="calibre_link-3110"></a>are powerful tools for processing batch operations over
    large datasets, they do not provide ways to read or write individual
    records efficiently. In this example, we’ll explore using HBase as the
    tool to fill this gap.</p><p class="calibre2">The existing weather dataset described in previous chapters contains
    observations for tens of thousands of stations over 100 years, and this
    data is growing without bound. In this example, we will build a simple
    online (as opposed to batch) interface that allows a user to navigate the
    different stations and page through their historical temperature
    observations in time order. We’ll build simple command-line Java
    applications for this, but it’s easy to see how the same techniques could
    be used to build a web application to do the same thing.</p><p class="calibre2">For the sake of this example, let us allow that the dataset is
    massive, that the observations run to the billions, and that the rate at
    which temperature updates arrive is significant—say, hundreds to thousands
    of updates per second from around the world and across the whole range of
    weather stations. Also, let us allow that it is a requirement that the
    online application must display the most up-to-date observation within a
    second or so of receipt.</p><p class="calibre2">The first size requirement should preclude our use of a simple RDBMS instance and make
      HBase a candidate store. The second latency requirement rules out plain HDFS. A MapReduce job
      could build initial indices that allowed random access over all of the observation data, but
      keeping up this index as the updates arrive is not what HDFS and MapReduce are good at.</p><div class="book" title="Schema Design"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4625">Schema Design</h3></div></div></div><p class="calibre2">In our example, there <a class="calibre" id="calibre_link-1899"></a><a class="calibre" id="calibre_link-3288"></a>will be two tables:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term"><code class="literal1">stations</code></span></dt><dd class="calibre8"><p class="calibre2">This table holds station data. Let the row key be the
            <code class="literal">stationid</code>. Let this table have a column family
            <code class="literal">info</code> that acts as a key-value
            dictionary for station information. Let the dictionary keys be the
            column names <code class="literal">info:name</code>,
            <code class="literal">info:location</code>, and <code class="literal">info:description</code>. This table is static,
            and in this case, the <code class="literal">info</code>
            family closely mirrors a typical RDBMS table design.</p></dd><dt class="calibre7"><span class="term"><code class="literal1">observations</code></span></dt><dd class="calibre8"><p class="calibre2">This table holds temperature observations. Let the row key
            be a composite key of <code class="literal">stationid</code> plus a
            reverse-order timestamp. Give this table a column family
            <code class="literal">data</code> that will contain one column,
            <code class="literal">airtemp</code>, with the observed temperature as the
            column value.</p></dd></dl></div><p class="calibre2">Our choice of schema is derived from knowing the most efficient
      way we can read from HBase. Rows and columns are stored in increasing
      lexicographical order. Though there are facilities for secondary
      indexing and regular expression matching, they come at a performance
      penalty. It is vital that you understand the most efficient way to query
      your data in order to choose the most effective setup for storing and
      accessing.</p><p class="calibre2">For the <code class="literal">stations</code> table, the choice of
      <code class="literal">stationid</code> as the key is obvious because we will
      always access information for a particular station by its ID. The
      <code class="literal">observations</code> table, however, uses a composite key
      that adds the observation timestamp at the end. This will group all
      observations for a particular station together, and by using a
      reverse-order timestamp (<code class="literal">Long.MAX_VALUE - timestamp</code>) and
      storing it as binary, observations for each station will be ordered with
      most recent observation first.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">We rely on the fact that station IDs are a fixed length. In some
        cases, you will need to zero-pad number components so row keys sort
        properly. Otherwise, you will run into the issue where 10 sorts before
        2, say, when only the byte order is considered (02 sorts before
          10).</p><p class="calibre2">Also, if your keys are integers, use a binary representation rather than persisting
          the string version of a number. The former consumes less space.</p></div><p class="calibre2">In the shell, define the tables as follows:</p><pre class="screen1"><code class="literal">hbase(main):001:0&gt;</code> <strong class="userinput"><code class="calibre9">create 'stations', {NAME =&gt; 'info'}</code></strong>
0 row(s) in 0.9600 seconds
<code class="literal">hbase(main):002:0&gt;</code> <strong class="userinput"><code class="calibre9">create 'observations', {NAME =&gt; 'data'}</code></strong>
0 row(s) in 0.1770 seconds</pre><div class="sidebar"><div class="sidebar-title">Wide Tables</div><p class="calibre2">All access in HBase is via primary key, <a class="calibre" id="calibre_link-3599"></a>so the key design should lend itself to how the data is
        going to be queried. One thing to keep in mind when designing schemas
        is that a defining attribute of <a class="ulink" href="http://en.wikipedia.org/wiki/Column-oriented_DBMS" target="_top">column(-family)-oriented
        stores</a>, such as HBase, is the ability to host wide and
        sparsely populated tables at no incurred cost.<sup class="calibre5">[<a class="firstname" href="#calibre_link-77" id="calibre_link-86">139</a>]</sup></p><p class="calibre2">There is no native database join facility in HBase, but wide
        tables can make it so that there is no need for database joins to pull
        from secondary or tertiary tables. A wide row can sometimes be made to
        hold all data that pertains to a particular primary key.</p></div></div><div class="book" title="Loading Data"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4626">Loading Data</h3></div></div></div><p class="calibre2">There are a relatively <a class="calibre" id="calibre_link-1895"></a>small number of stations, so their static data is easily inserted using any of
        the available interfaces. The example code includes a Java application for doing this, which
        is run as follows:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hbase HBaseStationImporter input/ncdc/metadata/stations-fixed-width.txt</code></strong></pre><p class="calibre2">However, let’s assume that there are billions of individual observations to be loaded.
        This kind of import is normally an extremely complex and long-running database operation,
        but MapReduce and HBase’s distribution model allow us to make full use of the cluster. We’ll
        copy the raw input data onto HDFS, and then run a MapReduce job that can read the input and
        write to HBase.</p><p class="calibre2"><a class="ulink" href="#calibre_link-78" title="Example&nbsp;20-3.&nbsp;A MapReduce application to import temperature data from HDFS into an HBase table">Example&nbsp;20-3</a> shows an example
      MapReduce job that imports observations to HBase from the same input
      files used in the previous chapters’ examples.</p><div class="example"><a id="calibre_link-78" class="calibre"></a><div class="example-title">Example&nbsp;20-3.&nbsp;A MapReduce application to import temperature data from HDFS
        into an HBase table</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">HBaseTemperatureImporter</code> <code class="k">extends</code> <code class="n">Configured</code> <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>
  
  <code class="k">static</code> <code class="k">class</code> <code class="nc">HBaseTemperatureMapper</code><code class="o">&lt;</code><code class="n">K</code><code class="o">&gt;</code> <code class="k">extends</code> <code class="n">Mapper</code><code class="o">&lt;</code><code class="n">LongWritable</code><code class="o">,</code> <code class="n">Text</code><code class="o">,</code> <code class="n">K</code><code class="o">,</code> <code class="n">Put</code><code class="o">&gt;</code> <code class="o">{</code>
    <code class="k">private</code> <code class="n">NcdcRecordParser</code> <code class="n">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="n">NcdcRecordParser</code><code class="o">();</code>

    <code class="nd">@Override</code>
    <code class="k">public</code> <code class="kt">void</code> <code class="nf">map</code><code class="o">(</code><code class="n">LongWritable</code> <code class="n">key</code><code class="o">,</code> <code class="n">Text</code> <code class="n">value</code><code class="o">,</code> <code class="n">Context</code> <code class="n">context</code><code class="o">)</code> <code class="k">throws</code>
        <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
      <code class="n">parser</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">value</code><code class="o">.</code><code class="na">toString</code><code class="o">());</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">isValidTemperature</code><code class="o">())</code> <code class="o">{</code>
        <code class="kt">byte</code><code class="o">[]</code> <code class="n">rowKey</code> <code class="o">=</code> <code class="n">RowKeyConverter</code><code class="o">.</code><code class="na">makeObservationRowKey</code><code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">getStationId</code><code class="o">(),</code>
            <code class="n">parser</code><code class="o">.</code><code class="na">getObservationDate</code><code class="o">().</code><code class="na">getTime</code><code class="o">());</code>
        <code class="n">Put</code> <code class="n">p</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Put</code><code class="o">(</code><code class="n">rowKey</code><code class="o">);</code>
        <code class="n">p</code><code class="o">.</code><code class="na">add</code><code class="o">(</code><code class="n">HBaseTemperatureQuery</code><code class="o">.</code><code class="na">DATA_COLUMNFAMILY</code><code class="o">,</code>
            <code class="n">HBaseTemperatureQuery</code><code class="o">.</code><code class="na">AIRTEMP_QUALIFIER</code><code class="o">,</code>
            <code class="n">Bytes</code><code class="o">.</code><code class="na">toBytes</code><code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">getAirTemperature</code><code class="o">()));</code>
        <code class="n">context</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="k">null</code><code class="o">,</code> <code class="n">p</code><code class="o">);</code>
      <code class="o">}</code>
    <code class="o">}</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="na">length</code> <code class="o">!=</code> <code class="mi">1</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="sb">"Usage: HBaseTemperatureImporter &lt;input&gt;"</code><code class="o">);</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>
    <code class="n">Job</code> <code class="n">job</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Job</code><code class="o">(</code><code class="n">getConf</code><code class="o">(),</code> <code class="n">getClass</code><code class="o">().</code><code class="na">getSimpleName</code><code class="o">());</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setJarByClass</code><code class="o">(</code><code class="n">getClass</code><code class="o">());</code>
    <code class="n">FileInputFormat</code><code class="o">.</code><code class="na">addInputPath</code><code class="o">(</code><code class="n">job</code><code class="o">,</code> <code class="k">new</code> <code class="n">Path</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]));</code>
    <code class="n">job</code><code class="o">.</code><code class="na">getConfiguration</code><code class="o">().</code><code class="na">set</code><code class="o">(</code><code class="n">TableOutputFormat</code><code class="o">.</code><code class="na">OUTPUT_TABLE</code><code class="o">,</code> <code class="sb">"observations"</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setMapperClass</code><code class="o">(</code><code class="n">HBaseTemperatureMapper</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setNumReduceTasks</code><code class="o">(</code><code class="mi">0</code><code class="o">);</code>
    <code class="n">job</code><code class="o">.</code><code class="na">setOutputFormatClass</code><code class="o">(</code><code class="n">TableOutputFormat</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>
    <code class="k">return</code> <code class="n">job</code><code class="o">.</code><code class="na">waitForCompletion</code><code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">?</code> <code class="mi">0</code> <code class="o">:</code> <code class="mi">1</code><code class="o">;</code>
  <code class="o">}</code>

  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="n">HBaseConfiguration</code><code class="o">.</code><code class="na">create</code><code class="o">(),</code>
        <code class="k">new</code> <code class="nf">HBaseTemperatureImporter</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2"><code class="literal">HBaseTemperatureImporter</code> has a nested class named
          <code class="literal">HBaseTemperatureMapper</code> that is like the
          <code class="literal">MaxTemperatureMapper</code> class from <a class="ulink" href="#calibre_link-79" title="Chapter&nbsp;6.&nbsp;Developing a MapReduce Application">Chapter&nbsp;6</a>. The
        outer class implements <code class="literal">Tool</code> and does the setup to launch the map-only
        job. <code class="literal">HBaseTemperatureMapper</code> takes the same input as
          <code class="literal">MaxTemperatureMapper</code> and does the same parsing—using the
          <code class="literal">NcdcRecordParser</code> introduced in <a class="ulink" href="#calibre_link-79" title="Chapter&nbsp;6.&nbsp;Developing a MapReduce Application">Chapter&nbsp;6</a>—to
        check for valid temperatures. But rather than writing valid temperatures to the output
        context, as <code class="literal">MaxTemperatureMapper</code> does, it creates a
          <code class="literal">Put</code> object to add those temperatures to the
          <code class="literal">observations</code> HBase table, in the <code class="literal">data:airtemp</code> column. (We are using static constants for <code class="literal">data</code>
        and <code class="literal">airtemp</code>, imported from the
          <code class="literal">HBaseTemperatureQuery</code> class described later.)</p><p class="calibre2">The row key for each observation is created in the
      <code class="literal">makeObservationRowKey()</code> method on
      <code class="literal">RowKeyConverter</code> from the station ID and
      observation time:</p><a id="calibre_link-4627" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">class</code> <code class="nc">RowKeyConverter</code> <code class="o">{</code>

  <code class="k">private</code> <code class="k">static</code> <code class="k">final</code> <code class="kt">int</code> <code class="n">STATION_ID_LENGTH</code> <code class="o">=</code> <code class="mi">12</code><code class="o">;</code>

  <code class="c2">/**</code>
<code class="c2">   * @return A row key whose format is: &lt;station_id&gt; &lt;reverse_order_timestamp&gt;</code>
<code class="c2">   */</code>
  <code class="k">public</code> <code class="k">static</code> <code class="kt">byte</code><code class="o">[]</code> <code class="nf">makeObservationRowKey</code><code class="o">(</code><code class="n">String</code> <code class="n">stationId</code><code class="o">,</code>
      <code class="kt">long</code> <code class="n">observationTime</code><code class="o">)</code> <code class="o">{</code>
    <code class="kt">byte</code><code class="o">[]</code> <code class="n">row</code> <code class="o">=</code> <code class="k">new</code> <code class="kt">byte</code><code class="o">[</code><code class="n">STATION_ID_LENGTH</code> <code class="o">+</code> <code class="n">Bytes</code><code class="o">.</code><code class="na">SIZEOF_LONG</code><code class="o">];</code>
    <code class="n">Bytes</code><code class="o">.</code><code class="na">putBytes</code><code class="o">(</code><code class="n">row</code><code class="o">,</code> <code class="mi">0</code><code class="o">,</code> <code class="n">Bytes</code><code class="o">.</code><code class="na">toBytes</code><code class="o">(</code><code class="n">stationId</code><code class="o">),</code> <code class="mi">0</code><code class="o">,</code> <code class="n">STATION_ID_LENGTH</code><code class="o">);</code>
    <code class="kt">long</code> <code class="n">reverseOrderTimestamp</code> <code class="o">=</code> <code class="n">Long</code><code class="o">.</code><code class="na">MAX_VALUE</code> <code class="o">-</code> <code class="n">observationTime</code><code class="o">;</code>
    <code class="n">Bytes</code><code class="o">.</code><code class="na">putLong</code><code class="o">(</code><code class="n">row</code><code class="o">,</code> <code class="n">STATION_ID_LENGTH</code><code class="o">,</code> <code class="n">reverseOrderTimestamp</code><code class="o">);</code>
    <code class="k">return</code> <code class="n">row</code><code class="o">;</code>
  <code class="o">}</code>
<code class="o">}</code></pre><p class="calibre2">The conversion takes advantage of the fact that the station ID is a fixed-length ASCII
        string. Like in the earlier example, we use HBase’s <code class="literal">Bytes</code> class for
        converting between byte arrays and common Java types. The
          <code class="literal">Bytes.SIZEOF_LONG</code> constant is used for calculating the size of the
        timestamp portion of the row key byte array. The <code class="literal">putBytes()</code> and
          <code class="literal">putLong()</code> methods are used to fill the station ID and timestamp
        portions of the key at the relevant offsets in the byte array.</p><p class="calibre2">The job is configured in the <code class="literal">run()</code> method to use HBase’s
          <code class="literal">TableOutputFormat</code>. The table to write to must be specified by
        setting the <code class="literal">TableOutputFormat.OUTPUT_TABLE</code> property in
        the job configuration.</p><p class="calibre2">It’s convenient to use <code class="literal">TableOutputFormat</code>
      since it manages the creation of an <code class="literal">HTable</code>
      instance for us, which otherwise we would do in the mapper’s
      <code class="literal">setup()</code> method (along with a call to
      <code class="literal">close()</code> in the
      <code class="literal">cleanup()</code> method).
      <code class="literal">TableOutputFormat</code> also disables the
      <code class="literal">HTable</code> auto-flush feature, so that calls to
        <code class="literal">put()</code> are buffered for greater efficiency.</p><p class="calibre2">The example code includes a class called <code class="literal">HBaseTemperatureDirectImporter</code> to demonstrate how to use an <code class="literal">HTable</code> directly from a MapReduce program. We can run the program with the following:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hbase HBaseTemperatureImporter input/ncdc/all</code></strong></pre><div class="book" title="Load distribution"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4628">Load distribution</h4></div></div></div><p class="calibre2">Watch for the phenomenon where an import walks in lockstep
        through the table, with all clients in concert pounding one of the
        table’s regions (and thus, a single node), then moving on to the next,
        and so on, rather than evenly distributing the load over all regions.
        This is usually brought on by some interaction between sorted input
        and how the splitter works. Randomizing the ordering of your row keys
        prior to insertion may help. In our example, given the distribution of
        <code class="literal">stationid</code> values and how
        <code class="literal">TextInputFormat</code> makes splits, the upload should
        be sufficiently distributed.</p><p class="calibre2">If a table is new, it will have only one region, and all updates will be to this single region until it splits. This will
        happen even if row keys are randomly distributed. This startup
        phenomenon means uploads run slowly at first, until there are
        sufficient regions distributed so all cluster members are able to
        participate in the uploads. Do not confuse this phenomenon with that
        noted in the previous paragraph.</p><p class="calibre2">Both of these problems can be avoided by using bulk loads,
        discussed next.</p></div><div class="book" title="Bulk load"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4629">Bulk load</h4></div></div></div><p class="calibre2">HBase has an efficient facility for <a class="calibre" id="calibre_link-1032"></a>bulk loading HBase by writing its internal data format
        directly into the filesystem from MapReduce. Going this route, it’s
        possible to load an HBase instance at rates that are an order of
        magnitude or more beyond those attainable by writing via the HBase
        client API.</p><p class="calibre2">Bulk loading is a two-step process. The <a class="calibre" id="calibre_link-1991"></a>first step uses
        <code class="literal">HFileOutputFormat2</code> to write HFiles to an HDFS
        directory using a MapReduce job. Since rows have to be written in
        order, the job must perform a total sort (see <a class="ulink" href="#calibre_link-80" title="Total Sort">Total Sort</a>) of the row keys. The
        <code class="literal">configureIncrementalLoad()</code> method of
        <code class="literal">HFileOutputFormat2</code> does all the necessary
        configuration for you.</p><p class="calibre2">The second step of the bulk load involves moving the HFiles from
        HDFS into an existing HBase table. The table can be live during this
        process. The example code includes a class called
        <code class="literal">HBaseTemperatureBulkImporter</code> for loading the
        observation data using a <a class="calibre" id="calibre_link-1896"></a>bulk load.</p></div></div><div class="book" title="Online Queries"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4630">Online Queries</h3></div></div></div><p class="calibre2">To implement the online <a class="calibre" id="calibre_link-1897"></a>query application, we will use the HBase Java API
      directly. Here it becomes clear how important your choice of schema and
      storage format is.</p><div class="book" title="Station queries"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4631">Station queries</h4></div></div></div><p class="calibre2">The simplest query will be to get the static station
        information. This is a single row lookup, performed using a
        <code class="literal">get()</code> operation. This type of query is simple in a
        traditional database, but HBase gives you additional control and
        flexibility. Using the <code class="literal">info</code> family as a key-value
        dictionary (column names as keys, column values as values), the code
        from <code class="literal">HBaseStationQuery</code> looks like this:</p><a id="calibre_link-4632" class="calibre"></a><pre class="screen1">  <code class="k">static</code> <code class="k">final</code> <code class="kt">byte</code><code class="o">[]</code> <code class="n">INFO_COLUMNFAMILY</code> <code class="o">=</code> <code class="n">Bytes</code><code class="o">.</code><code class="na">toBytes</code><code class="o">(</code><code class="sb">"info"</code><code class="o">);</code>
  <code class="k">static</code> <code class="k">final</code> <code class="kt">byte</code><code class="o">[]</code> <code class="n">NAME_QUALIFIER</code> <code class="o">=</code> <code class="n">Bytes</code><code class="o">.</code><code class="na">toBytes</code><code class="o">(</code><code class="sb">"name"</code><code class="o">);</code>
  <code class="k">static</code> <code class="k">final</code> <code class="kt">byte</code><code class="o">[]</code> <code class="n">LOCATION_QUALIFIER</code> <code class="o">=</code> <code class="n">Bytes</code><code class="o">.</code><code class="na">toBytes</code><code class="o">(</code><code class="sb">"location"</code><code class="o">);</code>
  <code class="k">static</code> <code class="k">final</code> <code class="kt">byte</code><code class="o">[]</code> <code class="n">DESCRIPTION_QUALIFIER</code> <code class="o">=</code> <code class="n">Bytes</code><code class="o">.</code><code class="na">toBytes</code><code class="o">(</code><code class="sb">"description"</code><code class="o">);</code>

  <code class="k">public</code> <code class="n">Map</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;</code> <code class="n">getStationInfo</code><code class="o">(</code><code class="n">HTable</code> <code class="n">table</code><code class="o">,</code> <code class="n">String</code> <code class="n">stationId</code><code class="o">)</code>
      <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="n">Get</code> <code class="n">get</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Get</code><code class="o">(</code><code class="n">Bytes</code><code class="o">.</code><code class="na">toBytes</code><code class="o">(</code><code class="n">stationId</code><code class="o">));</code>
    <code class="n">get</code><code class="o">.</code><code class="na">addFamily</code><code class="o">(</code><code class="n">INFO_COLUMNFAMILY</code><code class="o">);</code>
    <code class="n">Result</code> <code class="n">res</code> <code class="o">=</code> <code class="n">table</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="n">get</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">res</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">return</code> <code class="k">null</code><code class="o">;</code>
    <code class="o">}</code>
    <code class="n">Map</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;</code> <code class="n">resultMap</code> <code class="o">=</code> <code class="k">new</code> <code class="n">LinkedHashMap</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;();</code>
    <code class="n">resultMap</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="sb">"name"</code><code class="o">,</code> <code class="n">getValue</code><code class="o">(</code><code class="n">res</code><code class="o">,</code> <code class="n">INFO_COLUMNFAMILY</code><code class="o">,</code> <code class="n">NAME_QUALIFIER</code><code class="o">));</code>
    <code class="n">resultMap</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="sb">"location"</code><code class="o">,</code> <code class="n">getValue</code><code class="o">(</code><code class="n">res</code><code class="o">,</code> <code class="n">INFO_COLUMNFAMILY</code><code class="o">,</code>
        <code class="n">LOCATION_QUALIFIER</code><code class="o">));</code>
    <code class="n">resultMap</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="sb">"description"</code><code class="o">,</code> <code class="n">getValue</code><code class="o">(</code><code class="n">res</code><code class="o">,</code> <code class="n">INFO_COLUMNFAMILY</code><code class="o">,</code>
        <code class="n">DESCRIPTION_QUALIFIER</code><code class="o">));</code>
    <code class="k">return</code> <code class="n">resultMap</code><code class="o">;</code>
  <code class="o">}</code>

  <code class="k">private</code> <code class="k">static</code> <code class="n">String</code> <code class="nf">getValue</code><code class="o">(</code><code class="n">Result</code> <code class="n">res</code><code class="o">,</code> <code class="kt">byte</code><code class="o">[]</code> <code class="n">cf</code><code class="o">,</code> <code class="kt">byte</code><code class="o">[]</code> <code class="n">qualifier</code><code class="o">)</code> <code class="o">{</code>
    <code class="kt">byte</code><code class="o">[]</code> <code class="n">value</code> <code class="o">=</code> <code class="n">res</code><code class="o">.</code><code class="na">getValue</code><code class="o">(</code><code class="n">cf</code><code class="o">,</code> <code class="n">qualifier</code><code class="o">);</code>
    <code class="k">return</code> <code class="n">value</code> <code class="o">==</code> <code class="k">null</code><code class="o">?</code> <code class="sb">""</code><code class="o">:</code> <code class="n">Bytes</code><code class="o">.</code><code class="na">toString</code><code class="o">(</code><code class="n">value</code><code class="o">);</code>
  <code class="o">}</code></pre><p class="calibre2">In this example, <code class="literal">getStationInfo()</code> takes an
            <code class="literal">HTable</code> instance and a station ID. To get the station info, we use
            <code class="literal">get()</code>, passing a <code class="literal">Get</code> instance configured
          to retrieve all the column values for the row identified by the station ID in the defined
          column family, <code class="literal">INFO_</code><code class="literal">COLUMNFAMILY</code>.</p><p class="calibre2">The <code class="literal">get()</code> results are returned in a
        <code class="literal">Result</code>. It contains the row, and you can fetch
        cell values by stipulating the column cell you want. The
        <code class="literal">getStationInfo()</code> method converts the
        <code class="literal">Result</code> into a more friendly
        <code class="literal">Map</code> of <code class="literal">String</code> keys and
        values.</p><p class="calibre2">We can already see how there is a need for utility functions
        when using HBase. There are an increasing number of abstractions being
        built atop HBase to deal with this low-level interaction, but it’s
        important to understand how this works and how storage choices make a
        difference.</p><p class="calibre2">One of the strengths of HBase over a relational database is that you don’t have to
          specify all the columns up front. So, if each station now has at least these three attributes but there are hundreds of optional ones, in the
          future we can just insert them without modifying the schema. (Our application’s reading
          and writing code would, of course, need to be changed. The example code might change in
          this case to looping through <code class="literal">Result</code> rather than grabbing each value
          explicitly.)</p><p class="calibre2">Here’s an example of a station query:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hbase HBaseStationQuery 011990-99999</code></strong>
name    SIHCCAJAVRI                  
location        (unknown)
description     (unknown)</pre></div><div class="book" title="Observation queries"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4633">Observation queries</h4></div></div></div><p class="calibre2">Queries of the <code class="literal">observations</code> table take the form of a station ID, a
          start time, and a maximum number of rows to return. Since the rows are stored in reverse
          chronological order by station, queries will return observations that preceded the start
          time. The <code class="literal">getStationObservations()</code> method in <a class="ulink" href="#calibre_link-81" title="Example&nbsp;20-4.&nbsp;An application for retrieving a range of rows of weather station observations from an HBase table">Example&nbsp;20-4</a> uses an HBase scanner to iterate over the table rows.
          It returns a <code class="literal">NavigableMap&lt;Long, Integer&gt;</code>, where the key is
          the timestamp and the value is the temperature. Since the map sorts by key in ascending
          order, its entries are in chronological order.</p><div class="example"><a id="calibre_link-81" class="calibre"></a><div class="example-title">Example&nbsp;20-4.&nbsp;An application for retrieving a range of rows of weather
          station observations from an HBase table</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">HBaseTemperatureQuery</code> <code class="k">extends</code> <code class="n">Configured</code> <code class="k">implements</code> <code class="n">Tool</code> <code class="o">{</code>
  <code class="k">static</code> <code class="k">final</code> <code class="kt">byte</code><code class="o">[]</code> <code class="n">DATA_COLUMNFAMILY</code> <code class="o">=</code> <code class="n">Bytes</code><code class="o">.</code><code class="na">toBytes</code><code class="o">(</code><code class="sb">"data"</code><code class="o">);</code>
  <code class="k">static</code> <code class="k">final</code> <code class="kt">byte</code><code class="o">[]</code> <code class="n">AIRTEMP_QUALIFIER</code> <code class="o">=</code> <code class="n">Bytes</code><code class="o">.</code><code class="na">toBytes</code><code class="o">(</code><code class="sb">"airtemp"</code><code class="o">);</code>
  
  <code class="k">public</code> <code class="n">NavigableMap</code><code class="o">&lt;</code><code class="n">Long</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;</code> <code class="n">getStationObservations</code><code class="o">(</code><code class="n">HTable</code> <code class="n">table</code><code class="o">,</code>
      <code class="n">String</code> <code class="n">stationId</code><code class="o">,</code> <code class="kt">long</code> <code class="n">maxStamp</code><code class="o">,</code> <code class="kt">int</code> <code class="n">maxCount</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="kt">byte</code><code class="o">[]</code> <code class="n">startRow</code> <code class="o">=</code> <code class="n">RowKeyConverter</code><code class="o">.</code><code class="na">makeObservationRowKey</code><code class="o">(</code><code class="n">stationId</code><code class="o">,</code> <code class="n">maxStamp</code><code class="o">);</code>
    <code class="n">NavigableMap</code><code class="o">&lt;</code><code class="n">Long</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;</code> <code class="n">resultMap</code> <code class="o">=</code> <code class="k">new</code> <code class="n">TreeMap</code><code class="o">&lt;</code><code class="n">Long</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;();</code>
    <code class="n">Scan</code> <code class="n">scan</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Scan</code><code class="o">(</code><code class="n">startRow</code><code class="o">);</code>
    <code class="n">scan</code><code class="o">.</code><code class="na">addColumn</code><code class="o">(</code><code class="n">DATA_COLUMNFAMILY</code><code class="o">,</code> <code class="n">AIRTEMP_QUALIFIER</code><code class="o">);</code>
    <code class="n">ResultScanner</code> <code class="n">scanner</code> <code class="o">=</code> <code class="n">table</code><code class="o">.</code><code class="na">getScanner</code><code class="o">(</code><code class="n">scan</code><code class="o">);</code>
    <code class="k">try</code> <code class="o">{</code>
      <code class="n">Result</code> <code class="n">res</code><code class="o">;</code>
      <code class="kt">int</code> <code class="n">count</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code>
      <code class="k">while</code> <code class="o">((</code><code class="n">res</code> <code class="o">=</code> <code class="n">scanner</code><code class="o">.</code><code class="na">next</code><code class="o">())</code> <code class="o">!=</code> <code class="k">null</code> <code class="o">&amp;&amp;</code> <code class="n">count</code><code class="o">++</code> <code class="o">&lt;</code> <code class="n">maxCount</code><code class="o">)</code> <code class="o">{</code>
        <code class="kt">byte</code><code class="o">[]</code> <code class="n">row</code> <code class="o">=</code> <code class="n">res</code><code class="o">.</code><code class="na">getRow</code><code class="o">();</code>
        <code class="kt">byte</code><code class="o">[]</code> <code class="n">value</code> <code class="o">=</code> <code class="n">res</code><code class="o">.</code><code class="na">getValue</code><code class="o">(</code><code class="n">DATA_COLUMNFAMILY</code><code class="o">,</code> <code class="n">AIRTEMP_QUALIFIER</code><code class="o">);</code>
        <code class="n">Long</code> <code class="n">stamp</code> <code class="o">=</code> <code class="n">Long</code><code class="o">.</code><code class="na">MAX_VALUE</code> <code class="o">-</code>
            <code class="n">Bytes</code><code class="o">.</code><code class="na">toLong</code><code class="o">(</code><code class="n">row</code><code class="o">,</code> <code class="n">row</code><code class="o">.</code><code class="na">length</code> <code class="o">-</code> <code class="n">Bytes</code><code class="o">.</code><code class="na">SIZEOF_LONG</code><code class="o">,</code> <code class="n">Bytes</code><code class="o">.</code><code class="na">SIZEOF_LONG</code><code class="o">);</code>
        <code class="n">Integer</code> <code class="n">temp</code> <code class="o">=</code> <code class="n">Bytes</code><code class="o">.</code><code class="na">toInt</code><code class="o">(</code><code class="n">value</code><code class="o">);</code>
        <code class="n">resultMap</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="n">stamp</code><code class="o">,</code> <code class="n">temp</code><code class="o">);</code>
      <code class="o">}</code>
    <code class="o">}</code> <code class="k">finally</code> <code class="o">{</code>
      <code class="n">scanner</code><code class="o">.</code><code class="na">close</code><code class="o">();</code>
    <code class="o">}</code>
    <code class="k">return</code> <code class="n">resultMap</code><code class="o">;</code>
  <code class="o">}</code>

  <code class="k">public</code> <code class="kt">int</code> <code class="nf">run</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="na">length</code> <code class="o">!=</code> <code class="mi">1</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="sb">"Usage: HBaseTemperatureQuery &lt;station_id&gt;"</code><code class="o">);</code>
      <code class="k">return</code> <code class="o">-</code><code class="mi">1</code><code class="o">;</code>
    <code class="o">}</code>
    
    <code class="n">HTable</code> <code class="n">table</code> <code class="o">=</code> <code class="k">new</code> <code class="n">HTable</code><code class="o">(</code><code class="n">HBaseConfiguration</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">getConf</code><code class="o">()),</code> <code class="sb">"observations"</code><code class="o">);</code>
    <code class="k">try</code> <code class="o">{</code>
      <code class="n">NavigableMap</code><code class="o">&lt;</code><code class="n">Long</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;</code> <code class="n">observations</code> <code class="o">=</code>
          <code class="n">getStationObservations</code><code class="o">(</code><code class="n">table</code><code class="o">,</code> <code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">],</code> <code class="n">Long</code><code class="o">.</code><code class="na">MAX_VALUE</code><code class="o">,</code> <code class="mi">10</code><code class="o">).</code><code class="na">descendingMap</code><code class="o">();</code>
      <code class="k">for</code> <code class="o">(</code><code class="n">Map</code><code class="o">.</code><code class="na">Entry</code><code class="o">&lt;</code><code class="n">Long</code><code class="o">,</code> <code class="n">Integer</code><code class="o">&gt;</code> <code class="n">observation</code> <code class="o">:</code> <code class="n">observations</code><code class="o">.</code><code class="na">entrySet</code><code class="o">())</code> <code class="o">{</code>
        <code class="c2">// Print the date, time, and temperature</code>
        <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"%1$tF %1$tR\t%2$s\n"</code><code class="o">,</code> <code class="n">observation</code><code class="o">.</code><code class="na">getKey</code><code class="o">(),</code>
            <code class="n">observation</code><code class="o">.</code><code class="na">getValue</code><code class="o">());</code>
      <code class="o">}</code>
      <code class="k">return</code> <code class="mi">0</code><code class="o">;</code>
    <code class="o">}</code> <code class="k">finally</code> <code class="o">{</code>
      <code class="n">table</code><code class="o">.</code><code class="na">close</code><code class="o">();</code>
    <code class="o">}</code>
  <code class="o">}</code>

  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">exitCode</code> <code class="o">=</code> <code class="n">ToolRunner</code><code class="o">.</code><code class="na">run</code><code class="o">(</code><code class="n">HBaseConfiguration</code><code class="o">.</code><code class="na">create</code><code class="o">(),</code>
        <code class="k">new</code> <code class="nf">HBaseTemperatureQuery</code><code class="o">(),</code> <code class="n">args</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="n">exitCode</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The <code class="literal">run()</code> method calls
        <code class="literal">getStationObservations()</code>, asking for the 10
        most recent observations, which it turns back into descending order by
        calling <code class="literal">descendingMap()</code>. The observations are
        formatted and printed to the console (remember that the temperatures
        are in tenths of a degree). For example:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hbase HBaseTemperatureQuery 011990-99999</code></strong>
1902-12-31 20:00    -106
1902-12-31 13:00    -83
1902-12-30 20:00    -78
1902-12-30 13:00    -100
1902-12-29 20:00    -128
1902-12-29 13:00    -111
1902-12-29 06:00    -111
1902-12-28 20:00    -117
1902-12-28 13:00    -61
1902-12-27 20:00    -22</pre><p class="calibre2">The advantage of storing timestamps in reverse chronological order is that it lets us
          get the newest observations, which is often what we want in online applications. If the
          observations were stored with the actual timestamps, we would be able to get only the
          oldest observations for a given offset and limit efficiently. Getting the newest would
          mean getting all of the rows and then grabbing the newest off the end. It’s much more
          efficient to get the first <span class="calibre">n</span> rows, then exit the
          scanner (this is sometimes called an “early-out” <a class="calibre" id="calibre_link-3111"></a><a class="calibre" id="calibre_link-1898"></a>scenario).</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">HBase 0.98 added the ability to do reverse scans, which means
          it is now possible to store observations in chronological order and
          scan backward from a given starting row. Reverse scans are a few
          percent slower than forward scans. To reverse a scan, call <code class="literal">setReversed(true)</code> on the
          <code class="literal">Scan</code> object before starting the scan.</p></div></div></div></div><div class="book" title="HBase Versus RDBMS"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4634">HBase Versus RDBMS</h2></div></div></div><p class="calibre2">HBase and other <a class="calibre" id="calibre_link-1912"></a><a class="calibre" id="calibre_link-3141"></a>column-oriented databases are often compared to more traditional and popular
      relational databases, or RDBMSs. Although they differ dramatically in their implementations
      and in what they set out to accomplish, the fact that they are potential solutions to the same
      problems means that despite their enormous differences, the comparison is a fair one to
      make.</p><p class="calibre2">As described previously, HBase is a distributed, column-oriented data storage system. It
      picks up where Hadoop left off by providing random reads and writes on top of HDFS. It has
      been designed from the ground up with a focus on scale in every direction: tall in numbers of
      rows (billions), wide in numbers of columns (millions), and able to be horizontally
      partitioned and replicated across thousands of commodity nodes automatically. The table
      schemas mirror the physical storage, creating a system for efficient data structure
      serialization, storage, and retrieval. The burden is on the application developer to make use
      of this storage and retrieval in the right way.</p><p class="calibre2">Strictly speaking, an RDBMS is a database that follows <a class="ulink" href="http://en.wikipedia.org/wiki/Codd%27s_12_rules" target="_top">Codd’s 12 rules</a>. Typical RDBMSs
      are fixed-schema, row-oriented databases with ACID
      properties and a sophisticated SQL query engine. The emphasis is on strong consistency,
      referential integrity, abstraction from the physical layer, and complex queries through the
      SQL language. You can easily create secondary indexes; perform complex inner and outer joins;
      and count, sum, sort, group, and page your data across a number of tables, rows, and
      columns.</p><p class="calibre2">For a majority of small- to medium-volume applications, there is no substitute for the
      ease of use, flexibility, maturity, and powerful feature set of available open source RDBMS
      solutions such as MySQL and PostgreSQL. However, if you need to scale up in terms of dataset
      size, read/write concurrency, or both, you’ll soon find that the conveniences of an RDBMS come
      at an enormous performance penalty and make distribution inherently difficult. The scaling of
      an RDBMS usually involves breaking Codd’s rules, loosening ACID restrictions, forgetting
      conventional DBA wisdom, and, on the way, losing most of the desirable properties that made
      relational databases so convenient in the first place.</p><div class="book" title="Successful Service"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4635">Successful Service</h3></div></div></div><p class="calibre2">Here is a synopsis of how the typical RDBMS scaling story runs.
      The following list presumes a successful growing service:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Initial public launch</span></dt><dd class="calibre8"><p class="calibre2">Move from local workstation to a shared, remotely hosted MySQL instance with a
              well-defined schema.</p></dd><dt class="calibre7"><span class="term">Service becomes more popular; too many reads hitting the
          database</span></dt><dd class="calibre8"><p class="calibre2">Add memcached to cache common queries. Reads are now no
            longer strictly ACID; cached data must expire.</p></dd><dt class="calibre7"><span class="term">Service continues to grow in popularity; too many writes
          hitting the database</span></dt><dd class="calibre8"><p class="calibre2">Scale MySQL vertically by buying a beefed-up server with 16 cores, 128 GB of RAM,
              and banks of 15k RPM hard drives. Costly.</p></dd><dt class="calibre7"><span class="term">New features increase query complexity; now we have too many joins</span></dt><dd class="calibre8"><p class="calibre2">Denormalize your data to reduce joins. (That’s not what they
            taught me in DBA school!)</p></dd><dt class="calibre7"><span class="term">Rising popularity swamps the server; things are too
          slow</span></dt><dd class="calibre8"><p class="calibre2">Stop doing any server-side computations.</p></dd><dt class="calibre7"><span class="term">Some queries are still too slow</span></dt><dd class="calibre8"><p class="calibre2">Periodically prematerialize the most complex queries, and
            try to stop joining in most cases.</p></dd><dt class="calibre7"><span class="term">Reads are OK, but writes are getting slower and slower</span></dt><dd class="calibre8"><p class="calibre2">Drop secondary indexes and triggers (no indexes?).</p></dd></dl></div><p class="calibre2">At this point, there are no clear solutions for how to solve your
      scaling problems. In any case, you’ll need to begin to scale
      horizontally. You can attempt to build some type of partitioning on your
      largest tables, or look into some of the commercial solutions that
      provide multiple master capabilities.</p><p class="calibre2">Countless applications, businesses, and websites have successfully
      achieved scalable, fault-tolerant, and distributed data systems built on
      top of RDBMSs and are likely using many of the previous strategies. But
      what you end up with is something that is no longer a true RDBMS,
      sacrificing features and conveniences for compromises and complexities.
      Any form of slave replication or external caching introduces weak
      consistency into your now denormalized data. The inefficiency of joins
      and secondary indexes means almost all queries become primary key
      lookups. A multiwriter setup likely means no real joins at all, and
      distributed transactions are a nightmare. There’s now an incredibly
      complex network topology to manage with an entirely separate cluster for
      caching. Even with this system and the compromises made, you will still
      worry about your primary master crashing and the daunting possibility of
      having 10 times the data and 10 times the load in a few months.</p></div><div class="book" title="HBase"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4636">HBase</h3></div></div></div><p class="calibre2">Enter HBase, which <a class="calibre" id="calibre_link-1892"></a>has the following characteristics:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">No real indexes</span></dt><dd class="calibre8"><p class="calibre2">Rows are stored sequentially, as are the columns within each
            row. Therefore, no issues with index bloat, and insert performance
            is independent of table size.</p></dd><dt class="calibre7"><span class="term">Automatic partitioning</span></dt><dd class="calibre8"><p class="calibre2">As your tables grow, they will automatically be split into
            regions and distributed across all available nodes.</p></dd><dt class="calibre7"><span class="term">Scale linearly and automatically with new nodes</span></dt><dd class="calibre8"><p class="calibre2">Add a node, point it to the existing cluster, and run the
            regionserver. Regions will automatically rebalance, and load will
            spread evenly.</p></dd><dt class="calibre7"><span class="term">Commodity hardware</span></dt><dd class="calibre8"><p class="calibre2">Clusters are built on $1,000–$5,000 nodes rather than
            $50,000 nodes. RDBMSs are I/O hungry, requiring more costly
            hardware.</p></dd><dt class="calibre7"><span class="term">Fault tolerance</span></dt><dd class="calibre8"><p class="calibre2">Lots of nodes means each is relatively insignificant. No
            need to worry about individual node downtime.</p></dd><dt class="calibre7"><span class="term">Batch processing</span></dt><dd class="calibre8"><p class="calibre2">MapReduce integration allows fully parallel, distributed
            jobs against your data with locality awareness.</p></dd></dl></div><p class="calibre2">If you stay up at night worrying about your database (uptime,
      scale, or speed), you should seriously consider making a jump from the
      RDBMS world to HBase. Use a solution that was intended to scale rather
      than a solution based on stripping down and throwing money at what used
      to work. With HBase, the software is free, the hardware is cheap, and
      the distribution is <a class="calibre" id="calibre_link-1913"></a><a class="calibre" id="calibre_link-3142"></a>intrinsic.</p></div></div><div class="book" title="Praxis"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4637">Praxis</h2></div></div></div><p class="calibre2">In this section, we discuss <a class="calibre" id="calibre_link-1902"></a>some of the common issues users run into when running an
    HBase cluster under load.</p><div class="book" title="HDFS"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4638">HDFS</h3></div></div></div><p class="calibre2">HBase’s use of HDFS is <a class="calibre" id="calibre_link-1945"></a>very different from how it’s used by MapReduce. In
      MapReduce, generally, HDFS files are opened with their content streamed
      through a map task and then closed. In HBase, datafiles are opened on
      cluster startup and kept open so that we avoid paying the costs
      associated with opening files on each access. Because of this, HBase
      tends to see issues not normally encountered by MapReduce
      clients:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Running out of file descriptors</span></dt><dd class="calibre8"><p class="calibre2">Because we keep files open, on a loaded cluster it doesn’t
            take long before we run into system- and Hadoop-imposed limits.
            For instance, say we have a cluster that has three nodes, each
            running an instance of a datanode and a regionserver, and we’re
            running an upload into a table that is currently at 100 regions
            and 10 column families. Allow that each column family has on
            average two flush files. Doing the math, we can have 100 × 10 × 2,
            or 2,000, files open at any one time. Add to this total other
            miscellaneous descriptors consumed by outstanding scanners and
            Java libraries. Each open file consumes at least one descriptor
            over on the remote datanode.</p><p class="calibre2">The default limit on the number of file descriptors per process is 1,024. When we
              exceed the filesystem <em class="calibre10">ulimit</em>, we’ll see the
              complaint about “Too many open files” in logs, but often we’ll first see indeterminate
              behavior in HBase. The fix requires increasing the file descriptor ulimit count;
              10,240 is a common setting. Consult the <a class="ulink" href="http://hbase.apache.org/book.html" target="_top">HBase Reference Guide</a> for how to increase the
              ulimit on your cluster.</p></dd><dt class="calibre7"><span class="term">Running out of datanode threads</span></dt><dd class="calibre8"><p class="calibre2">Similarly, the <a class="calibre" id="calibre_link-1387"></a>Hadoop datanode has an upper bound on the number of threads it can run at
              any one time. Hadoop 1 had a low default of 256 for this setting
                (<code class="literal">dfs.datanode.max.xcievers</code>), which would cause HBase to behave
              erratically. Hadoop 2 increased the default to 4,096, so you are much less likely to
              see a problem for recent versions of HBase (which only run on Hadoop 2 and later). You
              can change the setting by configuring <code class="literal">dfs.datanode.max.transfer.threads</code> (the new name for this property) in
                <em class="calibre10">hdfs-site.xml</em>.</p></dd></dl></div></div><div class="book" title="UI"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4639">UI</h3></div></div></div><p class="calibre2">HBase runs a web server on the master to present a view on the
      state of your running cluster. By default, it listens on port 60010. The
      master UI displays a list of basic attributes such as software versions,
      cluster load, request rates, lists of cluster tables, and participating
      regionservers. Click on a regionserver in the master UI, and you are
      taken to the web server running on the individual regionserver. It lists
      the regions this server is carrying and basic metrics such as resources
      consumed and request rates.</p></div><div class="book" title="Metrics"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4640">Metrics</h3></div></div></div><p class="calibre2">Hadoop has a <a class="calibre" id="calibre_link-2693"></a>metrics system that can be used to emit vitals over a
      period to a <span class="calibre"><em class="calibre10">context</em></span> (this is covered in <a class="ulink" href="#calibre_link-33" title="Metrics and JMX">Metrics and JMX</a>). Enabling Hadoop metrics, and in particular
      tying them to Ganglia or emitting them via JMX, will give you views on
      what is happening on your cluster, both currently and in the recent
      past. HBase also adds metrics of its own—request rates, counts of
      vitals, resources used. See the file <em class="calibre10">hadoop-metrics2-hbase.properties</em> under the
      HBase <em class="calibre10">conf</em> directory.</p></div><div class="book" title="Counters"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4641">Counters</h3></div></div></div><p class="calibre2">At <a class="ulink" href="https://www.stumbleupon.com/" target="_top">StumbleUpon</a>, the <a class="calibre" id="calibre_link-1267"></a>first production feature deployed on HBase was keeping counters for the
          <span class="calibre">stumbleupon.com</span> frontend. Counters were previously kept
        in MySQL, but the rate of change was such that drops were frequent, and the load imposed by
        the counter writes was such that web designers self imposed limits on what was counted.
        Using the <code class="literal">incrementColumnValue()</code> method on
          <code class="literal">HTable</code>, counters can be incremented many thousands of times a
          <a class="calibre" id="calibre_link-1903"></a>second.</p></div></div><div class="book" title="Further Reading"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4642">Further Reading</h2></div></div></div><p class="calibre2">In this chapter, <a class="calibre" id="calibre_link-1893"></a>we only scratched the surface of what’s possible with HBase. For more in-depth
      information, consult the <a class="ulink" href="http://hbase.apache.org/book.html" target="_top">project’s Reference
        Guide</a>, <span class="calibre"><a class="ulink" href="http://shop.oreilly.com/product/0636920014348.do" target="_top">HBase: The Definitive
        Guide</a></span> by Lars George (O’Reilly, 2011, new edition forthcoming), or
        <span class="calibre"><a class="ulink" href="http://www.manning.com/dimidukkhurana/" target="_top">HBase in
          Action</a></span> by Nick<a class="calibre" id="calibre_link-3517"></a><a class="calibre" id="calibre_link-1807"></a> Dimiduk and Amandeep Khurana (Manning, 2012).</p></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" id="calibre_link-69"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-82">136</a>] </sup>But see the Apache Phoenix project, mentioned in <a class="ulink" href="#calibre_link-83" title="SQL-on-Hadoop Alternatives">SQL-on-Hadoop Alternatives</a>, and <a class="ulink" href="https://wiki.trafodion.org/" target="_top">Trafodion</a>, a transactional SQL database built on HBase.</p></div><div class="footnote" id="calibre_link-70"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-84">137</a>] </sup>Fay Chang et al., <a class="ulink" href="http://research.google.com/archive/bigtable.html" target="_top">“Bigtable: A
        Distributed Storage System for Structured Data,”</a> November 2006.</p></div><div class="footnote" id="calibre_link-72"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-85">138</a>] </sup>HBase doesn’t support indexing of other columns in the table
            (also known as <span class="calibre"><em class="calibre10">secondary indexes</em></span>). However, there are several
          strategies for supporting the types of query that secondary indexes
          provide, each with different trade-offs between storage space,
          processing load, and query execution time; see the <a class="ulink" href="http://hbase.apache.org/book.html" target="_top">HBase Reference Guide</a>
          for a discussion.</p></div><div class="footnote" id="calibre_link-77"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-86">139</a>] </sup>See Daniel J. Abadi, <a class="ulink" href="http://bit.ly/column-stores" target="_top">“Column-Stores for
                Wide and Sparse Data,”</a> January 2007.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-74"><section type="chapter" id="calibre_link-4643" title="Chapter&nbsp;21.&nbsp;ZooKeeper"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;21.&nbsp;ZooKeeper</h2></div></div></div><p class="calibre2">So far in this book, we have been studying <a class="calibre" id="calibre_link-3927"></a>large-scale data processing. This chapter is different: it is
  about building general distributed applications using Hadoop’s distributed
  coordination service, called ZooKeeper.</p><p class="calibre2">Writing distributed applications is hard. It’s hard primarily because
  of partial failure. When a message is sent across the network between two
  nodes and the network fails, the sender does not know whether the receiver
  got the message. It may have gotten through before the network failed, or it
  may not have. Or perhaps the receiver’s process died. The only way that the
  sender can find out what happened is to reconnect to the receiver and ask
  it. This is partial failure: when we don’t even know if an operation
  failed.</p><p class="calibre2">ZooKeeper can’t make partial failures go away, since they are
  intrinsic to distributed systems. It certainly does not hide partial
  failures, either.<sup class="calibre6">[<a class="firstname" href="#calibre_link-89" id="calibre_link-121">140</a>]</sup> But what ZooKeeper does do is give you a set of tools to build
  distributed applications that can safely handle partial failures.</p><p class="calibre2">ZooKeeper also has the following characteristics:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">ZooKeeper is simple</span></dt><dd class="calibre8"><p class="calibre2">ZooKeeper is, at its core, a stripped-down filesystem that exposes a few simple
          operations and some extra abstractions, such as ordering and notifications.</p></dd><dt class="calibre7"><span class="term">ZooKeeper is expressive</span></dt><dd class="calibre8"><p class="calibre2">The ZooKeeper primitives are a rich set of building blocks that can be used to build a
          large class of coordination data structures and protocols. Examples include distributed
          queues, distributed locks, and leader election among a group of peers.</p></dd><dt class="calibre7"><span class="term">ZooKeeper is highly available</span></dt><dd class="calibre8"><p class="calibre2">ZooKeeper runs on a collection of machines and is designed to be
        highly available, so applications can depend on it. ZooKeeper can help
        you avoid introducing single points of failure into your system, so
        you can build a reliable application.</p></dd><dt class="calibre7"><span class="term">ZooKeeper facilitates loosely coupled interactions</span></dt><dd class="calibre8"><p class="calibre2">ZooKeeper interactions support participants that do not need to know about one
          another. For example, ZooKeeper can be used as a rendezvous mechanism so that processes
          that otherwise don’t know of each other’s existence (or network details) can discover and
          interact with one another. Coordinating parties may not even be contemporaneous, since one
          process may leave a message in ZooKeeper that is read by another after the first has shut
          down.</p></dd><dt class="calibre7"><span class="term">ZooKeeper is a library</span></dt><dd class="calibre8"><p class="calibre2">ZooKeeper provides an open source, shared repository of implementations and recipes of
          common coordination patterns. Individual programmers are spared the burden of writing
          common protocols themselves (which is often difficult to get right). Over time, the
          community can add to and improve the libraries, which is to everyone’s benefit.</p></dd></dl></div><p class="calibre2">ZooKeeper is highly performant, too. At Yahoo!, where it was created,
  the throughput for a ZooKeeper cluster has been benchmarked at over 10,000
  operations per second for write-dominant workloads generated by hundreds of
  clients. For workloads where reads dominate, which is the norm, the
  throughput is several times higher.<sup class="calibre6">[<a class="firstname" href="#calibre_link-90" id="calibre_link-122">141</a>]</sup></p><div class="book" title="Installing and Running ZooKeeper"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4644">Installing and Running ZooKeeper</h2></div></div></div><p class="calibre2">When trying out ZooKeeper <a class="calibre" id="calibre_link-3945"></a>for the first time, it’s simplest to run it in standalone
    mode with a single ZooKeeper server. You can do this on a development
    machine, for example. ZooKeeper requires Java to run, so make sure you
    have it installed first.</p><p class="calibre2">Download a stable release of ZooKeeper from the <a class="ulink" href="http://zookeeper.apache.org/releases.html" target="_top">Apache ZooKeeper releases
    page</a>, and unpack the tarball in a suitable location:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">tar xzf zookeeper-<em class="replaceable1"><code class="calibre46">x.y.z</code></em>.tar.gz</code></strong></pre><p class="calibre2">ZooKeeper provides a few binaries to run and interact with the
    service, and it’s convenient to put the directory containing the binaries
    on your command-line path:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">export ZOOKEEPER_HOME=~/sw/zookeeper-<em class="replaceable1"><code class="calibre46">x.y.z</code></em></code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">export PATH=$PATH:$ZOOKEEPER_HOME/bin</code></strong></pre><p class="calibre2">Before running the ZooKeeper service, we need to set up a
    configuration file. The configuration file is conventionally called
    <em class="calibre10">zoo.cfg</em> and placed in the <em class="calibre10">conf</em> subdirectory (although you can also place
    it in <em class="calibre10">/etc/zookeeper</em>, or in the
    directory defined by <a class="calibre" id="calibre_link-3926"></a>the <code class="literal">ZOOCFGDIR</code> environment variable, if set).
    Here’s an example:</p><pre class="screen1"><code class="na">tickTime</code><code class="o">=</code><code class="sb">2000</code>
<code class="na">dataDir</code><code class="o">=</code><code class="sb">/Users/tom/zookeeper</code>
<code class="na">clientPort</code><code class="o">=</code><code class="sb">2181</code></pre><p class="calibre2">This is a standard Java properties file, and the three properties defined in this example
      are the minimum required for running ZooKeeper in standalone mode. Briefly, <code class="literal">tickTime</code> is the basic time unit in ZooKeeper
      (specified in milliseconds), <code class="literal">dataDir</code> is the local
      filesystem location where ZooKeeper stores persistent data, and <code class="literal">clientPort</code> is the port ZooKeeper listens on for client connections (2181 is a
      common choice). You should change <code class="literal">dataDir</code> to an appropriate
      setting for your system.</p><p class="calibre2">With a suitable configuration defined, we are now ready to start a
    local ZooKeeper server:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">zkServer.sh start</code></strong></pre><p class="calibre2">To check whether ZooKeeper is running, send the <code class="literal">ruok</code> command (“Are you OK?”) to the client port
    using <code class="literal">nc</code> (<code class="literal">telnet</code> works, too):</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">echo ruok | nc localhost 2181</code></strong>
imok</pre><p class="calibre2">That’s ZooKeeper saying, “I’m OK.” <a class="ulink" href="#calibre_link-91" title="Table&nbsp;21-1.&nbsp;ZooKeeper commands: the four-letter words">Table&nbsp;21-1</a> lists the
      commands, known as the “four-letter words,” for managing ZooKeeper.</p><div class="table"><a id="calibre_link-91" class="calibre"></a><div class="table-title">Table&nbsp;21-1.&nbsp;ZooKeeper commands: the four-letter words</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Category</td><td class="calibre20">Command</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">Server status</td><td class="calibre23"><code class="uri">ruok</code></td><td class="calibre25">Prints <code class="uri">imok</code> if the
            <a class="calibre" id="calibre_link-3256"></a>server is running and not in an error state.</td></tr><tr class="calibre26"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">conf</code></td><td class="calibre25">Prints the <a class="calibre" id="calibre_link-1226"></a>server configuration (from <em class="calibre10">zoo.cfg</em>).</td></tr><tr class="calibre19"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">envi</code></td><td class="calibre25">Prints the <a class="calibre" id="calibre_link-1570"></a>server environment, including ZooKeeper version, Java version, and other
              system properties.</td></tr><tr class="calibre26"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">srvr</code></td><td class="calibre25">Prints <a class="calibre" id="calibre_link-3511"></a>server statistics, including latency statistics, the number of znodes, and
              the server mode (standalone, leader, or follower).</td></tr><tr class="calibre19"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">stat</code></td><td class="calibre25">Prints <a class="calibre" id="calibre_link-3519"></a>server statistics and connected clients.</td></tr><tr class="calibre26"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">srst</code></td><td class="calibre25">Resets <a class="calibre" id="calibre_link-3510"></a>server statistics.</td></tr><tr class="calibre19"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">isro</code></td><td class="calibre25">Shows <a class="calibre" id="calibre_link-2155"></a>whether the server is in read-only (<code class="uri">ro</code>)
              mode (due to a network partition) or read/write mode (<code class="uri">rw</code>).</td></tr><tr class="calibre26"><td class="calibre23">Client connections</td><td class="calibre23"><code class="uri">dump</code></td><td class="calibre25">Lists all <a class="calibre" id="calibre_link-1550"></a>the sessions and ephemeral znodes for the ensemble.
            You must connect to the leader (see <code class="uri">srvr</code>) for this command.</td></tr><tr class="calibre19"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">cons</code></td><td class="calibre25">Lists <a class="calibre" id="calibre_link-1247"></a>connection statistics for all the server’s
            clients.</td></tr><tr class="calibre26"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">crst</code></td><td class="calibre25">Resets <a class="calibre" id="calibre_link-1291"></a>connection statistics.</td></tr><tr class="calibre19"><td class="calibre23">Watches</td><td class="calibre23"><code class="uri">wchs</code></td><td class="calibre25">Lists <a class="calibre" id="calibre_link-3778"></a>summary information for the server’s
            watches.</td></tr><tr class="calibre26"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">wchc</code></td><td class="calibre25">Lists <a class="calibre" id="calibre_link-3776"></a>all the server’s watches by connection. Caution: may impact server
              performance for a large number of watches.</td></tr><tr class="calibre19"><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">wchp</code></td><td class="calibre25">Lists <a class="calibre" id="calibre_link-3777"></a>all the server’s watches by znode path. Caution: may impact server
              performance for a large number of watches.</td></tr><tr class="calibre26"><td class="calibre27">Monitoring</td><td class="calibre27"><code class="uri">mntr</code></td><td class="calibre28">Lists <a class="calibre" id="calibre_link-2703"></a>server statistics in Java properties format, suitable as a source for
              monitoring systems such as Ganglia and Nagios.</td></tr></tbody></table></div></div><p class="calibre2">In addition to the <code class="literal">mntr</code> command,
    ZooKeeper exposes statistics via <a class="calibre" id="calibre_link-2207"></a><a class="calibre" id="calibre_link-2187"></a>JMX. For more details, see the <a class="ulink" href="http://zookeeper.apache.org/" target="_top">ZooKeeper documentation</a>. There
    are also monitoring tools and recipes in the <em class="calibre10">src/contrib</em> directory of the
    distribution.</p><p class="calibre2">From version 3.5.0 of ZooKeeper, there is an inbuilt web server for providing the same
      information as the four-letter words. Visit
        <span class="calibre"><em class="calibre10">http://localhost:8080/commands</em></span> for a list of <a class="calibre" id="calibre_link-3946"></a>commands.</p></div><div class="book" title="An Example"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4645">An Example</h2></div></div></div><p class="calibre2">Imagine a group of <a class="calibre" id="calibre_link-3939"></a><a class="calibre" id="calibre_link-1821"></a>servers that provide some service to clients. We want
    clients to be able to locate one of the servers so they can use the
    service. One of the challenges is maintaining the list of servers in the
    group.</p><p class="calibre2">The membership list clearly cannot be stored on a single node in the network, as the
      failure of that node would mean the failure of the whole system (we would like the list to be
      highly available). Suppose for a moment that we had a robust way of storing the list. We would
      still have the problem of how to remove a server from the list if it failed. Some process
      needs to be responsible for removing failed servers, but note that it can’t be the servers
      themselves, because they are no longer running!</p><p class="calibre2">What we are describing is not a passive distributed data structure,
    but an active one, and one that can change the state of an entry when some
    external event occurs. ZooKeeper provides this service, so let’s see how
    to build this group membership application (as it is known) with
    it.</p><div class="book" title="Group Membership in ZooKeeper"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4646">Group Membership in ZooKeeper</h3></div></div></div><p class="calibre2">One way of understanding ZooKeeper <a class="calibre" id="calibre_link-1825"></a><a class="calibre" id="calibre_link-3912"></a>is to think of it as providing a high-availability filesystem. It doesn’t have
        files and directories, but a unified concept of a node, called a
          <em class="calibre10">znode</em>, that acts both as a container of data (like a file) and a
        container of other znodes (like a directory). Znodes form a hierarchical namespace, and a
        natural way to build a membership list is to create a parent znode with the name of the
        group and child znodes with the names of the group members (servers). This is shown in
          <a class="ulink" href="#calibre_link-92" title="Figure&nbsp;21-1.&nbsp;ZooKeeper znodes">Figure&nbsp;21-1</a>.</p><div class="book"><div class="figure"><a id="calibre_link-92" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4647" class="calibre"></a><img alt="ZooKeeper znodes" src="images/000065.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;21-1.&nbsp;ZooKeeper znodes</div></div></div><p class="calibre2">In this example we won’t store data in any of the znodes, but in a real application you
        could imagine storing data about the members, such as hostnames, in their znodes.</p></div><div class="book" title="Creating the Group"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4648">Creating the Group</h3></div></div></div><p class="calibre2">Let’s introduce <a class="calibre" id="calibre_link-3914"></a><a class="calibre" id="calibre_link-1822"></a>ZooKeeper’s Java API by writing a program to create a
      znode for the group, which is <em class="calibre10">/zoo</em>
      in <a class="ulink" href="#calibre_link-93" title="Example&nbsp;21-1.&nbsp;A program to create a znode representing a group in ZooKeeper">Example&nbsp;21-1</a>.</p><div class="example"><a id="calibre_link-93" class="calibre"></a><div class="example-title">Example&nbsp;21-1.&nbsp;A program to create a znode representing a group in
        ZooKeeper</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">CreateGroup</code> <code class="k">implements</code> <code class="n">Watcher</code> <code class="o">{</code>
  
  <code class="k">private</code> <code class="k">static</code> <code class="k">final</code> <code class="kt">int</code> <code class="n">SESSION_TIMEOUT</code> <code class="o">=</code> <code class="mi">5000</code><code class="o">;</code>
  
  <code class="k">private</code> <code class="n">ZooKeeper</code> <code class="n">zk</code><code class="o">;</code>
  <code class="k">private</code> <code class="n">CountDownLatch</code> <code class="n">connectedSignal</code> <code class="o">=</code> <code class="k">new</code> <code class="n">CountDownLatch</code><code class="o">(</code><code class="mi">1</code><code class="o">);</code>
  
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">connect</code><code class="o">(</code><code class="n">String</code> <code class="n">hosts</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="n">zk</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ZooKeeper</code><code class="o">(</code><code class="n">hosts</code><code class="o">,</code> <code class="n">SESSION_TIMEOUT</code><code class="o">,</code> <code class="k">this</code><code class="o">);</code>
    <code class="n">connectedSignal</code><code class="o">.</code><code class="na">await</code><code class="o">();</code>
  <code class="o">}</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">process</code><code class="o">(</code><code class="n">WatchedEvent</code> <code class="n">event</code><code class="o">)</code> <code class="o">{</code> <code class="c2">// Watcher interface</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">event</code><code class="o">.</code><code class="na">getState</code><code class="o">()</code> <code class="o">==</code> <code class="n">KeeperState</code><code class="o">.</code><code class="na">SyncConnected</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">connectedSignal</code><code class="o">.</code><code class="na">countDown</code><code class="o">();</code>
    <code class="o">}</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">create</code><code class="o">(</code><code class="n">String</code> <code class="n">groupName</code><code class="o">)</code> <code class="k">throws</code> <code class="n">KeeperException</code><code class="o">,</code>
      <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="n">String</code> <code class="n">path</code> <code class="o">=</code> <code class="sb">"/"</code> <code class="o">+</code> <code class="n">groupName</code><code class="o">;</code>
    <code class="n">String</code> <code class="n">createdPath</code> <code class="o">=</code> <code class="n">zk</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">path</code><code class="o">,</code> <code class="k">null</code><code class="c2">/*data*/</code><code class="o">,</code> <code class="n">Ids</code><code class="o">.</code><code class="na">OPEN_ACL_UNSAFE</code><code class="o">,</code>
        <code class="n">CreateMode</code><code class="o">.</code><code class="na">PERSISTENT</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="sb">"Created "</code> <code class="o">+</code> <code class="n">createdPath</code><code class="o">);</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">close</code><code class="o">()</code> <code class="k">throws</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="n">zk</code><code class="o">.</code><code class="na">close</code><code class="o">();</code>
  <code class="o">}</code>

  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">CreateGroup</code> <code class="n">createGroup</code> <code class="o">=</code> <code class="k">new</code> <code class="n">CreateGroup</code><code class="o">();</code>
    <code class="n">createGroup</code><code class="o">.</code><code class="na">connect</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]);</code>
    <code class="n">createGroup</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">]);</code>
    <code class="n">createGroup</code><code class="o">.</code><code class="na">close</code><code class="o">();</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">When the <code class="literal">main()</code> method is run, it creates
      a <code class="literal">CreateGroup</code> instance and then calls
      its <code class="literal">connect()</code> method. This method instantiates
      a new <code class="literal">ZooKeeper</code> object, which is the
      central class of the client API and the one that maintains the
      connection between the client and the ZooKeeper service. The constructor
      takes three arguments: the first is the host address (and optional port,
      which defaults to 2181) of the ZooKeeper service;<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-94" id="calibre_link-123">142</a>]</sup> the second is the session timeout in milliseconds (which
      we set to 5 seconds), explained in more detail later; and the third is
      an instance of a <code class="literal">Watcher</code> object. The
      <code class="literal">Watcher</code> object receives callbacks
      from ZooKeeper to inform it of various events. In this scenario,
      <code class="literal">CreateGroup</code> is a <code class="literal">Watcher</code>, so we pass this to the <code class="literal">ZooKeeper</code> constructor.</p><p class="calibre2">When a <code class="literal">ZooKeeper</code> instance is
      created, it starts a thread to connect to the ZooKeeper service. The
      call to the constructor returns immediately, so it is important to wait
      for the connection to be established before using the <code class="literal">ZooKeeper</code> object. We make use of Java’s
      <code class="literal">CountDownLatch</code> class (in the <code class="literal">java.util.concurrent</code> package) to block until
      the <code class="literal">ZooKeeper</code> instance is ready. This
      is where the <code class="literal">Watcher</code> comes in. The
      <code class="literal">Watcher</code> interface has a single
      method:</p><a id="calibre_link-4649" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="kt">void</code> <code class="nf">process</code><code class="o">(</code><code class="n">WatchedEvent</code> <code class="n">event</code><code class="o">);</code></pre><p class="calibre2">When the client has connected to ZooKeeper, the <code class="literal">Watcher</code> receives a call to its <code class="literal">process()</code> method with an
      event indicating that it has connected. On receiving a connection event
      (represented by the <code class="literal">Watcher.Event.KeeperState</code> enum, with value
      <code class="literal">SyncConnected</code>),
      we decrement the counter in the <code class="literal">CountDownLatch</code>, using its <code class="literal">countDown()</code> method. The latch was created with
      a count of one, representing the number of events that need to occur
      before it releases all waiting threads. After calling <code class="literal">countDown()</code> once, the counter reaches zero and
      the <code class="literal">await()</code> method returns.</p><p class="calibre2">The <code class="literal">connect()</code> method has now returned, and the next method to
        be invoked on the <code class="literal">CreateGroup</code> is the
          <code class="literal">create()</code> method. In this method, we create a new ZooKeeper znode
        using the <code class="literal">create()</code> method on the <code class="literal">ZooKeeper</code> instance. The arguments it takes are the path (represented by a
        string), the contents of the znode (a byte array <code class="literal">null</code>
        here), an access control list (or ACL for short, which here is completely open, allowing any
        client to read from or write to the znode), and the nature of the znode to be
        created.</p><p class="calibre2">Znodes may be ephemeral or persistent. An <span class="calibre">ephemeral</span>
        znode will be deleted by the ZooKeeper service when the client that created it disconnects,
        either explicitly or because the client terminates for whatever reason. A <span class="calibre">persistent</span> znode, on the other hand, is not deleted when the
        client disconnects. We want the znode representing a group to live longer than the lifetime
        of the program that creates it, so we create a persistent znode.</p><p class="calibre2">The return value of the <code class="literal">create()</code> method
      is the path that was created by ZooKeeper. We use it to print a message
      that the path was successfully created. We will see how the path
      returned by <code class="literal">create()</code> may differ from the one
      passed into the method when we look at sequential znodes.</p><p class="calibre2">To see the program in action, we need to have ZooKeeper running on
      the local machine, and then we can use the <a class="calibre" id="calibre_link-3915"></a><a class="calibre" id="calibre_link-1823"></a>following:</p><pre class="screen1">% <strong class="userinput"><code class="calibre9">export </code></strong><strong class="userinput"><code class="calibre9">CLASSPATH=ch21-zk/target/classes/:$ZOOKEEPER_HOME/*:\
  $ZOOKEEPER_HOME/lib/*:$ZOOKEEPER_HOME/conf</code></strong>
% <strong class="userinput"><code class="calibre9">java CreateGroup localhost zoo</code></strong>
Created /zoo</pre></div><div class="book" title="Joining a Group"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4650">Joining a Group</h3></div></div></div><p class="calibre2">The next part of the application is a <a class="calibre" id="calibre_link-1826"></a><a class="calibre" id="calibre_link-3918"></a>program to register a member in a group. Each member will
      run as a program and join a group. When the program exits, it should be
      removed from the group, which we can do by creating an ephemeral znode
      that represents it in the ZooKeeper namespace.</p><p class="calibre2">The <code class="literal">JoinGroup</code> program
      implements this idea, and its listing is in <a class="ulink" href="#calibre_link-95" title="Example&nbsp;21-2.&nbsp;A program that joins a group">Example&nbsp;21-2</a>. The logic for creating and connecting to a
      <code class="literal">ZooKeeper</code> instance has been
      refactored into a base class, <code class="literal">ConnectionWatcher</code>, and appears in <a class="ulink" href="#calibre_link-96" title="Example&nbsp;21-3.&nbsp;A helper class that waits for the ZooKeeper connection to be established">Example&nbsp;21-3</a>.</p><div class="example"><a id="calibre_link-95" class="calibre"></a><div class="example-title">Example&nbsp;21-2.&nbsp;A program that joins a group</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">JoinGroup</code> <code class="k">extends</code> <code class="n">ConnectionWatcher</code> <code class="o">{</code>
  
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">join</code><code class="o">(</code><code class="n">String</code> <code class="n">groupName</code><code class="o">,</code> <code class="n">String</code> <code class="n">memberName</code><code class="o">)</code> <code class="k">throws</code> <code class="n">KeeperException</code><code class="o">,</code>
      <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="n">String</code> <code class="n">path</code> <code class="o">=</code> <code class="sb">"/"</code> <code class="o">+</code> <code class="n">groupName</code> <code class="o">+</code> <code class="sb">"/"</code> <code class="o">+</code> <code class="n">memberName</code><code class="o">;</code>
    <code class="n">String</code> <code class="n">createdPath</code> <code class="o">=</code> <code class="n">zk</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">path</code><code class="o">,</code> <code class="k">null</code><code class="c2">/*data*/</code><code class="o">,</code> <code class="n">Ids</code><code class="o">.</code><code class="na">OPEN_ACL_UNSAFE</code><code class="o">,</code>
      <code class="n">CreateMode</code><code class="o">.</code><code class="na">EPHEMERAL</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="sb">"Created "</code> <code class="o">+</code> <code class="n">createdPath</code><code class="o">);</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">JoinGroup</code> <code class="n">joinGroup</code> <code class="o">=</code> <code class="k">new</code> <code class="n">JoinGroup</code><code class="o">();</code>
    <code class="n">joinGroup</code><code class="o">.</code><code class="na">connect</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]);</code>
    <code class="n">joinGroup</code><code class="o">.</code><code class="na">join</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">],</code> <code class="n">args</code><code class="o">[</code><code class="mi">2</code><code class="o">]);</code>
    
    <code class="c2">// stay alive until process is killed or thread is interrupted</code>
    <code class="n">Thread</code><code class="o">.</code><code class="na">sleep</code><code class="o">(</code><code class="n">Long</code><code class="o">.</code><code class="na">MAX_VALUE</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><div class="example"><a id="calibre_link-96" class="calibre"></a><div class="example-title">Example&nbsp;21-3.&nbsp;A helper class that waits for the ZooKeeper connection to be
        established</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">ConnectionWatcher</code> <code class="k">implements</code> <code class="n">Watcher</code> <code class="o">{</code>
  
  <code class="k">private</code> <code class="k">static</code> <code class="k">final</code> <code class="kt">int</code> <code class="n">SESSION_TIMEOUT</code> <code class="o">=</code> <code class="mi">5000</code><code class="o">;</code>

  <code class="k">protected</code> <code class="n">ZooKeeper</code> <code class="n">zk</code><code class="o">;</code>
  <code class="k">private</code> <code class="n">CountDownLatch</code> <code class="n">connectedSignal</code> <code class="o">=</code> <code class="k">new</code> <code class="n">CountDownLatch</code><code class="o">(</code><code class="mi">1</code><code class="o">);</code>

  <code class="k">public</code> <code class="kt">void</code> <code class="nf">connect</code><code class="o">(</code><code class="n">String</code> <code class="n">hosts</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="n">zk</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ZooKeeper</code><code class="o">(</code><code class="n">hosts</code><code class="o">,</code> <code class="n">SESSION_TIMEOUT</code><code class="o">,</code> <code class="k">this</code><code class="o">);</code>
    <code class="n">connectedSignal</code><code class="o">.</code><code class="na">await</code><code class="o">();</code>
  <code class="o">}</code>
  
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">process</code><code class="o">(</code><code class="n">WatchedEvent</code> <code class="n">event</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">event</code><code class="o">.</code><code class="na">getState</code><code class="o">()</code> <code class="o">==</code> <code class="n">KeeperState</code><code class="o">.</code><code class="na">SyncConnected</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">connectedSignal</code><code class="o">.</code><code class="na">countDown</code><code class="o">();</code>
    <code class="o">}</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">close</code><code class="o">()</code> <code class="k">throws</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="n">zk</code><code class="o">.</code><code class="na">close</code><code class="o">();</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The code for <code class="literal">JoinGroup</code> is very
      similar to <code class="literal">CreateGroup</code>. It creates an
      ephemeral znode as a child of the group znode in its
      <code class="literal">join()</code> method, then simulates doing work of
      some kind by sleeping until the process is forcibly terminated. Later,
      you will see that upon termination, the ephemeral znode is removed by
      ZooKeeper.</p></div><div class="book" title="Listing Members in a Group"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4651">Listing Members in a Group</h3></div></div></div><p class="calibre2">Now we need a program to find the members in <a class="calibre" id="calibre_link-1827"></a><a class="calibre" id="calibre_link-3919"></a>a group (see <a class="ulink" href="#calibre_link-97" title="Example&nbsp;21-4.&nbsp;A program to list the members in a group">Example&nbsp;21-4</a>).</p><div class="example"><a id="calibre_link-97" class="calibre"></a><div class="example-title">Example&nbsp;21-4.&nbsp;A program to list the members in a group</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">ListGroup</code> <code class="k">extends</code> <code class="n">ConnectionWatcher</code> <code class="o">{</code>
    
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">list</code><code class="o">(</code><code class="n">String</code> <code class="n">groupName</code><code class="o">)</code> <code class="k">throws</code> <code class="n">KeeperException</code><code class="o">,</code>
      <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="n">String</code> <code class="n">path</code> <code class="o">=</code> <code class="sb">"/"</code> <code class="o">+</code> <code class="n">groupName</code><code class="o">;</code>
    
    <code class="k">try</code> <code class="o">{</code>
      <code class="n">List</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">children</code> <code class="o">=</code> <code class="n">zk</code><code class="o">.</code><code class="na">getChildren</code><code class="o">(</code><code class="n">path</code><code class="o">,</code> <code class="k">false</code><code class="o">);</code>
      <code class="k">if</code> <code class="o">(</code><code class="n">children</code><code class="o">.</code><code class="na">isEmpty</code><code class="o">())</code> <code class="o">{</code>
        <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"No members in group %s\n"</code><code class="o">,</code> <code class="n">groupName</code><code class="o">);</code>
        <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="mi">1</code><code class="o">);</code>
      <code class="o">}</code>
      <code class="k">for</code> <code class="o">(</code><code class="n">String</code> <code class="n">child</code> <code class="o">:</code> <code class="n">children</code><code class="o">)</code> <code class="o">{</code>
        <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="n">child</code><code class="o">);</code>
      <code class="o">}</code>
    <code class="o">}</code> <code class="k">catch</code> <code class="o">(</code><code class="n">KeeperException</code><code class="o">.</code><code class="na">NoNodeException</code> <code class="n">e</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"Group %s does not exist\n"</code><code class="o">,</code> <code class="n">groupName</code><code class="o">);</code>
      <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="mi">1</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">ListGroup</code> <code class="n">listGroup</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ListGroup</code><code class="o">();</code>
    <code class="n">listGroup</code><code class="o">.</code><code class="na">connect</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]);</code>
    <code class="n">listGroup</code><code class="o">.</code><code class="na">list</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">]);</code>
    <code class="n">listGroup</code><code class="o">.</code><code class="na">close</code><code class="o">();</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">In the <code class="literal">list()</code> method, we call <code class="literal">getChildren()</code> with a znode path and a watch
      flag to retrieve a list of child paths for the znode, which we print
      out. Placing a watch on a znode causes the registered <code class="literal">Watcher</code> to be triggered if the znode changes
      state. Although we’re not using it here, watching a znode’s children
      would permit a program to get notifications of members joining or
      leaving the group, or of the group being deleted.</p><p class="calibre2">We catch <code class="literal">KeeperException.NoNodeException</code>, which is
      thrown in the case when the group’s znode does not exist.</p><p class="calibre2">Let’s see <code class="literal">ListGroup</code> in action.
      As expected, the <code class="literal">zoo</code> group is empty,
      since we haven’t added any members yet:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">java ListGroup localhost zoo</code></strong>
No members in group zoo</pre><p class="calibre2">We can use <code class="literal">JoinGroup</code> to add
      some members. We launch them as background processes, since they don’t
      terminate on their own (due to the sleep statement):</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">java JoinGroup localhost zoo duck &amp;</code></strong>
<code class="literal">% </code><strong class="userinput"><code class="calibre9">java JoinGroup localhost zoo cow &amp;</code></strong>
<code class="literal">% </code><strong class="userinput"><code class="calibre9">java JoinGroup localhost zoo goat &amp;</code></strong>
<code class="literal">% </code><strong class="userinput"><code class="calibre9">goat_pid=$!</code></strong></pre><p class="calibre2">The last line saves the process ID of the Java process running the
      program that adds <code class="literal">goat</code> as a member.
      We need to remember the ID so that we can kill the process in a moment,
      after checking the members:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">java ListGroup localhost zoo</code></strong>
goat
duck
cow</pre><p class="calibre2">To remove a member, we kill its process:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">kill $goat_pid</code></strong></pre><p class="calibre2">And a few seconds later, it has disappeared from the group because
      the process’s ZooKeeper session has terminated (the timeout was set to 5
      seconds) and its associated ephemeral node has been removed:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">java ListGroup localhost zoo</code></strong>
duck
cow</pre><p class="calibre2">Let’s stand back and see what we’ve built here. We have a way of
      building up a list of a group of nodes that are participating in a
      distributed system. The nodes may have no knowledge of each other. A
      client that wants to use the nodes in the list to perform some work, for
      example, can discover the nodes without them being aware of the client’s
      existence.</p><p class="calibre2">Finally, note that group membership is not a substitution for
      handling network errors when communicating with a node. Even if a node
      is a group member, communications with it may fail, and such failures
      must be handled in the usual ways (retrying, trying a different member
      of the group, etc.).</p><div class="book" title="ZooKeeper command-line tools"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4652">ZooKeeper command-line tools</h4></div></div></div><p class="calibre2">ZooKeeper comes <a class="calibre" id="calibre_link-1195"></a>with a command-line tool for interacting with the
        ZooKeeper namespace. We can use it to list the znodes under the
        <em class="calibre10">/zoo</em> znode as follows:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">zkCli.sh -server localhost ls /zoo</code></strong>
[cow, duck]</pre><p class="calibre2">You can run the command without arguments to display usage
        <a class="calibre" id="calibre_link-1828"></a><a class="calibre" id="calibre_link-3920"></a>instructions.</p></div></div><div class="book" title="Deleting a Group"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4653">Deleting a Group</h3></div></div></div><p class="calibre2">To round off the example, let’s see how to <a class="calibre" id="calibre_link-1824"></a><a class="calibre" id="calibre_link-3916"></a>delete a group. The <code class="literal">ZooKeeper</code> class provides
        a <code class="literal">delete()</code> method that takes a path and a version number. ZooKeeper
        will delete a znode only if the version number specified is the same as the version number
        of the znode it is trying to delete—an optimistic locking mechanism that allows clients to
        detect conflicts over znode modification. You can bypass the version check, however, by
        using a version number of –1 to delete the znode regardless of its version number.</p><p class="calibre2">There is no recursive delete operation in ZooKeeper, so you have
      to delete child znodes before parents. This is what we do in the
      <code class="literal">DeleteGroup</code> class, which will remove
      a group and all its members (<a class="ulink" href="#calibre_link-98" title="Example&nbsp;21-5.&nbsp;A program to delete a group and its members">Example&nbsp;21-5</a>).</p><div class="example"><a id="calibre_link-98" class="calibre"></a><div class="example-title">Example&nbsp;21-5.&nbsp;A program to delete a group and its members</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">DeleteGroup</code> <code class="k">extends</code> <code class="n">ConnectionWatcher</code> <code class="o">{</code>
    
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">delete</code><code class="o">(</code><code class="n">String</code> <code class="n">groupName</code><code class="o">)</code> <code class="k">throws</code> <code class="n">KeeperException</code><code class="o">,</code>
      <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="n">String</code> <code class="n">path</code> <code class="o">=</code> <code class="sb">"/"</code> <code class="o">+</code> <code class="n">groupName</code><code class="o">;</code>
    
    <code class="k">try</code> <code class="o">{</code>
      <code class="n">List</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">children</code> <code class="o">=</code> <code class="n">zk</code><code class="o">.</code><code class="na">getChildren</code><code class="o">(</code><code class="n">path</code><code class="o">,</code> <code class="k">false</code><code class="o">);</code>
      <code class="k">for</code> <code class="o">(</code><code class="n">String</code> <code class="n">child</code> <code class="o">:</code> <code class="n">children</code><code class="o">)</code> <code class="o">{</code>
        <code class="n">zk</code><code class="o">.</code><code class="na">delete</code><code class="o">(</code><code class="n">path</code> <code class="o">+</code> <code class="sb">"/"</code> <code class="o">+</code> <code class="n">child</code><code class="o">,</code> <code class="o">-</code><code class="mi">1</code><code class="o">);</code>
      <code class="o">}</code>
      <code class="n">zk</code><code class="o">.</code><code class="na">delete</code><code class="o">(</code><code class="n">path</code><code class="o">,</code> <code class="o">-</code><code class="mi">1</code><code class="o">);</code>
    <code class="o">}</code> <code class="k">catch</code> <code class="o">(</code><code class="n">KeeperException</code><code class="o">.</code><code class="na">NoNodeException</code> <code class="n">e</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"Group %s does not exist\n"</code><code class="o">,</code> <code class="n">groupName</code><code class="o">);</code>
      <code class="n">System</code><code class="o">.</code><code class="na">exit</code><code class="o">(</code><code class="mi">1</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">DeleteGroup</code> <code class="n">deleteGroup</code> <code class="o">=</code> <code class="k">new</code> <code class="n">DeleteGroup</code><code class="o">();</code>
    <code class="n">deleteGroup</code><code class="o">.</code><code class="na">connect</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]);</code>
    <code class="n">deleteGroup</code><code class="o">.</code><code class="na">delete</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">1</code><code class="o">]);</code>
    <code class="n">deleteGroup</code><code class="o">.</code><code class="na">close</code><code class="o">();</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">Finally, we can delete the <code class="literal">zoo</code>
      group that we created <a class="calibre" id="calibre_link-3940"></a>earlier:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">java DeleteGroup localhost zoo</code></strong>
<code class="literal">% </code><strong class="userinput"><code class="calibre9">java ListGroup localhost zoo</code></strong>
Group zoo does not exist</pre></div></div><div class="book" title="The ZooKeeper Service"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4654">The ZooKeeper Service</h2></div></div></div><p class="calibre2">ZooKeeper is a highly available, high-performance coordination
    service. In this section, we look at the nature of the service it
    provides: its model, operations, and implementation.</p><div class="book" title="Data Model"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4655">Data Model</h3></div></div></div><p class="calibre2">ZooKeeper <a class="calibre" id="calibre_link-3938"></a>maintains a hierarchical tree of nodes called znodes. A
      znode stores data and has an associated ACL. ZooKeeper is designed for
      coordination (which typically uses small datafiles), not high-volume
      data storage, so there is a limit of 1 MB on the amount of data that may
      be stored in any znode.</p><p class="calibre2">Data access is atomic. A client reading the data stored in a znode will never receive
        only some of the data; either the data will be delivered in its entirety or the read will
        fail. Similarly, a write will replace all the data associated with a znode. ZooKeeper
        guarantees that the write will either succeed or fail; there is no such thing as a partial
        write, where only some of the data written by the client is stored. ZooKeeper does not
        support an append operation. These characteristics contrast with HDFS, which is designed for
          high-volume data storage with streaming data access
        and provides an append operation.</p><p class="calibre2">Znodes are referenced by paths, which in ZooKeeper are represented
      as slash-delimited Unicode character strings, like filesystem paths in
      Unix. Paths must be absolute, so they must begin with a slash character.
      Furthermore, they are canonical, which means that each path has a single
      representation, and so paths do not undergo resolution. For example, in
      Unix, a file with the path <em class="calibre10">/a/b</em> can
      equivalently be referred to by the path <em class="calibre10">/a/./b</em> because “.” refers to the current
      directory at the point it is encountered in the path. In ZooKeeper, “.”
      does not have this special meaning and is actually illegal as a path
      component (as is “..” for the parent of the current directory).</p><p class="calibre2">Path components are composed of Unicode characters, with a few
      restrictions (these are spelled out in the ZooKeeper reference
      documentation). The string “zookeeper” is a reserved word and may not be
      used as a path component. In particular, ZooKeeper uses the <em class="calibre10">/zookeeper</em> subtree to store management
      information, such as information on quotas.</p><p class="calibre2">Note that paths are not URIs, and they are represented in the Java API by a <code class="literal">java.lang.String</code>, rather than the Hadoop <code class="literal">Path</code> class (or the <code class="literal">java.net.URI</code> class, for that matter).</p><p class="calibre2">Znodes have some <a class="calibre" id="calibre_link-3072"></a><a class="calibre" id="calibre_link-3923"></a>properties that are very useful for building distributed
      applications, which we discuss in the following sections.</p><div class="book" title="Ephemeral znodes"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4656">Ephemeral znodes</h4></div></div></div><p class="calibre2">As we’ve seen, znodes can be one of two types: <a class="calibre" id="calibre_link-1575"></a><a class="calibre" id="calibre_link-3917"></a><a class="calibre" id="calibre_link-2981"></a><a class="calibre" id="calibre_link-3922"></a>ephemeral or persistent. A znode’s type is set at creation time and may not be
          changed later. An ephemeral znode is deleted by ZooKeeper when the creating client’s
          session ends. By contrast, a persistent znode is not tied to the client’s session and is
          deleted only when explicitly deleted by a client (not necessarily the one that created
          it). An ephemeral znode may not have children, not even ephemeral ones.</p><p class="calibre2">Even though ephemeral nodes are tied to a client session, they are visible to all
          clients (subject to their ACL policies, of course).</p><p class="calibre2">Ephemeral znodes are ideal for building applications that need
        to know when certain distributed resources are available. The example
        earlier in this chapter uses ephemeral znodes to implement a group
        membership service, so any process can discover the members of the
        group at any particular time.</p></div><div class="book" title="Sequence numbers"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4657">Sequence numbers</h4></div></div></div><p class="calibre2">A <em class="calibre10">sequential</em> znode is <a class="calibre" id="calibre_link-3348"></a><a class="calibre" id="calibre_link-3925"></a>given a sequence number by ZooKeeper as a part of its
        name. If a znode is created with the sequential flag set, then the
        value of a monotonically increasing counter (maintained by the parent
        znode) is appended to its name.</p><p class="calibre2">If a client asks to create a sequential znode with the name
        <em class="calibre10">/a/b-</em>, for example, the znode
        created may actually have the name <em class="calibre10">/a/b-3</em>.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-99" id="calibre_link-124">143</a>]</sup> If, later on, another sequential znode with the name
        <em class="calibre10">/a/b-</em> is created, it will be
        given a unique name with a larger value of the counter—for example,
        <em class="calibre10">/a/b-5</em>. In the Java API, the
        actual path given to sequential znodes is communicated back to the
        client as the return value of the <code class="literal">create()</code>
        call.</p><p class="calibre2">Sequence numbers can be used to impose a global ordering on
        events in a distributed system and may be used by the client to infer
        the ordering. In <a class="ulink" href="#calibre_link-100" title="A Lock Service">A Lock Service</a>, you will learn how to use
        sequential znodes to build a shared lock.</p></div><div class="book" title="Watches"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4658">Watches</h4></div></div></div><p class="calibre2">Watches <a class="calibre" id="calibre_link-3773"></a>allow clients to get notifications when a znode changes
        in some way. Watches are set by operations on the ZooKeeper service
        and are triggered by other operations on the service. For example, a
        client might call the <code class="literal">exists</code>
        operation on a znode, placing a watch on it at the same time. If the
        znode doesn’t exist, the <code class="literal">exists</code>
          operation will return <code class="literal">false</code>. If, some time later, the znode is created
        by a second client, the watch is triggered, notifying the first client
        of the znode’s creation. You will see precisely which operations
        trigger others in the next section.</p><p class="calibre2">Watchers are triggered only once.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-101" id="calibre_link-125">144</a>]</sup> To receive multiple notifications, a client needs to
        reregister the watch. So, if the client in the previous example wishes to
        receive further notifications for the znode’s existence (to be
        notified when it is deleted, for example), it needs to call the
        <code class="literal">exists</code> operation again to set a new
        watch.</p><p class="calibre2">There is an example in <a class="ulink" href="#calibre_link-102" title="A Configuration Service">A Configuration Service</a>
        demonstrating how to use watches to update configuration across a
        <a class="calibre" id="calibre_link-3073"></a><a class="calibre" id="calibre_link-3924"></a>cluster.</p></div></div><div class="book" title="Operations"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4659">Operations</h3></div></div></div><p class="calibre2">There are nine basic <a class="calibre" id="calibre_link-3947"></a><a class="calibre" id="calibre_link-3921"></a><a class="calibre" id="calibre_link-2837"></a>operations in ZooKeeper, listed in <a class="ulink" href="#calibre_link-103" title="Table&nbsp;21-2.&nbsp;Operations in the ZooKeeper service">Table&nbsp;21-2</a>.</p><div class="table"><a id="calibre_link-103" class="calibre"></a><div class="table-title">Table&nbsp;21-2.&nbsp;Operations in the ZooKeeper service</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Operation</td><td class="calibre21">Description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">create</code></td><td class="calibre25">Creates a <a class="calibre" id="calibre_link-1282"></a>znode (the parent znode must already
              exist)</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">delete</code></td><td class="calibre25">Deletes a <a class="calibre" id="calibre_link-1423"></a>znode (the znode must not have any
              children)</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">exists</code></td><td class="calibre25">Tests<a class="calibre" id="calibre_link-1587"></a> whether a znode exists and retrieves its
              metadata</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">getACL</code>, <code class="uri">setACL</code></td><td class="calibre25">Gets/sets <a class="calibre" id="calibre_link-1796"></a><a class="calibre" id="calibre_link-3380"></a>the ACL for a znode</td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">getChildren</code></td><td class="calibre25">Gets a list of <a class="calibre" id="calibre_link-1797"></a>the children of a znode</td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">getData</code>, <code class="uri">setData</code></td><td class="calibre25">Gets/sets the <a class="calibre" id="calibre_link-1798"></a><a class="calibre" id="calibre_link-3381"></a>data associated with a znode</td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">sync</code></td><td class="calibre28">Synchronizes <a class="calibre" id="calibre_link-3568"></a>a client’s view of a znode with ZooKeeper</td></tr></tbody></table></div></div><p class="calibre2">Update operations in ZooKeeper are conditional. A <code class="literal">delete</code> or <code class="literal">setData</code> operation has to specify the version
      number of the znode that is being updated (which is found from a
      previous <code class="literal">exists</code> call). If the version
      number does not match, the update will fail. Updates are a nonblocking
      operation, so a client that loses an update (because another process
      updated the znode in the meantime) can decide whether to try again or
      take some other action, and it can do so without blocking the progress
      of any other process.</p><p class="calibre2">Although ZooKeeper can be viewed as a filesystem, there are some
      filesystem primitives that it does away with in the name of simplicity.
      Because files are small and are written and read in their entirety,
      there is no need to provide open, close, or seek operations.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">The <code class="literal">sync</code> operation is not like
            <code class="literal">fsync()</code> in POSIX filesystems. As mentioned earlier, writes in
          ZooKeeper are atomic, and a successful write operation is guaranteed to have been written
          to persistent storage on a majority of ZooKeeper servers. However, it is permissible for
          reads to lag the latest state of the ZooKeeper service, and the <code class="literal">sync</code> operation exists to allow a client to bring itself up to date. This
          topic is covered in more detail in <a class="ulink" href="#calibre_link-104" title="Consistency">Consistency</a>.</p></div><div class="book" title="Multiupdate"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4660">Multiupdate</h4></div></div></div><p class="calibre2">There is another ZooKeeper <a class="calibre" id="calibre_link-2835"></a>operation, called <code class="literal">multi</code>, that batches
          together multiple primitive operations into a single unit that either succeeds or fails in
          its entirety. The situation where some of the primitive operations succeed and some fail
          can never arise.</p><p class="calibre2">Multiupdate is very useful for building structures in ZooKeeper that maintain some
          global invariant. One example is an undirected graph. Each vertex in the graph is
          naturally represented as a znode in ZooKeeper, and to add or remove an edge we need to
          update the two znodes corresponding to its vertices because each has a reference to the
          other. If we used only primitive ZooKeeper operations, it would be possible for another
          client to observe the graph in an inconsistent state, where one vertex is connected to
          another but the reverse connection is absent. Batching the updates on the two znodes into
          one <code class="literal">multi</code> operation ensures that the update is atomic,
          so a pair of vertices can never have a dangling connection.</p></div><div class="book" title="APIs"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4661">APIs</h4></div></div></div><p class="calibre2">There are two core language <a class="calibre" id="calibre_link-2834"></a>bindings for ZooKeeper clients, one for Java and one for
        C; there are also <code class="literal">contrib</code> bindings
        for Perl, Python, and REST clients. For each binding, there is a
        choice between performing operations synchronously or asynchronously.
        We’ve already seen the synchronous Java API. Here’s the signature for
        the <code class="literal">exists</code> operation, which returns
        either a <code class="literal">Stat</code> object that
        encapsulates the znode’s metadata or <code class="literal">null</code> if the znode doesn’t exist:</p><a id="calibre_link-4662" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="n">Stat</code> <code class="nf">exists</code><code class="o">(</code><code class="n">String</code> <code class="n">path</code><code class="o">,</code> <code class="n">Watcher</code> <code class="n">watcher</code><code class="o">)</code> <code class="k">throws</code> <code class="n">KeeperException</code><code class="o">,</code>
    <code class="n">InterruptedException</code></pre><p class="calibre2">The asynchronous equivalent, which is also found in the <code class="literal">ZooKeeper</code> class, looks like this:</p><a id="calibre_link-4663" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="kt">void</code> <code class="nf">exists</code><code class="o">(</code><code class="n">String</code> <code class="n">path</code><code class="o">,</code> <code class="n">Watcher</code> <code class="n">watcher</code><code class="o">,</code> <code class="n">StatCallback</code> <code class="n">cb</code><code class="o">,</code> <code class="n">Object</code> <code class="n">ctx</code><code class="o">)</code></pre><p class="calibre2">In the Java API, all the asynchronous methods have <code class="literal">void</code> return types, since the result of the
        operation is conveyed via a callback. The caller passes a callback
        implementation whose method is invoked when a response is received
        from ZooKeeper. In this case, the callback is the <code class="literal">StatCallback</code> interface, which has the
        following method:</p><a id="calibre_link-4664" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="kt">void</code> <code class="nf">processResult</code><code class="o">(</code><code class="kt">int</code> <code class="n">rc</code><code class="o">,</code> <code class="n">String</code> <code class="n">path</code><code class="o">,</code> <code class="n">Object</code> <code class="n">ctx</code><code class="o">,</code> <code class="n">Stat</code> <code class="n">stat</code><code class="o">);</code></pre><p class="calibre2">The <code class="literal">rc</code> argument is the return
        code, corresponding to the codes defined by <code class="literal">Keeper</code><code class="literal">Exception</code>. A nonzero code
        represents an exception, in which case the <code class="literal">stat</code> parameter will be <code class="literal">null</code>. The <code class="literal">path</code> and <code class="literal">ctx</code> arguments correspond to the equivalent
        arguments passed by the client to the <code class="literal">exists()</code> method, and can be used to identify
        the request for which this callback is a response. The <code class="literal">ctx</code> parameter can be an arbitrary object
        that may be used by the client when the path does not give enough
        context to disambiguate the request. If not needed, it may be set to
        <code class="literal">null</code>.</p><p class="calibre2">There are actually two C shared libraries. The single-threaded
        library, <code class="literal">zookeeper_st</code>, supports
        only the asynchronous API and is intended for platforms where the
        <code class="literal">pthread</code> library is not available or
        stable. Most developers will use the multithreaded library, <code class="literal">zookeeper_mt</code>, as it supports both the
        synchronous and asynchronous APIs. For details on how to build and use
        the C API, refer to the <em class="calibre10">README</em>
        file in the <em class="calibre10">src/c</em> directory of
        the ZooKeeper distribution.</p><div class="sidebar"><a id="calibre_link-4665" class="calibre"></a><div class="sidebar-title">Should I Use the Synchronous or Asynchronous API?</div><p class="calibre2">Both APIs offer the same functionality, so the one you use is
          largely a matter of style. The asynchronous API is appropriate if
          you have an event-driven programming model, for example.</p><p class="calibre2">The asynchronous API allows you to pipeline requests, which in
          some scenarios can offer better throughput. Imagine that you want to
          read a large batch of znodes and process them independently. Using
          the synchronous API, each read would block until it returned,
          whereas with the asynchronous API, you can fire off all the
          asynchronous reads very quickly and process the responses in a
          separate thread as they come back.</p></div></div><div class="book" title="Watch triggers"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-114">Watch triggers</h4></div></div></div><p class="calibre2">The read <a class="calibre" id="calibre_link-2836"></a><a class="calibre" id="calibre_link-3774"></a>operations <code class="literal">exists</code>,
        <code class="literal">getChildren</code>, and <code class="literal">getData</code> may have watches set on them, and
        the watches are triggered by write operations: <code class="literal">create</code>, <code class="literal">delete</code>, and <code class="literal">setData</code>. ACL operations do not participate
        in watches. When a watch is triggered, a watch event is generated, and
        the watch event’s type depends both on the watch and the operation
        that triggered it:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">A watch set on an <code class="literal">exists</code>
            operation will be triggered when the znode being watched is
            created, deleted, or has its data updated.</p></li><li class="listitem"><p class="calibre2">A watch set on a <code class="literal">getData</code>
            operation will be triggered when the znode being watched is
            deleted or has its data updated. No trigger can occur on creation
            because the znode must already exist for the <code class="literal">getData</code> operation to succeed.</p></li><li class="listitem"><p class="calibre2">A watch set on a <code class="literal">getChildren</code> operation will be triggered
            when a child of the znode being watched is created or deleted, or
            when the znode itself is deleted. You can tell whether the znode
            or its child was deleted by looking at the watch event type:
            <code class="literal">NodeDeleted</code> shows the znode was
            deleted, and <code class="literal">NodeChildrenChanged</code> indicates that it
            was a child that was deleted.</p></li></ul></div><p class="calibre2">The combinations are summarized in <a class="ulink" href="#calibre_link-105" title="Table&nbsp;21-3.&nbsp;Watch creation operations and their corresponding triggers">Table&nbsp;21-3</a>.</p><div class="table"><a id="calibre_link-105" class="calibre"></a><div class="table-title">Table&nbsp;21-3.&nbsp;Watch creation operations and their corresponding
          triggers</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">&nbsp;</td><td colspan="5" class="calibre21">Watch trigger</td></tr><tr class="calibre26"><td class="calibre20">&nbsp;</td><td colspan="2" class="calibre20">&nbsp;</td><td colspan="2" class="calibre20">&nbsp;</td><td class="calibre21">&nbsp;</td></tr><tr class="calibre19"><td class="calibre20">Watch creation</td><td class="calibre20">create znode</td><td class="calibre20">create child</td><td class="calibre20">delete znode</td><td class="calibre20">delete child</td><td class="calibre21">setData</td></tr></thead><tbody class="calibre22"><tr class="calibre48"><td class="calibre23"><code class="uri">exists</code></td><td class="calibre23"><code class="uri">NodeCreated</code></td><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">NodeDeleted</code></td><td class="calibre23">&nbsp;</td><td class="calibre25"><code class="uri">NodeDataChanged</code></td></tr><tr class="calibre49"><td class="calibre23"><code class="uri">getData</code></td><td class="calibre23">&nbsp;</td><td class="calibre23">&nbsp;</td><td class="calibre23"><code class="uri">NodeDeleted</code></td><td class="calibre23">&nbsp;</td><td class="calibre25"><code class="uri">NodeDataChanged</code></td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">getChildren</code></td><td class="calibre27">&nbsp;</td><td class="calibre27"><code class="uri">NodeChildrenChanged</code></td><td class="calibre27"><code class="uri">NodeDeleted</code></td><td class="calibre27"><code class="uri">NodeChildrenChanged</code></td><td class="calibre28">&nbsp;</td></tr></tbody></table></div></div><p class="calibre2">A watch event includes the path of the znode that was involved
        in the event, so for <code class="literal">NodeCreated</code>
        and <code class="literal">NodeDeleted</code> events, you can
        tell which node was created or deleted simply by inspecting the path.
        To discover which children have changed after a <code class="literal">NodeChildrenChanged</code> event, you need to call
        <code class="literal">getChildren</code> again to retrieve the
        new list of children. Similarly, to discover the new data for a
        <code class="literal">NodeDataChanged</code> event, you need to
        call <code class="literal">getData</code>. In both of these
        cases, the state of the znodes may have changed between receiving the
        watch event and performing the read operation, so you should bear this
        in mind when writing applications.</p></div><div class="book" title="ACLs"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4666">ACLs</h4></div></div></div><p class="calibre2">A znode is <a class="calibre" id="calibre_link-863"></a><a class="calibre" id="calibre_link-3913"></a><a class="calibre" id="calibre_link-860"></a>created with a list of ACLs, which determine who can perform certain
          operations on it.</p><p class="calibre2">ACLs depend on <a class="calibre" id="calibre_link-926"></a>authentication, the process by which the client
        identifies itself to ZooKeeper. There are a few authentication schemes
        that ZooKeeper provides:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term"><code class="literal1">digest</code></span></dt><dd class="calibre8"><p class="calibre2">The client is authenticated by a username and
              password.</p></dd><dt class="calibre7"><span class="term"><code class="literal1">sasl</code></span></dt><dd class="calibre8"><p class="calibre2">The client is authenticated using <a class="calibre" id="calibre_link-2303"></a><a class="calibre" id="calibre_link-930"></a>Kerberos.</p></dd><dt class="calibre7"><span class="term"><code class="literal1">ip</code></span></dt><dd class="calibre8"><p class="calibre2">The client is authenticated by its IP address.</p></dd></dl></div><p class="calibre2">Clients may authenticate themselves after establishing a
        ZooKeeper session. Authentication is optional, although a znode’s ACL
        may require an authenticated client, in which case the client must
        authenticate itself to access the znode. Here is an example of using
        the <code class="literal">digest</code> scheme to authenticate
        with a username and password:</p><a id="calibre_link-4667" class="calibre"></a><pre class="screen1"><code class="n">zk</code><code class="o">.</code><code class="na">addAuthInfo</code><code class="o">(</code><code class="sb">"digest"</code><code class="o">,</code> <code class="sb">"tom:secret"</code><code class="o">.</code><code class="na">getBytes</code><code class="o">());</code></pre><p class="calibre2">An ACL is the combination of an authentication scheme, an
        identity for that scheme, and a set of permissions. For example, if we
        wanted to give a client with the IP address <code class="literal">10.0.0.1</code> read access to a znode, we would
        set an ACL on the znode with the <code class="literal">ip</code>
        scheme, an ID of <code class="literal">10.0.0.1</code>, and
        <code class="literal">READ</code> permission. In Java, we would
        create the <code class="literal">ACL</code> object as
        follows:</p><a id="calibre_link-4668" class="calibre"></a><pre class="screen1"><code class="k">new</code> <code class="nf">ACL</code><code class="o">(</code><code class="n">Perms</code><code class="o">.</code><code class="na">READ</code><code class="o">,</code>
<code class="k">new</code> <code class="nf">Id</code><code class="o">(</code><code class="sb">"ip"</code><code class="o">,</code> <code class="sb">"10.0.0.1"</code><code class="o">));</code></pre><p class="calibre2">The full set of permissions are <a class="calibre" id="calibre_link-2975"></a>listed in <a class="ulink" href="#calibre_link-106" title="Table&nbsp;21-4.&nbsp;ACL permissions">Table&nbsp;21-4</a>. Note that
        the <code class="literal">exists</code> operation is not
        governed by an ACL permission, so any client may <a class="calibre" id="calibre_link-1283"></a><a class="calibre" id="calibre_link-3157"></a><a class="calibre" id="calibre_link-3809"></a><a class="calibre" id="calibre_link-1424"></a><a class="calibre" id="calibre_link-869"></a>call <code class="literal">exists</code> to find
        the <code class="literal">Stat</code> for a znode or to discover
        that a znode does not in fact exist.</p><div class="table"><a id="calibre_link-106" class="calibre"></a><div class="table-title">Table&nbsp;21-4.&nbsp;ACL permissions</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">ACL permission</td><td class="calibre21">Permitted operations</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">CREATE</code></td><td class="calibre25"><code class="uri">create</code> (a child
                znode)</td></tr><tr class="calibre26"><td rowspan="2" class="calibre23"><code class="uri">READ</code></td><td class="calibre25"><code class="uri">getChildren</code></td></tr><tr class="calibre19"><td class="calibre25"><code class="uri">getData</code></td></tr><tr class="calibre26"><td class="calibre23"><code class="uri">WRITE</code></td><td class="calibre25"><code class="uri">setData</code></td></tr><tr class="calibre19"><td class="calibre23"><code class="uri">DELETE</code></td><td class="calibre25"><code class="uri">delete</code> (a child
                znode)</td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">ADMIN</code></td><td class="calibre28"><code class="uri">setACL</code></td></tr></tbody></table></div></div><p class="calibre2">There are a number of predefined ACLs in the <code class="literal">ZooDefs.Ids</code> class, including <code class="literal">OPEN_ACL_UNSAFE</code>, which gives all permissions (except <code class="literal">ADMIN</code> permission) to everyone.</p><p class="calibre2">In addition, ZooKeeper has a pluggable authentication mechanism,
        which makes it possible to integrate third-party authentication
        systems if <a class="calibre" id="calibre_link-3948"></a>needed.</p></div></div><div class="book" title="Implementation"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4669">Implementation</h3></div></div></div><p class="calibre2">The ZooKeeper <a class="calibre" id="calibre_link-3944"></a>service can run in two modes. In <em class="calibre10">standalone mode</em>, there
        is a single ZooKeeper server, which is useful for testing due to its simplicity (it can even
        be embedded in unit tests) but provides no guarantees
        of high availability or resilience. In production, ZooKeeper runs <a class="calibre" id="calibre_link-3213"></a>in <em class="calibre10">replicated mode</em> on a cluster of machines called an
          <em class="calibre10">ensemble</em>. ZooKeeper achieves high availability through replication,
        and can provide a service as long as a majority of the machines in the ensemble are up. For
        example, in a five-node ensemble, any two machines can fail and the service will still work
        because a majority of three remain. Note that a six-node ensemble can also tolerate only two
        machines failing, because if three machines fail, the remaining three do not constitute a
        majority of the six. For this reason, it is usual to have an odd number of machines in an
        ensemble.</p><p class="calibre2">Conceptually, ZooKeeper is very simple: all it has to do is ensure
      that every modification to the tree of znodes is replicated to a
      majority of the ensemble. If a minority of the machines fail, then a
      minimum of one machine will survive with the latest state. The other
      remaining replicas will eventually catch up with this state.</p><p class="calibre2">The implementation of this simple idea, however, is nontrivial. ZooKeeper uses a
        protocol <a class="calibre" id="calibre_link-3910"></a>called <span class="calibre">Zab</span> that runs in two phases, which may
        be repeated indefinitely:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Phase 1: Leader election</span></dt><dd class="calibre8"><p class="calibre2">The machines in an <a class="calibre" id="calibre_link-2316"></a>ensemble go through a process of electing a
            distinguished member, called the <em class="calibre10">leader</em>.
            The other machines are termed <em class="calibre10">followers</em>.
            This phase is finished once a majority (or
            <em class="calibre10">quorum</em>) of followers have synchronized
            their state with the leader.</p></dd><dt class="calibre7"><span class="term">Phase 2: Atomic broadcast</span></dt><dd class="calibre8"><p class="calibre2">All write <a class="calibre" id="calibre_link-923"></a>requests are forwarded to the leader, which
            broadcasts the update to the followers. When a majority have
            persisted the change, the leader commits the update, and the
            client gets a response saying the update succeeded. The protocol
            for achieving consensus is designed to be atomic, so a change
            either succeeds or fails. It resembles a two-phase commit.</p></dd></dl></div><div class="sidebar"><a id="calibre_link-4670" class="calibre"></a><div class="sidebar-title">Does ZooKeeper Use Paxos?</div><p class="calibre2">No. ZooKeeper’s Zab protocol is not the same as the <a class="calibre" id="calibre_link-2959"></a>well-known Paxos algorithm.<sup class="calibre5">[<a class="firstname" href="#calibre_link-107" id="calibre_link-126">145</a>]</sup> Zab is similar, but it differs in several
        aspects of its operation, such as relying on TCP for its message
        ordering guarantees.<sup class="calibre5">[<a class="firstname" href="#calibre_link-108" id="calibre_link-127">146</a>]</sup></p><p class="calibre2">Google’s Chubby Lock Service,<sup class="calibre5">[<a class="firstname" href="#calibre_link-109" id="calibre_link-128">147</a>]</sup> which
        shares similar goals with ZooKeeper, is based on Paxos.</p></div><p class="calibre2">If the leader fails, the remaining machines hold another leader
      election and continue as before with the new leader. If the old leader
      later recovers, it then starts as a follower. Leader election is very
      fast, around 200 ms according to <a class="ulink" href="http://bit.ly/dist_coordination" target="_top">one published result</a>, so performance does not noticeably degrade during an
      election.</p><p class="calibre2">All machines in the ensemble write updates to disk before updating their in-memory
        copies of the znode tree. Read requests may be serviced from any machine, and because they
        involve only a lookup from memory, they are very fast.</p></div><div class="book" title="Consistency"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-104">Consistency</h3></div></div></div><p class="calibre2">Understanding the<a class="calibre" id="calibre_link-3936"></a> basis of ZooKeeper’s implementation helps in understanding the consistency
        guarantees that the service makes. The terms “leader” and “follower” for the machines in an
        ensemble are apt because they make the point that a follower may lag the leader by a number
        of updates. This is a consequence of the fact that only a majority and not all members of
        the ensemble need to have persisted a change before it is committed. A good mental model for
        ZooKeeper is of clients connected to ZooKeeper servers that are following the leader. A
        client may actually be connected to the leader, but it has no control over this and cannot
        even know if this is the case.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-110" id="calibre_link-129">148</a>]</sup> See <a class="ulink" href="#calibre_link-111" title="Figure&nbsp;21-2.&nbsp;Reads are satisfied by followers, whereas writes are committed by the leader">Figure&nbsp;21-2</a>.</p><div class="book"><div class="figure"><a id="calibre_link-111" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4671" class="calibre"></a><img alt="Reads are satisfied by followers, whereas writes are committed by the leader" src="images/000073.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;21-2.&nbsp;Reads are satisfied by followers, whereas writes are committed
        by the leader</div></div></div><p class="calibre2">Every update made to the znode tree is given a globally unique identifier, <a class="calibre" id="calibre_link-3956"></a>called a <span class="calibre">zxid</span> (which stands for “ZooKeeper
        transaction ID”). Updates are ordered, so if <span class="calibre">zxid</span>
        <span class="calibre">z</span><sub class="calibre50">1</sub> is less than <span class="calibre">z</span><sub class="calibre50">2</sub>, then <span class="calibre">z</span><sub class="calibre50">1</sub> happened before <span class="calibre">z</span><sub class="calibre50">2</sub>, according to ZooKeeper (which is the single
        authority on ordering in the distributed system).</p><p class="calibre2">The following guarantees for data consistency flow from
      ZooKeeper’s design:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Sequential consistency</span></dt><dd class="calibre8"><p class="calibre2">Updates from any particular client are applied in the order
            that they are sent. This means that if a client updates the znode
            <span class="calibre">z</span> to the value <span class="calibre">a</span>, and in a later operation, it updates
            <span class="calibre">z</span> to the value <span class="calibre">b</span>, then no client will ever see <span class="calibre">z</span> with value <span class="calibre">a</span> after it has seen it with value
            <span class="calibre">b</span> (if no other updates are made
            to <span class="calibre">z</span>).</p></dd><dt class="calibre7"><span class="term">Atomicity</span></dt><dd class="calibre8"><p class="calibre2">Updates either succeed or fail. This means that if an update
            fails, no client will ever see it.</p></dd><dt class="calibre7"><span class="term">Single system image</span></dt><dd class="calibre8"><p class="calibre2">A client will see the same view of the system, regardless of
            the server it connects to. This means that if a client connects to
            a new server during the same session, it will not see an older
            state of the system than the one it saw with the previous server.
            When a server fails and a client tries to connect to another in
            the ensemble, a server that is behind the one that failed will not
            accept connections from the client until it has caught up with the
            failed server.</p></dd><dt class="calibre7"><span class="term">Durability</span></dt><dd class="calibre8"><p class="calibre2">Once an update has succeeded, it will persist and will not
            be undone. This means updates will survive server failures.</p></dd><dt class="calibre7"><span class="term">Timeliness</span></dt><dd class="calibre8"><p class="calibre2">The lag in any client’s view of the system is bounded, so it
            will not be out of date by more than some multiple of tens of
            seconds. This means that rather than allow a client to see data
            that is very stale, a server will shut down, forcing the client to
            switch to a more up-to-date server.</p></dd></dl></div><p class="calibre2">For performance reasons, reads are satisfied from a ZooKeeper server’s memory and do not
        participate in the global ordering of writes. This property can lead to the appearance of
        inconsistent ZooKeeper states from clients that communicate through a mechanism outside
        ZooKeeper: for example, client A updates znode <span class="calibre">z</span> from
          <span class="calibre">a</span> to <span class="calibre">a’</span>, A tells B
        to read <span class="calibre">z</span>, and B reads the value of <span class="calibre">z</span> as <span class="calibre">a</span>, not <span class="calibre">a’</span>. This is perfectly compatible with the guarantees that
        ZooKeeper makes (the condition that it does <span class="calibre">not</span> promise
        is called “simultaneously consistent cross-client views”). To prevent this condition from
        happening, B should call <code class="literal">sync</code> on <span class="calibre">z</span> before reading <span class="calibre">z</span>’s value. The <code class="literal">sync</code> operation forces the ZooKeeper server to which B is
        connected to “catch up” with the leader, so that when B reads <span class="calibre">z</span>’s value, it will be the one that A set (or a later <a class="calibre" id="calibre_link-3937"></a>value).</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Slightly confusingly, the <code class="literal">sync</code> operation is available only as an
        <span class="calibre">asynchronous</span> call. This is because
        you don’t need to wait for it to return, since ZooKeeper guarantees
        that any subsequent operation will happen after the <code class="literal">sync</code> completes on the server, even if the
        operation is issued before the <code class="literal">sync</code>
        completes.</p></div></div><div class="book" title="Sessions"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4672">Sessions</h3></div></div></div><p class="calibre2">A ZooKeeper <a class="calibre" id="calibre_link-3951"></a>client is configured with the list of servers in the ensemble. On startup, it
        tries to connect to one of the servers in the list. If the connection fails, it tries
        another server in the list, and so on, until it either successfully connects to one of them
        or fails because all ZooKeeper servers are unavailable.</p><p class="calibre2">Once a connection has been made with a ZooKeeper server, the
      server creates a new session for the client. A session has a timeout
      period that is decided on by the application that creates it. If the
      server hasn’t received a request within the timeout period, it may
      expire the session. Once a session has expired, it may not be reopened,
      and any ephemeral nodes associated with the session will be lost.
      Although session expiry is a comparatively rare event, since sessions
      are long lived, it is important for applications to handle it (we will
      see how in <a class="ulink" href="#calibre_link-112" title="The Resilient ZooKeeper Application">The Resilient ZooKeeper Application</a>).</p><p class="calibre2">Sessions are kept alive by the client sending ping requests (also
      known as heartbeats) whenever the session is idle for longer than a
      certain period. (Pings are automatically sent by the ZooKeeper client
      library, so your code doesn’t need to worry about maintaining the
      session.) The period is chosen to be low enough to detect server failure
      (manifested by a read timeout) and reconnect to another server within
      the session timeout period.</p><p class="calibre2">Failover to <a class="calibre" id="calibre_link-1605"></a>another ZooKeeper server is handled automatically by the
      ZooKeeper client, and crucially, sessions (and associated ephemeral
      znodes) are still valid after another server takes over from the failed
      one.</p><p class="calibre2">During failover, the application will receive notifications of
      disconnections and connections to the service. Watch notifications will
      not be delivered while the client is disconnected, but they will be
      delivered when the client successfully reconnects. Also, if the
      application tries to perform an operation while the client is
      reconnecting to another server,
      the operation will fail. This underlines the importance of handling
      connection loss exceptions in real-world ZooKeeper applications
      (described in <a class="ulink" href="#calibre_link-112" title="The Resilient ZooKeeper Application">The Resilient ZooKeeper Application</a>).</p><div class="book" title="Time"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4673">Time</h4></div></div></div><p class="calibre2">There are several time <a class="calibre" id="calibre_link-3690"></a>parameters in ZooKeeper. The <em class="calibre10">tick
        time</em> is the fundamental period of time in ZooKeeper and is
        used by servers in the ensemble to define the schedule on which their
        interactions run.
        Other settings are defined in terms of tick time, or are at least
        constrained by it. The session timeout, for example, may not be less
        than 2 ticks or more than 20. If you attempt to set a session timeout
        outside this range, it will be modified to fall within the
        range.</p><p class="calibre2">A common tick time setting is 2 seconds (2,000 milliseconds).
        This translates to an allowable session timeout of between 4 and 40
        seconds.</p><p class="calibre2">There are a few considerations in selecting a session
          timeout. A low session timeout leads to faster detection of machine
        failure. In the group membership example, the session timeout is the
        time it takes for a failed machine to be removed from the group.
        Beware of setting the session timeout too low, however, because a busy
        network can cause packets to be delayed and may cause inadvertent
        session expiry. In such an event, a machine would appear to “flap”:
        leaving and then rejoining the group repeatedly in a short space of
        time.</p><p class="calibre2">Applications that create more complex ephemeral state should
        favor longer session timeouts, as the cost of reconstruction is
        higher. In some cases, it is possible to design the application so it
        can restart within the session timeout period and avoid session
        expiry. (This might be desirable to perform maintenance or upgrades.)
        Every session is given a unique identity and password by the server,
        and if these are passed to ZooKeeper while a connection is being made,
        it is possible to recover a session (as long as it hasn’t expired). An
        application can therefore arrange a graceful shutdown, whereby it
        stores the session identity and password to stable storage before
        restarting the process, retrieving the stored session identity and
        password, and recovering the session.</p><p class="calibre2">You should view this feature as an optimization that can help
        avoid expired sessions. It does not remove the need to handle session
        expiry, which can still occur if a machine fails unexpectedly, or even
        if an application is shut down gracefully but does not restart before
        its session expires, for whatever reason.</p><p class="calibre2">As a general rule, the larger the ZooKeeper ensemble, the larger
        the session timeout should be. Connection timeouts, read timeouts, and
        ping periods are all defined internally as a function of the number of
        servers in the ensemble, so as the ensemble grows, these periods
        decrease. Consider increasing the timeout if you experience frequent
        connection loss. You can monitor ZooKeeper metrics—such as request
        <a class="calibre" id="calibre_link-3952"></a>latency statistics—using JMX.</p></div></div><div class="book" title="States"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4674">States</h3></div></div></div><p class="calibre2">The <code class="literal">ZooKeeper</code> object
      transitions<a class="calibre" id="calibre_link-3953"></a><a class="calibre" id="calibre_link-3525"></a> through different states in its lifecycle (see <a class="ulink" href="#calibre_link-113" title="Figure&nbsp;21-3.&nbsp;ZooKeeper state transitions">Figure&nbsp;21-3</a>). You can query its state at any time by
      using the <code class="literal">getState()</code> method:</p><a id="calibre_link-4675" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="n">States</code> <code class="nf">getState</code><code class="o">()</code></pre><p class="calibre2"><code class="literal">States</code> is an enum representing
      the different states that a <code class="literal">ZooKeeper</code>
      object may be in. (Despite the enum’s name, an instance of <code class="literal">ZooKeeper</code> may be in only one state at a time.)
      A newly constructed <code class="literal">ZooKeeper</code>
      instance is in the <code class="literal">CONNECTING</code> state
      <a class="calibre" id="calibre_link-1238"></a>while it tries to establish a connection with the ZooKeeper
      service. Once a connection is established, it goes <a class="calibre" id="calibre_link-1237"></a>into the <code class="literal">CONNECTED</code>
      state.</p><div class="book"><div class="figure"><a id="calibre_link-113" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4676" class="calibre"></a><img alt="ZooKeeper state transitions" src="images/000083.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;21-3.&nbsp;ZooKeeper state transitions</div></div></div><p class="calibre2">A client using the <code class="literal">ZooKeeper</code>
      object can receive notifications of the state transitions by registering
      a <code class="literal">Watcher</code> object. On entering the
      <code class="literal">CONNECTED</code> state, the watcher receives
      a <code class="literal">WatchedEvent</code> whose <code class="literal">KeeperState</code> value is <code class="literal">SyncConnected</code>.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">A ZooKeeper <code class="literal">Watcher</code> object
        serves double duty: it can be used to be notified of changes in the
        ZooKeeper state (as described in this section), and it can be used to
        be notified of changes in znodes (described in <a class="ulink" href="#calibre_link-114" title="Watch triggers">Watch triggers</a>). The (default) watcher passed into the
        <code class="literal">ZooKeeper</code> object constructor is
        used for state changes, but znode changes may either use a dedicated
        instance of <code class="literal">Watcher</code> (by passing one
        in to the appropriate read operation) or share the default one if
        using the form of the read operation that takes a Boolean flag to
        specify whether to use a watcher.</p></div><p class="calibre2">The <code class="literal">ZooKeeper</code> instance may
      disconnect and reconnect to the ZooKeeper service, moving between the
      <code class="literal">CONNECTED</code> and <code class="literal">CONNECTING</code> states. If it disconnects, the
      watcher receives a <code class="literal">Disconnected</code>
      event. Note that these state transitions are initiated by the <code class="literal">ZooKeeper</code> instance itself, and it will
      automatically try to reconnect if the connection is lost.</p><p class="calibre2">The <code class="literal">ZooKeeper</code> instance may transition to a third <a class="calibre" id="calibre_link-1101"></a>state, <code class="literal">CLOSED</code>, if either the
          <code class="literal">close()</code> method is called or the session times out, as indicated
        by a <code class="literal">KeeperState</code> of type <code class="literal">Expired</code>. Once in the <code class="literal">CLOSED</code> state, the <code class="literal">ZooKeeper</code> object is no
        longer considered to be alive (this can be tested using the
          <code class="literal">isAlive()</code> method on <code class="literal">States</code>)
        and cannot be reused. To reconnect to the ZooKeeper service, the client must construct
          <a class="calibre" id="calibre_link-3954"></a><a class="calibre" id="calibre_link-3526"></a>a new <code class="literal">ZooKeeper</code>
        instance.</p></div></div><div class="book" title="Building Applications with ZooKeeper"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4677">Building Applications with ZooKeeper</h2></div></div></div><p class="calibre2">Having covered ZooKeeper in some depth, let’s turn back to writing
    some useful applications with
    it.</p><div class="book" title="A Configuration Service"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-102">A Configuration Service</h3></div></div></div><p class="calibre2">One of the most <a class="calibre" id="calibre_link-3929"></a>basic services that a distributed application needs is a configuration service,
        so that common pieces of configuration information can be shared by machines in a cluster.
        At the simplest level, ZooKeeper can act as a highly available store for configuration,
        allowing application participants to retrieve or update configuration files. Using ZooKeeper
        watches, it is possible to create an active configuration service, where interested clients
        are notified of changes in configuration.</p><p class="calibre2">Let’s write such a service. We make a couple of assumptions that simplify the
        implementation (they could be removed with a little more work). First, the only
        configuration values we need to store are strings, and keys are just znode paths, so we use
        a znode to store each key-value pair. Second, there is a single client performing updates at
        any one time. Among other things, this model fits with the idea of a master (such as the
        namenode in HDFS) that wishes to update information that its workers need to follow.</p><p class="calibre2">We wrap the code up in a class called <code class="literal">ActiveKeyValueStore</code>:</p><a id="calibre_link-4678" class="calibre"></a><pre class="screen1"><code class="k">public</code> <code class="k">class</code> <code class="nc">ActiveKeyValueStore</code> <code class="k">extends</code> <code class="n">ConnectionWatcher</code> <code class="o">{</code>

  <code class="k">private</code> <code class="k">static</code> <code class="k">final</code> <code class="n">Charset</code> <code class="n">CHARSET</code> <code class="o">=</code> <code class="n">Charset</code><code class="o">.</code><code class="na">forName</code><code class="o">(</code><code class="sb">"UTF-8"</code><code class="o">);</code>

  <code class="k">public</code> <code class="kt">void</code> <code class="nf">write</code><code class="o">(</code><code class="n">String</code> <code class="n">path</code><code class="o">,</code> <code class="n">String</code> <code class="n">value</code><code class="o">)</code> <code class="k">throws</code> <code class="n">InterruptedException</code><code class="o">,</code>
      <code class="n">KeeperException</code> <code class="o">{</code>
    <code class="n">Stat</code> <code class="n">stat</code> <code class="o">=</code> <code class="n">zk</code><code class="o">.</code><code class="na">exists</code><code class="o">(</code><code class="n">path</code><code class="o">,</code> <code class="k">false</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">stat</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">zk</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">path</code><code class="o">,</code> <code class="n">value</code><code class="o">.</code><code class="na">getBytes</code><code class="o">(</code><code class="n">CHARSET</code><code class="o">),</code> <code class="n">Ids</code><code class="o">.</code><code class="na">OPEN_ACL_UNSAFE</code><code class="o">,</code>
          <code class="n">CreateMode</code><code class="o">.</code><code class="na">PERSISTENT</code><code class="o">);</code>
    <code class="o">}</code> <code class="k">else</code> <code class="o">{</code>
      <code class="n">zk</code><code class="o">.</code><code class="na">setData</code><code class="o">(</code><code class="n">path</code><code class="o">,</code> <code class="n">value</code><code class="o">.</code><code class="na">getBytes</code><code class="o">(</code><code class="n">CHARSET</code><code class="o">),</code> <code class="o">-</code><code class="mi">1</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code>
<code class="o">}</code></pre><p class="calibre2">The contract of the <code class="literal">write()</code> method is
      that a key with the given value is written to ZooKeeper. It hides the
      difference between creating a new znode and updating an existing znode
      with a new value by testing first for the znode using the <code class="literal">exists</code> operation and then performing the
      appropriate operation. The other detail worth mentioning is the need to
      convert the string value to a byte array, for which we just use the
      <code class="literal">getBytes()</code> method with a
      UTF-8 encoding.</p><p class="calibre2">To illustrate the use of the <code class="literal">ActiveKeyValueStore</code>, consider a <code class="literal">ConfigUpdater</code> class that updates a
      configuration property with a value. The listing appears in <a class="ulink" href="#calibre_link-115" title="Example&nbsp;21-6.&nbsp;An application that updates a property in ZooKeeper at random times">Example&nbsp;21-6</a>.</p><div class="example"><a id="calibre_link-115" class="calibre"></a><div class="example-title">Example&nbsp;21-6.&nbsp;An application that updates a property in ZooKeeper at random
        times</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">ConfigUpdater</code> <code class="o">{</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="k">final</code> <code class="n">String</code> <code class="n">PATH</code> <code class="o">=</code> <code class="sb">"/config"</code><code class="o">;</code>
  
  <code class="k">private</code> <code class="n">ActiveKeyValueStore</code> <code class="n">store</code><code class="o">;</code>
  <code class="k">private</code> <code class="n">Random</code> <code class="n">random</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Random</code><code class="o">();</code>
  
  <code class="k">public</code> <code class="nf">ConfigUpdater</code><code class="o">(</code><code class="n">String</code> <code class="n">hosts</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="n">store</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ActiveKeyValueStore</code><code class="o">();</code>
    <code class="n">store</code><code class="o">.</code><code class="na">connect</code><code class="o">(</code><code class="n">hosts</code><code class="o">);</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">run</code><code class="o">()</code> <code class="k">throws</code> <code class="n">InterruptedException</code><code class="o">,</code> <code class="n">KeeperException</code> <code class="o">{</code>
    <code class="k">while</code> <code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">String</code> <code class="n">value</code> <code class="o">=</code> <code class="n">random</code><code class="o">.</code><code class="na">nextInt</code><code class="o">(</code><code class="mi">100</code><code class="o">)</code> <code class="o">+</code> <code class="sb">""</code><code class="o">;</code>
      <code class="n">store</code><code class="o">.</code><code class="na">write</code><code class="o">(</code><code class="n">PATH</code><code class="o">,</code> <code class="n">value</code><code class="o">);</code>
      <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"Set %s to %s\n"</code><code class="o">,</code> <code class="n">PATH</code><code class="o">,</code> <code class="n">value</code><code class="o">);</code>
      <code class="n">TimeUnit</code><code class="o">.</code><code class="na">SECONDS</code><code class="o">.</code><code class="na">sleep</code><code class="o">(</code><code class="n">random</code><code class="o">.</code><code class="na">nextInt</code><code class="o">(</code><code class="mi">10</code><code class="o">));</code>
    <code class="o">}</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">ConfigUpdater</code> <code class="n">configUpdater</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ConfigUpdater</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]);</code>
    <code class="n">configUpdater</code><code class="o">.</code><code class="na">run</code><code class="o">();</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">The program is simple. A <code class="literal">ConfigUpdater</code> has an
          <code class="literal">ActiveKeyValueStore</code> that connects to ZooKeeper in the
          <code class="literal">ConfigUpdater</code>’s constructor. The
          <code class="literal">run()</code> method loops forever, updating the <em class="calibre10">/config</em> znode at random times with random values.</p><p class="calibre2">Next, let’s look at how to read the <em class="calibre10">/config</em> configuration property. First, we add
      a read method to <code class="literal">ActiveKeyValueStore</code>:</p><a id="calibre_link-4679" class="calibre"></a><pre class="screen1">  <code class="k">public</code> <code class="n">String</code> <code class="nf">read</code><code class="o">(</code><code class="n">String</code> <code class="n">path</code><code class="o">,</code> <code class="n">Watcher</code> <code class="n">watcher</code><code class="o">)</code> <code class="k">throws</code> <code class="n">InterruptedException</code><code class="o">,</code>
      <code class="n">KeeperException</code> <code class="o">{</code>
    <code class="kt">byte</code><code class="o">[]</code> <code class="n">data</code> <code class="o">=</code> <code class="n">zk</code><code class="o">.</code><code class="na">getData</code><code class="o">(</code><code class="n">path</code><code class="o">,</code> <code class="n">watcher</code><code class="o">,</code> <code class="k">null</code><code class="c2">/*stat*/</code><code class="o">);</code>
    <code class="k">return</code> <code class="k">new</code> <code class="nf">String</code><code class="o">(</code><code class="n">data</code><code class="o">,</code> <code class="n">CHARSET</code><code class="o">);</code>
  <code class="o">}</code></pre><p class="calibre2">The <code class="literal">getData()</code> method of ZooKeeper takes
      the path, a <code class="literal">Watcher</code>, and a <code class="literal">Stat</code> object. The <code class="literal">Stat</code> object is filled in with values by
      <code class="literal">getData()</code> and is used to pass information back
      to the caller. In this way, the caller can get both the data and the
      metadata for a znode, although in this case, we pass a <code class="literal">null</code> <code class="literal">Stat</code> because we are not interested in the
      metadata.</p><p class="calibre2">As a consumer of the service, <code class="literal">ConfigWatcher</code> (see <a class="ulink" href="#calibre_link-116" title="Example&nbsp;21-7.&nbsp;An application that watches for updates of a property in ZooKeeper and prints them to the console">Example&nbsp;21-7</a>) creates an <code class="literal">ActiveKeyValueStore</code> and, after starting, calls
      the store’s <code class="literal">read()</code> method (in its <code class="literal">displayConfig()</code> method) to pass a reference to
      itself as the watcher. It displays the initial value of the
      configuration that it reads.</p><div class="example"><a id="calibre_link-116" class="calibre"></a><div class="example-title">Example&nbsp;21-7.&nbsp;An application that watches for updates of a property in
        ZooKeeper and prints them to the console</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">ConfigWatcher</code> <code class="k">implements</code> <code class="n">Watcher</code> <code class="o">{</code>
  
  <code class="k">private</code> <code class="n">ActiveKeyValueStore</code> <code class="n">store</code><code class="o">;</code>
  
  <code class="k">public</code> <code class="nf">ConfigWatcher</code><code class="o">(</code><code class="n">String</code> <code class="n">hosts</code><code class="o">)</code> <code class="k">throws</code> <code class="n">IOException</code><code class="o">,</code> <code class="n">InterruptedException</code> <code class="o">{</code>
    <code class="n">store</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ActiveKeyValueStore</code><code class="o">();</code>
    <code class="n">store</code><code class="o">.</code><code class="na">connect</code><code class="o">(</code><code class="n">hosts</code><code class="o">);</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">displayConfig</code><code class="o">()</code> <code class="k">throws</code> <code class="n">InterruptedException</code><code class="o">,</code> <code class="n">KeeperException</code> <code class="o">{</code>
    <code class="n">String</code> <code class="n">value</code> <code class="o">=</code> <code class="n">store</code><code class="o">.</code><code class="na">read</code><code class="o">(</code><code class="n">ConfigUpdater</code><code class="o">.</code><code class="na">PATH</code><code class="o">,</code> <code class="k">this</code><code class="o">);</code>
    <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"Read %s as %s\n"</code><code class="o">,</code> <code class="n">ConfigUpdater</code><code class="o">.</code><code class="na">PATH</code><code class="o">,</code> <code class="n">value</code><code class="o">);</code>
  <code class="o">}</code>

  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="kt">void</code> <code class="nf">process</code><code class="o">(</code><code class="n">WatchedEvent</code> <code class="n">event</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">event</code><code class="o">.</code><code class="na">getType</code><code class="o">()</code> <code class="o">==</code> <code class="n">EventType</code><code class="o">.</code><code class="na">NodeDataChanged</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">try</code> <code class="o">{</code>
        <code class="n">displayConfig</code><code class="o">();</code>
      <code class="o">}</code> <code class="k">catch</code> <code class="o">(</code><code class="n">InterruptedException</code> <code class="n">e</code><code class="o">)</code> <code class="o">{</code>
        <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="sb">"Interrupted. Exiting."</code><code class="o">);</code>        
        <code class="n">Thread</code><code class="o">.</code><code class="na">currentThread</code><code class="o">().</code><code class="na">interrupt</code><code class="o">();</code>
      <code class="o">}</code> <code class="k">catch</code> <code class="o">(</code><code class="n">KeeperException</code> <code class="n">e</code><code class="o">)</code> <code class="o">{</code>
        <code class="n">System</code><code class="o">.</code><code class="na">err</code><code class="o">.</code><code class="na">printf</code><code class="o">(</code><code class="sb">"KeeperException: %s. Exiting.\n"</code><code class="o">,</code> <code class="n">e</code><code class="o">);</code>        
      <code class="o">}</code>
    <code class="o">}</code>
  <code class="o">}</code>
  
  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <code class="n">ConfigWatcher</code> <code class="n">configWatcher</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ConfigWatcher</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]);</code>
    <code class="n">configWatcher</code><code class="o">.</code><code class="na">displayConfig</code><code class="o">();</code>
    
    <code class="c2">// stay alive until process is killed or thread is interrupted</code>
    <code class="n">Thread</code><code class="o">.</code><code class="na">sleep</code><code class="o">(</code><code class="n">Long</code><code class="o">.</code><code class="na">MAX_VALUE</code><code class="o">);</code>
  <code class="o">}</code>
<code class="o">}</code></pre></div></div><p class="calibre2">When the <code class="literal">ConfigUpdater</code> updates
      the znode, ZooKeeper causes the watcher to fire with an event type of
      <code class="literal">EventType.NodeDataChanged</code>. <code class="literal">ConfigWatcher</code> acts on this event in its
      <code class="literal">process()</code> method by reading and displaying the
      latest version of the config.</p><p class="calibre2">Because watches are one-time signals, we tell ZooKeeper of the new watch each time we
        call <code class="literal">read()</code> on <code class="literal">ActiveKeyValueStore</code>, which ensures we see future updates. We are not guaranteed
        to receive every update, though, because the znode may have been updated (possibly many
        times) during the span of time between the receipt of the watch event and the next read, and
        as the client has no watch registered during that period, it is not
        notified.
        For the configuration service, this is not a problem, because clients care only about the
        latest value of a property, as it takes precedence over previous values. However, in general
        you should be aware of this potential limitation.</p><p class="calibre2">Let’s see the code in action. Launch the <code class="literal">ConfigUpdater</code> in one terminal window:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">java ConfigUpdater localhost</code></strong>
Set /config to 79
Set /config to 14
Set /config to 78</pre><p class="calibre2">Then launch the <code class="literal">ConfigWatcher</code>
      in another window immediately <a class="calibre" id="calibre_link-3930"></a>afterward:</p><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">java ConfigWatcher localhost</code></strong>
Read /config as 79
Read /config as 14
Read /config as 78</pre></div><div class="book" title="The Resilient ZooKeeper Application"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-112">The Resilient ZooKeeper Application</h3></div></div></div><p class="calibre2">The first<a class="calibre" id="calibre_link-3934"></a> of the <a class="ulink" href="http://bit.ly/dist_computing" target="_top">Fallacies of Distributed Computing</a> states that “the network is reliable.” As they stand, our programs so far have been
      assuming a reliable network, so when they run on a real network, they
      can fail in several ways. Let’s examine some possible failure modes and what
      we can do to correct them so that our programs are resilient in the face
      of failure.</p><p class="calibre2">Every ZooKeeper operation in the <a class="calibre" id="calibre_link-2831"></a><a class="calibre" id="calibre_link-1581"></a>Java API declares two types of exception in its <code class="literal">throws</code>
      clause: <code class="literal">InterruptedException</code> and
      <code class="literal">KeeperException</code>.</p><div class="book" title="InterruptedException"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4680">InterruptedException</h4></div></div></div><p class="calibre2">An <code class="literal">InterruptedException</code> is
        thrown if the operation is interrupted. There is a standard Java
        mechanism for canceling blocking methods, which is to call
        <code class="literal">interrupt()</code> on the thread from which the
        blocking method was called. A successful cancellation will result in
        an <code class="literal">InterruptedException</code>. ZooKeeper
        adheres to this standard, so you can cancel a ZooKeeper operation in
        this way. Classes or libraries that use ZooKeeper usually should
        propagate the <code class="literal">InterruptedException</code>
          so that their clients can cancel their operations.<sup class="calibre6">[<a class="firstname" href="#calibre_link-117" id="calibre_link-130">149</a>]</sup></p><p class="calibre2">An <code class="literal">InterruptedException</code> does not indicate a
          failure, but rather that the operation has been canceled, so in the configuration
          application example it is appropriate to propagate the exception, causing the application
          to terminate.</p></div><div class="book" title="KeeperException"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4681">KeeperException</h4></div></div></div><p class="calibre2">A <code class="literal">KeeperException</code> is thrown
        if the ZooKeeper server signals an error or if there is a
        communication problem with the server. For different error cases,
        there are various subclasses of <code class="literal">Keeper</code><code class="literal">Exception</code>. For example, <code class="literal">KeeperException.NoNodeException</code> is a
        subclass of <code class="literal">KeeperException</code> that is
        thrown if you try to perform an operation on a znode that doesn’t
        exist.</p><p class="calibre2">Every subclass of <code class="literal">KeeperException</code> has a corresponding code
        with information about the type of error. For example, for <code class="literal">KeeperException.NoNodeException</code>, the code is
        <code class="literal">KeeperException.Code.NONODE</code> (an
        enum value).</p><p class="calibre2">There are two ways, then, to handle <code class="literal">KeeperException</code>: either catch <code class="literal">KeeperException</code> and test its code to determine what remedying action to take,
          or catch the equivalent <code class="literal">KeeperException</code> subclasses and perform the appropriate action in each catch
          block.</p><p class="calibre2"><code class="literal">KeeperException</code>s fall into
        three broad categories.</p><div class="book" title="State exceptions"><div class="titlepage1"><div class="book"><div class="book"><h5 class="title8" id="calibre_link-4682">State exceptions</h5></div></div></div><p class="calibre2">A state exception <a class="calibre" id="calibre_link-3527"></a><a class="calibre" id="calibre_link-3955"></a>occurs when the operation fails because it cannot be
          applied to the znode tree. State exceptions usually happen because
          another process is mutating a znode at the same time. For example, a
          <code class="literal">setData</code> operation with a version
          number will fail with a <code class="literal">KeeperException.BadVersionException</code> if the
          znode is updated by another process first because the version number
          does not match. The programmer is usually aware that this kind of
          conflict is possible and will code to deal with it.</p><p class="calibre2">Some state exceptions indicate an error in the program, such
          as <code class="literal">KeeperException.</code><code class="literal">NoChildrenForEphemeralsException</code>,
          which is thrown when trying to create a child znode of an ephemeral
          znode.</p></div><div class="book" title="Recoverable exceptions"><div class="titlepage1"><div class="book"><div class="book"><h5 class="title8" id="calibre_link-4683">Recoverable exceptions</h5></div></div></div><p class="calibre2">Recoverable exceptions are those from which the application
          can recover within the same ZooKeeper session. A recoverable
          exception is manifested by <code class="literal">KeeperException.ConnectionLossException</code>,
          which means that the connection to ZooKeeper has been lost. ZooKeeper
          will try to reconnect, and in most cases the reconnection will
          succeed and ensure that the session is intact.</p><p class="calibre2">However, ZooKeeper cannot tell if the operation that failed with a <code class="literal">KeeperException.ConnectionLossException</code> was applied. This is
            an example of partial failure (which we introduced at the beginning of the chapter). The
            onus is therefore on the programmer to deal with the uncertainty, and the action that
            should be taken depends on the application.</p><p class="calibre2">At this point, it is useful to make a distinction between
          <em class="calibre10">idempotent</em> and
          <em class="calibre10">nonidempotent</em> operations. An idempotent
          operation is one that may be applied one or more times with the same
          result, such as a read request or an unconditional <code class="literal">setData</code>. These can simply be
          retried.</p><p class="calibre2">A nonidempotent operation cannot be retried indiscriminately, as the effect of
            applying it multiple times is not the same as that of applying it once. The program
            needs a way of detecting whether its update was applied by encoding information in the
            znode’s pathname or its data. We discuss how to deal with failed nonidempotent operations in <a class="ulink" href="#calibre_link-118" title="Recoverable exceptions">Recoverable exceptions</a>, when we look at the implementation of a lock
            service.</p></div><div class="book" title="Unrecoverable exceptions"><div class="titlepage1"><div class="book"><div class="book"><h5 class="title8" id="calibre_link-4684">Unrecoverable exceptions</h5></div></div></div><p class="calibre2">In some cases, the ZooKeeper session becomes invalid—perhaps because of a timeout or
          because the session was closed (both of these scenarios get a
          <code class="literal">KeeperException.SessionExpiredException</code>),
          or perhaps because authentication failed (<code class="literal">KeeperException.AuthFailedException</code>). In
          any case, all ephemeral nodes associated with the session will be
          lost, so the application needs to rebuild its state before
          reconnecting to ZooKeeper.</p></div></div><div class="book" title="A reliable configuration service"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4685">A reliable configuration service</h4></div></div></div><p class="calibre2">Going back to the <code class="literal">write()</code> method in
        <code class="literal">ActiveKeyValueStore</code>, recall that it
        is composed of an <code class="literal">exists</code> operation
        followed by either a <code class="literal">create</code> or a
        <code class="literal">setData</code>:</p><a id="calibre_link-4686" class="calibre"></a><pre class="screen1">  <code class="k">public</code> <code class="kt">void</code> <code class="nf">write</code><code class="o">(</code><code class="n">String</code> <code class="n">path</code><code class="o">,</code> <code class="n">String</code> <code class="n">value</code><code class="o">)</code> <code class="k">throws</code> <code class="n">InterruptedException</code><code class="o">,</code>
      <code class="n">KeeperException</code> <code class="o">{</code>
    <code class="n">Stat</code> <code class="n">stat</code> <code class="o">=</code> <code class="n">zk</code><code class="o">.</code><code class="na">exists</code><code class="o">(</code><code class="n">path</code><code class="o">,</code> <code class="k">false</code><code class="o">);</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">stat</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
      <code class="n">zk</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">path</code><code class="o">,</code> <code class="n">value</code><code class="o">.</code><code class="na">getBytes</code><code class="o">(</code><code class="n">CHARSET</code><code class="o">),</code> <code class="n">Ids</code><code class="o">.</code><code class="na">OPEN_ACL_UNSAFE</code><code class="o">,</code>
          <code class="n">CreateMode</code><code class="o">.</code><code class="na">PERSISTENT</code><code class="o">);</code>
    <code class="o">}</code> <code class="k">else</code> <code class="o">{</code>
      <code class="n">zk</code><code class="o">.</code><code class="na">setData</code><code class="o">(</code><code class="n">path</code><code class="o">,</code> <code class="n">value</code><code class="o">.</code><code class="na">getBytes</code><code class="o">(</code><code class="n">CHARSET</code><code class="o">),</code> <code class="o">-</code><code class="mi">1</code><code class="o">);</code>
    <code class="o">}</code>
  <code class="o">}</code></pre><p class="calibre2">Taken as a whole, the <code class="literal">write()</code> method is
        idempotent, so we can afford to unconditionally retry it. Here’s a
        modified version of the <code class="literal">write()</code> method that
        retries in a loop. It is set to try a maximum number of retries
        (<code class="literal">MAX_RETRIES</code>) and sleeps for
        <code class="literal">RETRY_PERIOD_SECONDS</code> between each
        attempt:</p><a id="calibre_link-4687" class="calibre"></a><pre class="screen1">  <code class="k">public</code> <code class="kt">void</code> <code class="nf">write</code><code class="o">(</code><code class="n">String</code> <code class="n">path</code><code class="o">,</code> <code class="n">String</code> <code class="n">value</code><code class="o">)</code> <code class="k">throws</code> <code class="n">InterruptedException</code><code class="o">,</code>
      <code class="n">KeeperException</code> <code class="o">{</code>
    <code class="kt">int</code> <code class="n">retries</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code>
    <code class="k">while</code> <code class="o">(</code><code class="k">true</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">try</code> <code class="o">{</code>
        <code class="n">Stat</code> <code class="n">stat</code> <code class="o">=</code> <code class="n">zk</code><code class="o">.</code><code class="na">exists</code><code class="o">(</code><code class="n">path</code><code class="o">,</code> <code class="k">false</code><code class="o">);</code>
        <code class="k">if</code> <code class="o">(</code><code class="n">stat</code> <code class="o">==</code> <code class="k">null</code><code class="o">)</code> <code class="o">{</code>
          <code class="n">zk</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">path</code><code class="o">,</code> <code class="n">value</code><code class="o">.</code><code class="na">getBytes</code><code class="o">(</code><code class="n">CHARSET</code><code class="o">),</code> <code class="n">Ids</code><code class="o">.</code><code class="na">OPEN_ACL_UNSAFE</code><code class="o">,</code>
              <code class="n">CreateMode</code><code class="o">.</code><code class="na">PERSISTENT</code><code class="o">);</code>
        <code class="o">}</code> <code class="k">else</code> <code class="o">{</code>
          <code class="n">zk</code><code class="o">.</code><code class="na">setData</code><code class="o">(</code><code class="n">path</code><code class="o">,</code> <code class="n">value</code><code class="o">.</code><code class="na">getBytes</code><code class="o">(</code><code class="n">CHARSET</code><code class="o">),</code> <code class="n">stat</code><code class="o">.</code><code class="na">getVersion</code><code class="o">());</code>
        <code class="o">}</code>
        <code class="k">return</code><code class="o">;</code>
      <code class="o">}</code> <code class="k">catch</code> <code class="o">(</code><code class="n">KeeperException</code><code class="o">.</code><code class="na">SessionExpiredException</code> <code class="n">e</code><code class="o">)</code> <code class="o">{</code>
        <code class="k">throw</code> <code class="n">e</code><code class="o">;</code>
      <code class="o">}</code> <code class="k">catch</code> <code class="o">(</code><code class="n">KeeperException</code> <code class="n">e</code><code class="o">)</code> <code class="o">{</code>
        <code class="k">if</code> <code class="o">(</code><code class="n">retries</code><code class="o">++</code> <code class="o">==</code> <code class="n">MAX_RETRIES</code><code class="o">)</code> <code class="o">{</code>
          <code class="k">throw</code> <code class="n">e</code><code class="o">;</code>
        <code class="o">}</code>
        <code class="c2">// sleep then retry</code>
        <code class="n">TimeUnit</code><code class="o">.</code><code class="na">SECONDS</code><code class="o">.</code><code class="na">sleep</code><code class="o">(</code><code class="n">RETRY_PERIOD_SECONDS</code><code class="o">);</code>
      <code class="o">}</code>
    <code class="o">}</code>
  <code class="o">}</code></pre><p class="calibre2">The code is careful not to retry <code class="literal">KeeperException.SessionExpiredException</code>,
        because when a session expires, the <code class="literal">ZooKeeper</code> object enters the <code class="literal">CLOSED</code> state, from which it can never
        reconnect (refer to <a class="ulink" href="#calibre_link-113" title="Figure&nbsp;21-3.&nbsp;ZooKeeper state transitions">Figure&nbsp;21-3</a>). We simply
        rethrow the exception<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-119" id="calibre_link-131">150</a>]</sup> and let the caller create a new <code class="literal">ZooKeeper</code> instance, so that the whole
        <code class="literal">write()</code> method can be retried. A simple way
        to create a new instance is to create a new <code class="literal">ConfigUpdater</code> (which we’ve actually renamed
        <code class="literal">ResilientConfigUpdater</code>) to recover
        from an expired session:</p><pre class="screen1">  <code class="k">public</code> <code class="k">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="k">throws</code> <code class="n">Exception</code> <code class="o">{</code>
    <span class="calibre24"><strong class="calibre9"><code class="kc">while</code> <code class="o1">(</code><code class="kc">true</code><code class="o1">)</code> <code class="o1">{</code>
      <code class="kc">try</code> <code class="o1">{</code></strong></span>
        <code class="n">ResilientConfigUpdater</code> <code class="n">configUpdater</code> <code class="o">=</code>
          <code class="k">new</code> <code class="nf">ResilientConfigUpdater</code><code class="o">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]);</code>
        <code class="n">configUpdater</code><code class="o">.</code><code class="na">run</code><code class="o">();</code>
      <span class="calibre24"><strong class="calibre9"><code class="o1">}</code> <code class="kc">catch</code> <code class="o1">(</code><code class="n1">KeeperException</code><code class="o1">.</code><code class="na1">SessionExpiredException</code> <code class="n1">e</code><code class="o1">)</code> <code class="o1">{</code>
        <code class="c8">// start a new session</code>
      <code class="o1">}</code> <code class="kc">catch</code> <code class="o1">(</code><code class="n1">KeeperException</code> <code class="n1">e</code><code class="o1">)</code> <code class="o1">{</code>
        <code class="c8">// already retried, so exit</code>
        <code class="n1">e</code><code class="o1">.</code><code class="na1">printStackTrace</code><code class="o1">();</code>
        <code class="kc">break</code><code class="o1">;</code>
      <code class="o1">}</code>
    <code class="o1">}</code></strong></span>
  <code class="o">}</code></pre><p class="calibre2">An alternative way of dealing with session expiry would be to look for a <code class="literal">KeeperState</code> of type <code class="literal">Expired</code>
          in the watcher (that would be the <code class="literal">ConnectionWatcher</code> in
          the example here), and create a new connection when this is detected. This way, we would
          just keep retrying the <code class="literal">write()</code> method, even if we got a
            <code class="literal">KeeperException.SessionExpiredException</code>, since the
          connection should eventually be reestablished. Regardless of the precise mechanics of how
          we recover from an expired session, the important point is that it is a different kind of
          failure from connection loss and needs to be handled differently.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">There’s actually another failure mode that we’ve ignored here.
          When the <code class="literal">ZooKeeper</code> object is
          created, it tries to connect to a ZooKeeper server. If the
          connection fails or times out, then it tries another server in the
          ensemble. If, after trying all of the servers in the ensemble, it
          can’t connect, then it throws an <code class="literal">IOException</code>. The likelihood of all
          ZooKeeper servers being unavailable is low; nevertheless, some
          applications may choose to retry the operation in a loop until
          ZooKeeper is available.</p></div><p class="calibre2">This is just one strategy for retry handling. There are many others, such as using
          exponential backoff, where the period between retries is multiplied by a constant
            each<a class="calibre" id="calibre_link-3935"></a><a class="calibre" id="calibre_link-1582"></a><a class="calibre" id="calibre_link-2832"></a> time.</p></div></div><div class="book" title="A Lock Service"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-100">A Lock Service</h3></div></div></div><p class="calibre2">A <span class="calibre">distributed lock</span> is a <a class="calibre" id="calibre_link-3931"></a><a class="calibre" id="calibre_link-2349"></a>mechanism for providing mutual exclusion between a collection of processes. At
        any one time, only a single process may hold the lock. Distributed locks can be used for
        leader election in a large distributed system, where the leader is the process that holds
        the lock at any point in time.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">Do not confuse ZooKeeper’s own <a class="calibre" id="calibre_link-2317"></a>leader election with a general leader election service,
        which can be built using ZooKeeper primitives (and in fact, one
        implementation is included with ZooKeeper). ZooKeeper’s own leader
        election is not exposed publicly, unlike the type of general leader
        election service we are describing here, which is designed to be used
        by distributed systems that need to agree upon a master
        process.</p></div><p class="calibre2">To implement a distributed lock using ZooKeeper, we use sequential znodes to impose an
        order on the processes vying for the lock. The idea is simple: first, designate a lock
        znode, typically describing the entity being locked on (say, <em class="calibre10">/leader</em>); then, clients that want to acquire the lock create sequential
        ephemeral znodes as children of the lock znode. At any point in time, the client with the
        lowest sequence number holds the lock. For example, if two clients create the znodes at
          <em class="calibre10">/leader/lock-1</em> and <em class="calibre10">/leader/lock-2</em> around the same time, then the client that
        created <em class="calibre10">/leader/lock-1</em> holds the lock, since its znode
        has the lowest sequence number. The ZooKeeper service is the arbiter of order because it
        assigns the sequence numbers.</p><p class="calibre2">The lock may be released simply by deleting the znode <em class="calibre10">/leader/lock-1</em>; alternatively, if the client process dies, it will be deleted
        by virtue of being an ephemeral znode. The client that created <em class="calibre10">/leader/lock-2</em> will then hold the lock because it has the next lowest sequence
        number. It ensures it will be notified that it has the lock by creating a watch that fires
        when znodes go away.</p><p class="calibre2">The pseudocode for lock acquisition is as follows:</p><div class="book"><ol class="orderedlist"><li class="listitem"><p class="calibre2">Create an ephemeral sequential znode named <em class="calibre10">lock-</em> under the lock znode, and remember
          its actual pathname (the return value of the <code class="literal">create</code> operation).</p></li><li class="listitem"><p class="calibre2">Get the children of the lock znode and set a watch.</p></li><li class="listitem"><p class="calibre2">If the pathname of the znode created in step 1 has the lowest number of the children
            returned in step 2, then the lock has been acquired. Exit.</p></li><li class="listitem"><p class="calibre2">Wait for the notification from the watch set in step 2, and go to step 2.</p></li></ol></div><div class="book" title="The herd effect"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4688">The herd effect</h4></div></div></div><p class="calibre2">Although this algorithm is correct, there are some problems with
        it. The first problem is that this implementation suffers from
        <a class="calibre" id="calibre_link-1989"></a>the <em class="calibre10">herd effect</em>. Consider
        hundreds or thousands of clients, all trying to acquire the lock. Each
        client places a watch on the lock znode for changes in its set of
        children. Every time the lock is released or another process starts
        the lock acquisition process, the watch fires, and every client
        receives a notification. The “herd effect” refers to a large number of
        clients being notified of the same event when only a small number of
        them can actually proceed. In this case, only one client will
        successfully acquire the lock, and the process of maintaining and
        sending watch events to all clients causes traffic spikes, which put
        pressure on the ZooKeeper servers.</p><p class="calibre2">To avoid the herd effect, the condition for notification needs to be refined. The key
          observation for implementing locks is that a client needs to be notified only when the
          child znode with the <span class="calibre">previous</span> sequence number goes
          away, not when any child znode is deleted (or created). In our example, if clients have
          created the znodes <em class="calibre10">/leader/lock-1</em>, <em class="calibre10">/leader/lock-2</em>, and <em class="calibre10">/leader/lock-3</em>, then the client holding <em class="calibre10">/leader/lock-3</em> needs to be notified only when <em class="calibre10">/leader/lock-2</em> disappears. It does not need to be notified when <em class="calibre10">/leader/lock-1</em>
          disappears or when a new znode, <em class="calibre10">/leader/lock-4</em>, is added.</p></div><div class="book" title="Recoverable exceptions"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-118">Recoverable exceptions</h4></div></div></div><p class="calibre2">Another problem with <a class="calibre" id="calibre_link-1583"></a><a class="calibre" id="calibre_link-2833"></a>the lock algorithm as it stands is that it doesn’t
        handle the case when the create operation fails due to connection
        loss. Recall that in this case we do not know whether the operation
        succeeded or failed. Creating a sequential znode is a nonidempotent operation, so we can’t simply
          retry, because if the first <code class="literal">create</code> had succeeded we would have an orphaned
        znode that would never be deleted (until the client session ended, at
        least). Deadlock would be the unfortunate result.</p><p class="calibre2">The problem is that after reconnecting, the client can’t tell
        whether it created any of the child znodes. By embedding an identifier
        in the znode name, if it suffers a connection loss, it can check to
        see whether any of the children of the lock node have its identifier
        in their names. If a child contains its identifier, it knows that the
          <code class="literal">create</code> operation succeeded, and it shouldn’t create another child
        znode. If no child has the identifier in its name, the client can
        safely create a new sequential child znode.</p><p class="calibre2">The client’s session identifier is a long integer that is unique
        for the ZooKeeper service and therefore ideal for the purpose of
        identifying a client across connection loss events. The session
        identifier can be obtained by calling the
        <code class="literal">getSessionId()</code> method on the <code class="literal">ZooKeeper</code> Java
        class.</p><p class="calibre2">The ephemeral sequential znode should be created with a name of
        the form <em class="calibre10">lock-<em class="replaceable"><code class="replaceable">&lt;sessionId&gt;</code></em>-</em>, so
        that when the sequence number is appended by ZooKeeper, the name
        becomes <em class="calibre10">lock-<em class="replaceable"><code class="replaceable">&lt;sessionId&gt;</code></em>-<em class="replaceable"><code class="replaceable">&lt;sequenceNumber&gt;</code></em></em>.
        The sequence numbers are unique to the parent, not to the name of the
        child, so this technique allows the child znodes to identify their
        creators as well as impose an order of creation.</p></div><div class="book" title="Unrecoverable exceptions"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4689">Unrecoverable exceptions</h4></div></div></div><p class="calibre2">If a client’s ZooKeeper session expires, the ephemeral znode
        created by the client will be deleted, effectively relinquishing the
        lock (or at least forfeiting the client’s turn to acquire the lock). The
        application using the lock should realize that it no longer holds the
        lock, clean up its state, and then start again by creating a new lock
        object and trying to acquire it. Notice that it is the application
        that controls this process, not the lock implementation, since it
        cannot second-guess how the application needs to clean up its
        state.</p></div><div class="book" title="Implementation"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4690">Implementation</h4></div></div></div><p class="calibre2">Accounting for all of the failure modes is nontrivial, so
        implementing a distributed lock correctly is a delicate matter.
        ZooKeeper comes with a production-quality lock implementation in Java
        called <code class="literal">WriteLock</code> that is very easy
        for clients to <a class="calibre" id="calibre_link-3932"></a><a class="calibre" id="calibre_link-2350"></a>use.</p></div></div><div class="book" title="More Distributed Data Structures and Protocols"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4691">More Distributed Data Structures and Protocols</h3></div></div></div><p class="calibre2">There are many <a class="calibre" id="calibre_link-3933"></a><a class="calibre" id="calibre_link-1365"></a>distributed data structures and protocols that can be
      built with ZooKeeper, such as barriers, queues, and two-phase commit.
      One interesting thing to note is that these are synchronous protocols,
      even though we use asynchronous ZooKeeper primitives (such as
      notifications) to build them.</p><p class="calibre2">The <a class="ulink" href="http://zookeeper.apache.org/" target="_top">ZooKeeper
      website</a> describes several such data structures and protocols in
      pseudocode. ZooKeeper comes with implementations of some of these
      standard recipes (including locks, leader election, and queues); they
      can be found in the <em class="calibre10">recipes</em>
      directory of the distribution.</p><p class="calibre2">The <a class="ulink" href="http://curator.apache.org/" target="_top">Apache Curator
      project</a> also provides an <a class="calibre" id="calibre_link-889"></a>extensive set of ZooKeeper recipes, as well as a simplified
      ZooKeeper client.</p><div class="book" title="BookKeeper and Hedwig"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4692">BookKeeper and Hedwig</h4></div></div></div><p class="calibre2"><em class="calibre10">BookKeeper</em> is a highly <a class="calibre" id="calibre_link-1020"></a>available and reliable logging service. It can be used
        to provide write-ahead logging, which is a common technique for
        ensuring data integrity in storage systems. In a system using
        write-ahead logging, every write operation is written to the
        transaction log before it is applied. Using this procedure, we don’t
        have to write the data to permanent storage after every write
        operation, because in the event of a system failure, the latest state
        may be recovered by replaying the transaction log for any writes that
        were not applied.</p><p class="calibre2">BookKeeper clients create logs called
        <em class="calibre10">ledgers</em>, and each record appended to a ledger
        is called a <em class="calibre10">ledger entry</em>, which is simply a
        byte array. Ledgers are managed by <em class="calibre10">bookies</em>,
        which are servers that replicate the ledger data. Note that ledger
        data is not stored in ZooKeeper; only metadata is.</p><p class="calibre2">Traditionally, the challenge has been to make systems that use
        write-ahead logging robust in the face of failure of the node writing
        the transaction log. This is usually done by replicating the
        transaction log in some manner. HDFS high availability, described
        , uses a
        group of journal nodes to provide a highly available edit log.
        Although it is similar to BookKeeper, it is a dedicated service
        written for HDFS, and it doesn’t use ZooKeeper as the coordination
        engine.</p><p class="calibre2"><em class="calibre10">Hedwig</em> is a topic-based i<a class="calibre" id="calibre_link-1987"></a>publish-subscribe system built on BookKeeper. Thanks to
        its ZooKeeper underpinnings, Hedwig is a highly available service and
        guarantees message delivery even if subscribers are offline for
        extended periods of time.</p><p class="calibre2">BookKeeper is a ZooKeeper subproject, and you can find more
        information on how to use it, as well as Hedwig, at <a class="ulink" href="http://zookeeper.apache.org/bookkeeper/" target="_top">http://zookeeper.apache.org/bookkeeper/</a>.</p></div></div></div><div class="book" title="ZooKeeper in Production"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4693">ZooKeeper in Production</h2></div></div></div><p class="calibre2">In production, you <a class="calibre" id="calibre_link-3949"></a>should run ZooKeeper in replicated mode. Here, we will cover
    some of the considerations for running an ensemble of ZooKeeper servers.
    However, this section is not exhaustive, so you should consult the <a class="ulink" href="http://bit.ly/admin_guide" target="_top">ZooKeeper
    Administrator’s Guide</a> for detailed, up-to-date instructions,
    including supported platforms, recommended hardware, maintenance
    procedures, dynamic reconfiguration (to change the servers in a running
    ensemble), and configuration properties.</p><div class="book" title="Resilience and Performance"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4694">Resilience and Performance</h3></div></div></div><p class="calibre2">ZooKeeper machines should be located to minimize the impact of
      machine and network failure. In practice, this means that servers should
      be spread across racks, power supplies, and switches, so that the
      failure of any one of these does not cause the ensemble to lose a
      majority of its servers.</p><p class="calibre2">For applications that require low-latency service (on the order of
      a few milliseconds), it is important to run all the servers in an
      ensemble in a single data center. Some use cases don’t require
      low-latency responses, however, which makes it feasible to spread
      servers across data centers (at least two per data center) for extra
      resilience. Example applications in this category are leader election
      and distributed coarse-grained locking, both of which have relatively
      infrequent state changes, so the overhead of a few tens of milliseconds
      incurred by inter-data-center messages is not significant relative to
      the overall functioning of the service.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">ZooKeeper has the concept of an <em class="calibre10">observer
        node</em>, which is like a non-voting follower. Because they do
        not participate in the vote for consensus during write requests,
        observers allow a ZooKeeper cluster to improve read performance without hurting
        write performance.<sup class="calibre5">[<a class="firstname" href="#calibre_link-120" id="calibre_link-132">151</a>]</sup> Observers can be used to good advantage to allow a
        ZooKeeper cluster to span data centers without impacting latency as
        much as regular voting followers. This is achieved by placing
        the voting members in one data center and observers in the
        other.</p></div><p class="calibre2">ZooKeeper is a highly available system, and it is critical that it
      can perform its functions in a timely manner. Therefore, ZooKeeper
      should run on machines that are dedicated to ZooKeeper alone. Having
      other applications contend for resources can cause ZooKeeper’s
      performance to degrade significantly.</p><p class="calibre2">Configure ZooKeeper to keep its transaction log on a different disk drive from its
        snapshots. By default, both go in the directory specified by the <code class="literal">dataDir</code> property, <a class="calibre" id="calibre_link-1368"></a>but if you specify a location <a class="calibre" id="calibre_link-1374"></a>for <code class="literal">dataLogDir</code>, the transaction log will be
        written there. By having its own dedicated device (not just a partition), a ZooKeeper server
        can maximize the rate at which it writes log entries to disk, which it does sequentially
        without seeking. Because all writes go through the leader, write throughput does not scale
        by adding servers, so it is crucial that writes are as fast as possible.</p><p class="calibre2">If the process swaps to disk, performance will be adversely
      affected. This can be avoided by setting the Java heap size to less than
      the amount of unused physical memory on the machine. From its
      configuration directory, the ZooKeeper <a class="calibre" id="calibre_link-3298"></a>scripts will source a file called <em class="calibre10">java.env</em>, which can be used to set <a class="calibre" id="calibre_link-2297"></a>the <code class="literal">JVMFLAGS</code>
      environment variable to specify the heap size (and any other desired JVM
      arguments).</p></div><div class="book" title="Configuration"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4695">Configuration</h3></div></div></div><p class="calibre2">Each server in the ensemble of ZooKeeper servers has a numeric
      identifier that is unique within the ensemble and must fall between 1
      and 255. The server number is specified in plain text in a file named
      <em class="calibre10">myid</em> in the directory specified by
      the <code class="literal">dataDir</code> property.</p><p class="calibre2">Setting each server number is only half of the job. We also need
      to give every server all the identities and network locations of the
      others in the ensemble. The ZooKeeper configuration file must include a
      line for each server, of the form:</p><pre class="screen1"><code class="si">server.</code><em class="replaceable"><code class="replaceable"><code class="err1">n</code></code></em><code class="o">=</code><em class="replaceable"><code class="replaceable"><code class="err1">hostname</code></code></em><code class="o">:</code><em class="replaceable"><code class="replaceable"><code class="err1">port</code></code></em><code class="o">:</code><em class="replaceable"><code class="replaceable"><code class="err1">port</code></code></em></pre><p class="calibre2">The value of <code class="literal"><em class="replaceable"><code class="replaceable">n</code></em></code> is replaced by
      the server number. There are two port settings: the first is the port
      that followers use to connect to the leader, and the second is used for
      leader election. Here is a sample configuration for a three-machine
      replicated ZooKeeper ensemble:</p><pre class="screen1"><code class="na">tickTime</code><code class="o">=</code><code class="sb">2000</code>
<code class="na">dataDir</code><code class="o">=</code><code class="sb">/disk1/zookeeper</code>
<code class="na">dataLogDir</code><code class="o">=</code><code class="sb">/disk2/zookeeper</code>
<code class="na">clientPort</code><code class="o">=</code><code class="sb">2181</code>
<code class="na">initLimit</code><code class="o">=</code><code class="sb">5</code>
<code class="na">syncLimit</code><code class="o">=</code><code class="sb">2</code>
<code class="na">server.1</code><code class="o">=</code><code class="sb">zookeeper1:2888:3888</code>
<code class="na">server.2</code><code class="o">=</code><code class="sb">zookeeper2:2888:3888</code>
<code class="na">server.3</code><code class="o">=</code><code class="sb">zookeeper3:2888:3888</code></pre><p class="calibre2">Servers listen on three ports: 2181 for client connections; 2888
      for follower connections, if they are the leader; and 3888 for other
      server connections during the leader election phase. When a ZooKeeper
      server starts up, it reads the <em class="calibre10">myid</em>
      file to determine which server it is, and then reads the configuration
      file to determine the ports it should listen on and to discover the
      network addresses of the other servers in the ensemble.</p><p class="calibre2">Clients connecting to this ZooKeeper ensemble should use <code class="literal">zookeeper1:2181,zookeeper2:2181,zookeeper3:2181</code>
      as the host string in the constructor for the <code class="literal">ZooKeeper</code> object.</p><p class="calibre2">In replicated mode, <a class="calibre" id="calibre_link-3214"></a>there are two extra mandatory <a class="calibre" id="calibre_link-2099"></a><a class="calibre" id="calibre_link-3569"></a>properties: <code class="literal">initLimit</code>
      and <code class="literal">syncLimit</code>,
      both measured in multiples of <code class="literal">tickTime</code>.</p><p class="calibre2"><code class="literal">initLimit</code> is the amount of time
      to allow for followers to connect to and sync with the leader. If a
      majority of followers fail to sync within this period, the leader
      renounces its leadership status and another leader election takes place.
      If this happens often (and you can discover if this is the case because
      it is logged), it is a sign that the setting is too low.</p><p class="calibre2"><code class="literal">syncLimit</code> is the amount of time
      to allow a follower to sync with the leader. If a follower fails to sync
      within this period, it will restart itself. Clients that were attached
      to this follower will connect to another one.</p><p class="calibre2">These are the minimum settings needed to get up and running with a
      cluster of ZooKeeper servers. There are, however, more configuration
      options, particularly for tuning performance, which are documented in
      the <a class="calibre" id="calibre_link-3950"></a><a class="ulink" href="http://bit.ly/zookeeper_admin" target="_top">ZooKeeper Administrator’s Guide</a>.</p></div></div><div class="book" title="Further Reading"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4696">Further Reading</h2></div></div></div><p class="calibre2">For more in-depth information about <a class="calibre" id="calibre_link-3928"></a>ZooKeeper, see <a class="ulink" href="http://shop.oreilly.com/product/0636920028901.do" target="_top"><span class="calibre"><em class="calibre10">ZooKeeper</em></span></a>
    by Flavio Junqueira and Benjamin Reed (O’Reilly, 2013).</p></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" id="calibre_link-89"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-121">140</a>] </sup>This is the message of Jim Waldo et al. in <a class="ulink" href="http://www.eecs.harvard.edu/~waldo/Readings/waldo-94.pdf" target="_top">“A Note on Distributed
          Computing”</a> (Sun Microsystems, November 1994). Distributed
        programming is fundamentally different from local programming, and the differences cannot
        simply be papered over.</p></div><div class="footnote" id="calibre_link-90"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-122">141</a>] </sup>Detailed benchmarks are available in the excellent paper <a class="ulink" href="http://bit.ly/wait-free_coordination" target="_top">“ZooKeeper:
          Wait-free coordination for Internet-scale systems,”</a> by Patrick Hunt et al. (USENIX
        Annual Technology Conference, 2010).</p></div><div class="footnote" type="footnote" id="calibre_link-94"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-123">142</a>] </sup>For a replicated ZooKeeper service, this parameter is the
          comma-separated list of servers (host and optional port) in the
          ensemble.</p></div><div class="footnote" type="footnote" id="calibre_link-99"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-124">143</a>] </sup>It is conventional (but not required) to have a trailing
            dash on pathnames for sequential nodes, to make their sequence
            numbers easy to read and parse (by the application).</p></div><div class="footnote" type="footnote" id="calibre_link-101"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-125">144</a>] </sup>Except for callbacks for connection events, which do not
            need reregistration.</p></div><div class="footnote" id="calibre_link-107"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-126">145</a>] </sup>Leslie Lamport, <a class="ulink" href="http://bit.ly/simple-paxos" target="_top">“Paxos Made
          Simple,”</a> <span class="calibre">ACM SIGACT News</span> December 2001.</p></div><div class="footnote" id="calibre_link-108"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-127">146</a>] </sup>Zab is described in Benjamin Reed and Flavio Junqueira’s <a class="ulink" href="http://bit.ly/ordered_protocol" target="_top">“A simple totally ordered broadcast
          protocol,”</a> <span class="calibre">LADIS ’08 Proceedings of the 2nd Workshop on
                Large-Scale Distributed Systems and Middleware</span>, 2008.</p></div><div class="footnote" id="calibre_link-109"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-128">147</a>] </sup>Mike Burrows, <a class="ulink" href="http://research.google.com/archive/chubby.html" target="_top">“The Chubby Lock
          Service for Loosely-Coupled Distributed Systems,”</a> November 2006.</p></div><div class="footnote" type="footnote" id="calibre_link-110"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-129">148</a>] </sup>It is possible to configure ZooKeeper so that the leader does not accept client
            connections. In this case, its only job is to coordinate updates. Do this by <a class="calibre" id="calibre_link-2318"></a>setting the <code class="literal">leaderServes</code> property to
              <code class="literal">no</code>. This is recommended for ensembles of more than
            three servers.</p></div><div class="footnote" id="calibre_link-117"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-130">149</a>] </sup>For more detail, see the excellent article <a class="ulink" href="http://www.ibm.com/developerworks/java/library/j-jtp05236.html" target="_top">“Java theory and practice: Dealing
            with InterruptedException”</a> by Brian Goetz (IBM, May 2006).</p></div><div class="footnote" type="footnote" id="calibre_link-119"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-131">150</a>] </sup>Another way of writing the code would be to have a single
            catch block, just for <code class="literal">KeeperException</code>, and a test to see
            whether its code has the value <code class="literal">KeeperException.Code.SESSIONEXPIRED</code>.
            They both behave in the same way, so which method you use is
            simply a matter of style.</p></div><div class="footnote" id="calibre_link-120"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-132">151</a>] </sup>This is discussed in more detail in <a class="ulink" href="http://bit.ly/scaling_zookeeper" target="_top">“Observers: Making ZooKeeper Scale Even Further”</a>
              by Henry Robinson (Cloudera, December
              2009).</p></div></div></section></div>

<div class="calibre1" id="calibre_link-207"><div class="book" type="part" id="calibre_link-4697" title="Part&nbsp;V.&nbsp;Case Studies"><div class="book"><div class="book"><div class="book"><h1 class="title6">Part&nbsp;V.&nbsp;Case Studies</h1></div></div></div></div></div>

<div class="calibre1" id="calibre_link-282"><section type="chapter" id="calibre_link-4698" title="Chapter&nbsp;22.&nbsp;Composable Data at Cerner"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;22.&nbsp;Composable Data at Cerner</h2></div><div class="book"><div class="book"><div class="author2"><h3 class="author1"><span class="firstname">Ryan</span> <span class="firstname">Brush</span></h3></div><div class="author2"><h3 class="author1"><span class="firstname">Micah</span> <span class="firstname">Whitacre</span></h3></div></div></div></div></div><p class="calibre2">Healthcare <a class="calibre" id="calibre_link-1023"></a><a class="calibre" id="calibre_link-3784"></a>information <a class="calibre" id="calibre_link-1068"></a><a class="calibre" id="calibre_link-1074"></a>technology is often a story of automating existing
    processes. This is changing. Demands to improve care quality and control
    its costs are growing, creating a need for better systems to support those
    goals. Here we look at how Cerner is using the Hadoop ecosystem to make
    sense of healthcare and—building on that knowledge—to help solve such
    problems.</p><div class="book" title="From CPUs to Semantic Integration"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4699">From CPUs to Semantic Integration</h2></div></div></div><p class="calibre2">Cerner has long <a class="calibre" id="calibre_link-1075"></a>been focused on applying technology to healthcare, with
      much of our history emphasizing electronic medical records. However, new
      problems required a broader approach, which led us to look into
      Hadoop.</p><p class="calibre2">In 2009, we needed to create better search indexes of medical records. This led to
      processing needs not easily solved with other architectures. The search indexes required
      expensive processing of clinical documentation: extracting terms from the documentation and
      resolving their relationships with other terms. For instance, if a user typed “heart disease,”
      we wanted documents discussing a myocardial infarction to be returned. This processing was
      quite expensive—it can take several seconds of CPU time for larger documents—and we wanted to
      apply it to many millions of documents. In short, we needed to throw a lot of CPUs at the
      problem, and be cost effective in the process.</p><p class="calibre2">Among other options, we considered a staged event-driven architecture (SEDA) approach to
      ingest documents at scale. But Hadoop stood out for one important need: we wanted to reprocess
      the many millions of documents frequently, in a small number of hours or faster. The logic for
      knowledge extraction from the clinical documents was rapidly improving, and we needed to roll
      improvements out to the world quickly. In Hadoop, this simply meant running a new version of a
      MapReduce job over data already in place. The process documents were then loaded into a
      cluster of Apache Solr servers to support application queries.</p><p class="calibre2">These early successes set the stage for more involved projects. This type of system and
      its data can be used as an empirical basis to help control costs and improve care across
      entire populations. And since healthcare data is often fragmented across systems and
      institutions, we needed to first bring in all of that data and make sense of it.</p><p class="calibre2">With dozens of data sources and formats, and even standardized
      data models subject to interpretation, we were facing an enormous
      semantic integration problem. Our biggest challenge was not the
      <span class="calibre">size</span> of the data—we knew Hadoop could
      scale to our needs—but the sheer complexity of cleaning, managing, and
      transforming it for our needs. We needed higher-level tools to manage
      that complexity.</p></div><div class="book" title="Enter Apache Crunch"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4700">Enter Apache Crunch</h2></div></div></div><p dir="ltr" class="calibre2">Bringing together <a class="calibre" id="calibre_link-1076"></a><a class="calibre" id="calibre_link-1293"></a>and analyzing such disparate datasets creates a lot of
      demands, but a few stood out:</p><div class="book"><ul class="itemizedlist"><li class="listitem" dir="ltr"><p dir="ltr" class="calibre2">We needed to split many processing steps into
          modules that could easily be assembled into a sophisticated
          pipeline.</p></li><li class="listitem" dir="ltr"><p dir="ltr" class="calibre2">We needed to offer a higher-level programming model
          than raw MapReduce.</p></li><li class="listitem" dir="ltr"><p dir="ltr" class="calibre2">We needed to work with the complex structure of medical records, which
          have several hundred unique fields and several levels of nested substructures.</p></li></ul></div><p class="calibre2">We explored a variety of options in this case, including Pig, Hive, and Cascading. Each
      of these worked well, and we continue to use Hive for ad hoc analysis, but they were unwieldy
      when applying arbitrary logic to our complex data structures. Then we heard of Crunch (see
        <a class="ulink" href="#calibre_link-283" title="Chapter&nbsp;18.&nbsp;Crunch">Chapter&nbsp;18</a>), a project led by Josh Wills that is similar to the FlumeJava
      system from Google. Crunch offers a simple Java-based programming model and static type
      checking of records—a perfect fit for our community of Java developers and the type of data we
      were working with.</p></div><div class="book" title="Building a Complete Picture"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4701">Building a Complete Picture</h2></div></div></div><p dir="ltr" class="calibre2">Understanding and <a class="calibre" id="calibre_link-1077"></a>managing healthcare at scale requires significant amounts of clean, normalized,
      and relatable data. Unfortunately, such data is typically spread across a number of sources,
      making it difficult and error prone to consolidate. Hospitals, doctors’ offices, clinics, and
      pharmacies each hold portions of a person’s records in industry-standard formats such as CCDs
      (Continuity of Care Documents), HL7 (Health Level 7, a healthcare data interchange format),
      CSV files, or proprietary formats.</p><p dir="ltr" class="calibre2">Our challenge is to take this data; transform it into a clean, integrated
      representation; and use it to create registries that help patients manage specific conditions,
      measure operational aspects of healthcare, and support a variety of analytics, as shown in
        <a class="ulink" href="#calibre_link-284" title="Figure&nbsp;22-1.&nbsp;Operational data flow">Figure&nbsp;22-1</a>.</p><div class="figure"><a id="calibre_link-284" class="calibre"></a><div class="book"><div class="book"><img alt="Operational data flow" src="images/000001.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;22-1.&nbsp;Operational data flow</div></div><p class="calibre2">An essential step is to create a clean, semantically integrated
      basis we can build on, which is the focus of this case study. We start
      by normalizing data to a common structure. Earlier versions of this
      system used different models, but have since migrated to Avro for
      storing and sharing data between processing steps. <a class="ulink" href="#calibre_link-285" title="Example&nbsp;22-1.&nbsp;Avro IDL for common data types">Example&nbsp;22-1</a> shows a simplified Avro IDL to
      illustrate how our common structures look.</p><div class="example"><a id="calibre_link-285" class="calibre"></a><div class="example-title">Example&nbsp;22-1.&nbsp;Avro IDL for common data types</div><div class="book"><pre class="screen">@namespace("com.cerner.example")
protocol PersonProtocol {

  record Demographics {
    string firstName;
    string lastName;
    string dob;
    ...
  }

  record LabResult {
    string personId;
    string labDate;
    int labId;
    int labTypeId;
    int value;
  }

  record Medication {
    string personId;
    string medicationId;
    string dose;
    string doseUnits;
    string frequency;
    ...
  }

  record Diagnosis {
    string personId;
    string diagnosisId;
    string date;
    ...
  }

  record Allergy {
    string personId;
    int allergyId;
    int substanceId;
    ...
  }

  /**
   * Represents a person's record from a single source.
   */
  record PersonRecord {
    string personId;
    Demographics demographics;
    array&lt;LabResult&gt; labResults;
    array&lt;Allergy&gt; allergies;
    array&lt;Medication&gt; medications;
    array&lt;Diagnosis&gt; diagnoses;
    . . .
  }
}</pre></div></div><p dir="ltr" class="calibre2">Note that a variety of data types are all nested in a
      common person record rather than in separate datasets. This supports the
      most common usage pattern for this data—looking at a complete
      record—without requiring downstream operations to do a number of
      expensive joins between datasets.</p><p dir="ltr" class="calibre2">A series of Crunch pipelines are used to manipulate the
      data into a <code class="literal">PCollection</code><code class="literal">&lt;PersonRecord&gt;</code>
      hiding the complexity of each source and providing a simple interface to
      interact with the raw, normalized record data. Behind the scenes, each
      <code class="literal">PersonRecord</code> can be stored in HDFS or as a row in
      HBase with the individual data elements spread throughout column
      families and qualifiers. The result of the aggregation looks like the
      data in <a class="ulink" href="#calibre_link-286" title="Table&nbsp;22-1.&nbsp;Aggregated data">Table&nbsp;22-1</a>.</p><div class="table"><a id="calibre_link-286" class="calibre"></a><div class="table-title">Table&nbsp;22-1.&nbsp;Aggregated data</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Source</td><td class="calibre20">Person ID</td><td class="calibre20">Person demographics</td><td class="calibre21">Data</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">Doctor’s office</td><td class="calibre23">12345</td><td class="calibre23">Abraham Lincoln ...</td><td class="calibre25">Diabetes diagnosis, lab results</td></tr><tr class="calibre26"><td class="calibre23">Hospital</td><td class="calibre23">98765</td><td class="calibre23">Abe Lincoln ...</td><td class="calibre25">Flu diagnosis</td></tr><tr class="calibre19"><td class="calibre23">Pharmacy</td><td class="calibre23">98765</td><td class="calibre23">Abe Lincoln ...</td><td class="calibre25">Allergies, medications</td></tr><tr class="calibre26"><td class="calibre27">Clinic</td><td class="calibre27">76543</td><td class="calibre27">A. Lincoln ...</td><td class="calibre28">Lab results</td></tr></tbody></table></div></div><p class="calibre2">Consumers wishing to retrieve data from a collection of authorized
      sources call a “retriever” API that simply produces a Crunch
      <code class="literal">PCollection</code> of requested data:</p><pre class="screen1"><code class="n">Set</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">sources</code> <code class="o">=</code> <code class="o">...;</code>
<code class="n">PCollection</code><code class="o">&lt;</code><code class="n">PersonRecord</code><code class="o">&gt;</code> <code class="n">personRecords</code> <code class="o">=</code>
    <code class="n">RecordRetriever</code><code class="o">.</code><code class="na">getData</code><code class="o">(</code><code class="n">pipeline</code><code class="o">,</code> <code class="n">sources</code><code class="o">);</code></pre><p class="calibre2">This retriever pattern allows consumers to load datasets while
      being insulated from how and where they are physically stored. At the
      time of this writing, some use of this pattern is being replaced by the
      emerging <a class="ulink" href="http://kitesdk.org/" target="_top">Kite SDK</a> for managing
      data in Hadoop. Each entry in the retrieved
      <code class="literal">PCollection&lt;PersonRecord&gt;</code> represents a
      person’s complete medical record within the context of a single
      source.</p></div><div class="book" title="Integrating Healthcare Data"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4702">Integrating Healthcare Data</h2></div></div></div><p class="calibre2">There are dozens of processing steps between raw data and answers
        to healthcare-related questions. Here we look at one: bringing together
        data for a single person from multiple <a class="calibre" id="calibre_link-1078"></a>sources.</p><p class="calibre2">Unfortunately, the lack <a class="calibre" id="calibre_link-1080"></a>of a common patient identifier in the United States,
      combined with noisy data such as variations in a person’s name and
      demographics between systems, makes it difficult to accurately unify a
      person’s data across sources. Information spread across multiple sources
      might look like <a class="ulink" href="#calibre_link-287" title="Table&nbsp;22-2.&nbsp;Data from multiple sources">Table&nbsp;22-2</a>.</p><div class="table"><a id="calibre_link-287" class="calibre"></a><div class="table-title">Table&nbsp;22-2.&nbsp;Data from multiple sources</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Source</td><td class="calibre20">Person ID</td><td class="calibre20">First name</td><td class="calibre20">Last name</td><td class="calibre20">Address</td><td class="calibre21">Gender</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">Doctor’s office</td><td class="calibre23">12345</td><td class="calibre23">Abraham</td><td class="calibre23">Lincoln</td><td class="calibre23">1600 Pennsylvania Ave.</td><td class="calibre25">M</td></tr><tr class="calibre26"><td class="calibre23">Hospital</td><td class="calibre23">98765</td><td class="calibre23">Abe</td><td class="calibre23">Lincoln</td><td class="calibre23">Washington, DC</td><td class="calibre25">M</td></tr><tr class="calibre19"><td class="calibre23">Hospital</td><td class="calibre23">45678</td><td class="calibre23">Mary Todd</td><td class="calibre23">Lincoln</td><td class="calibre23">1600 Pennsylvania Ave.</td><td class="calibre25">F</td></tr><tr class="calibre26"><td class="calibre27">Clinic</td><td class="calibre27">76543</td><td class="calibre27">A.</td><td class="calibre27">Lincoln</td><td class="calibre27">Springfield, IL</td><td class="calibre28">M</td></tr></tbody></table></div></div><p class="calibre2">This is typically resolved in healthcare by a system called an <em class="calibre10">Enterprise
        Master Patient Index</em> (EMPI). An EMPI can be fed data from multiple systems and
      determine which records are indeed for the same person. This is achieved in a variety of ways,
      ranging from humans explicitly stating relationships to sophisticated algorithms that identify
      commonality.</p><p class="calibre2">In some cases, we can load EMPI information from external systems,
      and in others we compute it within Hadoop. The key is that we can expose
      this information for use in our Crunch-based pipelines. The result is a
      <code class="literal">PCollection&lt;EMPIRecord&gt;</code> with the data
      structured as follows:</p><pre class="screen1">@namespace("com.cerner.example")
protocol EMPIProtocol {

  record PersonRecordId {
    string sourceId;
    string personId
  }

  /**
   * Represents an EMPI match.
   */
  record EMPIRecord {
    string empiId;
    array&lt;PersonRecordId&gt; personIds;
  }
}</pre><p class="calibre2">Given EMPI information for the data in this structure,
      <code class="literal">PCollection&lt;EMPI</code><code class="literal">Record&gt;</code> would
      contain data like that shown in <a class="ulink" href="#calibre_link-288" title="Table&nbsp;22-3.&nbsp;EMPI data">Table&nbsp;22-3</a>.</p><div class="table"><a id="calibre_link-288" class="calibre"></a><div class="table-title">Table&nbsp;22-3.&nbsp;EMPI data</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">EMPI identifier</td><td class="calibre21">PersonRecordIds (&lt;SourceId, PersonId&gt;)</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">EMPI-1</code></td><td class="calibre25"><div class="book"><p class="calibre2"><code class="uri">&lt;offc-135,&nbsp;12345&gt;<br class="calibre3">
&lt;hspt-246,&nbsp;98765&gt;<br class="calibre3">
&lt;clnc-791,&nbsp;76543&gt;</code></p></div></td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">EMPI-2</code></td><td class="calibre28"><code class="uri">&lt;hspt-802,
              45678&gt;</code></td></tr></tbody></table></div></div><p class="calibre2">In order to group a person’s medical records in a single location based upon the
      provided <code class="literal">PCollection&lt;EMPIRecord&gt;</code> and
        <code class="literal">PCollection&lt;PersonRecord&gt;</code>, the collections must be converted
      into a <code class="literal">PTable</code>, keyed by a common key. In this situation, a
        <code class="literal">Pair&lt;String, String&gt;</code>, where the first value is the
        <code class="literal">sourceId</code> and the second is the <code class="literal">personId</code>, will
      guarantee a unique key to use for joining.</p><p class="calibre2">The first step is to extract the common key from each
      <code class="literal">EMPIRecord</code> in the collection:</p><pre class="screen1"><code class="n">PCollection</code><code class="o">&lt;</code><code class="n">EMPIRecord</code><code class="o">&gt;</code> <code class="n">empiRecords</code> <code class="o">=</code> <code class="o">...;</code>
<code class="n">PTable</code><code class="o">&lt;</code><code class="n">Pair</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;,</code> <code class="n">EMPIRecord</code><code class="o">&gt;</code> <code class="n">keyedEmpiRecords</code> <code class="o">=</code>
    <code class="n">empiRecords</code><code class="o">.</code><code class="na">parallelDo</code><code class="o">(</code>
  <code class="k">new</code> <code class="n">DoFn</code><code class="o">&lt;</code><code class="n">EMPIRecord</code><code class="o">,</code> <code class="n">Pair</code><code class="o">&lt;</code><code class="n">Pair</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;,</code> <code class="n">EMPIRecord</code><code class="o">&gt;&gt;()</code> <code class="o">{</code>
    <code class="nd">@Override</code>
    <code class="k">public</code> <code class="kt">void</code> <code class="nf">process</code><code class="o">(</code><code class="n">EMPIRecord</code> <code class="n">input</code><code class="o">,</code>
        <code class="n">Emitter</code><code class="o">&lt;</code><code class="n">Pair</code><code class="o">&lt;</code><code class="n">Pair</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;,</code> <code class="n">EMPIRecord</code><code class="o">&gt;&gt;</code> <code class="n">emitter</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">for</code> <code class="o">(</code><code class="n">PersonRecordId</code> <code class="nd">recordId:</code> <code class="n">input</code><code class="o">.</code><code class="na">getPersonIds</code><code class="o">())</code> <code class="o">{</code>
        <code class="n">emitter</code><code class="o">.</code><code class="na">emit</code><code class="o">(</code><code class="n">Pair</code><code class="o">.</code><code class="na">of</code><code class="o">(</code>
            <code class="n">Pair</code><code class="o">.</code><code class="na">of</code><code class="o">(</code><code class="n">recordId</code><code class="o">.</code><code class="na">getSourceId</code><code class="o">(),</code> <code class="n">recordId</code><code class="o">.</code><code class="na">getPersonId</code><code class="o">()),</code> <code class="n">input</code><code class="o">));</code>
      <code class="o">}</code>
    <code class="o">}</code>
  <code class="o">},</code> <code class="n">tableOf</code><code class="o">(</code><code class="n">pairs</code><code class="o">(</code><code class="n">strings</code><code class="o">(),</code> <code class="n">strings</code><code class="o">()),</code> <code class="n">records</code><code class="o">(</code><code class="n">EMPIRecord</code><code class="o">.</code><code class="na">class</code><code class="o">)</code>
<code class="o">);</code></pre><p class="calibre2">Next, the same key needs to be extracted from each
      <code class="literal">PersonRecord</code>:</p><pre class="screen1"><code class="n">PCollection</code><code class="o">&lt;</code><code class="n">PersonRecord</code><code class="o">&gt;</code> <code class="n">personRecords</code> <code class="o">=</code> <code class="o">...;</code>
<code class="n">PTable</code><code class="o">&lt;</code><code class="n">Pair</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;,</code> <code class="n">PersonRecord</code><code class="o">&gt;</code> <code class="n">keyedPersonRecords</code> <code class="o">=</code> <code class="n">personRecords</code><code class="o">.</code><code class="na">by</code><code class="o">(</code>
    <code class="k">new</code> <code class="n">MapFn</code><code class="o">&lt;</code><code class="n">PersonRecord</code><code class="o">,</code> <code class="n">Pair</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;&gt;()</code> <code class="o">{</code>
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="n">Pair</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;</code> <code class="n">map</code><code class="o">(</code><code class="n">PersonRecord</code> <code class="n">input</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">return</code> <code class="n">Pair</code><code class="o">.</code><code class="na">of</code><code class="o">(</code><code class="n">input</code><code class="o">.</code><code class="na">getSourceId</code><code class="o">(),</code> <code class="n">input</code><code class="o">.</code><code class="na">getPersonId</code><code class="o">());</code>
  <code class="o">}</code>
<code class="o">},</code> <code class="n">pairs</code><code class="o">(</code><code class="n">strings</code><code class="o">(),</code> <code class="n">strings</code><code class="o">()));</code></pre><p class="calibre2">Joining the two <code class="literal">PTable</code> objects will return a
        <code class="literal">PTable&lt;Pair&lt;String, String&gt;, Pair</code><code class="literal">&lt;EMPIRecord, PersonRecord&gt;&gt;</code>.
      In this situation, the keys are no longer useful, so we change the table to be keyed by the
      EMPI identifier:</p><pre class="screen1"><code class="n">PTable</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">PersonRecord</code><code class="o">&gt;</code> <code class="n">personRecordKeyedByEMPI</code> <code class="o">=</code> <code class="n">keyedPersonRecords</code>
    <code class="o">.</code><code class="na">join</code><code class="o">(</code><code class="n">keyedEmpiRecords</code><code class="o">)</code>
    <code class="o">.</code><code class="na">values</code><code class="o">()</code>
    <code class="o">.</code><code class="na">by</code><code class="o">(</code><code class="k">new</code> <code class="n">MapFn</code><code class="o">&lt;</code><code class="n">Pair</code><code class="o">&lt;</code><code class="n">PersonRecord</code><code class="o">,</code> <code class="n">EMPIRecord</code><code class="o">&gt;&gt;()</code> <code class="o">{</code>
  <code class="nd">@Override</code>
  <code class="k">public</code> <code class="n">String</code> <code class="nf">map</code><code class="o">(</code><code class="n">Pair</code><code class="o">&lt;</code><code class="n">PersonRecord</code><code class="o">,</code> <code class="n">EMPIRecord</code><code class="o">&gt;</code> <code class="n">input</code><code class="o">)</code> <code class="o">{</code>
    <code class="k">return</code> <code class="n">input</code><code class="o">.</code><code class="na">second</code><code class="o">().</code><code class="na">getEmpiId</code><code class="o">();</code>
  <code class="o">}</code>
<code class="o">},</code> <code class="n">strings</code><code class="o">()));</code></pre><p class="calibre2">The final step is to group the table by its key to ensure all of
      the data is aggregated together for processing as a complete
      collection:</p><pre class="screen1"><code class="n">PGroupedTable</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">PersonRecord</code><code class="o">&gt;</code> <code class="n">groupedPersonRecords</code> <code class="o">=</code>
    <code class="n">personRecordKeyedByEMPI</code><code class="o">.</code><code class="na">groupByKey</code><code class="o">();</code></pre><p class="calibre2">The <code class="literal">PGroupedTable</code> would contain data like
      that in <a class="ulink" href="#calibre_link-289" title="Table&nbsp;22-4.&nbsp;Grouped EMPI data">Table&nbsp;22-4</a>.</p><p class="calibre2">This logic to unify data sources is the first step of a larger execution flow. Other
        Crunch functions downstream build on these steps to meet many client needs. In a common use
        case, a number of problems are solved by loading the contents of the unified <code class="literal">PersonRecord</code>s into a rules-based
        <u style="
    text-decoration: underline 0.14em green;
">processing model</u> to emit new clinical knowledge. For instance, we may run rules over those
        records to determine if a diabetic is receiving recommended care, and to indicate areas that
        can be improved. Similar rule sets exist for a variety of needs, ranging from general wellness
        to managing complicated conditions. The logic can be complicated and with a lot of variance
        between use cases, but it is all hosted in functions composed in a Crunch <a class="calibre" id="calibre_link-1081"></a>pipeline.</p><div class="table"><a id="calibre_link-289" class="calibre"></a><div class="table-title">Table&nbsp;22-4.&nbsp;Grouped EMPI data</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="calibre30"><col class="calibre30"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">EMPI identifier</td><td class="calibre21">Iterable&lt;PersonRecord&gt;</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">EMPI-1</code></td><td class="calibre25"><div class="book"><p class="calibre2"><code class="uri">{<br class="calibre3">
&nbsp;&nbsp;"personId":&nbsp;"12345",<br class="calibre3">
&nbsp;&nbsp;"demographics":&nbsp;{<br class="calibre3">
&nbsp;&nbsp;&nbsp;&nbsp;"firstName":&nbsp;"Abraham",&nbsp;"lastName":&nbsp;"Lincoln",&nbsp;...<br class="calibre3">
&nbsp;&nbsp;},<br class="calibre3">
&nbsp;&nbsp;"labResults":&nbsp;[...]<br class="calibre3">
},<br class="calibre3">
{<br class="calibre3">
&nbsp;&nbsp;"personId":&nbsp;"98765",<br class="calibre3">
&nbsp;&nbsp;"demographics":&nbsp;{<br class="calibre3">
&nbsp;&nbsp;&nbsp;&nbsp;"firstName":&nbsp;"Abe",&nbsp;"lastName":&nbsp;"Lincoln",&nbsp;...<br class="calibre3">
&nbsp;&nbsp;},<br class="calibre3">
&nbsp;&nbsp;"diagnoses":&nbsp;[...]<br class="calibre3">
},<br class="calibre3">
{<br class="calibre3">
&nbsp;&nbsp;"personId":&nbsp;"98765",<br class="calibre3">
&nbsp;&nbsp;"demographics":&nbsp;{<br class="calibre3">
&nbsp;&nbsp;&nbsp;&nbsp;"firstName":&nbsp;"Abe",&nbsp;"lastName":&nbsp;"Lincoln",&nbsp;...<br class="calibre3">
&nbsp;&nbsp;},<br class="calibre3">
&nbsp;&nbsp;"medications":&nbsp;[...]},<br class="calibre3">
{<br class="calibre3">
&nbsp;&nbsp;"personId":&nbsp;"76543",<br class="calibre3">
&nbsp;&nbsp;"demographics":&nbsp;{<br class="calibre3">
&nbsp;&nbsp;&nbsp;&nbsp;"firstName":&nbsp;"A.",&nbsp;"lastName":&nbsp;"Lincoln",&nbsp;...<br class="calibre3">
&nbsp;&nbsp;}<br class="calibre3">
&nbsp;&nbsp;...<br class="calibre3">
}</code></p></div></td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">EMPI-2</code></td><td class="calibre28"><div class="book"><p class="calibre2"><code class="uri">{<br class="calibre3">
&nbsp;&nbsp;"personId":&nbsp;"45678",<br class="calibre3">
&nbsp;&nbsp;"demographics":&nbsp;{<br class="calibre3">
&nbsp;&nbsp;&nbsp;&nbsp;"firstName":&nbsp;"Mary&nbsp;Todd",&nbsp;"lastName":&nbsp;"Lincoln",&nbsp;...<br class="calibre3">
&nbsp;&nbsp;}<br class="calibre3">
&nbsp;&nbsp;...<br class="calibre3">
}</code></p></div></td></tr></tbody></table></div></div></div><div class="book" title="Composability over Frameworks"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4703">Composability over Frameworks</h2></div></div></div><p class="calibre2">The patterns described here take<a class="calibre" id="calibre_link-1079"></a> on a particular class of problem in healthcare centered
      around the person. However, this data can serve as the basis for
      understanding operational and systemic properties of healthcare as well,
      creating new demands on our ability to transform and analyze it.</p><p class="calibre2">Libraries like Crunch help us meet emerging demands because they help make our data and
      processing logic composable. Rather than a single, static framework for data processing, we
      can modularize functions and datasets and reuse them as new needs emerge. <a class="ulink" href="#calibre_link-290" title="Figure&nbsp;22-2.&nbsp;Composable datasets and functions">Figure&nbsp;22-2</a> shows how components can be wired into one another in novel
        ways, with each box implemented as one or more Crunch <code class="literal">DoFns</code>. Here we leverage person records
      to identify diabetics and recommend health management programs, while using those composable
      pieces to integrate operational data and drive analytics of the health system.</p><div class="figure"><a id="calibre_link-290" class="calibre"></a><div class="book"><div class="book"><img alt="Composable datasets and functions" src="images/000008.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;22-2.&nbsp;Composable datasets and functions</div></div><p class="calibre2">Composability also makes iterating through new problem spaces
      easier. When creating a new view of data to answer a new class of
      question, we can tap into existing datasets and transformations and emit
      our new version. As the problem becomes better understood, that view can
      be replaced or updated iteratively. Ultimately, these new functions and
      datasets can be contributed back and leveraged for new needs. The result
      is a growing catalog of datasets to support growing demands to
      understand the data.</p><p class="calibre2">Processing is orchestrated with Oozie. Every time new data
      arrives, a new dataset is created with a unique identifier in a
      well-defined location in HDFS. Oozie coordinators watch that location
      and simply launch Crunch jobs to create downstream datasets, which may
      subsequently be picked up by other coordinators. At the time of this
      writing, datasets and updates are identified by UUIDs to keep them
      unique. However, we are in the process of placing new data in
      timestamp-based partitions in order to better work with Oozie’s nominal
      time model.</p></div><div class="book" title="Moving Forward"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4704">Moving Forward</h2></div></div></div><p class="calibre2">We are looking to two <a class="calibre" id="calibre_link-1082"></a>major steps to maximize the value from this system more
        efficiently.</p><p class="calibre2">First, we want to create prescriptive practices around the Hadoop
      ecosystem and its supporting libraries. A number of good practices are
      defined in this book and elsewhere, but they often require significant
      expertise to implement effectively. We are using and building libraries
      that make such patterns explicit and accessible to a larger audience.
      Crunch offers some good examples of this, with a variety of join and
      processing patterns built into the library.</p><p class="calibre2">Second, our growing catalog of datasets has created a demand for
      simple and prescriptive data management to complement the processing
      features offered by Crunch. We have been adopting the Kite SDK to meet
      this need in some use cases, and expect to expand its use over
      time.</p><p class="calibre2">The end goal is a secure, scalable catalog of data to support many
      needs in healthcare, including problems that have not yet emerged.
      Hadoop has shown it can scale to our data and processing needs, and
      higher-level libraries are now making it usable by a larger audience for
      many <a class="calibre" id="calibre_link-1069"></a>problems.</p></div></section></div>

<div class="calibre1" id="calibre_link-349"><section type="chapter" id="calibre_link-4705" title="Chapter&nbsp;23.&nbsp;Biological Data Science: Saving Lives with Software"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;23.&nbsp;Biological Data Science: Saving Lives with Software</h2></div><div class="book"><div class="book"><div class="author2"><h3 class="author1"><span class="firstname">Matt</span> <span class="firstname">Massie</span></h3></div></div></div></div></div><p class="calibre2">It’s <a class="calibre" id="calibre_link-2649"></a>hard to believe <a class="calibre" id="calibre_link-1064"></a><a class="calibre" id="calibre_link-993"></a>a decade has passed since the <a class="ulink" href="http://research.google.com/archive/mapreduce.html" target="_top">MapReduce paper</a> appeared at OSDI’04. It’s also hard to overstate the impact
    that paper had on the tech industry; the MapReduce paradigm opened
    distributed programming to nonexperts and enabled large-scale data
    processing on clusters built using commodity hardware. The open source
    community responded by creating open source MapReduce-based systems, like
    Apache Hadoop and Spark, that enabled data scientists and engineers to
    formulate and solve problems at a scale unimagined before.</p><p class="calibre2">While the tech industry was being transformed by MapReduce-based
    systems, biology was experiencing its own metamorphosis driven by
    second-generation (or “next-generation”) sequencing technology; see <a class="ulink" href="#calibre_link-350" title="Figure&nbsp;23-1.&nbsp;Timeline of big data technology and cost of sequencing a genome">Figure&nbsp;23-1</a>. Sequencing machines are scientific
    instruments that read the chemical “letters” (A, C, T, and G) that make up
    your genome: your complete set of genetic material. To have your genome
    sequenced when the MapReduce paper was published cost about $20 million
    and took many months to complete; today, it costs just a few thousand
    dollars and takes only a few days. While the first human genome took
    decades to create, in 2014 alone an estimated 228,000 genomes were
    sequenced worldwide.<sup class="calibre6">[<a class="firstname" href="#calibre_link-351" id="calibre_link-374">152</a>]</sup> This estimate implies around 20 petabytes (PB) of sequencing data
    were generated in 2014 worldwide.</p><div class="figure"><a id="calibre_link-350" class="calibre"></a><div class="book"><div class="mediaobject"><table class="calibre51"><tbody><tr class="calibre19"><td class="calibre28"><img alt="Timeline of big data technology and cost of sequencing a genome" src="images/000017.jpg" class="calibre52"></td></tr></tbody></table></div></div><div class="figure-title">Figure&nbsp;23-1.&nbsp;Timeline of big data technology and cost of sequencing a
      genome</div></div><p class="calibre2">The plummeting cost of sequencing points to superlinear growth of
    genomics data over the coming years. This DNA data deluge has left
    biological data scientists struggling to process data in a timely and
    scalable way using current genomics software. The <a class="ulink" href="https://amplab.cs.berkeley.edu" target="_top">AMPLab</a> is a research lab in
    the Computer Science Division at UC Berkeley focused on creating novel big
    data systems and applications. For example, Apache Spark (see <a class="ulink" href="#calibre_link-352" title="Chapter&nbsp;19.&nbsp;Spark">Chapter&nbsp;19</a>) is one system that grew out of the AMPLab. Spark recently broke the world record for the Daytona Gray Sort, sorting
    100 TB in just 23 minutes. The team at <a class="ulink" href="http://databricks.com" target="_top">Databricks</a> that broke the record also
    demonstrated they could sort 1 PB in less than 4 hours!</p><p class="calibre2">Consider this amazing possibility: we have technology today that
    could analyze every genome collected in 2014 on the order of days using a
    few hundred machines.</p><p class="calibre2">While the AMPLab identified genomics as the ideal big data
    application for technical reasons, there are also more important
    compassionate reasons: the timely processing of biological data saves
    lives. This short use case will focus on systems we use and have
    developed, with our partners and the open source community, to quickly
    analyze large biological <a class="calibre" id="calibre_link-994"></a>datasets.</p><div class="book" title="The Structure of DNA"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4706">The Structure of DNA</h2></div></div></div><p class="calibre2">The discovery in 1953 by <a class="calibre" id="calibre_link-1006"></a><a class="calibre" id="calibre_link-1534"></a><a class="calibre" id="calibre_link-1288"></a><a class="calibre" id="calibre_link-3775"></a>Francis Crick and James D. Watson, using experimental data
      collected by Rosalind Franklin and Maurice Wilkins, that DNA has a
      double helix structure was one of the greatest scientific discoveries of
      the 20th century. Their <span class="calibre"><em class="calibre10">Nature</em></span> article entitled
      “Molecular Structure of Nucleic Acids: A Structure for Deoxyribose
      Nucleic Acid” contains one of the most profound and understated
      sentences in science:</p><div class="blockquote"><blockquote class="blockquote1"><p class="calibre4">It has not escaped our notice that the specific pairing we have
        postulated immediately suggests a possible copying mechanism for the
        genetic material.</p></blockquote></div><p class="calibre2">This “specific pairing” referred to the observation that the bases
      adenine (A) and thymine (T) always pair together and guanine (G) and
      cytosine (C) always pair together; see <a class="ulink" href="#calibre_link-353" title="Figure&nbsp;23-2.&nbsp;DNA double helix structure">Figure&nbsp;23-2</a>. This deterministic pairing
      enables a “copying mechanism”: the DNA double helix unwinds and
      complementary base pairs snap into place, creating two exact copies of
      the original DNA strand.</p><div class="figure"><a id="calibre_link-353" class="calibre"></a><div class="book"><div class="book"><img alt="DNA double helix structure" src="images/000025.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;23-2.&nbsp;DNA double helix structure</div></div></div><div class="book" title="The Genetic Code: Turning DNA Letters into Proteins"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4707">The Genetic Code: Turning DNA Letters into Proteins</h2></div></div></div><p class="calibre2">Without proteins, there is no life. <a class="calibre" id="calibre_link-999"></a><a class="calibre" id="calibre_link-1529"></a>DNA serves as a recipe for creating proteins. A protein is
      a chain of amino acids that folds into a specific 3D shape<sup class="calibre6">[<a class="firstname" href="#calibre_link-354" id="calibre_link-375">153</a>]</sup> to serve a particular structure or function. As there are
      a total of 20 amino acids<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-355" id="calibre_link-376">154</a>]</sup> and only four letters in the DNA alphabet (A, C, T, G),
      nature groups these letters in words, called
      <span class="calibre"><em class="calibre10">codons</em></span>. Each codon is three bases long (since two
      bases would only support 4<sup class="calibre6">2</sup>=16 amino
      acids).</p><p class="calibre2">In 1968, Har Gobind Khorana, Robert W. Holley, and Marshall Nirenberg received the Nobel
            Prize in Physiology or Medicine for successfully mapping amino acids associated with
            each of the 64 codons. Each codon encodes a single amino acid, or designates the start
            and stop positions (see <a class="ulink" href="#calibre_link-356" title="Table&nbsp;23-1.&nbsp;Codon table">Table&nbsp;23-1</a>). Since there are 64 possible codons and only 20 amino acids,
            multiple codons correspond to some of the amino acids.</p><div class="table"><a id="calibre_link-356" class="calibre"></a><div class="table-title">Table&nbsp;23-1.&nbsp;Codon table</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="acid"><col class="acid"><col class="split"><col class="acid"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Amino acid</td><td class="calibre20">Codon(s)</td><td class="calibre20">Amino acid</td><td class="calibre21">Codon(s)</td></tr></thead><tfoot class="calibre53"><tr class="calibre19"><td class="calibre27">START!</td><td class="calibre27">AUG</td><td class="calibre27">STOP!</td><td class="calibre28">UAA or UGA or UAG</td></tr></tfoot><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">Alanine</td><td class="calibre23">GC{U,C,A,G}</td><td class="calibre23">Leucine</td><td class="calibre25">UU{A,G} or CU{U,C,A,G}</td></tr><tr class="calibre26"><td class="calibre23">Arginine</td><td class="calibre23">CG{U,C,A,G} or AG{A,G}</td><td class="calibre23">Lysine</td><td class="calibre25">AA{A,G}</td></tr><tr class="calibre19"><td class="calibre23">Asparagine</td><td class="calibre23">AA{U,C}</td><td class="calibre23">Methionine</td><td class="calibre25">AUG</td></tr><tr class="calibre26"><td class="calibre23">Aspartic acid</td><td class="calibre23">GA{U,C}</td><td class="calibre23">Phenylalanine</td><td class="calibre25">UU{U,C}</td></tr><tr class="calibre19"><td class="calibre23">Cysteine</td><td class="calibre23">UG{U,C}</td><td class="calibre23">Proline</td><td class="calibre25">CC{U,C,A,G}</td></tr><tr class="calibre26"><td class="calibre23">Glutamic acid</td><td class="calibre23">GA{A,G}</td><td class="calibre23">Threonine</td><td class="calibre25">AC{U,C,A,G}</td></tr><tr class="calibre19"><td class="calibre23">Glutamine</td><td class="calibre23">CA{A,G}</td><td class="calibre23">Serine</td><td class="calibre25">UC{U,C,A,G} or AG{U,C}</td></tr><tr class="calibre26"><td class="calibre23">Glycine</td><td class="calibre23">GG{U,C,A,G}</td><td class="calibre23">Tryptophan</td><td class="calibre25">UGG</td></tr><tr class="calibre19"><td class="calibre23">Histidine</td><td class="calibre23">CA{U,C}</td><td class="calibre23">Tyrosine</td><td class="calibre25">UA{U,C}</td></tr><tr class="calibre26"><td class="calibre23">Isoleucine</td><td class="calibre23">AU{U,C,A}</td><td class="calibre23">Valine</td><td class="calibre25">GU{U,C,A,G}</td></tr></tbody></table></div></div><p class="calibre2">Because every organism on Earth evolved from the same common
      ancestor, <span class="calibre"><em class="calibre10">every organism on Earth uses the same genetic code,
      with few variations</em></span>. Whether the organism is a tree, worm,
      fungus, or cheetah, the codon <code class="literal">UGG</code>
      encodes tryptophan. Mother Nature has been the ultimate practitioner of
      code reuse over the last few billion years.</p><p class="calibre2">DNA is not directly used to synthesize amino acids. Instead, a process called
                <span class="calibre"><em class="calibre10">transcription</em></span> copies the DNA sequence that codes for a protein
            into <span class="calibre"><em class="calibre10">messenger RNA</em></span> (mRNA). These mRNA carry information from the
            nuclei of your cells to the surrounding <span class="calibre"><em class="calibre10">cytoplasm</em></span> to create
            proteins in a process called <span class="calibre"><em class="calibre10">translation</em></span>.</p><p class="calibre2">You probably noticed that
            this lookup table doesn’t have the DNA letter T (for thymine) and has a new letter U
            (for uracil). During <span class="calibre"><em class="calibre10">transcription</em></span>, U is substituted for T:</p><pre class="screen1"><code class="literal">$</code> <strong class="userinput"><code class="calibre9">echo "ATGGTGACTCCTACATGA" | sed 's/T/U/g' | fold -w 3</code></strong>
AUG
GUG
ACU
CCU
ACA
UGA</pre><p class="calibre2">Looking up these codons in the codon table, we can determine that this particular DNA
            strand will translate into a protein with the following amino acids in a chain:
            methionine, valine, threonine, proline, and threonine. This is a contrived example, but
            it logically demonstrates how DNA instructs the creation of proteins that make you
            uniquely <span class="calibre"><em class="calibre10">you</em></span>. It’s a marvel that science has allowed us to
            understand the language of DNA, including the start and stop punctuations.</p></div><div class="book" title="Thinking of DNA as Source Code"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4708">Thinking of DNA as Source Code</h2></div></div></div><p class="calibre2">At the cellular level, your <a class="calibre" id="calibre_link-997"></a><a class="calibre" id="calibre_link-1532"></a>body is a completely distributed system. Nothing is
      centralized. It’s like a cluster of 37.2 trillion<sup class="calibre6">[<a class="firstname" href="#calibre_link-357" id="calibre_link-377">155</a>]</sup> cells executing the same code: your DNA.</p><p class="calibre2">If you think of your DNA as source code, here are some things to
      consider:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">The source is comprised of only four characters: A, C, T, and
          G.</p></li><li class="listitem"><p class="calibre2">The source has two contributors, your mother and father, who
          contributed 3.2 billion letters each. In fact, the reference genome
          provided by the Genome Reference Consortium (GRC) is nothing more
          than an ASCII file with 3.2 billion characters inside.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-358" id="calibre_link-378">156</a>]</sup></p></li><li class="listitem"><p class="calibre2">The source is broken up into 25 separate files called
                        <em class="calibre10">chromosomes</em> that each hold varying fractions of the
                    source. The files are numbered, and tend to get smaller in size, with chromosome
                    1 holding ~250 million characters and chromosome 22 holding only ~50 million.
                    There are also the X, Y, and mitochondrial chromosomes. The term
                        <span class="calibre"><em class="calibre10">chromosome</em></span> basically means “colored thing,” from a time
                    when biologists could stain them but didn’t know what they were.</p></li><li class="listitem"><p class="calibre2">The source is executed on your biological machinery three letters (i.e., a codon) at
                    a time, using the genetic code explained previously—not unlike a Turing machine
                    that reads chemical letters instead of paper ribbon.</p></li><li class="listitem"><p class="calibre2">The source has about 20,000 functions, called <em class="calibre10">genes</em>, which
                    each create a protein when executed. The location of each gene in the source is
                    called the <em class="calibre10">locus</em>. You can think of a gene as a specific
                    range of contiguous base positions on a chromosome. For example, the BRCA1 gene
                    implicated in breast cancer can be found on chromosome 17 from positions
                    41,196,312 to 41,277,500. A gene is like a “pointer” or “address,” whereas
                    alleles (described momentarily) are the actual content. Everyone has the BRCA1
                    gene, but not everyone has alleles that put them at risk.</p></li><li class="listitem"><p class="calibre2">A <em class="calibre10">haplotype</em> is similar to an object in
          object-oriented programming languages that holds specific functions
          (genes) that are typically inherited together.</p></li><li class="listitem"><p class="calibre2">The source has two definitions for each gene, called
                    <span class="calibre"><em class="calibre10">alleles</em></span>—one from your mother and one from your father—which
                    are found at the same position of paired chromosomes (while the cells in your
                    body are <em class="calibre10">diploid</em>—that is, they have two alleles per
                    gene—there are organisms that are <em class="calibre10">triploid</em>,
                        <em class="calibre10">tetraploid</em>, etc.). Both alleles are executed and the
                    resultant proteins interact to create a specific
                        <em class="calibre10">phenotype</em>. For example, proteins that make or degrade
                    eye color pigment lead to a particular phenotype, or an observable
                    characteristic (e.g., blue eyes). If the alleles you inherit from your parents
                    are identical, you’re <em class="calibre10">homozygous</em> for that allele;
                    otherwise, you’re <em class="calibre10">heterozygous</em>.</p></li><li class="listitem"><p class="calibre2">A <em class="calibre10">single-nucleic polymorphism</em> (SNP), pronounced “snip,” is a
                    single-character change in the source code (e.g., from <code class="literal">ACT</code><span class="calibre"><strong class="calibre24"><code class="calibre9">G</code></strong></span><code class="literal">ACTG</code> to <code class="literal">ACT</code><span class="calibre"><strong class="calibre24"><code class="calibre9">T</code></strong></span><code class="literal">ACTG</code>).</p></li><li class="listitem"><p class="calibre2">An <em class="calibre10">indel</em> is short for <span class="calibre"><em class="calibre10">insert-delete</em></span> and
                    represents an insertion or deletion from the reference genome. For example, if
                    the reference has <code class="literal">CCTGACTG</code> and your sample
                    has four characters inserted—say, <code class="literal">CCTG</code><span class="calibre"><strong class="calibre24"><code class="calibre9">CCTA</code></strong></span><code class="literal">ACTG</code>—then
                    it is an indel.</p></li><li class="listitem"><p class="calibre2">Only 0.5% of the source gets translated into the proteins that sustain your life.
                    That portion of the source is called your <em class="calibre10">exome</em>. A human
                    exome requires a few gigabytes to store in compressed binary files.</p></li><li class="listitem"><p class="calibre2">The other 99.5% of the source is commented out and serves as word padding
                        (<em class="calibre10">introns</em>); it is used to regulate when genes are
                    turned on, repeat, and so on.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-359" id="calibre_link-379">157</a>]</sup> A <span class="calibre"><em class="calibre10">whole genome</em></span> requires a few hundred gigabytes
                    to store in compressed binary files.</p></li><li class="listitem"><p class="calibre2">Every cell of your body has the same source,<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-360" id="calibre_link-380">158</a>]</sup> but it can be selectively commented out by
                        <em class="calibre10">epigenetic</em> factors like <em class="calibre10">DNA
                        methylation</em> and <em class="calibre10">histone modification</em>, not
                    unlike an <code class="literal">#ifdef</code> statement for each cell type
                    (e.g., <code class="literal">#ifdef RETINA</code> or <code class="literal">#ifdef LIVER</code>). These factors are responsible for
                    making cells in your retina operate differently than cells in your liver.</p></li><li class="listitem"><p class="calibre2">The process of <em class="calibre10">variant calling</em> is
          similar to running <em class="calibre10">diff</em> between
          two different DNA sources.</p></li></ul></div><p class="calibre2">These analogies aren’t meant to be taken too literally, but hopefully they helped
            familiarize you with some <a class="calibre" id="calibre_link-998"></a><a class="calibre" id="calibre_link-1533"></a>genomics terminology.</p></div><div class="book" title="The Human Genome Project and Reference Genomes"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4709">The Human Genome Project and Reference Genomes</h2></div></div></div><p class="calibre2">In 1953, Watson <a class="calibre" id="calibre_link-1000"></a><a class="calibre" id="calibre_link-2063"></a><a class="calibre" id="calibre_link-1530"></a><a class="calibre" id="calibre_link-1004"></a><a class="calibre" id="calibre_link-3203"></a>and Crick discovered the structure of DNA, and in 1965 Nirenberg, with help
            from his NIH colleagues, cracked the genetic code, which expressed the rules for
            translating DNA or mRNA into proteins. Scientists knew that there were millions of human
            proteins but didn’t have a complete survey of the human genome, which made it impossible
            to fully understand the genes responsible for protein synthesis. For example, if each
            protein was created by a single gene, that would imply millions of protein-coding genes
            in the human genome.</p><p class="calibre2">In 1990, the Human Genome Project set out to determine all the chemical base pairs that
            make up human DNA. This collaborative, international research program published the
            first human genome in April of 2003,<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-361" id="calibre_link-381">159</a>]</sup> at an estimated cost of $3.8 billion. The Human Genome Project generated an
        estimated $796 billion in economic impact, equating to a return on investment (ROI) of 141:1.<sup class="calibre6">[<a class="firstname" href="#calibre_link-362" id="calibre_link-382">160</a>]</sup> The Human Genome Project found about 20,500 genes—significantly fewer than
            the millions you would expect with a simple 1:1 model of gene to protein, since proteins
            can be assembled from a combination of genes, post-translational processes during
            folding, and other mechanisms.</p><p class="calibre2">While this first human genome took over a decade to build, once created, it made
            “bootstrapping” the subsequent sequencing of other genomes much easier. For the first
            genome, scientists were operating in the dark. They had no reference to search as a
            roadmap for constructing the full genome. There is no technology to date that can read a
            whole genome from start to finish; instead, there are many techniques that vary in the
            speed, accuracy, and length of DNA fragments they can read. Scientists in the Human
            Genome Project had to sequence the genome in pieces, with different pieces being more
            easily sequenced by different technologies. Once you have a complete human genome,
            subsequent human genomes become much easier to construct; you can use the first genome
            as a reference for the second. The fragments from the second genome can be pattern
            matched to the first, similar to having the picture on a jigsaw puzzle’s box to help
            inform the placement of the puzzle pieces. It helps that most coding sequences are
            highly conserved, and <span class="calibre"><em class="calibre10">variants</em></span> only occur at 1 in 1,000
            loci.</p><p class="calibre2">Shortly after the Human <a class="calibre" id="calibre_link-1794"></a><a class="calibre" id="calibre_link-1808"></a>Genome Project was completed, the <a class="ulink" href="http://genomereference.org" target="_top">Genome Reference Consortium
      (GRC)</a>, an international collection of academic and research
      institutes, was formed to improve the representation of reference
      genomes. The GRC publishes a new human reference that serves as
      something like a common coordinate system or map to help analyze new
      genomes. The latest human reference genome, released in February 2014,
      was named <code class="literal">GRCh38</code>; it replaced <code class="literal">GRCh37</code>, which was released five years
      prior.</p></div><div class="book" title="Sequencing and Aligning DNA"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4710">Sequencing and Aligning DNA</h2></div></div></div><p class="calibre2">Second-generation <a class="calibre" id="calibre_link-1005"></a><a class="calibre" id="calibre_link-1531"></a>sequencing is rapidly evolving, with numerous hardware vendors and new sequencing
      methods being developed about every six months; however, a common feature of all these
      technologies is the use of massively parallel methods, where thousands or even millions of
      reactions occur simultaneously. The double-stranded DNA is split down the middle, the single
      strands are copied many times, and the copies are randomly shredded into small fragments of
      different lengths called <em class="calibre10">reads</em>, which are placed into the sequencer.
      The sequencer reads the “letters” in each of these reads, in parallel for high throughput, and
      outputs a raw ASCII file containing each read (e.g., <code class="literal">AGTTTCGGGATC...</code>), as well as a quality estimate for each letter read, to be used
      for downstream analysis.</p><p class="calibre2">A piece of software called an <span class="calibre"><em class="calibre10">aligner</em></span> takes each read and works to
      find its position in the reference genome (see <a class="ulink" href="#calibre_link-363" title="Figure&nbsp;23-3.&nbsp;Aligning reads to a reference genome, from Wikipedia">Figure&nbsp;23-3</a>).<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-364" id="calibre_link-383">161</a>]</sup> A complete human genome is about 3 billion base (A, C, T, G) pairs long.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-365" id="calibre_link-384">162</a>]</sup> The reference genome (e.g., <code class="literal">GRCh38</code>) acts like
      the picture on a puzzle box, presenting the overall contours and colors of the human genome.
      Each short read is like a puzzle piece that needs to be fit into position as closely as
      possible. A common metric is “edit distance,” which quantifies the number of operations
      necessary to transform one string to another. Identical strings have an edit distance of zero,
      and an indel of one letter has an edit distance of one. Since humans are 99.9% identical to
      one another, most of the reads will fit to the reference quite well and have a low edit
      distance. The challenge with building a good aligner is handling idiosyncratic reads.</p><div class="figure"><a id="calibre_link-363" class="calibre"></a><div class="book"><div class="book"><img alt="Aligning reads to a reference genome, from Wikipedia" src="images/000033.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;23-3.&nbsp;Aligning reads to a reference genome, from <a class="ulink" href="http://bit.ly/mapping_reads" target="_top">Wikipedia</a></div></div></div><div class="book" title="ADAM, A Scalable Genome Analysis Platform"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4711">ADAM, A Scalable Genome Analysis Platform</h2></div></div></div><p class="calibre2">Aligning the reads to a <a class="calibre" id="calibre_link-995"></a><a class="calibre" id="calibre_link-867"></a><a class="calibre" id="calibre_link-1527"></a>reference genome is only the first of a series of steps necessary to generate
      reports that are useful in a clinical or research setting. The early stages of this processing
      pipeline look similar to any other extract-transform-load (ETL) pipelines that need data
      deduplication and normalization before analysis.</p><p class="calibre2">The sequencing process duplicates genomic DNA, so it’s possible that the same DNA reads
      are generated multiple times; these duplicates need to be marked. The sequencer also provides
      a quality estimate for each DNA “letter” that it reads, which has sequencer-specific biases
      that need to be adjusted. Aligners often misplace reads that have indels (inserted or deleted
      sequences) that need to be repositioned on the reference genome. Currently, this preprocessing
      is done using single-purpose tools launched by shell scripts on a single machine. These tools
      take multiple days to finish the processing of whole genomes. The process is disk bound, with
      each stage writing a new file to be read into subsequent stages, and is an ideal use case for
      applying general-purpose big data technology. ADAM is able to handle the same preprocessing in
      under two hours.</p><p class="calibre2">ADAM is a genome analysis platform that focuses on rapidly processing petabytes of
      high-coverage, whole genome data. ADAM relies on Apache Avro, Parquet, and Spark. These
      systems provide many benefits when used together, since they:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">Allow developers to focus on algorithms without needing to worry about distributed
          system failures</p></li><li class="listitem"><p class="calibre2">Enable jobs to be run locally on a single machine, on an in-house cluster, or in the
          cloud <span class="calibre"><em class="calibre10">without changing code</em></span></p></li><li class="listitem"><p class="calibre2">Compress legacy genomic formats and provide predicate pushdown
          and projection for performance</p></li><li class="listitem"><p class="calibre2">Provide an agile way of customizing and evolving data
          formats</p></li><li class="listitem"><p class="calibre2">Are designed to easily scale out using only commodity
          hardware</p></li><li class="listitem"><p class="calibre2">Are shared with a standard Apache 2.0 license<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-366" id="calibre_link-385">163</a>]</sup></p></li></ul></div><div class="book" title="Literate programming with the Avro interface description language (IDL)"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4712">Literate programming with the Avro interface description language (IDL)</h3></div></div></div><p class="calibre2">The <a class="ulink" href="http://samtools.github.io/hts-specs/SAMv1.pdf" target="_top">Sequence Alignment/Map
          (SAM) specification</a> defines the mandatory fields listed in <a class="ulink" href="#calibre_link-367" title="Table&nbsp;23-2.&nbsp;Mandatory fields in the SAM format">Table&nbsp;23-2</a>.</p><div class="table"><a id="calibre_link-367" class="calibre"></a><div class="table-title">Table&nbsp;23-2.&nbsp;Mandatory fields in the SAM format</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="col"><col class="field"><col class="field"><col class="regex"><col class="desc"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Col</td><td class="calibre20">Field</td><td class="calibre20">Type</td><td class="calibre20">Regexp/Range</td><td class="calibre21">Brief description</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">1</td><td class="calibre23"><code class="uri">QNAME</code></td><td class="calibre23"><code class="uri">String</code></td><td class="calibre23"><code class="uri">[!-?A-~]{1,255}</code></td><td class="calibre25">Query template NAME</td></tr><tr class="calibre26"><td class="calibre23">2</td><td class="calibre23"><code class="uri">FLAG</code></td><td class="calibre23"><code class="uri">Int</code></td><td class="calibre23"><code class="uri">[0, 2<sup class="calibre37">16</sup>-1]</code></td><td class="calibre25">bitwise FLAG</td></tr><tr class="calibre19"><td class="calibre23">3</td><td class="calibre23"><code class="uri">RNAME</code></td><td class="calibre23"><code class="uri">String</code></td><td class="calibre23"><code class="uri">\*|[!-()+-&lt;&gt;-~][!-~]*</code></td><td class="calibre25">Reference sequence NAME</td></tr><tr class="calibre26"><td class="calibre23">4</td><td class="calibre23"><code class="uri">POS</code></td><td class="calibre23"><code class="uri">Int</code></td><td class="calibre23"><code class="uri">[0,2<sup class="calibre37">31</sup>-1]</code></td><td class="calibre25">1-based leftmost mapping POSition</td></tr><tr class="calibre19"><td class="calibre23">5</td><td class="calibre23"><code class="uri">MAPQ</code></td><td class="calibre23"><code class="uri">Int</code></td><td class="calibre23"><code class="uri">[0,2<sup class="calibre37">8</sup>-1]</code></td><td class="calibre25">MAPping Quality</td></tr><tr class="calibre26"><td class="calibre23">6</td><td class="calibre23"><code class="uri">CIGAR</code></td><td class="calibre23"><code class="uri">String</code></td><td class="calibre23"><code class="uri">\*|([0-9]+[MIDNSHPX=])+</code></td><td class="calibre25">CIGAR string</td></tr><tr class="calibre19"><td class="calibre23">7</td><td class="calibre23"><code class="uri">RNEXT</code></td><td class="calibre23"><code class="uri">String</code></td><td class="calibre23"><code class="uri">\*|=|[!-()+-&gt;&lt;-~][!-~]*</code></td><td class="calibre25">Ref. name of the mate/NEXT read</td></tr><tr class="calibre26"><td class="calibre23">8</td><td class="calibre23"><code class="uri">PNEXT</code></td><td class="calibre23"><code class="uri">Int</code></td><td class="calibre23"><code class="uri">[0,2<sup class="calibre37">31</sup>-1]</code></td><td class="calibre25">Position of the mate/NEXT read</td></tr><tr class="calibre19"><td class="calibre23">9</td><td class="calibre23"><code class="uri">TLEN</code></td><td class="calibre23"><code class="uri">Int</code></td><td class="calibre23"><code class="uri">[-2<sup class="calibre37">31</sup>+1,2<sup class="calibre37">31</sup>-1]</code></td><td class="calibre25">observed Template LENgth</td></tr><tr class="calibre26"><td class="calibre23">10</td><td class="calibre23"><code class="uri">SEQ</code></td><td class="calibre23"><code class="uri">String</code></td><td class="calibre23"><code class="uri">\*|[A-Za-z=.]+</code></td><td class="calibre25">segment SEQuence</td></tr><tr class="calibre19"><td class="calibre27">11</td><td class="calibre27"><code class="uri">QUAL</code></td><td class="calibre27"><code class="uri">String</code></td><td class="calibre27"><code class="uri">[!-~]</code></td><td class="calibre28">ASCII of Phred-scaled base QUALity+33</td></tr></tbody></table></div></div><p class="calibre2">Any developers who want to implement this specification need to translate this English
        spec into their computer language of choice. In ADAM, we have chosen instead to use literate
        programming with a spec defined in Avro IDL. For example, the mandatory fields for SAM can
        be easily expressed in a simple Avro record:</p><pre class="screen1">record AlignmentRecord {
  string qname;
  int flag;
  string rname;
  int pos;
  int mapq;
  string cigar;
  string rnext;
  int pnext;
  int tlen;
  string seq;
  string qual;
}</pre><p class="calibre2">Avro is able to autogenerate native Java (or C++, Python, etc.) classes for reading
        and writing data and provides standard interfaces (e.g., Hadoop’s <code class="literal">InputFormat</code>) to make integration with numerous systems easy. Avro is also
          designed to make schema evolution easier. In fact, the <a class="ulink" href="http://bit.ly/bdg-formats" target="_top">ADAM schemas</a> we use today have evolved to be more sophisticated, expressive, and customized to express a variety of genomic models such as structural variants, genotypes, variant calling annotations, variant effects, and more.</p><p class="calibre2">UC Berkeley is a member of the <a class="ulink" href="http://genomicsandhealth.org/" target="_top">Global
          Alliance for Genomics &amp; Health</a>, a non-governmental, public-private partnership
        consisting of more than 220 organizations across 30 nations, with the goal of maximizing the
        potential of genomics medicine through effective and responsible data sharing. The Global
        Alliance has embraced this literate programming approach and publishes <a class="ulink" href="https://github.com/ga4gh/schemas" target="_top">its schemas</a> in Avro IDL as well. Using Avro has allowed researchers around the world to talk about data
        at the logical level, without concern for computer languages or on-disk formats.</p></div><div class="book" title="Column-oriented access with Parquet"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4713">Column-oriented access with Parquet</h3></div></div></div><p class="calibre2">The SAM and BAM<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-368" id="calibre_link-386">164</a>]</sup> file formats are <span class="calibre"><em class="calibre10">row-oriented</em></span>: the data for each record is
        stored together as a single line of text or a binary record. (See <a class="ulink" href="#calibre_link-369" title="Other File Formats and Column-Oriented Formats">Other File Formats and Column-Oriented Formats</a> for further discussion of row- versus
        column-oriented formats.) A single paired-end read in a SAM file might look like
        this:</p><pre class="screen1">read1   99 chrom1  7 30 8M2I4M1D3M = 37  39 TTAGATAAAGGATACTG *
read1  147 chrom1 37 30 9M         =  7 -39 CAGCGGCAT         * NM:i:1</pre><p class="calibre2">A typical SAM/BAM file contains many millions of rows, one for
        each DNA read that came off the sequencer. The preceding text fragment
        translates loosely into the view shown in <a class="ulink" href="#calibre_link-370" title="Table&nbsp;23-3.&nbsp;Logical view of SAM fragment">Table&nbsp;23-3</a>.</p><div class="table"><a id="calibre_link-370" class="calibre"></a><div class="table-title">Table&nbsp;23-3.&nbsp;Logical view of SAM fragment</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="col"><col class="rname"><col class="col"><col class="col"><col class="rname"><col class="rname"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Name</td><td class="calibre20">Reference</td><td class="calibre20">Position</td><td class="calibre20">MapQ</td><td class="calibre20">CIGAR</td><td class="calibre21">Sequence</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23"><code class="uri">read1</code></td><td class="calibre23"><code class="uri">chromosome1</code></td><td class="calibre23">7</td><td class="calibre23">30</td><td class="calibre23"><code class="uri">8M2I4M1D3M</code></td><td class="calibre25"><code class="uri">TTAGATAAAGGATACTG</code></td></tr><tr class="calibre26"><td class="calibre27"><code class="uri">read1</code></td><td class="calibre27"><code class="uri">chromosome1</code></td><td class="calibre27">37</td><td class="calibre27">30</td><td class="calibre27"><code class="uri">9M</code></td><td class="calibre28"><code class="uri">CAGCGGCAT</code></td></tr></tbody></table></div></div><p class="calibre2">In this example, the read, identified as <code class="literal">read1</code>, was mapped to the reference genome at
        <code class="literal">chromosome1</code>, positions 7 and 37.
        This is called a “paired-end” read as it represents a single strand of
        DNA that was read from each end by the sequencer. By analogy, it’s
        like reading an array of length 150 from <code class="literal">0..50</code> and <code class="literal">150..100</code>.</p><p class="calibre2">The <code class="literal">MapQ</code> score represents the
        probability that the sequence is mapped to the reference correctly.
        <code class="literal">MapQ</code> scores of 20, 30, and 40 have
        a probability of being correct of 99%, 99.9%, and 99.99%,
        respectively. To calculate the probability of error from a <code class="literal">MapQ</code> score, use the expression
        10<sup class="calibre6">(-MapQ/10)</sup> (e.g.,
        10<sup class="calibre6">(-30/10)</sup> is a probability of
        0.001).</p><p class="calibre2">The <code class="literal">CIGAR</code> explains how the
        individual nucleotides in the DNA sequence map to the
        reference.<sup class="calibre6">[<a class="firstname" type="noteref" href="#calibre_link-371" id="calibre_link-387">165</a>]</sup> The <code class="literal">Sequence</code> is, of
        course, the DNA sequence that was mapped to the reference.</p><p class="calibre2">There is a stark mismatch between the SAM/BAM
        <span class="calibre"><em class="calibre10">row-oriented</em></span> on-disk format and the
        <span class="calibre"><em class="calibre10">column-oriented</em></span> <u style="
    text-decoration: underline .14em;
">access pattern</u>s common to genome
        analysis. Consider the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">A range query to find data for a particular gene linked to
            breast cancer, named BRCA1: “Find all reads that cover chromosome
            17 from position 41,196,312 to 41,277,500”</p></li><li class="listitem"><p class="calibre2">A simple filter to find poorly mapped reads: “Find all reads
            with a <code class="literal">MapQ</code> less than
              10”</p></li><li class="listitem"><p class="calibre2">A search of all reads with insertions or deletions, called
            <span class="calibre"><em class="calibre10">indels</em></span>: “Find all reads that contain <code class="literal">I</code> or <code class="literal">D</code> in the <code class="literal">CIGAR</code> string”</p></li><li class="listitem"><p class="calibre2">Count the number of unique <span class="calibre"><em class="calibre10">k</em></span>-mers:
            “Read every <code class="literal">Sequence</code> and
            generate all possible substrings of length <span class="calibre"><em class="calibre10">k</em></span>
            in the string”</p></li></ul></div><p class="calibre2">Parquet’s predicate pushdown feature allows us to rapidly filter
        reads for analysis (e.g., finding a gene, ignoring poorly mapped reads).
        Projection allows for precise materialization of only the columns of
        interest (e.g., reading only the sequences for
          <span class="calibre"><em class="calibre10">k</em></span>-mer counting).</p><p class="calibre2">Additionally, a number of the fields have low cardinality, making them ideal for data
        compression techniques like run-length encoding (RLE). For example, given that humans have
        only 23 pairs of chromosomes, the <code class="literal">Reference</code> field will
        have only a few dozen unique values (e.g., <code class="literal">chromosome1</code>,
          <code class="literal">chromosome17</code>, etc.). We have found that storing BAM
        records inside Parquet files results in ~20% compression. Using the <code class="literal">PrintFooter</code> command in Parquet, we have found that quality scores can be
        run-length encoded and bit-packed to compress ~48%, but they still take up ~70% of the total
        space. <br>We’re looking forward to Parquet 2.0, so we can use delta encoding on the quality
        scores to compress the file size even more.</p></div><div class="book" title="A simple example: k-mer counting using Spark and ADAM"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4714">A simple example: <span class="firstname"><em class="calibre54">k</em></span>-mer counting using
        Spark and ADAM</h3></div></div></div><p class="calibre2">Let’s do “word count” for genomics: counting <span class="calibre"><em class="calibre10">k</em></span>-mers. The term
          <span class="calibre"><em class="calibre10">k-mers</em></span> refers to all the possible subsequences of length
          <span class="calibre"><em class="calibre10">k</em></span> for a read. For example, if you have a read with the sequence
          <code class="literal">AGATCTGAAG</code>, the 3-mers for that sequence would be
          <code class="literal">['AGA', 'GAT', 'ATC', 'TCT', 'CTG', 'TGA', 'GAA',
          'AAG']</code>. While this is a trivial example, <span class="calibre"><em class="calibre10">k</em></span>-mers are useful
        when building structures like De Bruijn graphs for sequence assembly. In this example, we
        are going to generate all the possible 21-mers from our reads, count them, and then write
        the totals to a text file.</p><p class="calibre2">This example assumes that you’ve already created a <code class="literal">SparkContext</code> named <code class="literal">sc</code>. First, we create a Spark RDD of <code class="literal">AlignmentRecord</code>s using a pushdown predicate
        to remove low-quality reads and a projection to only materialize the
        <code class="literal">sequence</code> field in each
        read:</p><pre class="screen1"><code class="c2">// Load reads from 'inputPath' into an RDD for analysis</code>
<code class="k">val</code> <code class="n">adamRecords</code><code class="k">:</code> <code class="kt">RDD</code><code class="o">[</code><code class="kt">AlignmentRecord</code><code class="o">]</code> <code class="k">=</code> <code class="n">sc</code><code class="o">.</code><code class="n">adamLoad</code><code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="n">inputPath</code><code class="o">,</code>

  <code class="c2">// Filter out all low-quality reads that failed vendor quality checks</code>
  <code class="n">predicate</code> <code class="k">=</code> <code class="nc">Some</code><code class="o">(</code><code class="n">classOf</code><code class="o">[</code><code class="kt">HighQualityReadsPredicate</code><code class="o">]),</code>

  <code class="c2">// Only materialize the 'sequence' from each record</code>
  <code class="n">projection</code> <code class="k">=</code> <code class="nc">Some</code><code class="o">(</code><code class="nc">Projection</code><code class="o">(</code><code class="nc">AlignmentRecordField</code><code class="o">.</code><code class="n">sequence</code><code class="o">)))</code></pre><p class="calibre2">Since Parquet is a column-oriented storage format, it can
        rapidly materialize only the <code class="literal">sequence</code> column and quickly skip over the
        unwanted fields. Next, we walk over each <code class="literal">sequence</code> using a sliding window of length
        <span class="calibre"><em class="calibre10">k</em></span>=21, emit a count of <code class="literal">1L</code>, and then <code class="literal">reduceByKey</code> using the
        <span class="calibre"><em class="calibre10">k</em></span>-mer subsequence as the key to get the total
        counts for the input file:</p><pre class="screen1"><code class="c2">// The length of k-mers we want to count</code>
<code class="k">val</code> <code class="n">kmerLength</code> <code class="k">=</code> <code class="mi">21</code>

<code class="c2">// Process the reads into an RDD of tuples with k-mers and counts</code>
<code class="k">val</code> <code class="n">kmers</code><code class="k">:</code> <code class="kt">RDD</code><code class="o">[(</code><code class="kt">String</code>, <code class="kt">Long</code><code class="o">)]</code> <code class="k">=</code> <code class="n">adamRecords</code><code class="o">.</code><code class="n">flatMap</code><code class="o">(</code><code class="n">read</code> <code class="k">=&gt;</code> <code class="o">{</code>
  <code class="n">read</code><code class="o">.</code><code class="n">getSequence</code>
    <code class="o">.</code><code class="n">toString</code>
    <code class="o">.</code><code class="n">sliding</code><code class="o">(</code><code class="n">kmerLength</code><code class="o">)</code>
    <code class="o">.</code><code class="n">map</code><code class="o">(</code><code class="n">k</code> <code class="k">=&gt;</code> <code class="o">(</code><code class="n">k</code><code class="o">,</code> <code class="mi">1L</code><code class="o">))</code>
<code class="o">}).</code><code class="n">reduceByKey</code> <code class="o">{</code> <code class="k">case</code> <code class="o">(</code><code class="n">a</code><code class="o">,</code> <code class="n">b</code><code class="o">)</code> <code class="k">=&gt;</code> <code class="n">a</code> <code class="o">+</code> <code class="n">b</code><code class="o">}</code>

<code class="c2">// Print the k-mers as a text file to the 'outputPath'</code>
<code class="n">kmers</code><code class="o">.</code><code class="n">map</code> <code class="o">{</code> <code class="k">case</code> <code class="o">(</code><code class="n">kmer</code><code class="o">,</code> <code class="n">count</code><code class="o">)</code> <code class="k">=&gt;</code> <code class="sb">s"</code><code class="si">$count</code><code class="sb">,</code><code class="si">$kmer</code><code class="sb">"</code><code class="o">}</code>
  <code class="o">.</code><code class="n">saveAsTextFile</code><code class="o">(</code><code class="n">args</code><code class="o">.</code><code class="n">outputPath</code><code class="o">)</code></pre><p class="calibre2">When run on sample <code class="literal">NA21144</code>,
          chromosome 11 in the 1000 Genomes project,<sup class="calibre6">[<a class="firstname" href="#calibre_link-372" id="calibre_link-388">166</a>]</sup> this job outputs the
        following:</p><pre class="screen1">AAAAAAAAAAAAAAAAAAAAAA, 124069
TTTTTTTTTTTTTTTTTTTTTT, 120590
ACACACACACACACACACACAC, 41528
GTGTGTGTGTGTGTGTGTGTGT, 40905
CACACACACACACACACACACA, 40795
TGTGTGTGTGTGTGTGTGTGTG, 40329
TAATCCCAGCACTTTGGGAGGC, 32122
TGTAATCCCAGCACTTTGGGAG, 31206
CTGTAATCCCAGCACTTTGGGA, 30809
GCCTCCCAAAGTGCTGGGATTA, 30716
...</pre><p class="calibre2">ADAM can do much more than just count
        <span class="calibre"><em class="calibre10">k</em></span>-mers. Aside from the preprocessing stages
        already mentioned—duplicate marking, base quality score recalibration,
        and indel realignment—it also:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">Calculates coverage read depth at each variant in a Variant
            Call Format (VCF) file</p></li><li class="listitem"><p class="calibre2">Counts the
            <span class="calibre"><em class="calibre10">k</em></span>-mers/<span class="calibre"><em class="calibre10">q</em></span>-mers from a
            read dataset</p></li><li class="listitem"><p class="calibre2">Loads gene annotations from a Gene Transfer Format (GTF)
            file and outputs the corresponding gene models</p></li><li class="listitem"><p class="calibre2">Prints statistics on all the reads in a read dataset (e.g.,
            % mapped to reference, number of duplicates, reads mapped cross-chromosome, etc.)</p></li><li class="listitem"><p class="calibre2">Launches legacy variant callers, pipes reads into <span class="calibre">stdin</span>, and saves output from <span class="calibre">stdout</span></p></li><li class="listitem"><p class="calibre2">Comes with a basic genome browser to view reads in a web
            browser</p></li></ul></div><p class="calibre2">However, the most important thing ADAM provides is an open,
          scalable platform. All artifacts are published to <a class="ulink" href="http://search.maven.org/" target="_top">Maven Central</a>
        (search for group ID <code class="literal">org.bdgenomics</code>) to make it easy for
        developers to benefit from the foundation ADAM provides. ADAM data is
        stored in Avro and Parquet, so you can also use systems like SparkSQL,
        Impala, Apache Pig, Apache Hive, or others to analyze the data. ADAM
        also supports job written in Scala, Java, and Python, with more
        language support on the way.</p><p class="calibre2">At Scala.IO in Paris in 2014, Andy Petrella and Xavier Tordoir used Spark’s MLlib
          <span class="calibre">k</span>-means with ADAM for population stratification across
        the 1000 Genomes dataset (population stratification is the process of assigning an
        individual genome to an ancestral group). They found that ADAM/Spark improved performance by
        a factor of <a class="calibre" id="calibre_link-996"></a><a class="calibre" id="calibre_link-868"></a><a class="calibre" id="calibre_link-1528"></a>150.</p></div></div><div class="book" title="From Personalized Ads to Personalized Medicine"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4715">From Personalized Ads to Personalized Medicine</h2></div></div></div><p class="calibre2">While ADAM is <a class="calibre" id="calibre_link-1002"></a>designed to rapidly and scalably analyze aligned reads, it does not align the
      reads itself; instead, ADAM relies on standard short-reads aligners. The <a class="ulink" href="http://snap.cs.berkeley.edu/" target="_top">Scalable Nucleotide Alignment Program</a> (SNAP) is a
      collaborative effort including participants from Microsoft Research, UC San Francisco, and the
      AMPLab as well as open source developers, shared with an Apache 2.0 license. The SNAP aligner
      is as accurate as the current best-of-class aligners, like BWA-mem, Bowtie2, and Novalign, but
      runs between 3 and 20 times faster. This speed advantage is important when doctors are racing
      to identify a pathogen.</p><p class="calibre2">In 2013, a boy went to the University of Wisconsin Hospital and Clinics’ Emergency
      Department three times in four months with symptoms of encephalitis: fevers and headaches. He
      was eventually hospitalized without a successful diagnosis after numerous blood tests, brain
      scans, and biopsies. Five weeks later, he began having seizures that required he be placed
      into a medically induced coma. In desperation, doctors sampled his spinal fluid and sent it to
      an experimental program led by Charles Chiu at UC San Francisco, where it was sequenced for
      analysis. The speed and accuracy of SNAP allowed UCSF to quickly filter out all human DNA and,
      from the remaining 0.02% of the reads, identify a rare infectious bacterium, <span class="calibre">Leptospira santarosai</span>. They reported the discovery to the Wisconsin
      doctors <span class="calibre"><em class="calibre10">just two days after they sent the sample</em></span>. The boy was treated
      with antibiotics for 10 days, awoke from his coma, and was discharged from the hospital two
      weeks later.<sup class="calibre6">[<a class="firstname" href="#calibre_link-373" id="calibre_link-389">167</a>]</sup></p><p class="calibre2">If you’re interested in learning more about the system the Chiu lab used—called <a class="ulink" href="http://chiulab.ucsf.edu/surpi/" target="_top">Sequence-based Ultra-Rapid Pathogen Identification
        (SURPI)</a>—they have generously shared their software with a permissive BSD license and
      provide an Amazon EC2 Machine Image (AMI) with SURPI preinstalled. SURPI collects 348,922
      unique bacterial sequences and 1,193,607 unique virus sequences from numerous sources and
      saves them in 29 SNAP-indexed databases, each approximately 27 GB in size, for fast
      search.</p><p class="calibre2">Today, more data is analyzed for personalized advertising than personalized medicine,
      but that will not be the case in the future. With personalized medicine, people receive
      customized healthcare that takes into consideration their unique DNA profiles. As the price of
      sequencing drops and more people have their genomes sequenced, the increase in statistical
      power will allow researchers to understand the genetic mechanisms underlying diseases and fold
      these discoveries into the personalized medical model, to improve treatment for subsequent
      patients. While only 25 PB of genomic data were generated worldwide this year, next year that
      number will likely be <a class="calibre" id="calibre_link-1003"></a>100 PB.</p></div><div class="book" title="Join In"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4716">Join In</h2></div></div></div><p class="calibre2">While we’re <a class="calibre" id="calibre_link-1001"></a>off to a great start, the ADAM project is still an experimental platform and needs
      further development. If you’re interested in learning more about programming on ADAM or want
      to contribute code, take a look at <span class="calibre"><a class="ulink" href="http://shop.oreilly.com/product/0636920035091.do" target="_top">Advanced Analytics with Spark:
          Patterns for Learning from Data at Scale</a></span> by Sandy Ryza et al.
      (O’Reilly, 2014), which includes a chapter on analyzing genomics data with ADAM and Spark. You
      can find us at <a class="ulink" href="http://bdgenomics.org" target="_top">http://bdgenomics.org</a>, on IRC at #adamdev, or <a class="calibre" id="calibre_link-1065"></a>on Twitter at <a class="ulink" href="https://twitter.com/bigdatagenomics" target="_top">@bigdatagenomics</a>.</p></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" id="calibre_link-351"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-374">152</a>] </sup>See Antonio Regalado, <a class="ulink" href="http://bit.ly/genome_sequencing" target="_top">“EmTech: Illumina Says 228,000 Human Genomes Will Be Sequenced This Year,”</a> September 24, 2014.</p></div><div class="footnote" id="calibre_link-354"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-375">153</a>] </sup>This process is called <span class="calibre"><em class="calibre10">protein folding</em></span>. The <a class="ulink" href="http://folding.stanford.edu/" target="_top">Folding@home</a>
          allows volunteers to donate CPU cycles to help researchers determine
          the mechanisms of protein folding.</p></div><div class="footnote" type="footnote" id="calibre_link-355"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-376">154</a>] </sup>There are also a few nonstandard amino acids not shown in the
          table that are encoded differently.</p></div><div class="footnote" id="calibre_link-357"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-377">155</a>] </sup>See Eva Bianconi et al., <a class="ulink" href="http://bit.ly/cell_estimate" target="_top">“An estimation of the number of cells in the human body,”</a> <span class="calibre"><em class="calibre10">Annals of Human Biology</em></span>, November/December 2013.</p></div><div class="footnote" type="footnote" id="calibre_link-358"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-378">156</a>] </sup>You might expect this to be 6.4 billion letters, but the
              reference genome is, for better or worse, a
              <em class="calibre10">haploid</em> representation of the average of
              dozens of individuals.</p></div><div class="footnote" type="footnote" id="calibre_link-359"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-379">157</a>] </sup>Only about 28% of your DNA is transcribed into nascent RNA, and after
                            RNA splicing, only about 1.5% of the RNA is left to code for proteins.
                            Evolutionary selection occurs at the DNA level, with most of your DNA
                            providing support to the other 0.5% or being deselected altogether (as
                            more fitting DNA evolves). There are some cancers that appear to be
                            caused by dormant regions of DNA being resurrected, so to speak.</p></div><div class="footnote" type="footnote" id="calibre_link-360"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-380">158</a>] </sup>There is actually, on average, about 1 error for each billion DNA
                            “letters” copied. So, each cell isn’t <span class="calibre"><em class="calibre10">exactly</em></span> the
                            same.</p></div><div class="footnote" type="footnote" id="calibre_link-361"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-381">159</a>] </sup>Intentionally 50 years after Watson and Crick’s discovery of the 3D structure
                    of DNA.</p></div><div class="footnote" id="calibre_link-362"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-382">160</a>] </sup>Jonathan Max Gitlin, <a class="ulink" href="http://www.genome.gov/27544383" target="_top">“Calculating the economic impact of the Human Genome Project,”</a> June 2013.</p></div><div class="footnote" type="footnote" id="calibre_link-364"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-383">161</a>] </sup>There is also a second approach, <span class="calibre">de novo</span> assembly,
          where reads are put into a graph data structure to create long sequences without mapping
          to a reference genome.</p></div><div class="footnote" type="footnote" id="calibre_link-365"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-384">162</a>] </sup>Each base is about 3.4 angstroms, so the DNA from a single human cell stretches over 2
          meters end to end!</p></div><div class="footnote" type="footnote" id="calibre_link-366"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-385">163</a>] </sup>Unfortunately, some of the more popular software in genomics has an ill-defined
              or custom, restrictive license. Clean open source licensing and source code are
              necessary for science to make it easier to reproduce and understand results.</p></div><div class="footnote" type="footnote" id="calibre_link-368"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-386">164</a>] </sup>BAM is the compressed binary version of the SAM format.</p></div><div class="footnote" type="footnote" id="calibre_link-371"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-387">165</a>] </sup>The first record’s Compact Idiosyncratic Gap Alignment
            Report (CIGAR) string is translated as “8 matches (8M), 2 inserts
            (2I), 4 matches (4M), 1 delete (1D), 3 matches (3M).”</p></div><div class="footnote" id="calibre_link-372"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-388">166</a>] </sup>Arguably the most popular publicly available dataset, found at <a class="ulink" href="http://www.1000genomes.org" target="_top">http://www.1000genomes.org</a>.</p></div><div class="footnote" id="calibre_link-373"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-389">167</a>] </sup>Michael Wilson et al., <a class="ulink" href="http://www.nejm.org/doi/pdf/10.1056/NEJMoa1401268" target="_top">“Actionable Diagnosis of Neuroleptospirosis by Next-Generation Sequencing,”</a> <span class="calibre"><em class="calibre10">New England
          Journal of Medicine</em></span>, June 2014.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-410"><section type="chapter" id="calibre_link-4717" title="Chapter&nbsp;24.&nbsp;Cascading"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Chapter&nbsp;24.&nbsp;Cascading</h2></div><div class="book"><div class="book"><div class="author2"><h3 class="author1"><span class="firstname">Chris K.</span> <span class="firstname">Wensel</span></h3></div></div></div></div></div><p class="calibre2">Cascading <a class="calibre" id="calibre_link-3783"></a>is <a class="calibre" id="calibre_link-1066"></a><a class="calibre" id="calibre_link-1049"></a>an open source Java library and API that provides an
    abstraction layer for MapReduce. It allows developers to build complex,
    mission-critical data processing applications that run on Hadoop
    clusters.</p><p class="calibre2">The Cascading project began in the summer of 2007. Its first public
    release, version 0.1, launched in January 2008. Version 1.0 was released
    in January 2009. Binaries, source code, and add-on modules can be
    downloaded from the <a class="ulink" href="http://www.cascading.org/" target="_top">project
    website</a>.</p><p class="calibre2">Map and reduce operations offer powerful primitives. However, they
    tend to be at the wrong level of granularity for creating sophisticated,
    highly composable code that can be shared among different developers.
    Moreover, many developers find it difficult to “think” in terms of
    MapReduce when faced with real-world problems.</p><p class="calibre2">To address the first issue, Cascading substitutes the keys and
    values used in MapReduce with simple field names and a data tuple model,
    where a tuple is simply a list of values. For the second issue, Cascading
    departs from map and reduce operations directly by introducing
    higher-level abstractions as alternatives: <code class="literal">Function</code>s, <code class="literal">Filter</code>s, <code class="literal">Aggregator</code>s, and <code class="literal">Buffer</code>s.</p><p class="calibre2">Other alternatives began to emerge at about the same time as the
    project’s initial public release, but Cascading was designed to complement
    them. Consider that most of these alternative frameworks impose pre- and
    post-conditions, or other expectations.</p><p class="calibre2">For example, in several other MapReduce tools, you must preformat,
    filter, or import your data into HDFS prior to running the application.
    That step of preparing the data must be performed outside of the
    programming abstraction. In contrast, Cascading provides the means to
    prepare and manage your data as integral parts of the programming
    abstraction.</p><p class="calibre2">This case study begins with an introduction to the main concepts of
    Cascading, then finishes with an overview of how <a class="ulink" href="http://www.sharethis.com/" target="_top">ShareThis</a> uses Cascading in its
    infrastructure.</p><p class="calibre2">See the <a class="ulink" href="http://www.cascading.org/documentation/" target="_top">Cascading User Guide</a> on the project website for a more
    in-depth presentation of the <u>Cascading</u> <u style="
    text-decoration: underline 0.14em green;
">processing model</u>.</p><div class="book" title="Fields, Tuples, and Pipes"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4718">Fields, Tuples, and Pipes</h2></div></div></div><p class="calibre2">The MapReduce model <a class="calibre" id="calibre_link-1052"></a>uses keys and values to link input data to the map
      function, the map function to the reduce function, and the reduce
      function to the output data.</p><p class="calibre2">But as we know, real-world Hadoop applications usually consist of
      more than one MapReduce job chained together. Consider the canonical
      word count example implemented in MapReduce. If you needed to sort the
      numeric counts in descending order, which is not an unlikely
      requirement, it would need to be done in a second MapReduce job.</p><p class="calibre2">So, in the abstract, keys and values not only bind map to reduce,
      but reduce to the next map, and then to the next reduce, and so on
      (<a class="ulink" href="#calibre_link-411" title="Figure&nbsp;24-1.&nbsp;Counting and sorting in MapReduce">Figure&nbsp;24-1</a>). That is, key-value pairs are
      sourced from input files and stream through chains of map and reduce
      operations, and finally rest in an output file. When you implement
      enough of these chained MapReduce applications, you start to see a
      well-defined set of key-value manipulations used over and over again to
      modify the key-value data stream.</p><div class="book"><div class="figure"><a id="calibre_link-411" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4719" class="calibre"></a><img alt="Counting and sorting in MapReduce" src="images/000042.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;24-1.&nbsp;Counting and sorting in MapReduce</div></div></div><p class="calibre2">Cascading simplifies this by abstracting away keys and values and replacing them with
      tuples that have corresponding field names, similar in concept to tables and column names in a
      relational database. During processing, streams of these fields and tuples are then
      manipulated as they pass through user-defined operations linked together by pipes (<a class="ulink" href="#calibre_link-412" title="Figure&nbsp;24-2.&nbsp;Pipes linked by fields and tuples">Figure&nbsp;24-2</a>).</p><div class="book"><div class="figure"><a id="calibre_link-412" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4720" class="calibre"></a><img alt="Pipes linked by fields and tuples" src="images/000050.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;24-2.&nbsp;Pipes linked by fields and tuples</div></div></div><p class="calibre2">So, MapReduce keys and values are reduced to:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term"><a id="calibre_link-4721" class="calibre"></a>Fields</span></dt><dd class="calibre8"><p class="calibre2">A field is a collection of either <code class="literal">String</code> names (such as
            “first_name”), numeric positions (such as 2 or –1, for the third and last positions,
            respectively), or a combination of both. So, fields are
            used to declare the names of values in a tuple and to select values by name from a
            tuple. The latter is like a SQL <code class="literal">select</code> call.</p></dd><dt class="calibre7"><span class="term"><a id="calibre_link-4722" class="calibre"></a>Tuples</span></dt><dd class="calibre8"><p class="calibre2">A tuple is simply an array of
            <code class="literal">java.lang.Comparable</code> objects. A tuple is
            very much like a database row or record.</p></dd></dl></div><p class="calibre2">And the map and reduce operations are abstracted behind one or
      more pipe instances (<a class="ulink" href="#calibre_link-413" title="Figure&nbsp;24-3.&nbsp;Pipe types">Figure&nbsp;24-3</a>):</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term"><a id="calibre_link-4723" class="calibre"></a><code class="literal1">Each</code></span></dt><dd class="calibre8"><p class="calibre2">The <code class="literal">Each</code> pipe processes a single input tuple at
            a time. It may apply either a <code class="literal">Function</code> or a <code class="literal">Filter</code> operation (described shortly) to
            the input tuple.</p></dd><dt class="calibre7"><span class="term"><a id="calibre_link-4724" class="calibre"></a><code class="literal1">GroupBy</code></span></dt><dd class="calibre8"><p class="calibre2">The <code class="literal">GroupBy</code> pipe groups tuples on grouping fields. It behaves just
            like the SQL <code class="literal">GROUP BY</code> statement. It can also merge multiple input tuple
            streams into a single stream if they all share the same field names.</p></dd><dt class="calibre7"><span class="term"><a id="calibre_link-4725" class="calibre"></a><code class="literal1">CoGroup</code></span></dt><dd class="calibre8"><p class="calibre2">The <code class="literal">CoGroup</code> pipe joins multiple tuple streams
            together by common field names, and it also groups the tuples by
            the common grouping fields. All standard join types (inner, outer,
            etc.) and custom joins can be used across two or more tuple
            streams.</p></dd><dt class="calibre7"><span class="term"><code class="literal1">Every</code></span></dt><dd class="calibre8"><p class="calibre2">The <code class="literal">Every</code> pipe processes a single grouping of
            tuples at a time, where the group was grouped by a
            <code class="literal">GroupBy</code> or <code class="literal">CoGroup</code> pipe. The
              <code class="literal">Every</code> pipe may apply either an <code class="literal">Aggregator</code> or a <code class="literal">Buffer</code>
            operation to the grouping.</p></dd><dt class="calibre7"><span class="term"><code class="literal1">SubAssembly</code></span></dt><dd class="calibre8"><p class="calibre2">The <code class="literal">SubAssembly</code> pipe allows for nesting of
            assemblies inside a single pipe, which can, in turn, be nested in
            more complex assemblies.</p></dd></dl></div><div class="book"><div class="figure"><a id="calibre_link-413" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4726" class="calibre"></a><img alt="Pipe types" src="images/000059.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;24-3.&nbsp;Pipe types</div></div></div><p class="calibre2">All these pipes are chained together by the developer into “pipe
      assemblies,” in which each assembly can have many input tuple streams
      (sources) and many output tuple streams (sinks). See <a class="ulink" href="#calibre_link-414" title="Figure&nbsp;24-4.&nbsp;A simple PipeAssembly">Figure&nbsp;24-4</a>.</p><div class="book"><div class="figure"><a id="calibre_link-414" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4727" class="calibre"></a><img alt="A simple PipeAssembly" src="images/000067.jpg" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;24-4.&nbsp;A simple PipeAssembly</div></div></div><p class="calibre2">On the surface, this might seem more complex than the traditional
      MapReduce model. And admittedly, there are more concepts here than map,
      reduce, key, and value. But in practice, there are many more concepts
      that must all work in tandem to provide different behaviors.</p><p class="calibre2">For example, a developer who wanted to provide a “secondary
      sorting” of reducer values would need to implement a map, a
      reduce, a “composite” key (two keys nested in a parent key), a value, a
      partitioner, an “output value grouping” comparator, and an “output key”
      comparator, all of which would be coupled to one another in varying
      ways, and very likely would not be reusable in subsequent
      applications.</p><p class="calibre2">In Cascading, this would be one line of code: <code class="literal">new
        GroupBy(</code><em class="replaceable"><code class="replaceable">&lt;previous&gt;</code></em><code class="literal">,</code> <em class="replaceable"><code class="replaceable">&lt;grouping fields&gt;</code></em><code class="literal">,</code> <em class="replaceable"><code class="replaceable">&lt;secondary sorting
      fields&gt;</code></em><code class="literal">)</code>, where <em class="replaceable"><code class="replaceable">&lt;previous&gt;</code></em> is the pipe that came
      <a class="calibre" id="calibre_link-1053"></a>before.</p></div><div class="book" title="Operations"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4728">Operations</h2></div></div></div><p class="calibre2">As mentioned <a class="calibre" id="calibre_link-1055"></a>earlier, Cascading departs from MapReduce by introducing
      alternative operations that are applied either to individual tuples or
      groups of tuples (<a class="ulink" href="#calibre_link-415" title="Figure&nbsp;24-5.&nbsp;Operation types">Figure&nbsp;24-5</a>):</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term"><a id="calibre_link-4729" class="calibre"></a><code class="literal1">Function</code></span></dt><dd class="calibre8"><p class="calibre2">A <code class="literal">Function</code> operates on individual input tuples
            and may return zero or more output tuples for every one input.
            Functions are applied by the <code class="literal">Each</code> pipe.</p></dd><dt class="calibre7"><span class="term"><a id="calibre_link-4730" class="calibre"></a><code class="literal1">Filter</code></span></dt><dd class="calibre8"><p class="calibre2">A <code class="literal">Filter</code> is a special kind of function that returns a Boolean value
            indicating whether the current input tuple should be removed from the tuple stream. A
              <code class="literal">Function</code> could serve this purpose,
            but the <code class="literal">Filter</code> is optimized for this case, and many filters can be grouped
            by “logical” filters such as <code class="literal">AND</code>, <code class="literal">OR</code>, <code class="literal">XOR</code>, and
              <code class="literal">NOT</code>, rapidly creating more complex filtering operations.</p></dd><dt class="calibre7"><span class="term"><a id="calibre_link-4731" class="calibre"></a><code class="literal1">Aggregator</code></span></dt><dd class="calibre8"><p class="calibre2">An <code class="literal">Aggregator</code> performs some operation against a group of tuples,
            where the grouped tuples are by a common set of
            field values (for example, all tuples having the
            same “last-name” value). Common <code class="literal">Aggregator</code> implementations would be <code class="literal">Sum</code>, <code class="literal">Count</code>,
              <code class="literal">Average</code>, <code class="literal">Max</code>, and <code class="literal">Min</code>.</p></dd><dt class="calibre7"><span class="term"><a id="calibre_link-4732" class="calibre"></a><code class="literal1">Buffer</code></span></dt><dd class="calibre8"><p class="calibre2">A <code class="literal">Buffer</code> is similar to an <code class="literal">Aggregator</code>, except it is
            optimized to act as a “sliding window” across all the tuples in a unique grouping. This
            is useful when the developer needs to efficiently insert missing values in an ordered
            set of tuples (such as a missing date or duration) or create a running average. Usually
              <code class="literal">Aggregator</code> is the operation of choice when working with groups of tuples,
              since many <code class="literal">Aggregator</code>s can be chained together very
            efficiently, but sometimes a <code class="literal">Buffer</code> is the best tool for the job.</p></dd></dl></div><div class="figure"><a id="calibre_link-415" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4733" class="calibre"></a><img alt="Operation types" src="images/000075.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;24-5.&nbsp;Operation types</div></div><p class="calibre2">Operations are bound to pipes when the pipe assembly is created
      (<a class="ulink" href="#calibre_link-416" title="Figure&nbsp;24-6.&nbsp;An assembly of operations">Figure&nbsp;24-6</a>).</p><div class="book"><div class="figure"><a id="calibre_link-416" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4734" class="calibre"></a><img alt="An assembly of operations" src="images/000085.jpg" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;24-6.&nbsp;An assembly of operations</div></div></div><p class="calibre2">The <code class="literal">Each</code> and <code class="literal">Every</code> pipes provide a simple mechanism for
      selecting some or all values out of an input tuple before the values are passed to its child operation.
      And there is a simple mechanism for merging the operation results with the original input
      tuple to create the output tuple. Without going into great detail, this allows for each
      operation to care only about argument tuple values and fields, not the whole set of fields in
      the current input tuple. Subsequently, operations can be reusable across applications in the
      same way that Java methods can be reusable.</p><p class="calibre2">For example, in Java, a method declared as <code class="literal">concatenate</code><code class="literal">(String
      first, String</code> <code class="literal">second)</code> is more abstract than<code class="literal">
      concatenate(Person person)</code>. In the second case, the
      <code class="literal">concatenate()</code> function must “know” about the
      <code class="literal">Person</code> object; in the first case, it is agnostic to where
      the data came from. Cascading operations exhibit this same <a class="calibre" id="calibre_link-1056"></a>quality.</p></div><div class="book" title="Taps, Schemes, and Flows"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4735">Taps, Schemes, and Flows</h2></div></div></div><p class="calibre2">In many of the <a class="calibre" id="calibre_link-1060"></a>previous diagrams, there are references to “sources” and
      “sinks.” In Cascading, all data is read from or written to
      <code class="literal">Tap</code> instances, but is converted to and from tuple
      instances via <code class="literal">Scheme</code> objects:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term"><a id="calibre_link-4736" class="calibre"></a><code class="literal1">Tap</code></span></dt><dd class="calibre8"><p class="calibre2">A <code class="literal">Tap</code> is responsible for the “how” and “where”
            parts of accessing data. For example, is the data on HDFS or the
            local filesystem? In Amazon S3 or over HTTP?</p></dd><dt class="calibre7"><span class="term"><a id="calibre_link-4737" class="calibre"></a><code class="literal1">Scheme</code></span></dt><dd class="calibre8"><p class="calibre2">A <code class="literal">Scheme</code> is responsible for reading raw data
            and converting it to a tuple and/or writing a tuple out into raw
            data, where this “raw” data can be lines of text, Hadoop binary
            sequence files, or some proprietary format.</p></dd></dl></div><p class="calibre2">Note that <code class="literal">Tap</code>s are not part of a pipe assembly, and so they are not a type of
        <code class="literal">Pipe</code>. But they are connected with pipe assemblies when they are made
      cluster executable. When a pipe assembly is connected with the necessary
      number of source and sink <code class="literal">Tap</code>
      instances, we get a <code class="literal">Flow</code>. The <code class="literal">Tap</code>s either emit or capture the field names
      the pipe assembly expects. That is, if a <code class="literal">Tap</code> emits a tuple
      with the field name “line” (by reading data from a file on HDFS), the
      head of the pipe assembly must be expecting a “line” value as well.
      Otherwise, the process that connects the pipe assembly with the <code class="literal">Tap</code>s will immediately fail with an
      error.</p><p class="calibre2">So pipe assemblies are really data process definitions, and are
      not “executable” on their own. They must be connected to source and sink
      <code class="literal">Tap</code> instances before they can run on
      a cluster. This separation between <code class="literal">Tap</code>s and pipe assemblies is part of what makes
      Cascading so powerful.</p><p class="calibre2">If you think of a pipe assembly like a Java class, then a <code class="literal">Flow</code> is like a Java object instance (<a class="ulink" href="#calibre_link-417" title="Figure&nbsp;24-7.&nbsp;A Flow">Figure&nbsp;24-7</a>). That is, the same pipe assembly can be
      “instantiated” many times into new <code class="literal">Flow</code>s, in the same application, without fear of
      any interference between them. This allows pipe assemblies to be created
      and shared like standard Java libraries.</p><div class="book"><div class="figure"><a id="calibre_link-417" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4738" class="calibre"></a><img alt="A Flow" src="images/000003.jpg" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;24-7.&nbsp;A Flow</div></div></div></div><div class="book" title="Cascading in Practice"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4739">Cascading in Practice</h2></div></div></div><p class="calibre2">Now that we know what <a class="calibre" id="calibre_link-1050"></a>Cascading is and have a good idea of how it works, what
      does an application written in Cascading look like? See <a class="ulink" href="#calibre_link-418" title="Example&nbsp;24-1.&nbsp;Word count and sort">Example&nbsp;24-1</a>.</p><div class="example"><a id="calibre_link-418" class="calibre"></a><div class="example-title">Example&nbsp;24-1.&nbsp;Word count and sort</div><div class="book"><pre class="screen"><code class="n">Scheme</code> <code class="n">sourceScheme</code> <code class="o">=</code>
    <code class="k">new</code> <code class="nf">TextLine</code><code class="o">(</code><code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"line"</code><code class="o">));</code> <a class="calibre" href="#calibre_link-419" id="calibre_link-428"><img alt="1" src="images/000010.png" class="calibre29"></a>
<code class="n">Tap</code> <code class="n">source</code> <code class="o">=</code>
    <code class="k">new</code> <code class="nf">Hfs</code><code class="o">(</code><code class="n">sourceScheme</code><code class="o">,</code> <code class="n">inputPath</code><code class="o">);</code> <a class="calibre" href="#calibre_link-420" id="calibre_link-430"><img alt="2" src="images/000019.png" class="calibre29"></a>

<code class="n">Scheme</code> <code class="n">sinkScheme</code> <code class="o">=</code> <code class="k">new</code> <code class="n">TextLine</code><code class="o">();</code> <a class="calibre" href="#calibre_link-421" id="calibre_link-429"><img alt="3" src="images/000027.png" class="calibre29"></a>
<code class="n">Tap</code> <code class="n">sink</code> <code class="o">=</code>
    <code class="k">new</code> <code class="nf">Hfs</code><code class="o">(</code><code class="n">sinkScheme</code><code class="o">,</code> <code class="n">outputPath</code><code class="o">,</code> <code class="n">SinkMode</code><code class="o">.</code><code class="na">REPLACE</code><code class="o">);</code> <a class="calibre" href="#calibre_link-420" id="calibre_link-431"><img alt="4" src="images/000036.png" class="calibre29"></a>

<code class="n">Pipe</code> <code class="n">assembly</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Pipe</code><code class="o">(</code><code class="sb">"wordcount"</code><code class="o">);</code> <a class="calibre" href="#calibre_link-422" id="calibre_link-432"><img alt="5" src="images/000045.png" class="calibre29"></a>

<code class="n">String</code> <code class="n">regexString</code> <code class="o">=</code> <code class="sb">"(?&lt;!\\pL)(?=\\pL)[^ ]*(?&lt;=\\pL)(?!\\pL)"</code><code class="o">;</code>
<code class="n">Function</code> <code class="n">regex</code> <code class="o">=</code> <code class="k">new</code> <code class="n">RegexGenerator</code><code class="o">(</code><code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"word"</code><code class="o">),</code> <code class="n">regexString</code><code class="o">);</code>
<code class="n">assembly</code> <code class="o">=</code>
    <code class="k">new</code> <code class="nf">Each</code><code class="o">(</code><code class="n">assembly</code><code class="o">,</code> <code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"line"</code><code class="o">),</code> <code class="n">regex</code><code class="o">);</code> <a class="calibre" href="#calibre_link-423" id="calibre_link-433"><img alt="6" src="images/000053.png" class="calibre29"></a>

<code class="n">assembly</code> <code class="o">=</code>
    <code class="k">new</code> <code class="nf">GroupBy</code><code class="o">(</code><code class="n">assembly</code><code class="o">,</code> <code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"word"</code><code class="o">));</code> <a class="calibre" href="#calibre_link-424" id="calibre_link-434"><img alt="7" src="images/000062.png" class="calibre29"></a>

<code class="n">Aggregator</code> <code class="n">count</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Count</code><code class="o">(</code><code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"count"</code><code class="o">));</code>
<code class="n">assembly</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Every</code><code class="o">(</code><code class="n">assembly</code><code class="o">,</code> <code class="n">count</code><code class="o">);</code> <a class="calibre" href="#calibre_link-425" id="calibre_link-435"><img alt="8" src="images/000070.png" class="calibre29"></a>

<code class="n">assembly</code> <code class="o">=</code>
    <code class="k">new</code> <code class="nf">GroupBy</code><code class="o">(</code><code class="n">assembly</code><code class="o">,</code> <code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"count"</code><code class="o">),</code> <code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"word"</code><code class="o">));</code> <a class="calibre" href="#calibre_link-426" id="calibre_link-436"><img alt="9" src="images/000078.png" class="calibre29"></a>

<code class="n">FlowConnector</code> <code class="n">flowConnector</code> <code class="o">=</code> <code class="k">new</code> <code class="n">FlowConnector</code><code class="o">();</code>
<code class="n">Flow</code> <code class="n">flow</code> <code class="o">=</code>
    <code class="n">flowConnector</code><code class="o">.</code><code class="na">connect</code><code class="o">(</code><code class="sb">"word-count"</code><code class="o">,</code> <code class="n">source</code><code class="o">,</code> <code class="n">sink</code><code class="o">,</code> <code class="n">assembly</code><code class="o">);</code> <a class="calibre" href="#calibre_link-427" id="calibre_link-437"><img alt="10" src="images/000089.png" class="calibre29"></a>

<code class="n">flow</code><code class="o">.</code><code class="na">complete</code><code class="o">();</code><a class="calibre" href="#calibre_link-427" id="calibre_link-438"><img alt="11" src="images/000006.png" class="calibre29"></a></pre></div></div><div class="book"><dl class="book"><dt class="calibre7"><a id="calibre_link-419" class="calibre"></a><a href="#calibre_link-428" class="calibre"><img alt="1" src="images/000010.png" class="calibre29"></a> </dt><dd class="calibre55"><p class="calibre56">We create a new <code class="literal">Scheme</code> that reads
          simple text files and emits a new <code class="literal">Tuple</code> for
          each line in a field named “line,” as declared by the
          <code class="literal">Fields</code> instance.</p></dd><dt class="calibre7"><a id="calibre_link-421" class="calibre"></a><a href="#calibre_link-429" class="calibre"><img alt="3" src="images/000027.png" class="calibre29"></a> </dt><dd class="calibre55"><p class="calibre56">We create a new <code class="literal">Scheme</code> that writes simple text files and
          expects a <code class="literal">Tuple</code> with any number of fields/values. If there is more
          than one value, they will be tab-delimited in the output file.</p></dd><dt class="calibre7"><a id="calibre_link-420" class="calibre"></a><a href="#calibre_link-430" class="calibre"><img alt="2" src="images/000019.png" class="calibre29"></a> <a href="#calibre_link-431" class="calibre"><img alt="4" src="images/000036.png" class="calibre29"></a> </dt><dd class="calibre55"><p class="calibre56">We create source and sink <code class="literal">Tap</code> instances
          that reference the input file and output directory, respectively.
          The sink <code class="literal">Tap</code> will overwrite any file that may
          already exist.</p></dd><dt class="calibre7"><a id="calibre_link-422" class="calibre"></a><a href="#calibre_link-432" class="calibre"><img alt="5" src="images/000045.png" class="calibre29"></a> </dt><dd class="calibre55"><p class="calibre56">We construct the head of our pipe assembly and name it
          “wordcount.” This name is used to bind the source and sink <code class="literal">Tap</code>s to the assembly. Multiple heads or
          tails would require unique names.</p></dd><dt class="calibre7"><a id="calibre_link-423" class="calibre"></a><a href="#calibre_link-433" class="calibre"><img alt="6" src="images/000053.png" class="calibre29"></a> </dt><dd class="calibre55"><p class="calibre56">We construct an <code class="literal">Each</code> pipe with a
          function that will parse the “line” field into a new
          <code class="literal">Tuple</code> for each word encountered.</p></dd><dt class="calibre7"><a id="calibre_link-424" class="calibre"></a><a href="#calibre_link-434" class="calibre"><img alt="7" src="images/000062.png" class="calibre29"></a> </dt><dd class="calibre55"><p class="calibre56">We construct a <code class="literal">GroupBy</code> pipe
          that will create a new <code class="literal">Tuple</code> grouping for
          each unique value in the field “word.”</p></dd><dt class="calibre7"><a id="calibre_link-425" class="calibre"></a><a href="#calibre_link-435" class="calibre"><img alt="8" src="images/000070.png" class="calibre29"></a> </dt><dd class="calibre55"><p class="calibre56">We construct an <code class="literal">Every</code> pipe with an
          <code class="literal">Aggregator</code> that will count the
          number of <code class="literal">Tuple</code>s in every unique
          word group. The result is stored in a field named “count.”</p></dd><dt class="calibre7"><a id="calibre_link-426" class="calibre"></a><a href="#calibre_link-436" class="calibre"><img alt="9" src="images/000078.png" class="calibre29"></a> </dt><dd class="calibre55"><p class="calibre56">We construct a <code class="literal">GroupBy</code> pipe that will
          create a new <code class="literal">Tuple</code> grouping for each unique
          value in the field “count” and secondary sort each value in the
          field “word.” The result will be a list of “count” and “word” values
          with “count” sorted in increasing order.</p></dd><dt class="calibre7"><a id="calibre_link-427" class="calibre"></a><a href="#calibre_link-437" class="calibre"><img alt="10" src="images/000089.png" class="calibre29"></a> <a href="#calibre_link-438" class="calibre"><img alt="11" src="images/000006.png" class="calibre29"></a> </dt><dd class="calibre55"><p class="calibre56">We connect the pipe assembly to its sources and sinks in a
          <code class="literal">Flow</code>, and then execute the
          <code class="literal">Flow</code> on the cluster.</p></dd></dl></div><p class="calibre2">In the example, we count the words encountered in the input
      document, and we sort the counts in their natural order (ascending). If some words have the same “count” value, these words are sorted in
      their natural order (alphabetical).</p><p class="calibre2">One obvious problem with this example is that some words might
      have uppercase letters in some instances—for example, “the” and “The” when the word comes
      at the beginning of a sentence. We might consider inserting a new
      operation to force all the words to lowercase, but we realize that all future
      applications that need to parse words from documents should have the
      same behavior, so we’ll instead create a reusable pipe called <code class="literal">SubAssembly</code>, just like we
      would by creating a subroutine in a traditional application (see <a class="ulink" href="#calibre_link-439" title="Example&nbsp;24-2.&nbsp;Creating a SubAssembly">Example&nbsp;24-2</a>).</p><div class="example"><a id="calibre_link-439" class="calibre"></a><div class="example-title">Example&nbsp;24-2.&nbsp;Creating a SubAssembly</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="k">class</code> <code class="nc">ParseWordsAssembly</code> <code class="k">extends</code> <code class="n">SubAssembly</code> <a class="calibre" href="#calibre_link-440" id="calibre_link-443"><img alt="1" src="images/000010.png" class="calibre29"></a>
  <code class="o">{</code>
  <code class="k">public</code> <code class="nf">ParseWordsAssembly</code><code class="o">(</code><code class="n">Pipe</code> <code class="n">previous</code><code class="o">)</code>
    <code class="o">{</code>
    <code class="n">String</code> <code class="n">regexString</code> <code class="o">=</code> <code class="sb">"(?&lt;!\\pL)(?=\\pL)[^ ]*(?&lt;=\\pL)(?!\\pL)"</code><code class="o">;</code>
    <code class="n">Function</code> <code class="n">regex</code> <code class="o">=</code> <code class="k">new</code> <code class="n">RegexGenerator</code><code class="o">(</code><code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"word"</code><code class="o">),</code> <code class="n">regexString</code><code class="o">);</code>
    <code class="n">previous</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Each</code><code class="o">(</code><code class="n">previous</code><code class="o">,</code> <code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"line"</code><code class="o">),</code> <code class="n">regex</code><code class="o">);</code>

    <code class="n">String</code> <code class="n">exprString</code> <code class="o">=</code> <code class="sb">"word.toLowerCase()"</code><code class="o">;</code>
    <code class="n">Function</code> <code class="n">expression</code> <code class="o">=</code>
        <code class="k">new</code> <code class="nf">ExpressionFunction</code><code class="o">(</code><code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"word"</code><code class="o">),</code> <code class="n">exprString</code><code class="o">,</code> <code class="n">String</code><code class="o">.</code><code class="na">class</code><code class="o">);</code> <a class="calibre" href="#calibre_link-441" id="calibre_link-444"><img alt="2" src="images/000019.png" class="calibre29"></a>
    <code class="n">previous</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Each</code><code class="o">(</code><code class="n">previous</code><code class="o">,</code> <code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"word"</code><code class="o">),</code> <code class="n">expression</code><code class="o">);</code>

    <code class="n">setTails</code><code class="o">(</code><code class="n">previous</code><code class="o">);</code> <a class="calibre" href="#calibre_link-442" id="calibre_link-445"><img alt="3" src="images/000027.png" class="calibre29"></a>
    <code class="o">}</code>
  <code class="o">}</code></pre></div></div><div class="book"><dl class="book"><dt class="calibre7"><a id="calibre_link-440" class="calibre"></a><a href="#calibre_link-443" class="calibre"><img alt="1" src="images/000010.png" class="calibre29"></a> </dt><dd class="calibre55"><p class="calibre56">We subclass the <code class="literal">SubAssembly</code> class,
          which is itself a kind of <code class="literal">Pipe</code>.</p></dd><dt class="calibre7"><a id="calibre_link-441" class="calibre"></a><a href="#calibre_link-444" class="calibre"><img alt="2" src="images/000019.png" class="calibre29"></a> </dt><dd class="calibre55"><p class="calibre56">We create a Java expression function that will call
          <code class="literal">toLowerCase()</code> on the
          <code class="literal">String</code> value in the field named “word.” We
          must also pass in the Java type the expression expects “word” to
          be—in this case, <code class="literal">String</code>. (<a class="ulink" href="http://www.janino.net/" target="_top">Janino</a> is used under the
          covers.)</p></dd><dt class="calibre7"><a id="calibre_link-442" class="calibre"></a><a href="#calibre_link-445" class="calibre"><img alt="3" src="images/000027.png" class="calibre29"></a> </dt><dd class="calibre55"><p class="calibre56">We tell the <code class="literal">SubAssembly</code> superclass where the tail ends of our
          pipe subassembly are.</p></dd></dl></div><p class="calibre2">First, we create a <code class="literal">SubAssembly</code> pipe to hold
      our “parse words” pipe assembly. Because this is a Java class, it can be
      reused in any other application, as long as there is an incoming field
      named “word” (<a class="ulink" href="#calibre_link-446" title="Example&nbsp;24-3.&nbsp;Extending word count and sort with a SubAssembly">Example&nbsp;24-3</a>). Note that there are ways to
      make this function even more generic, but they are covered in the
        <a class="ulink" href="http://www.cascading.org/documentation/" target="_top">Cascading User Guide</a>.</p><div class="example"><a id="calibre_link-446" class="calibre"></a><div class="example-title">Example&nbsp;24-3.&nbsp;Extending word count and sort with a SubAssembly</div><div class="book"><pre class="screen"><code class="n">Scheme</code> <code class="n">sourceScheme</code> <code class="o">=</code> <code class="k">new</code> <code class="n">TextLine</code><code class="o">(</code><code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"line"</code><code class="o">));</code>
<code class="n">Tap</code> <code class="n">source</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Hfs</code><code class="o">(</code><code class="n">sourceScheme</code><code class="o">,</code> <code class="n">inputPath</code><code class="o">);</code>

<code class="n">Scheme</code> <code class="n">sinkScheme</code> <code class="o">=</code> <code class="k">new</code> <code class="n">TextLine</code><code class="o">(</code><code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"word"</code><code class="o">,</code> <code class="sb">"count"</code><code class="o">));</code>
<code class="n">Tap</code> <code class="n">sink</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Hfs</code><code class="o">(</code><code class="n">sinkScheme</code><code class="o">,</code> <code class="n">outputPath</code><code class="o">,</code> <code class="n">SinkMode</code><code class="o">.</code><code class="na">REPLACE</code><code class="o">);</code>

<code class="n">Pipe</code> <code class="n">assembly</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Pipe</code><code class="o">(</code><code class="sb">"wordcount"</code><code class="o">);</code>

<code class="n">assembly</code> <code class="o">=</code>
    <code class="k">new</code> <code class="nf">ParseWordsAssembly</code><code class="o">(</code><code class="n">assembly</code><code class="o">);</code> <a class="calibre" href="#calibre_link-447" id="calibre_link-448"><img alt="1" src="images/000010.png" class="calibre29"></a>

<code class="n">assembly</code> <code class="o">=</code> <code class="k">new</code> <code class="n">GroupBy</code><code class="o">(</code><code class="n">assembly</code><code class="o">,</code> <code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"word"</code><code class="o">));</code>

<code class="n">Aggregator</code> <code class="n">count</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Count</code><code class="o">(</code><code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"count"</code><code class="o">));</code>
<code class="n">assembly</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Every</code><code class="o">(</code><code class="n">assembly</code><code class="o">,</code> <code class="n">count</code><code class="o">);</code>

<code class="n">assembly</code> <code class="o">=</code> <code class="k">new</code> <code class="n">GroupBy</code><code class="o">(</code><code class="n">assembly</code><code class="o">,</code> <code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"count"</code><code class="o">),</code> <code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"word"</code><code class="o">));</code>

<code class="n">FlowConnector</code> <code class="n">flowConnector</code> <code class="o">=</code> <code class="k">new</code> <code class="n">FlowConnector</code><code class="o">();</code>
<code class="n">Flow</code> <code class="n">flow</code> <code class="o">=</code> <code class="n">flowConnector</code><code class="o">.</code><code class="na">connect</code><code class="o">(</code><code class="sb">"word-count"</code><code class="o">,</code> <code class="n">source</code><code class="o">,</code> <code class="n">sink</code><code class="o">,</code> <code class="n">assembly</code><code class="o">);</code>

<code class="n">flow</code><code class="o">.</code><code class="na">complete</code><code class="o">();</code></pre></div></div><div class="book"><dl class="book"><dt class="calibre7"><a id="calibre_link-447" class="calibre"></a><a href="#calibre_link-448" class="calibre"><img alt="1" src="images/000010.png" class="calibre29"></a> </dt><dd class="calibre55"><p class="calibre56">We replace <code class="literal">Each</code> from the previous
          example with our <code class="literal">ParseWordsAssembly</code>
          pipe.</p></dd></dl></div><p class="calibre2">Finally, we just substitute in our new
      <code class="literal">SubAssembly</code> right where the previous
      <code class="literal">Every</code> and word parser function were used in the
      previous example. This nesting can continue as deep as <a class="calibre" id="calibre_link-1051"></a>necessary.</p></div><div class="book" title="Flexibility"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4740">Flexibility</h2></div></div></div><p class="calibre2">Let’s take a step <a class="calibre" id="calibre_link-1054"></a>back and see what this new model has given us—or better
      yet, what it has taken away.</p><p class="calibre2">You see, we no longer think in terms of MapReduce jobs, or
      <code class="literal">Mapper</code> and <code class="literal">Reducer</code> interface implementations and how to
      bind or link subsequent MapReduce jobs to the ones that precede them.
      During runtime, the Cascading “planner” figures out the optimal way to
      partition the pipe assembly into MapReduce jobs and manages the linkages
      between them (<a class="ulink" href="#calibre_link-449" title="Figure&nbsp;24-8.&nbsp;How a Flow translates to chained MapReduce jobs">Figure&nbsp;24-8</a>).</p><div class="book"><div class="figure"><a id="calibre_link-449" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4741" class="calibre"></a><img alt="How a Flow translates to chained MapReduce jobs" src="images/000015.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;24-8.&nbsp;How a Flow translates to chained MapReduce jobs</div></div></div><p class="calibre2">Because of this, developers can build applications of arbitrary granularity. They can
      start with a small application that just filters a logfile, then iteratively build more
      features into the application as needed.</p><p class="calibre2">Since Cascading is an API and not a syntax like strings of SQL, it is more flexible.
      First off, developers can create domain-specific languages (DSLs) using their favorite
      languages, such as Groovy, JRuby, Jython, Scala, and others (see the <a class="ulink" href="http://www.cascading.org/" target="_top">project site</a> for examples). Second, developers can
      extend various parts of Cascading, such as allowing custom Thrift or JSON objects to be read
      and written to and allowing them to be passed through the tuple stream.</p></div><div class="book" title="Hadoop and Cascading at ShareThis"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4742">Hadoop and Cascading at ShareThis</h2></div></div></div><p class="calibre2"><a class="ulink" href="http://www.sharethis.com" target="_top">ShareThis</a> is a sharing <a class="calibre" id="calibre_link-1057"></a><a class="calibre" id="calibre_link-3387"></a>network that makes it simple to share any online content. With the click of a
      button on a web page or browser plug-in, ShareThis allows users to seamlessly access their
      contacts and networks from anywhere online and share the content via email, IM, Facebook,
      Digg, mobile SMS, and similar services, without ever leaving the current page. Publishers can
      deploy the ShareThis button to tap into the service’s universal sharing capabilities to drive
      traffic, stimulate viral activity, and track the sharing of online content. ShareThis also
      simplifies social media services by reducing clutter on web pages and providing instant
      distribution of content across social networks, affiliate groups, and communities.</p><p class="calibre2">As ShareThis users share pages and information through the online
      widgets, a continuous stream of events enter the ShareThis network.
      These events are first filtered and processed, and then handed to
      various backend systems, including AsterData, Hypertable, and Katta.</p><p class="calibre2">The volume of these events can be huge; too large to process with traditional systems.
      This data can also be very “dirty” thanks to “injection attacks” from rogue systems, browser
      bugs, or faulty widgets. For this reason, the developers at ShareThis chose to deploy Hadoop
      as the preprocessing and orchestration frontend to their backend systems. They also chose to
      use Amazon Web Services to host their servers on the Elastic Computing Cloud (EC2) and provide
      long-term storage on the Simple Storage Service (S3), with an eye toward leveraging Elastic
      MapReduce (EMR).</p><p class="calibre2">In this overview, we will focus on the “log processing pipeline” (<a class="ulink" href="#calibre_link-450" title="Figure&nbsp;24-9.&nbsp;The ShareThis log processing pipeline">Figure&nbsp;24-9</a>). This pipeline simply takes data stored in an S3
      bucket, processes it (as described shortly), and stores the results back into another bucket.
      The Simple Queue Service (SQS) is used to coordinate the events that mark the start and
      completion of data processing runs. Downstream, other processes pull data to load into
      AsterData, pull URL lists from Hypertable to source a
      web crawl, or pull crawled page data to create Lucene indexes for use by Katta. Note that
      Hadoop is central to the ShareThis architecture. It is used to coordinate the processing and
      movement of data between architectural components.</p><div class="figure"><a id="calibre_link-450" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4743" class="calibre"></a><img alt="The ShareThis log processing pipeline" src="images/000023.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;24-9.&nbsp;The ShareThis log processing pipeline</div></div><p class="calibre2">With Hadoop as the frontend, all the event logs can be parsed, filtered, cleaned, and
      organized by a set of rules before ever being loaded into the AsterData cluster or used by any
      other component. AsterData is a clustered data warehouse that can support large datasets and
      that allows for complex ad hoc queries using a standard SQL syntax. ShareThis chose to clean
      and prepare the incoming datasets on the Hadoop cluster and then to load that data into the
      AsterData cluster for ad hoc analysis and reporting. Though that process would have been
      possible with AsterData, it made a lot of sense to use Hadoop as the first stage in the
      processing pipeline to offset load on the main data warehouse.</p><p class="calibre2">Cascading was chosen as the primary data processing API to simplify the development
      process, codify how data is coordinated between architectural components, and provide the
      developer-facing interface to those components. This represents a departure from more
      “traditional” Hadoop use cases, which essentially just query stored data. Cascading and Hadoop
      together provide a better and simpler structure for the complete solution, end to end, and
      thus provide more value to the users.</p><p class="calibre2">For the developers, Cascading made it easy to start with a simple unit test (created by
      subclassing <code class="literal">cascading.ClusterTestCase</code>) that did simple text parsing and
      then to layer in more processing rules while keeping the
      application logically organized for maintenance. Cascading aided this organization in a couple
      of ways. First, standalone operations (<code class="literal">Function</code>s, <code class="literal">Filter</code>s, etc.) could be written and tested independently. Second,
      the application was segmented into stages: one for parsing, one for rules, and a final stage
      for binning/collating the data, all via the <code class="literal">SubAssembly</code> base class
      described earlier.</p><p class="calibre2">The data coming from the ShareThis loggers looks a lot like Apache
      logs, with date/timestamps, share URLs, referrer URLs, and a bit of
      metadata. To use the data for analysis downstream, the URLs needed to be
      unpacked (parsing query-string data, domain names, etc.). So, a top-level
      <span class="calibre"><code class="literal">SubAssembly</code></span> was
      created to encapsulate the parsing, and child subassemblies were nested
      inside to handle specific fields if they were sufficiently complex to
      parse.</p><p class="calibre2">The same was done for applying rules. As every <code class="literal">Tuple</code> passed through the rules <code class="literal">SubAssembly</code>, it was marked as “bad”
      if any of the rules were triggered. Along with the “bad” tag, a
      description of why the record was bad was added to the <code class="literal">Tuple</code> for later review.</p><p class="calibre2">Finally, a splitter <code class="literal">SubAssembly</code>
      was created to do two things. First, it allowed for the tuple stream to
      split into two: one stream for “good” data and one for “bad” data.
      Second, the splitter binned the data into intervals, such as every hour.
      To do this, only two operations were necessary: the first to create the
      interval from the <span class="calibre">timestamp</span> value
      already present in the stream, and the second to use the <span class="calibre">interval</span> and <span class="calibre">good/bad</span> metadata to create a directory path
      (for example, <span class="calibre"><em class="calibre10">05/good/</em></span>, where “05” is 5 a.m. and
        “good” means the <code class="literal">Tuple</code> passed all the rules). This path would then be
      used by the Cascading <code class="literal">TemplateTap</code>, a
      special <code class="literal">Tap</code> that can dynamically
      output tuple streams to different locations based on values in the
      <code class="literal">Tuple</code>. In this case, the <code class="literal">TemplateTap</code> used the “path” value to create
      the final output path.</p><p class="calibre2">The developers also created a fourth <code class="literal">SubAssembly</code>—this one to apply Cascading
        <code class="literal">Assertions</code> during unit testing. These assertions double-checked that
      rules and parsing subassemblies did their job.</p><p class="calibre2">In the unit test in <a class="ulink" href="#calibre_link-451" title="Example&nbsp;24-4.&nbsp;Unit testing a Flow">Example&nbsp;24-4</a>, we see the
      splitter isn’t being tested, but it is added in another integration test
      not shown.</p><div class="example"><a id="calibre_link-451" class="calibre"></a><div class="example-title">Example&nbsp;24-4.&nbsp;Unit testing a Flow</div><div class="book"><pre class="screen"><code class="k">public</code> <code class="kt">void</code> <code class="nf">testLogParsing</code><code class="o">()</code> <code class="k">throws</code> <code class="n">IOException</code>
  <code class="o">{</code>
  <code class="n">Hfs</code> <code class="n">source</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Hfs</code><code class="o">(</code><code class="k">new</code> <code class="n">TextLine</code><code class="o">(</code><code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"line"</code><code class="o">)),</code> <code class="n">sampleData</code><code class="o">);</code>
  <code class="n">Hfs</code> <code class="n">sink</code> <code class="o">=</code>
      <code class="k">new</code> <code class="nf">Hfs</code><code class="o">(</code><code class="k">new</code> <code class="n">TextLine</code><code class="o">(),</code> <code class="n">outputPath</code> <code class="o">+</code> <code class="sb">"/parser"</code><code class="o">,</code> <code class="n">SinkMode</code><code class="o">.</code><code class="na">REPLACE</code><code class="o">);</code>

  <code class="n">Pipe</code> <code class="n">pipe</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Pipe</code><code class="o">(</code><code class="sb">"parser"</code><code class="o">);</code>

  <code class="c2">// split "line" on tabs</code>
  <code class="n">pipe</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Each</code><code class="o">(</code><code class="n">pipe</code><code class="o">,</code> <code class="k">new</code> <code class="n">Fields</code><code class="o">(</code><code class="sb">"line"</code><code class="o">),</code> <code class="k">new</code> <code class="n">RegexSplitter</code><code class="o">(</code><code class="sb">"\t"</code><code class="o">));</code>

  <code class="n">pipe</code> <code class="o">=</code> <code class="k">new</code> <code class="n">LogParser</code><code class="o">(</code><code class="n">pipe</code><code class="o">);</code>

  <code class="n">pipe</code> <code class="o">=</code> <code class="k">new</code> <code class="n">LogRules</code><code class="o">(</code><code class="n">pipe</code><code class="o">);</code>
  <code class="c2">// testing only assertions</code>
  <code class="n">pipe</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ParserAssertions</code><code class="o">(</code><code class="n">pipe</code><code class="o">);</code>

  <code class="n">Flow</code> <code class="n">flow</code> <code class="o">=</code> <code class="k">new</code> <code class="n">FlowConnector</code><code class="o">().</code><code class="na">connect</code><code class="o">(</code><code class="n">source</code><code class="o">,</code> <code class="n">sink</code><code class="o">,</code> <code class="n">pipe</code><code class="o">);</code>

  <code class="n">flow</code><code class="o">.</code><code class="na">complete</code><code class="o">();</code> <code class="c2">// run the test flow</code>

  <code class="c2">// Verify there are 98 tuples and 2 fields, and matches the regex pattern</code>
  <code class="c2">// For TextLine schemes the tuples are { "offset", "line" }</code>
  <code class="n">validateLength</code><code class="o">(</code><code class="n">flow</code><code class="o">,</code> <code class="mi">98</code><code class="o">,</code> <code class="mi">2</code><code class="o">,</code> <code class="n">Pattern</code><code class="o">.</code><code class="na">compile</code><code class="o">(</code><code class="sb">"^[0-9]+(\\t[^\\t]*){19}$"</code><code class="o">));</code>
  <code class="o">}</code></pre></div></div><p class="calibre2">For integration and deployment, many of the features built into
      Cascading allowed for easier integration with external systems and for
      greater process tolerance.</p><p class="calibre2">In production, all the subassemblies are joined and planned into a <code class="literal">Flow</code>, but instead of just source and sink <code class="literal">Tap</code>s, trap <code class="literal">Tap</code>s were planned in (<a class="ulink" href="#calibre_link-452" title="Figure&nbsp;24-10.&nbsp;The ShareThis log processing flow">Figure&nbsp;24-10</a>). Normally, when an operation throws an
      exception from a remote mapper or reducer task, the <code class="literal">Flow</code>
      will fail and kill all its managed MapReduce jobs. When a <code class="literal">Flow</code> has traps, any exceptions are caught and the data causing the exception is
      saved to the <code class="literal">Tap</code> associated with the current trap. Then the
      next <code class="literal">Tuple</code> is processed without stopping the <code class="literal">Flow</code>. Sometimes you want your <code class="literal">Flow</code>s to fail on errors, but in this case, the ShareThis developers knew they
      could go back and look at the “failed” data and update their unit tests while the production
      system kept running. Losing a few hours of processing time was worse than losing a couple of
      bad records.</p><div class="book"><div class="figure"><a id="calibre_link-452" class="calibre"></a><div class="book"><div class="book"><a id="calibre_link-4744" class="calibre"></a><img alt="The ShareThis log processing flow" src="images/000031.png" class="calibre29"></div></div><div class="figure-title">Figure&nbsp;24-10.&nbsp;The ShareThis log processing flow</div></div></div><p class="calibre2">Using Cascading’s event listeners, Amazon SQS could be integrated.
      When a <code class="literal">Flow</code> finishes, a message is
      sent to notify other systems that there is data ready to be picked up
      from Amazon S3. On failure, a different message is sent, alerting other
      processes.</p><p class="calibre2">The remaining downstream processes pick up where the log
      processing pipeline leaves off on different independent clusters. The
      log processing pipeline today runs once a day; there is no need to
      keep a 100-node cluster sitting around for the 23 hours it has nothing
      to do, so it is decommissioned and recommissioned 24 hours later.</p><p class="calibre2">In the future, it would be trivial to increase this interval on
      smaller clusters to every 6 hours, or 1 hour, as the business demands.
      Independently, other clusters are booting and shutting down at different
      intervals based on the needs of the business units responsible for those
      components. For example, the web crawler component (using Bixo, a
      Cascading-based web-crawler toolkit developed by EMI and ShareThis) may
      run continuously on a small cluster with a companion Hypertable cluster.
      This on-demand model works very well with Hadoop, where each cluster can
      be tuned for the kind of workload it is expected to <a class="calibre" id="calibre_link-1058"></a><a class="calibre" id="calibre_link-3388"></a>handle.</p></div><div class="book" title="Summary"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4745">Summary</h2></div></div></div><p class="calibre2">Hadoop is a very <a class="calibre" id="calibre_link-1059"></a>powerful platform for processing and coordinating the
      movement of data across various architectural components. Its only
      drawback is that the primary computing model is MapReduce.</p><p class="calibre2">Cascading aims to help developers build powerful applications quickly and simply,
      through a well-reasoned API, without needing to think in MapReduce and while leaving the heavy
      lifting of data distribution, replication, distributed process management, and liveness to
      Hadoop.</p><p class="calibre2">Read more about Cascading, join the online community, and download
      sample applications by <a class="calibre" id="calibre_link-1067"></a>visiting the <a class="ulink" href="http://www.cascading.org/" target="_top">project website</a>.</p></div></section></div>

<div class="calibre1" id="calibre_link-28"><section type="appendix" id="calibre_link-4746" title="Appendix&nbsp;A.&nbsp;Installing Apache Hadoop"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Appendix&nbsp;A.&nbsp;Installing Apache Hadoop</h2></div></div></div><p class="calibre2">It’s easy to install <a class="calibre" id="calibre_link-1837"></a>Hadoop on a single machine to try it out. (For installation on
  a cluster, refer to <a class="ulink" href="#calibre_link-1" title="Chapter&nbsp;10.&nbsp;Setting Up a Hadoop Cluster">Chapter&nbsp;10</a>.)</p><p class="calibre2">In this appendix, we cover how to install Hadoop Common, HDFS,
  MapReduce, and YARN using a binary tarball release from the Apache Software
  Foundation. Instructions for installing the other projects covered in this
  book are included at the start of the relevant chapters.</p><div class="note" title="Tip"><h3 class="title3">Tip</h3><p class="calibre2">Another option is to use a virtual machine (such as Cloudera’s QuickStart VM) that comes
      with all the Hadoop services preinstalled and configured.</p></div><p class="calibre2">The instructions that follow are suitable for Unix-based systems,
  including Mac OS X (which is not a production platform, but is fine for
  development).</p><div class="book" title="Prerequisites"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4747">Prerequisites</h2></div></div></div><p class="calibre2">Make sure you have a suitable version of Java installed. You can
    check the <a class="ulink" href="http://wiki.apache.org/hadoop/HadoopJavaVersions" target="_top">Hadoop wiki</a>
    to find which version you need. The following command confirms that Java
    was installed correctly:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">java -version</code></strong>
java version "1.7.0_25"
Java(TM) SE Runtime Environment (build 1.7.0_25-b15)
Java HotSpot(TM) 64-Bit Server VM (build 23.25-b01, mixed mode)</pre></div><div class="book" title="Installation"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4748">Installation</h2></div></div></div><p class="calibre2">Start by deciding which user you’d like to run Hadoop as. For trying
    out Hadoop or developing Hadoop programs, you can run Hadoop on a single
    machine using your own user account.</p><p class="calibre2">Download a stable release, which is packaged as a gzipped tar file,
    from the <a class="ulink" href="http://hadoop.apache.org/common/releases.html" target="_top">Apache
    Hadoop releases page</a>, and unpack it somewhere on your
    filesystem:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">tar xzf hadoop-<em class="replaceable1"><code class="calibre46">x.y.z</code></em>.tar.gz</code></strong></pre><p class="calibre2">Before you can run Hadoop, you need to tell it where Java is located
    on your system. If you have the <code class="literal">JAVA_HOME</code> environment variable set to point to a
    suitable Java installation, that will be used, and you don’t have to
    configure anything further. (It is often set in a shell startup file, such
    as <em class="calibre10">~/.bash_profile</em> or <em class="calibre10">~/.bashrc</em>.) Otherwise, you can set the Java
    installation that Hadoop uses by editing <em class="calibre10">conf/hadoop-env.sh</em> and specifying the <code class="literal">JAVA_HOME</code> variable. <a class="calibre" id="calibre_link-2201"></a>For example, on my Mac, I changed the line to read:</p><pre class="screen1">export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_25.jdk/Contents/Home</pre><p class="calibre2">to point to the installed version of Java.</p><p class="calibre2">It’s very convenient to create an environment variable that points
    to the Hadoop installation<a class="calibre" id="calibre_link-1873"></a> directory (<code class="literal">HADOOP_HOME</code>,
    by convention) and to put the Hadoop binary directories on your
    command-line path. For example:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">export HADOOP_HOME=~/sw/hadoop-<em class="replaceable1"><code class="calibre46">x.y.z</code></em></code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</code></strong></pre><p class="calibre2">Note that the <em class="calibre10">sbin</em> directory contains the scripts for
      running Hadoop daemons, so it should be included if you plan to run the daemons on your local
      machine.</p><p class="calibre2">Check that Hadoop runs by typing:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop version</code></strong>
Hadoop 2.5.1
Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 2e18d179e4a8065
b6a9f29cf2de9451891265cce
Compiled by jenkins on 2014-09-05T23:11Z
Compiled with protoc 2.5.0
From source with checksum 6424fcab95bfff8337780a181ad7c78
This command was run using /Users/tom/sw/hadoop-2.5.1/share/hadoop/common/hadoop
-common-2.5.1.jar</pre></div><div class="book" title="Configuration"><div class="titlepage1"><div class="book"><div class="book"><h2 class="title2" id="calibre_link-4749">Configuration</h2></div></div></div><p class="calibre2">Each component in Hadoop is configured using an XML file. Common
    properties go in <em class="calibre10">core-site.xml</em>, and
    properties pertaining to HDFS, MapReduce, and YARN go into the
    appropriately named file: <em class="calibre10">hdfs-site.xml</em>, <em class="calibre10">mapred-site.xml</em>, and <em class="calibre10">yarn-site.xml</em>. These files are all located in
    the <em class="calibre10">etc/hadoop</em> subdirectory.</p><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">You can see the default settings for all the properties that are
      governed by these configuration files by looking in the <em class="calibre10">share/doc</em> directory hierarchy of your Hadoop
      installation for files called <em class="calibre10">core-default.xml</em>, <em class="calibre10">hdfs-default.xml</em>, <em class="calibre10">mapred-default.xml</em>, and <em class="calibre10">yarn-default.xml</em>.</p></div><p class="calibre2">Hadoop can be run in one of three modes:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Standalone (or local) mode</span></dt><dd class="calibre8"><p class="calibre2">There are no daemons running and everything runs in a single
          JVM. Standalone mode is suitable for running MapReduce programs
          during development, since it is easy to test and debug them.</p></dd><dt class="calibre7"><span class="term">Pseudodistributed mode</span></dt><dd class="calibre8"><p class="calibre2">The Hadoop daemons run on the local machine, thus simulating a
          cluster on a small scale.</p></dd><dt class="calibre7"><span class="term">Fully distributed mode</span></dt><dd class="calibre8"><p class="calibre2">The Hadoop daemons run on a cluster of machines. This setup is
          described in <a class="ulink" href="#calibre_link-1" title="Chapter&nbsp;10.&nbsp;Setting Up a Hadoop Cluster">Chapter&nbsp;10</a>.</p></dd></dl></div><p class="calibre2">To run Hadoop in a particular mode, you need to do two things: set
    the appropriate properties, and start the Hadoop daemons. <a class="ulink" href="#calibre_link-461" title="Table&nbsp;A-1.&nbsp;Key configuration properties for different modes">Table&nbsp;A-1</a> shows the minimal set of properties to
    configure each mode. In standalone mode, the local filesystem and the
    local MapReduce job runner are used. In the <a class="calibre" id="calibre_link-1736"></a><a class="calibre" id="calibre_link-1484"></a><a class="calibre" id="calibre_link-2534"></a><a class="calibre" id="calibre_link-3887"></a><a class="calibre" id="calibre_link-3859"></a>distributed modes, the HDFS and YARN daemons are started,
    and MapReduce is configured to use YARN.</p><div class="table"><a id="calibre_link-461" class="calibre"></a><div class="table-title">Table&nbsp;A-1.&nbsp;Key configuration properties for different modes</div><div class="book"><table class="calibre16"><colgroup class="calibre17"><col class="c9"><col class="newcol1"><col class="calibre36"><col class="c10"><col class="c11"></colgroup><thead class="calibre18"><tr class="calibre19"><td class="calibre20">Component</td><td class="calibre20">Property</td><td class="calibre20">Standalone</td><td class="calibre20">Pseudodistributed</td><td class="calibre21">Fully distributed</td></tr></thead><tbody class="calibre22"><tr class="calibre19"><td class="calibre23">Common</td><td class="calibre23"><code class="uri">fs.defaultFS</code></td><td class="calibre23"><code class="uri">file:///</code>
            (default)</td><td class="calibre23"><code class="uri">hdfs://localhost/</code></td><td class="calibre25"><code class="uri">hdfs://<em class="replaceable"><code class="calibre44">namenode</code></em>/</code></td></tr><tr class="calibre26"><td class="calibre23">HDFS</td><td class="calibre23"><code class="uri">dfs.replication</code></td><td class="calibre23">N/A</td><td class="calibre23"><code class="uri">1</code></td><td class="calibre25"><code class="uri">3</code> (default)</td></tr><tr class="calibre19"><td class="calibre23">MapReduce</td><td class="calibre23"><code class="uri">mapreduce.framework.name</code></td><td class="calibre23"><code class="uri">local</code> (default)</td><td class="calibre23"><code class="uri">yarn</code></td><td class="calibre25"><code class="uri">yarn</code></td></tr><tr class="calibre26"><td rowspan="2" class="calibre27">YARN</td><td class="calibre23"><code class="uri">yarn.resourcemanager.hostname</code></td><td class="calibre23">N/A</td><td class="calibre23"><code class="uri">localhost</code></td><td class="calibre25"><code class="uri"><em class="replaceable"><code class="calibre44">resourcemanager</code></em></code></td></tr><tr class="calibre19"><td class="calibre27"><code class="uri">yarn.nodemanager.aux-services</code></td><td class="calibre27">N/A</td><td class="calibre27"><code class="uri">mapreduce_shuffle</code></td><td class="calibre28"><code class="uri">mapreduce_shuffle</code></td></tr></tbody></table></div></div><p class="calibre2">You can read more about configuration in <a class="ulink" href="#calibre_link-30" title="Hadoop Configuration">Hadoop Configuration</a>.</p><div class="book" title="Standalone Mode"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4750">Standalone Mode</h3></div></div></div><p class="calibre2">In standalone mode, <a class="calibre" id="calibre_link-3518"></a><a class="calibre" id="calibre_link-2343"></a>there is no further action to take, since the default
      properties are set for standalone mode and there are no daemons to
      run.</p></div><div class="book" title="Pseudodistributed Mode"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4751">Pseudodistributed Mode</h3></div></div></div><p class="calibre2">In pseudodistributed mode, the <a class="calibre" id="calibre_link-3078"></a>configuration files should be created with the following
      contents and placed in the <em class="calibre10">etc/hadoop</em> directory. Alternatively, you can
      copy the <em class="calibre10">etc/hadoop</em> directory to
      another location, and then place the <em class="calibre10">*-site.xml</em> configuration files there. The
      advantage of this approach is that it separates configuration settings
      from the installation files. If you do this, you need to set the
      <code class="literal">HADOOP_CONF_DIR</code> environment variable
      to the alternative location, or make sure you start the daemons with the
        <code class="literal">--config</code> option:</p><a id="calibre_link-4752" class="calibre"></a><pre class="screen1"><code class="cp">&lt;?xml version="1.0"?&gt;</code>
<code class="c1">&lt;!-- core-site.xml --&gt;</code>
<code class="nt">&lt;configuration&gt;</code>
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>fs.defaultFS<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>hdfs://localhost/<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
<code class="nt">&lt;/configuration&gt;</code></pre><a id="calibre_link-4753" class="calibre"></a><pre class="screen1"><code class="cp">&lt;?xml version="1.0"?&gt;</code>
<code class="c1">&lt;!-- hdfs-site.xml --&gt;</code>
<code class="nt">&lt;configuration&gt;</code>
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>dfs.replication<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>1<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
<code class="nt">&lt;/configuration&gt;</code></pre><pre class="screen1"><code class="cp">&lt;?xml version="1.0"?&gt;</code>
<code class="c1">&lt;!-- mapred-site.xml --&gt;</code>
<code class="nt">&lt;configuration&gt;</code>
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>mapreduce.framework.name<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>yarn<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
<code class="nt">&lt;/configuration&gt;</code></pre><a id="calibre_link-4754" class="calibre"></a><pre class="screen1"><code class="cp">&lt;?xml version="1.0"?&gt;</code>
<code class="c1">&lt;!-- yarn-site.xml --&gt;</code>
<code class="nt">&lt;configuration&gt;</code>
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>yarn.resourcemanager.hostname<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>localhost<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
  <code class="nt">&lt;property&gt;</code>
    <code class="nt">&lt;name&gt;</code>yarn.nodemanager.aux-services<code class="nt">&lt;/name&gt;</code>
    <code class="nt">&lt;value&gt;</code>mapreduce_shuffle<code class="nt">&lt;/value&gt;</code>
  <code class="nt">&lt;/property&gt;</code>
<code class="nt">&lt;/configuration&gt;</code></pre><div class="book" title="Configuring SSH"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4755">Configuring SSH</h4></div></div></div><p class="calibre2">In pseudodistributed mode, we have to start <a class="calibre" id="calibre_link-3514"></a>daemons, and to do that using the supplied scripts we need to have SSH
          installed. Hadoop doesn’t actually distinguish between pseudodistributed and fully
          distributed modes; it merely starts daemons on the set of hosts in the cluster (defined by
          the <em class="calibre10">slaves</em> file) by SSHing to each host and starting
          a daemon process. Pseudodistributed mode is just a special case of fully distributed mode
          in which the (single) host is localhost, so we need to make sure that we can SSH to
          localhost and log in without having to enter a password.</p><p class="calibre2">First, make sure that SSH is installed and a server is running.
        On Ubuntu, for example, this is achieved with:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">sudo apt-get install ssh</code></strong></pre><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">On Mac OS X, make sure Remote Login (under System
          Preferences<span class="calibre">→</span>Sharing) is enabled for
          the current user (or all users).</p></div><p class="calibre2">Then, to enable passwordless login, generate a new SSH key with an empty
          passphrase:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa</code></strong>
<strong class="userinput"><code class="calibre9"><code class="calibre9">% </code>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</code></strong></pre><p class="calibre2">You may also need to run <em class="calibre10">ssh-add</em> if you are running <em class="calibre10">ssh-agent</em>.</p><p class="calibre2">Test that you can connect with:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">ssh localhost</code></strong></pre><p class="calibre2">If successful, you should not have to type in a password.</p></div><div class="book" title="Formatting the HDFS filesystem"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4756">Formatting the HDFS filesystem</h4></div></div></div><p class="calibre2">Before HDFS can be used <a class="calibre" id="calibre_link-1944"></a>for the first time, the filesystem must be formatted. This is done by running
          the following command:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hdfs namenode -format</code></strong></pre></div><div class="book" title="Starting and stopping the daemons"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4757">Starting and <a class="calibre" id="calibre_link-1331"></a>stopping the daemons</h4></div></div></div><p class="calibre2">To start the HDFS, YARN, and MapReduce daemons, type:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">start-dfs.sh</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">start-yarn.sh</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">mr-jobhistory-daemon.sh start historyserver</code></strong></pre><div class="note" title="Note"><h3 class="title3">Note</h3><p class="calibre2">If you have placed configuration files outside the default
          <em class="calibre10">conf</em> directory, either export
          the <code class="literal">HADOOP_CONF_DIR</code> environment
          variable before running the scripts, or start the daemons with the
          <code class="literal">--config</code> option, which takes an
          absolute path to the configuration directory:</p><pre class="screen2"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">start-dfs.sh --config <em class="replaceable1"><code class="calibre46">path-to-config-directory</code></em></code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">start-yarn.sh --config <em class="replaceable1"><code class="calibre46">path-to-config-directory</code></em></code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">mr-jobhistory-daemon.sh --config <em class="replaceable1"><code class="calibre46">path-to-config-directory</code></em> 
  start historyserver</code></strong></pre></div><p class="calibre2">The following daemons will be started on your local machine: a namenode, a secondary
          namenode, a datanode (HDFS), a resource manager, a node manager (YARN), and a history
          server (MapReduce). You can check whether the daemons started successfully by looking at the logfiles in the <em class="calibre10">logs</em> directory (in the Hadoop installation directory) or
            by looking at the web UIs, at <span class="calibre">http://localhost:50070/</span> for the namenode, <span class="calibre">http://localhost:8088/</span> for the resource manager, and <span class="calibre">http://localhost:19888/</span> for the history server. You can also
          use Java’s <code class="literal">jps</code> command to see whether the processes are
          running.</p><p class="calibre2">Stopping the daemons is done as follows:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">mr-jobhistory-daemon.sh stop historyserver</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">stop-yarn.sh</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">stop-dfs.sh</code></strong></pre></div><div class="book" title="Creating a user directory"><div class="titlepage1"><div class="book"><div class="book"><h4 class="title7" id="calibre_link-4758">Creating a user directory</h4></div></div></div><p class="calibre2">Create a home directory <a class="calibre" id="calibre_link-1502"></a>for yourself by running the <a class="calibre" id="calibre_link-3079"></a>following:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">hadoop fs -mkdir -p /user/$USER</code></strong></pre></div></div><div class="book" title="Fully Distributed Mode"><div class="titlepage1"><div class="book"><div class="book"><h3 class="author1" id="calibre_link-4759">Fully Distributed Mode</h3></div></div></div><p class="calibre2">Setting up a cluster of <a class="calibre" id="calibre_link-1761"></a>machines brings many additional considerations, so this
      mode is <a class="calibre" id="calibre_link-1838"></a>covered in <a class="ulink" href="#calibre_link-1" title="Chapter&nbsp;10.&nbsp;Setting Up a Hadoop Cluster">Chapter&nbsp;10</a>.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-566"><section type="appendix" id="calibre_link-4760" title="Appendix&nbsp;B.&nbsp;Cloudera’s Distribution Including Apache Hadoop"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Appendix&nbsp;B.&nbsp;Cloudera’s Distribution Including Apache
  Hadoop</h2></div></div></div><p class="calibre2">Cloudera’s Distribution Including Apache Hadoop (hereafter <em class="calibre10">CDH</em>) is an
    integrated Apache Hadoop–based stack containing all the components needed for <a class="calibre" id="calibre_link-1072"></a><a class="calibre" id="calibre_link-1102"></a>production, tested and packaged to work together. Cloudera makes the distribution
    available in a number of different formats: Linux packages, virtual machine images, tarballs,
    and tools for running CDH in the cloud. CDH is free, released under the Apache 2.0 license, and
    available at <a class="ulink" href="http://www.cloudera.com/cdh" target="_top">http://www.cloudera.com/cdh</a>.</p><p class="calibre2">As of CDH 5, the following components are included, many of which are
  covered elsewhere in this book:</p><div class="book"><dl class="book"><dt class="calibre7"><span class="term">Apache Avro</span></dt><dd class="calibre8"><p class="calibre2">A cross-language data serialization library; includes rich data
        structures, a fast/compact binary format, and RPC</p></dd><dt class="calibre7"><span class="term">Apache Crunch</span></dt><dd class="calibre8"><p class="calibre2">A high-level Java API for writing data processing pipelines that
        can run on MapReduce or Spark</p></dd><dt class="calibre7"><span class="term">Apache DataFu (incubating)</span></dt><dd class="calibre8"><p class="calibre2">A library of useful statistical UDFs for doing large-scale analyses</p></dd><dt class="calibre7"><span class="term">Apache Flume</span></dt><dd class="calibre8"><p class="calibre2">Highly reliable, configurable streaming data collection</p></dd><dt class="calibre7"><span class="term">Apache Hadoop</span></dt><dd class="calibre8"><p class="calibre2">Highly scalable data storage (HDFS), resource management (YARN),
        and processing (MapReduce)</p></dd><dt class="calibre7"><span class="term">Apache HBase</span></dt><dd class="calibre8"><p class="calibre2">Column-oriented real-time database for random read/write access</p></dd><dt class="calibre7"><span class="term">Apache Hive</span></dt><dd class="calibre8"><p class="calibre2">SQL-like queries and tables for large datasets</p></dd><dt class="calibre7"><span class="term">Hue</span></dt><dd class="calibre8"><p class="calibre2">Web UI to make it easy to work with Hadoop data</p></dd><dt class="calibre7"><span class="term">Cloudera Impala</span></dt><dd class="calibre8"><p class="calibre2">Interactive, low-latency SQL queries on HDFS or HBase</p></dd><dt class="calibre7"><span class="term">Kite SDK</span></dt><dd class="calibre8"><p class="calibre2">APIs, examples, and docs for building apps on top of
        Hadoop</p></dd><dt class="calibre7"><span class="term">Apache Mahout</span></dt><dd class="calibre8"><p class="calibre2">Scalable machine-learning and data-mining algorithms</p></dd><dt class="calibre7"><span class="term">Apache Oozie</span></dt><dd class="calibre8"><p class="calibre2">Workflow scheduler for interdependent Hadoop jobs</p></dd><dt class="calibre7"><span class="term">Apache Parquet (incubating)</span></dt><dd class="calibre8"><p class="calibre2">An efficient columnar storage format for nested data</p></dd><dt class="calibre7"><span class="term">Apache Pig</span></dt><dd class="calibre8"><p class="calibre2">Data flow language for exploring large datasets</p></dd><dt class="calibre7"><span class="term">Cloudera Search</span></dt><dd class="calibre8"><p class="calibre2">Free-text, Google-style search of Hadoop data</p></dd><dt class="calibre7"><span class="term">Apache Sentry (incubating)</span></dt><dd class="calibre8"><p class="calibre2">Granular, role-based access control for Hadoop users</p></dd><dt class="calibre7"><span class="term">Apache Spark</span></dt><dd class="calibre8"><p class="calibre2">A cluster computing framework for large-scale in-memory data
        processing in Scala, Java, and Python</p></dd><dt class="calibre7"><span class="term">Apache Sqoop</span></dt><dd class="calibre8"><p class="calibre2">Efficient transfer of data between structured data stores (like
        relational databases) and Hadoop</p></dd><dt class="calibre7"><span class="term">Apache ZooKeeper</span></dt><dd class="calibre8"><p class="calibre2">Highly available coordination service for distributed
        applications</p></dd></dl></div><p class="calibre2">Cloudera also provides <em class="calibre10">Cloudera Manager</em> for
  deploying and operating Hadoop clusters running CDH.</p><p class="calibre2">To download CDH and Cloudera Manager, <a class="calibre" id="calibre_link-1073"></a><a class="calibre" id="calibre_link-1103"></a>visit <a class="ulink" href="http://www.cloudera.com/downloads" target="_top">http://www.cloudera.com/downloads</a>.</p></section></div>

<div class="calibre1" id="calibre_link-464"><section type="appendix" id="calibre_link-4761" title="Appendix&nbsp;C.&nbsp;Preparing the NCDC Weather Data"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Appendix&nbsp;C.&nbsp;Preparing the NCDC Weather Data</h2></div></div></div><p class="calibre2">This appendix gives a <a class="calibre" id="calibre_link-2779"></a><a class="calibre" id="calibre_link-2773"></a>runthrough of the steps taken to prepare the raw weather datafiles so they are in a
    form that is amenable to analysis using Hadoop. If you want to get a copy of the data to process
    using Hadoop, you can do so by following the instructions given at the <a class="ulink" href="http://www.hadoopbook.com/" target="_top">website that accompanies
      this book</a>. The rest of this appendix explains how the raw weather datafiles were processed.</p><p class="calibre2">The raw data is provided as a collection of tar files, compressed with bzip2. Each year’s
    worth of readings comes in a separate file. Here’s a partial directory listing of the
    files:</p><pre class="screen1">1901.tar.bz2
1902.tar.bz2
1903.tar.bz2
...
2000.tar.bz2</pre><p class="calibre2">Each tar file contains a file for each weather station’s readings for the year, compressed
    with gzip. (The fact that the files in the archive are compressed makes the bzip2 compression on
    the archive itself redundant.) For example:</p><pre class="screen1"><code class="literal">%</code> <strong class="userinput"><code class="calibre9">tar jxf 1901.tar.bz2</code></strong>
<code class="literal">%</code> <strong class="userinput"><code class="calibre9">ls 1901 | head</code></strong>
029070-99999-1901.gz
029500-99999-1901.gz
029600-99999-1901.gz
029720-99999-1901.gz
029810-99999-1901.gz
227070-99999-1901.gz</pre><p class="calibre2">Because there are tens of thousands of weather stations, the whole
  dataset is made up of a large number of relatively small files. It’s
  generally easier and more efficient to process a smaller number of
  relatively large files in Hadoop (see <a class="ulink" href="#calibre_link-301" title="Small files and CombineFileInputFormat">Small files and CombineFileInputFormat</a>), so in
  this case, I concatenated the decompressed files for a whole year into a
  single file, named by the year. I did this using a MapReduce program, to
  take advantage of its parallel processing capabilities. Let’s take a closer
  look at the program.</p><p class="calibre2">The program has only a map function. No reduce function is needed
  because the map does all the file processing in parallel with no combine
  stage. The processing can be done with a Unix script, so the Streaming
  interface to MapReduce is appropriate in this case; see <a class="ulink" href="#calibre_link-705" title="Example&nbsp;C-1.&nbsp;Bash script to process raw NCDC datafiles and store them in HDFS">Example&nbsp;C-1</a>.</p><div class="example"><a id="calibre_link-705" class="calibre"></a><div class="example-title">Example&nbsp;C-1.&nbsp;Bash script to process raw NCDC datafiles and store them in HDFS</div><div class="book"><pre class="screen"><code class="c1">#!/usr/bin/env bash</code>

<code class="c1"># NLineInputFormat gives a single line: key is offset, value is S3 URI</code>
<code class="nb">read </code>offset s3file

<code class="c1"># Retrieve file from S3 to local disk</code>
<code class="nb">echo</code> <code class="sb">"reporter:status:Retrieving </code><code class="nv">$s3file</code><code class="sb">"</code> &gt;<code class="p">&amp;</code>2
<code class="nv">$HADOOP_HOME</code>/bin/hadoop fs -get <code class="nv">$s3file</code> .

<code class="c1"># Un-bzip and un-tar the local file</code>
<code class="nv">target</code><code class="o">=</code><code class="sb">`</code>basename <code class="nv">$s3file</code> .tar.bz2<code class="sb">`</code>
mkdir -p <code class="nv">$target</code>
<code class="nb">echo</code> <code class="sb">"reporter:status:Un-tarring </code><code class="nv">$s3file</code><code class="sb"> to </code><code class="nv">$target</code><code class="sb">"</code> &gt;<code class="p">&amp;</code>2
tar jxf <code class="sb">`</code>basename <code class="nv">$s3file</code><code class="sb">`</code> -C <code class="nv">$target</code>

<code class="c1"># Un-gzip each station file and concat into one file</code>
<code class="nb">echo</code> <code class="sb">"reporter:status:Un-gzipping </code><code class="nv">$target</code><code class="sb">"</code> &gt;<code class="p">&amp;</code>2
<code class="k">for</code> file in <code class="nv">$target</code>/*/*
<code class="k">do</code>
  gunzip -c <code class="nv">$file</code> &gt;&gt; <code class="nv">$target</code>.all
  <code class="nb">echo</code> <code class="sb">"reporter:status:Processed </code><code class="nv">$file</code><code class="sb">"</code> &gt;<code class="p">&amp;</code>2
<code class="k">done</code>

<code class="c1"># Put gzipped version into HDFS</code>
<code class="nb">echo</code> <code class="sb">"reporter:status:Gzipping </code><code class="nv">$target</code><code class="sb"> and putting in HDFS"</code> &gt;<code class="p">&amp;</code>2
gzip -c <code class="nv">$target</code>.all <code class="p">|</code> <code class="nv">$HADOOP_HOME</code>/bin/hadoop fs -put - gz/<code class="nv">$target</code>.gz</pre></div></div><p class="calibre2">The input is a small text file (<em class="calibre10">ncdc_files.txt</em>) listing
    all the files to be processed (the files start out on S3, so they are referenced using S3 URIs
    that Hadoop understands). Here is a sample:</p><pre class="screen1">s3n://hadoopbook/ncdc/raw/isd-1901.tar.bz2
s3n://hadoopbook/ncdc/raw/isd-1902.tar.bz2
...
s3n://hadoopbook/ncdc/raw/isd-2000.tar.bz2</pre><p class="calibre2">Because the input format is specified to be <code class="literal">NLineInputFormat</code>, each mapper receives one line of input, which contains the file
    it has to process. The processing is explained in the script, but briefly, it unpacks the bzip2
    file and then concatenates each station file into a single file for the whole year. Finally, the
    file is gzipped and copied into HDFS. Note the use of <code class="literal">hadoop fs -put
      -</code> to consume from standard input.</p><p class="calibre2">Status messages are echoed to standard error with a <code class="literal">reporter:status</code> prefix so that they get interpreted as MapReduce status updates.
    This tells Hadoop that the script is making progress and is not hanging.</p><p class="calibre2">The script to run the Streaming job is as follows:</p><a id="calibre_link-4762" class="calibre"></a><pre class="screen1"><code class="literal">% </code><strong class="userinput"><code class="calibre9">hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
  -D mapred.reduce.tasks=0 \
  -D mapred.map.tasks.speculative.execution=false \
  -D mapred.task.timeout=12000000 \
  -input ncdc_files.txt \
  -inputformat org.apache.hadoop.mapred.lib.NLineInputFormat \
  -output output \
  -mapper load_ncdc_map.sh \
  -file load_ncdc_map.sh</code></strong></pre><p class="calibre2">I set the number of reduce tasks to zero, since this is a map-only
  job. I also turned off speculative execution so duplicate tasks wouldn’t
  write the same files (although the approach discussed in <a class="ulink" href="#calibre_link-706" title="Task side-effect files">Task side-effect files</a> would have worked, too). The task timeout
  was set to a high value so that Hadoop doesn’t kill tasks that are taking a
  long time (for example, when unarchiving files or copying to HDFS, when no
  progress is reported).</p><p class="calibre2">Finally, the files were archived on S3 by copying them from HDFS
  <a class="calibre" id="calibre_link-2780"></a><a class="calibre" id="calibre_link-2774"></a>using <em class="calibre10">distcp</em>.</p></section></div>

<div class="calibre1" id="calibre_link-249"><section type="appendix" id="calibre_link-4763" title="Appendix&nbsp;D.&nbsp;The Old and New Java MapReduce APIs"><div class="titlepage"><div class="book"><div class="book"><h2 class="title1">Appendix&nbsp;D.&nbsp;The Old and New Java MapReduce APIs</h2></div></div></div><p class="calibre2">The Java MapReduce API used <a class="calibre" id="calibre_link-2486"></a>throughout this book is called the “new API,” and it replaces the older,
    functionally equivalent API. Although Hadoop ships with both the old and new MapReduce APIs,
    they are not compatible with each other. Should you wish to use the old API, you can, since the
    code for all the MapReduce examples in this book is available for the old API on the book’s
    website (in the <code class="literal">oldapi</code> package).</p><p class="calibre2">There are several notable differences between the two APIs:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><p class="calibre2">The new API is in the <code class="literal">org.apache.hadoop.mapreduce</code> package (and
      subpackages). The old API can still be found in <code class="literal">org.apache.hadoop.mapred</code>.</p></li><li class="listitem"><p class="calibre2">The new API favors abstract classes over interfaces, since these
      are easier to evolve. This means that you can add a method (with a
      default implementation) to an abstract class without breaking old
      implementations of the class.<sup class="calibre6">[<a class="firstname" href="#calibre_link-785" id="calibre_link-788">168</a>]</sup> For example, the <code class="literal">Mapper</code> and <code class="literal">Reducer</code> interfaces in the old API are abstract
      classes in the new API.</p></li><li class="listitem"><p class="calibre2">The new API makes extensive use of <em class="calibre10">context
      objects</em> that allow the user code to communicate with the
      MapReduce system. The new <code class="literal">Context</code>,
      for example, essentially unifies the role of the <code class="literal">JobConf</code>, the <code class="literal">OutputCollector</code>, and the <code class="literal">Reporter</code> from the old API.</p></li><li class="listitem"><p class="calibre2">In both APIs, key-value record pairs are pushed to the mapper and
      reducer, but in addition, the new API allows both mappers and reducers
      to control the execution flow by overriding the <code class="literal">run()</code> method. For example, records can be
      processed in batches, or the execution can be terminated before all the
      records have been processed. In the old API, this is possible for
      mappers by writing a <code class="literal">MapRunnable</code>, but
      no equivalent exists for reducers.</p></li><li class="listitem"><p class="calibre2">Job control is performed through the <code class="literal">Job</code> class in the new API, rather than the old
      <code class="literal">JobClient</code>, which no longer exists in
      the new API.</p></li><li class="listitem"><p class="calibre2">Configuration has been unified in the new API. The old API has a
      special <code class="literal">JobConf</code> object for job
      configuration, which is an extension of Hadoop’s vanilla <code class="literal">Configuration</code> object (used for configuring
      daemons; see <a class="ulink" href="#calibre_link-41" title="The Configuration API">The Configuration API</a>). In the new API,
      job configuration is done through a <code class="literal">Configuration</code>, possibly via some of the
      helper methods on <code class="literal">Job</code>.</p></li><li class="listitem"><p class="calibre2">Output files are named slightly differently: in the old API both
      map and reduce outputs are named <em class="calibre10">part-<em class="replaceable"><code class="replaceable">nnnnn</code></em></em>,
      whereas in the new API map outputs are named <em class="calibre10">part-m-<em class="replaceable"><code class="replaceable">nnnnn</code></em></em> and
      reduce outputs are named <em class="calibre10">part-r-<em class="replaceable"><code class="replaceable">nnnnn</code></em></em>
      (where <em class="calibre10"><em class="replaceable"><code class="replaceable">nnnnn</code></em></em> is an
      integer designating the part number, starting from 00000).</p></li><li class="listitem"><p class="calibre2">User-overridable methods in the new API are declared to throw
      <code class="literal">java.lang.InterruptedException</code>. This
      means that you can write your code to be responsive to interrupts so
      that the framework can gracefully cancel long-running operations if it
      needs to.<sup class="calibre6">[<a class="firstname" href="#calibre_link-786" id="calibre_link-789">169</a>]</sup></p></li><li class="listitem"><p class="calibre2">In the new API, the <code class="literal">reduce()</code> method
      passes values as a <code class="literal">java.lang.Iterable</code>, rather than a <code class="literal">java.lang.Iterator</code> (as the old API does). This
        change makes it easier to iterate over the values using Java’s <code class="literal">for</code>-<code class="literal">each</code>
      loop construct:</p><a id="calibre_link-4764" class="calibre"></a><pre class="screen1"><code class="k">for</code> <code class="o">(</code><code class="n">VALUEIN</code> <code class="n">value</code> <code class="o">:</code> <code class="n">values</code><code class="o">)</code> <code class="o">{</code> <code class="o">...</code> <code class="o">}</code></pre></li></ul></div><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">Programs using the new API that were compiled against Hadoop 1 need
    to be recompiled to run against Hadoop 2. This is because some classes in
    the new MapReduce API changed to interfaces between the Hadoop 1 and
    Hadoop 2 releases. The symptom is an error at runtime like the
    following:</p><pre class="screen2">java.lang.IncompatibleClassChangeError: Found interface
org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected</pre></div><p class="calibre2"><a class="ulink" href="#calibre_link-787" title="Example&nbsp;D-1.&nbsp;Application to find the maximum temperature, using the old MapReduce API">Example&nbsp;D-1</a> shows the <code class="literal">MaxTemperature</code> application (from <a class="ulink" href="#calibre_link-756" title="Java MapReduce">Java MapReduce</a>) rewritten to use the old API. The differences
  are highlighted in <a class="calibre" id="calibre_link-2487"></a>bold.</p><div class="caution" type="warning" title="Warning"><h3 class="title4">Warning</h3><p class="calibre2">When converting your <code class="literal">Mapper</code> and <code class="literal">Reducer</code>
      classes to the new API, don’t forget to change the signatures of the
        <code class="literal">map()</code> and <code class="literal">reduce()</code> methods to the new
      form. Just changing your class to extend the new <code class="literal">Mapper</code> or
        <code class="literal">Reducer</code> classes will <span class="calibre">not</span> produce
      a compilation error or warning, because these classes provide identity forms of the
        <code class="literal">map()</code> and <code class="literal">reduce()</code> methods (respectively).
      Your mapper or reducer code, however, will not be invoked, which can lead to some
      hard-to-diagnose errors.</p><p class="calibre2">Annotating your <code class="literal">map()</code> and
    <code class="literal">reduce()</code> methods with the <code class="literal">@Override</code> annotation will allow the Java
    compiler to catch these errors.</p></div><div class="example"><a id="calibre_link-787" class="calibre"></a><div class="example-title">Example&nbsp;D-1.&nbsp;Application to find the maximum temperature, using the old
    MapReduce API</div><div class="book"><pre class="screen">public class OldMaxTemperature {
  
  static class OldMaxTemperatureMapper <span class="calibre24"><strong class="calibre9">extends MapReduceBase</strong></span>
      <span class="calibre24"><strong class="calibre9">implements Mapper</strong></span>&lt;LongWritable, Text, Text, IntWritable&gt; {
  
    private static final int MISSING = 9999;
    
    @Override
    public void map(LongWritable key, Text value,
        <span class="calibre24"><strong class="calibre9">OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter</strong></span>)
        throws IOException {
      
      String line = value.toString();
      String year = line.substring(15, 19);
      int airTemperature;
      if (line.charAt(87) == '+') { // parseInt doesn't like leading plus signs
        airTemperature = Integer.parseInt(line.substring(88, 92));
      } else {
        airTemperature = Integer.parseInt(line.substring(87, 92));
      }
      String quality = line.substring(92, 93);
      if (airTemperature != MISSING &amp;&amp; quality.matches("[01459]")) {
        <span class="calibre24"><strong class="calibre9">output.collect</strong></span>(new Text(year), new IntWritable(airTemperature));
      }
    }
  }
  
  static class OldMaxTemperatureReducer <span class="calibre24"><strong class="calibre9">extends MapReduceBase</strong></span>
    <span class="calibre24"><strong class="calibre9">implements Reducer</strong></span>&lt;Text, IntWritable, Text, IntWritable&gt; {

    @Override
    public void reduce(Text key, <span class="calibre24"><strong class="calibre9">Iterator</strong></span>&lt;IntWritable&gt; values,
        <span class="calibre24"><strong class="calibre9">OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter</strong></span>)
        throws IOException {
      
      int maxValue = Integer.MIN_VALUE;
      while (<span class="calibre24"><strong class="calibre9">values.hasNext()</strong></span>) {
        maxValue = Math.max(maxValue, <span class="calibre24"><strong class="calibre9">values.next().get()</strong></span>);
      }
      <span class="calibre24"><strong class="calibre9">output.collect</strong></span>(key, new IntWritable(maxValue));
    }
  }

  public static void main(String[] args) throws IOException {
    if (args.length != 2) {
      System.err.println("Usage: OldMaxTemperature &lt;input path&gt; &lt;output path&gt;");
      System.exit(-1);
    }
    
    <span class="calibre24"><strong class="calibre9">JobConf conf = new JobConf(OldMaxTemperature.class);</strong></span>
    <span class="calibre24"><strong class="calibre9">conf</strong></span>.setJobName("Max temperature");

    FileInputFormat.addInputPath(<span class="calibre24"><strong class="calibre9">conf</strong></span>, new Path(args[0]));
    FileOutputFormat.setOutputPath(<span class="calibre24"><strong class="calibre9">conf</strong></span>, new Path(args[1]));
    
    <span class="calibre24"><strong class="calibre9">conf</strong></span>.setMapperClass(OldMaxTemperatureMapper.class);
    <span class="calibre24"><strong class="calibre9">conf</strong></span>.setReducerClass(OldMaxTemperatureReducer.class);

    <span class="calibre24"><strong class="calibre9">conf</strong></span>.setOutputKeyClass(Text.class);
    <span class="calibre24"><strong class="calibre9">conf</strong></span>.setOutputValueClass(IntWritable.class);

    <span class="calibre24"><strong class="calibre9">JobClient.runJob(conf);</strong></span>
  }
}</pre></div></div><div class="book" type="footnotes"><br class="calibre3"><hr class="calibre14"><div class="footnote" id="calibre_link-785"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-788">168</a>] </sup>Technically, such a change would almost certainly break implementations that already
            define a method with the same signature as Jim des Rivières explains in <a class="ulink" href="http://bit.ly/adding_api_method" target="_top">“Evolving Java-based APIs,”</a> for all practical purposes this is treated as a compatible change.</p></div><div class="footnote" id="calibre_link-786"><p class="calibre2"><sup class="calibre5">[<a class="firstname" href="#calibre_link-789">169</a>] </sup><a class="ulink" href="http://bit.ly/interruptedexception" target="_top">“Java theory and practice: Dealing
          with InterruptedException”</a> by Brian Goetz explains this
          technique in detail.</p></div></div></section></div>

<div class="calibre1" id="calibre_link-857"><div class="index" type="index" id="calibre_link-4765" title="Index"><div class="book"><div class="book"><div class="book"><h2 class="title10">Index</h2></div></div></div><div class="index"><div class="book"><h3 class="author1">A</h3><dl class="book"><dt class="calibre57">AbstractAvroEventSerializer class, <a class="indexterm" href="#calibre_link-858">File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">access control lists (ACLs), <a class="indexterm" href="#calibre_link-859">An example</a>, <a class="indexterm" href="#calibre_link-860">ACLs</a></dt><dd class="calibre8"></dd><dt class="calibre57">accumulators, <a class="indexterm" href="#calibre_link-861">Accumulators</a></dt><dd class="calibre8"></dd><dt class="calibre57">ACLs (access control lists), <a class="indexterm" href="#calibre_link-862">An example</a>, <a class="indexterm" href="#calibre_link-863">ACLs</a></dt><dd class="calibre8"></dd><dt class="calibre57">action nodes, <a class="indexterm" href="#calibre_link-864">Apache Oozie</a>, <a class="indexterm" href="#calibre_link-865">Defining an Oozie workflow</a></dt><dd class="calibre8"></dd><dt class="calibre57">actions, RDD, <a class="indexterm" href="#calibre_link-866">Transformations and Actions</a></dt><dd class="calibre8"></dd><dt class="calibre57">ADAM platform, <a class="indexterm" href="#calibre_link-867">ADAM, A Scalable Genome Analysis Platform</a>–<a class="indexterm" href="#calibre_link-868">A simple example: k-mer counting using
        Spark and ADAM</a></dt><dd class="calibre8"></dd><dt class="calibre57">ADMIN permission (ACL), <a class="indexterm" href="#calibre_link-869">ACLs</a></dt><dd class="calibre8"></dd><dt class="calibre57">administration (see system administration)</dt><dd class="calibre8"></dd><dt class="calibre57">agents (Flume)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-870">Flume</a></dt><dd class="calibre8"></dd><dt class="calibre57">distribution process, <a class="indexterm" href="#calibre_link-871">Distribution: Agent Tiers</a>–<a class="indexterm" href="#calibre_link-872">Delivery Guarantees</a></dt><dd class="calibre8"></dd><dt class="calibre57">example of, <a class="indexterm" href="#calibre_link-873">An Example</a>–<a class="indexterm" href="#calibre_link-874">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">HDFS sinks and, <a class="indexterm" href="#calibre_link-875">The HDFS Sink</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Aggregate class, <a class="indexterm" href="#calibre_link-876">Crunch Libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">aggregating data in Hive tables, <a class="indexterm" href="#calibre_link-877">Sorting and Aggregating</a></dt><dd class="calibre8"></dd><dt class="calibre57">Aggregator interface, <a class="indexterm" href="#calibre_link-878">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">Aggregators class, <a class="indexterm" href="#calibre_link-879">An Example</a>, <a class="indexterm" href="#calibre_link-880">combineValues()</a></dt><dd class="calibre8"></dd><dt class="calibre57">aliases, defining, <a class="indexterm" href="#calibre_link-881">A Filter UDF</a></dt><dd class="calibre8"></dd><dt class="calibre57">ALTER TABLE statement (Hive), <a class="indexterm" href="#calibre_link-882">Partitions</a>, <a class="indexterm" href="#calibre_link-883">Altering Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">Amazon Web Services, <a class="indexterm" href="#calibre_link-884">Data!</a>, <a class="indexterm" href="#calibre_link-885">A Brief History of Apache Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">Ant build tool, <a class="indexterm" href="#calibre_link-886">Setting Up the Development Environment</a>, <a class="indexterm" href="#calibre_link-887">Packaging a Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">Apache Avro (see Avro)</dt><dd class="calibre8"></dd><dt class="calibre57">Apache Commons Logging API, <a class="indexterm" href="#calibre_link-888">Hadoop Logs</a></dt><dd class="calibre8"></dd><dt class="calibre57">Apache Crunch (see Crunch)</dt><dd class="calibre8"></dd><dt class="calibre57">Apache Curator project, <a class="indexterm" href="#calibre_link-889">More Distributed Data Structures and Protocols</a></dt><dd class="calibre8"></dd><dt class="calibre57">Apache Flume (see Flume)</dt><dd class="calibre8"></dd><dt class="calibre57">Apache Mesos, <a class="indexterm" href="#calibre_link-890">Executors and Cluster Managers</a></dt><dd class="calibre8"></dd><dt class="calibre57">Apache Oozie</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-891">Apache Oozie</a></dt><dd class="calibre8"></dd><dt class="calibre57">defining workflows, <a class="indexterm" href="#calibre_link-892">Defining an Oozie workflow</a>–<a class="indexterm" href="#calibre_link-893">Defining an Oozie workflow</a></dt><dd class="calibre8"></dd><dt class="calibre57">packaging and deploying workflow
            applications, <a class="indexterm" href="#calibre_link-894">Packaging and deploying an Oozie workflow application</a></dt><dd class="calibre8"></dd><dt class="calibre57">running workflow job, <a class="indexterm" href="#calibre_link-895">Running an Oozie workflow job</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Apache Parquet (see Parquet)</dt><dd class="calibre8"></dd><dt class="calibre57">Apache Phoenix, <a class="indexterm" href="#calibre_link-896">SQL-on-Hadoop Alternatives</a></dt><dd class="calibre8"></dd><dt class="calibre57">Apache Slider, <a class="indexterm" href="#calibre_link-897">Application Lifespan</a></dt><dd class="calibre8"></dd><dt class="calibre57">Apache Software Foundation, <a class="indexterm" href="#calibre_link-898">Beyond Batch</a>, <a class="indexterm" href="#calibre_link-899">Building YARN Applications</a></dt><dd class="calibre8"></dd><dt class="calibre57">Apache Spark (see Spark)</dt><dd class="calibre8"></dd><dt class="calibre57">Apache Tez, <a class="indexterm" href="#calibre_link-900">Execution engines</a></dt><dd class="calibre8"></dd><dt class="calibre57">Apache Thrift, <a class="indexterm" href="#calibre_link-901">Serialization IDL</a></dt><dd class="calibre8"></dd><dt class="calibre57">Apache Twill, <a class="indexterm" href="#calibre_link-902">Building YARN Applications</a></dt><dd class="calibre8"></dd><dt class="calibre57">APPEND write mode, <a class="indexterm" href="#calibre_link-903">Existing outputs</a></dt><dd class="calibre8"></dd><dt class="calibre57">application IDs, <a class="indexterm" href="#calibre_link-904">Launching a Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">application masters</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-905">Anatomy of a YARN Application Run</a></dt><dd class="calibre8"></dd><dt class="calibre57">decommissioning old nodes, <a class="indexterm" href="#calibre_link-906">Decommissioning old nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">failure considerations, <a class="indexterm" href="#calibre_link-907">Application Master Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">job completion, <a class="indexterm" href="#calibre_link-908">Job Completion</a></dt><dd class="calibre8"></dd><dt class="calibre57">job initialization process, <a class="indexterm" href="#calibre_link-909">Job Initialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">jobtrackers and, <a class="indexterm" href="#calibre_link-910">YARN Compared to MapReduce 1</a></dt><dd class="calibre8"></dd><dt class="calibre57">node manager failure, <a class="indexterm" href="#calibre_link-911">Node Manager Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">progress and status updates, <a class="indexterm" href="#calibre_link-912">Progress and Status Updates</a></dt><dd class="calibre8"></dd><dt class="calibre57">resource manager failure, <a class="indexterm" href="#calibre_link-913">Resource Manager Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">task assignments, <a class="indexterm" href="#calibre_link-914">Task Assignment</a></dt><dd class="calibre8"></dd><dt class="calibre57">task execution, <a class="indexterm" href="#calibre_link-915">Task Execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">task failure, <a class="indexterm" href="#calibre_link-916">Task Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">unmanaged, <a class="indexterm" href="#calibre_link-917">Anatomy of a YARN Application Run</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">ArrayFile class, <a class="indexterm" href="#calibre_link-918">MapFile variants</a></dt><dd class="calibre8"></dd><dt class="calibre57">ArrayPrimitiveWritable class, <a class="indexterm" href="#calibre_link-919">Writable collections</a></dt><dd class="calibre8"></dd><dt class="calibre57">ArrayWritable class, <a class="indexterm" href="#calibre_link-920">Writable collections</a></dt><dd class="calibre8"></dd><dt class="calibre57">ASSERT statement (Pig Latin), <a class="indexterm" href="#calibre_link-921">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">Astrometry.net project, <a class="indexterm" href="#calibre_link-922">Data!</a></dt><dd class="calibre8"></dd><dt class="calibre57">atomic broadcast, <a class="indexterm" href="#calibre_link-923">Implementation</a></dt><dd class="calibre8"></dd><dt class="calibre57">audit logs (HDFS), <a class="indexterm" href="#calibre_link-924">Hadoop Logs</a>, <a class="indexterm" href="#calibre_link-925">Audit Logging</a></dt><dd class="calibre8"></dd><dt class="calibre57">authentication</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">ACLs and, <a class="indexterm" href="#calibre_link-926">ACLs</a></dt><dd class="calibre8"></dd><dt class="calibre57">delegation tokens and, <a class="indexterm" href="#calibre_link-927">Delegation Tokens</a></dt><dd class="calibre8"></dd><dt class="calibre57">Kerberos, <a class="indexterm" href="#calibre_link-928">Kerberos and Hadoop</a>–<a class="indexterm" href="#calibre_link-929">An example</a>, <a class="indexterm" href="#calibre_link-930">ACLs</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">authorization process, <a class="indexterm" href="#calibre_link-931">Kerberos and Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">AVG function (Pig Latin), <a class="indexterm" href="#calibre_link-932">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">Avro</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-933">Serialization IDL</a>, <a class="indexterm" href="#calibre_link-934">Avro</a>–<a class="indexterm" href="#calibre_link-935">Avro</a></dt><dd class="calibre8"></dd><dt class="calibre57">binary storage format and, <a class="indexterm" href="#calibre_link-936">Binary storage formats: Sequence files, Avro datafiles, Parquet
        files, RCFiles, and ORCFiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">Crunch and, <a class="indexterm" href="#calibre_link-937">Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">data types and schemas, <a class="indexterm" href="#calibre_link-938">Avro Data Types and Schemas</a>–<a class="indexterm" href="#calibre_link-939">Avro Data Types and Schemas</a></dt><dd class="calibre8"></dd><dt class="calibre57">datafile format, <a class="indexterm" href="#calibre_link-940">Other File Formats and Column-Oriented Formats</a>, <a class="indexterm" href="#calibre_link-941">Avro Datafiles</a>–<a class="indexterm" href="#calibre_link-942">Avro Datafiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">Flume support, <a class="indexterm" href="#calibre_link-943">File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive support, <a class="indexterm" href="#calibre_link-944">Text and Binary File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">interoperability, <a class="indexterm" href="#calibre_link-945">Interoperability</a>–<a class="indexterm" href="#calibre_link-946">Avro Tools</a></dt><dd class="calibre8"></dd><dt class="calibre57">languages and framework support, <a class="indexterm" href="#calibre_link-947">Avro in Other Languages</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapReduce support, <a class="indexterm" href="#calibre_link-948">Avro MapReduce</a>–<a class="indexterm" href="#calibre_link-949">Sorting Using Avro MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">Parquet and, <a class="indexterm" href="#calibre_link-950">Avro, Protocol Buffers, and Thrift</a>–<a class="indexterm" href="#calibre_link-951">Projection and read schemas</a></dt><dd class="calibre8"></dd><dt class="calibre57">schema resolution, <a class="indexterm" href="#calibre_link-952">Schema Resolution</a>–<a class="indexterm" href="#calibre_link-953">Schema Resolution</a></dt><dd class="calibre8"></dd><dt class="calibre57">serialization and deserialization, <a class="indexterm" href="#calibre_link-954">In-Memory Serialization and Deserialization</a>–<a class="indexterm" href="#calibre_link-955">The Specific API</a></dt><dd class="calibre8"></dd><dt class="calibre57">sort capabilities, <a class="indexterm" href="#calibre_link-956">Sort Order</a>, <a class="indexterm" href="#calibre_link-957">Sorting Using Avro MapReduce</a>–<a class="indexterm" href="#calibre_link-958">Sorting Using Avro MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">Sqoop support, <a class="indexterm" href="#calibre_link-959">Text and Binary File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">tools supported, <a class="indexterm" href="#calibre_link-960">Avro Tools</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">avro.java.string property, <a class="indexterm" href="#calibre_link-961">Avro Data Types and Schemas</a></dt><dd class="calibre8"></dd><dt class="calibre57">AvroAsTextInputFormat class, <a class="indexterm" href="#calibre_link-962">Avro in Other Languages</a></dt><dd class="calibre8"></dd><dt class="calibre57">AvroParquetOutputFormat class, <a class="indexterm" href="#calibre_link-963">Parquet MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">AvroParquetReader class, <a class="indexterm" href="#calibre_link-964">Avro, Protocol Buffers, and Thrift</a></dt><dd class="calibre8"></dd><dt class="calibre57">AvroParquetWriter class, <a class="indexterm" href="#calibre_link-965">Avro, Protocol Buffers, and Thrift</a></dt><dd class="calibre8"></dd><dt class="calibre57">AvroReadSupport class, <a class="indexterm" href="#calibre_link-966">Projection and read schemas</a></dt><dd class="calibre8"></dd><dt class="calibre57">AvroStorage function (Pig Latin), <a class="indexterm" href="#calibre_link-967">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">awk tool, <a class="indexterm" href="#calibre_link-968">Analyzing the Data with Unix Tools</a>–<a class="indexterm" href="#calibre_link-969">Analyzing the Data with Unix Tools</a>, <a class="indexterm" href="#calibre_link-970">Running a Distributed MapReduce Job</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">B</h3><dl class="book"><dt class="calibre57">B-Tree data structure, <a class="indexterm" href="#calibre_link-971">Relational Database Management Systems</a></dt><dd class="calibre8"></dd><dt class="calibre57">backups</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">data, <a class="indexterm" href="#calibre_link-972">Data backups</a></dt><dd class="calibre8"></dd><dt class="calibre57">metadata, <a class="indexterm" href="#calibre_link-973">Metadata backups</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">balancer tool, <a class="indexterm" href="#calibre_link-974">Keeping an HDFS Cluster Balanced</a>, <a class="indexterm" href="#calibre_link-975">Balancer</a>, <a class="indexterm" href="#calibre_link-976">Filesystem balancer</a></dt><dd class="calibre8"></dd><dt class="calibre57">Baldeschwieler, Eric, <a class="indexterm" href="#calibre_link-977">A Brief History of Apache Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">bandwidth, measuring between nodes, <a class="indexterm" href="#calibre_link-978">Anatomy of a File Read</a></dt><dd class="calibre8"></dd><dt class="calibre57">batch processing</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">Flume support, <a class="indexterm" href="#calibre_link-979">Batching</a></dt><dd class="calibre8"></dd><dt class="calibre57">limitations of, <a class="indexterm" href="#calibre_link-980">Beyond Batch</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">batchSize property, <a class="indexterm" href="#calibre_link-981">Batching</a></dt><dd class="calibre8"></dd><dt class="calibre57">Beeline service (Hive), <a class="indexterm" href="#calibre_link-982">Hive Services</a></dt><dd class="calibre8"></dd><dt class="calibre57">benchmarking clusters, <a class="indexterm" href="#calibre_link-983">Benchmarking a Hadoop Cluster</a>–<a class="indexterm" href="#calibre_link-984">User Jobs</a></dt><dd class="calibre8"></dd><dt class="calibre57">binary formats</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">for data storage, <a class="indexterm" href="#calibre_link-985">Binary storage formats: Sequence files, Avro datafiles, Parquet
        files, RCFiles, and ORCFiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">FixedLengthInputFormat class, <a class="indexterm" href="#calibre_link-986">FixedLengthInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapFileOutputFormat class, <a class="indexterm" href="#calibre_link-987">MapFileOutputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">SequenceFileAsBinaryInputFormat class, <a class="indexterm" href="#calibre_link-988">SequenceFileAsBinaryInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">SequenceFileAsBinaryOutputFormat class, <a class="indexterm" href="#calibre_link-989">SequenceFileAsBinaryOutputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">SequenceFileAsTextInputFormat class, <a class="indexterm" href="#calibre_link-990">SequenceFileAsTextInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">SequenceFileInputFormat class, <a class="indexterm" href="#calibre_link-991">SequenceFileInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">SequenceFileOutputFormat class, <a class="indexterm" href="#calibre_link-992">SequenceFileOutputFormat</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">biological data science case study</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-993">Biological Data Science: Saving Lives with Software</a>–<a class="indexterm" href="#calibre_link-994">Biological Data Science: Saving Lives with Software</a></dt><dd class="calibre8"></dd><dt class="calibre57">ADAM platform, <a class="indexterm" href="#calibre_link-995">ADAM, A Scalable Genome Analysis Platform</a>–<a class="indexterm" href="#calibre_link-996">A simple example: k-mer counting using
        Spark and ADAM</a></dt><dd class="calibre8"></dd><dt class="calibre57">DNA as source code, <a class="indexterm" href="#calibre_link-997">Thinking of DNA as Source Code</a>–<a class="indexterm" href="#calibre_link-998">Thinking of DNA as Source Code</a></dt><dd class="calibre8"></dd><dt class="calibre57">genetic code, <a class="indexterm" href="#calibre_link-999">The Genetic Code: Turning DNA Letters into Proteins</a></dt><dd class="calibre8"></dd><dt class="calibre57">Human Genome Project, <a class="indexterm" href="#calibre_link-1000">The Human Genome Project and Reference Genomes</a></dt><dd class="calibre8"></dd><dt class="calibre57">join in, <a class="indexterm" href="#calibre_link-1001">Join In</a></dt><dd class="calibre8"></dd><dt class="calibre57">personalized medicine and, <a class="indexterm" href="#calibre_link-1002">From Personalized Ads to Personalized Medicine</a>–<a class="indexterm" href="#calibre_link-1003">From Personalized Ads to Personalized Medicine</a></dt><dd class="calibre8"></dd><dt class="calibre57">reference genomes, <a class="indexterm" href="#calibre_link-1004">The Human Genome Project and Reference Genomes</a></dt><dd class="calibre8"></dd><dt class="calibre57">sequencing and aligning DNA, <a class="indexterm" href="#calibre_link-1005">Sequencing and Aligning DNA</a></dt><dd class="calibre8"></dd><dt class="calibre57">structure of DNA, <a class="indexterm" href="#calibre_link-1006">The Structure of DNA</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">blacklisting node managers, <a class="indexterm" href="#calibre_link-1007">Node Manager Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">block access tokens, <a class="indexterm" href="#calibre_link-1008">Delegation Tokens</a></dt><dd class="calibre8"></dd><dt class="calibre57">blockpoolID identifier, <a class="indexterm" href="#calibre_link-1009">Namenode directory structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">blocks and block sizes</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1010">Blocks</a>–<a class="indexterm" href="#calibre_link-1011">Blocks</a></dt><dd class="calibre8"></dd><dt class="calibre57">block caching, <a class="indexterm" href="#calibre_link-1012">Block Caching</a></dt><dd class="calibre8"></dd><dt class="calibre57">checking blocks, <a class="indexterm" href="#calibre_link-1013">Filesystem check (fsck)</a>–<a class="indexterm" href="#calibre_link-1014">Finding the blocks for a file</a></dt><dd class="calibre8"></dd><dt class="calibre57">input splits and, <a class="indexterm" href="#calibre_link-1015">TextInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">Parquet and, <a class="indexterm" href="#calibre_link-1016">Parquet Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">setting, <a class="indexterm" href="#calibre_link-1017">HDFS block size</a></dt><dd class="calibre8"></dd><dt class="calibre57">setting for HDFS, <a class="indexterm" href="#calibre_link-1018">FileInputFormat input splits</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">BloomMapFile class, <a class="indexterm" href="#calibre_link-1019">MapFile variants</a></dt><dd class="calibre8"></dd><dt class="calibre57">BookKeeper service, <a class="indexterm" href="#calibre_link-1020">BookKeeper and Hedwig</a></dt><dd class="calibre8"></dd><dt class="calibre57">BooleanWritable class, <a class="indexterm" href="#calibre_link-1021">Writable wrappers for Java primitives</a></dt><dd class="calibre8"></dd><dt class="calibre57">broadcast variables, <a class="indexterm" href="#calibre_link-1022">Broadcast Variables</a></dt><dd class="calibre8"></dd><dt class="calibre57">Brush, Ryan, <a class="indexterm" href="#calibre_link-1023">Composable Data at Cerner</a></dt><dd class="calibre8"></dd><dt class="calibre57">buckets, Hive tables and, <a class="indexterm" href="#calibre_link-1024">Partitions and Buckets</a>, <a class="indexterm" href="#calibre_link-1025">Buckets</a>–<a class="indexterm" href="#calibre_link-1026">Buckets</a></dt><dd class="calibre8"></dd><dt class="calibre57">buffer size, <a class="indexterm" href="#calibre_link-1027">Buffer size</a></dt><dd class="calibre8"></dd><dt class="calibre57">built-in counters</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1028">Built-in Counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">job counters, <a class="indexterm" href="#calibre_link-1029">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">task counters, <a class="indexterm" href="#calibre_link-1030">Built-in Counters</a>–<a class="indexterm" href="#calibre_link-1031">Task counters</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">bulk loading, <a class="indexterm" href="#calibre_link-1032">Bulk load</a></dt><dd class="calibre8"></dd><dt class="calibre57">ByteArrayOutputStream class (Java), <a class="indexterm" href="#calibre_link-1033">The Writable Interface</a></dt><dd class="calibre8"></dd><dt class="calibre57">BytesWritable class, <a class="indexterm" href="#calibre_link-1034">BytesWritable</a>, <a class="indexterm" href="#calibre_link-1035">Processing a whole file as a record</a></dt><dd class="calibre8"></dd><dt class="calibre57">BYTES_READ counter, <a class="indexterm" href="#calibre_link-1036">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">BYTES_WRITTEN counter, <a class="indexterm" href="#calibre_link-1037">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">ByteWritable class, <a class="indexterm" href="#calibre_link-1038">Writable wrappers for Java primitives</a></dt><dd class="calibre8"></dd><dt class="calibre57">bzip2 compression, <a class="indexterm" href="#calibre_link-1039">Compression</a>–<a class="indexterm" href="#calibre_link-1040">Codecs</a>, <a class="indexterm" href="#calibre_link-1041">Native libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">BZip2Codec class, <a class="indexterm" href="#calibre_link-1042">Codecs</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">C</h3><dl class="book"><dt class="calibre57">C library (libhdfs), <a class="indexterm" href="#calibre_link-1043">C</a></dt><dd class="calibre8"></dd><dt class="calibre57">Cafarella, Mike, <a class="indexterm" href="#calibre_link-1044">A Brief History of Apache Hadoop</a>, <a class="indexterm" href="#calibre_link-1045">Backdrop</a></dt><dd class="calibre8"></dd><dt class="calibre57">Capacity Scheduler (YARN), <a class="indexterm" href="#calibre_link-1046">Capacity Scheduler Configuration</a>–<a class="indexterm" href="#calibre_link-1047">Queue placement</a></dt><dd class="calibre8"></dd><dt class="calibre57">Cartesian class, <a class="indexterm" href="#calibre_link-1048">Crunch Libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">Cascading library case study</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1049">Cascading</a></dt><dd class="calibre8"></dd><dt class="calibre57">application example, <a class="indexterm" href="#calibre_link-1050">Cascading in Practice</a>–<a class="indexterm" href="#calibre_link-1051">Cascading in Practice</a></dt><dd class="calibre8"></dd><dt class="calibre57">fields, tuples, and pipes, <a class="indexterm" href="#calibre_link-1052">Fields, Tuples, and Pipes</a>–<a class="indexterm" href="#calibre_link-1053">Fields, Tuples, and Pipes</a></dt><dd class="calibre8"></dd><dt class="calibre57">flexibility, <a class="indexterm" href="#calibre_link-1054">Flexibility</a></dt><dd class="calibre8"></dd><dt class="calibre57">operations, <a class="indexterm" href="#calibre_link-1055">Operations</a>–<a class="indexterm" href="#calibre_link-1056">Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">ShareThis sharing network, <a class="indexterm" href="#calibre_link-1057">Hadoop and Cascading at ShareThis</a>–<a class="indexterm" href="#calibre_link-1058">Hadoop and Cascading at ShareThis</a></dt><dd class="calibre8"></dd><dt class="calibre57">summary, <a class="indexterm" href="#calibre_link-1059">Summary</a></dt><dd class="calibre8"></dd><dt class="calibre57">taps, schemes, and flows, <a class="indexterm" href="#calibre_link-1060">Taps, Schemes, and Flows</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">case sensitivity</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">HiveQL, <a class="indexterm" href="#calibre_link-1061">The Hive Shell</a>, <a class="indexterm" href="#calibre_link-1062">Writing a UDF</a></dt><dd class="calibre8"></dd><dt class="calibre57">Pig Latin, <a class="indexterm" href="#calibre_link-1063">Structure</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">case studies</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">biological data science, <a class="indexterm" href="#calibre_link-1064">Biological Data Science: Saving Lives with Software</a>–<a class="indexterm" href="#calibre_link-1065">Join In</a></dt><dd class="calibre8"></dd><dt class="calibre57">Cascading library, <a class="indexterm" href="#calibre_link-1066">Cascading</a>–<a class="indexterm" href="#calibre_link-1067">Summary</a></dt><dd class="calibre8"></dd><dt class="calibre57">composable data at Cerner, <a class="indexterm" href="#calibre_link-1068">Composable Data at Cerner</a>–<a class="indexterm" href="#calibre_link-1069">Moving Forward</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">cat command, <a class="indexterm" href="#calibre_link-1070">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">cd command, <a class="indexterm" href="#calibre_link-1071">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">CDH (Cloudera’s Distribution including Apache Hadoop), <a class="indexterm" href="#calibre_link-1072">Cloudera’s Distribution Including Apache
  Hadoop</a>–<a class="indexterm" href="#calibre_link-1073">Cloudera’s Distribution Including Apache
  Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">Cerner case study, <a class="indexterm" href="#calibre_link-1074">Composable Data at Cerner</a></dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1075">From CPUs to Semantic Integration</a></dt><dd class="calibre8"></dd><dt class="calibre57">Apache Crunch and, <a class="indexterm" href="#calibre_link-1076">Enter Apache Crunch</a></dt><dd class="calibre8"></dd><dt class="calibre57">building complete picture, <a class="indexterm" href="#calibre_link-1077">Building a Complete Picture</a>–<a class="indexterm" href="#calibre_link-1078">Integrating Healthcare Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">composability over frameworks, <a class="indexterm" href="#calibre_link-1079">Composability over Frameworks</a></dt><dd class="calibre8"></dd><dt class="calibre57">integrating healthcare data, <a class="indexterm" href="#calibre_link-1080">Integrating Healthcare Data</a>–<a class="indexterm" href="#calibre_link-1081">Integrating Healthcare Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">moving forward, <a class="indexterm" href="#calibre_link-1082">Moving Forward</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">ChainMapper class, <a class="indexterm" href="#calibre_link-1083">Decomposing a Problem into MapReduce Jobs</a>, <a class="indexterm" href="#calibre_link-1084">MapReduce Library Classes</a></dt><dd class="calibre8"></dd><dt class="calibre57">ChainReducer class, <a class="indexterm" href="#calibre_link-1085">Decomposing a Problem into MapReduce Jobs</a>, <a class="indexterm" href="#calibre_link-1086">MapReduce Library Classes</a></dt><dd class="calibre8"></dd><dt class="calibre57">channel property, <a class="indexterm" href="#calibre_link-1087">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">Channels class, <a class="indexterm" href="#calibre_link-1088">Crunch Libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">channels property, <a class="indexterm" href="#calibre_link-1089">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">CharSequence interface (Java), <a class="indexterm" href="#calibre_link-1090">Avro Data Types and Schemas</a></dt><dd class="calibre8"></dd><dt class="calibre57">CHECKPOINT write mode, <a class="indexterm" href="#calibre_link-1091">Existing outputs</a>, <a class="indexterm" href="#calibre_link-1092">Checkpointing a Pipeline</a></dt><dd class="calibre8"></dd><dt class="calibre57">checkpointing process</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">running against namenodes, <a class="indexterm" href="#calibre_link-1093">The filesystem image and edit log</a></dt><dd class="calibre8"></dd><dt class="calibre57">running against pipelines, <a class="indexterm" href="#calibre_link-1094">Checkpointing a Pipeline</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">ChecksumFileSystem class, <a class="indexterm" href="#calibre_link-1095">Data Integrity</a>, <a class="indexterm" href="#calibre_link-1096">ChecksumFileSystem</a></dt><dd class="calibre8"></dd><dt class="calibre57">checksumming data, <a class="indexterm" href="#calibre_link-1097">Data Integrity</a>–<a class="indexterm" href="#calibre_link-1098">ChecksumFileSystem</a></dt><dd class="calibre8"></dd><dt class="calibre57">clear command, <a class="indexterm" href="#calibre_link-1099">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">Closeable interface (Java), <a class="indexterm" href="#calibre_link-1100">Writing a SequenceFile</a></dt><dd class="calibre8"></dd><dt class="calibre57">CLOSED state (ZooKeeper), <a class="indexterm" href="#calibre_link-1101">States</a></dt><dd class="calibre8"></dd><dt class="calibre57">Cloudera’s Distribution including Apache Hadoop (CDH), <a class="indexterm" href="#calibre_link-1102">Cloudera’s Distribution Including Apache
  Hadoop</a>–<a class="indexterm" href="#calibre_link-1103">Cloudera’s Distribution Including Apache
  Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">CLUSTER BY clause (Hive), <a class="indexterm" href="#calibre_link-1104">Sorting and Aggregating</a></dt><dd class="calibre8"></dd><dt class="calibre57">cluster managers, <a class="indexterm" href="#calibre_link-1105">Executors and Cluster Managers</a></dt><dd class="calibre8"></dd><dt class="calibre57">CLUSTERED BY clause (Hive), <a class="indexterm" href="#calibre_link-1106">Buckets</a></dt><dd class="calibre8"></dd><dt class="calibre57">clusterID identifier, <a class="indexterm" href="#calibre_link-1107">Namenode directory structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">ClusterMapReduceTestCase class, <a class="indexterm" href="#calibre_link-1108">Testing the Driver</a></dt><dd class="calibre8"></dd><dt class="calibre57">clusters</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">administration tools and, <a class="indexterm" href="#calibre_link-1109">dfsadmin</a>–<a class="indexterm" href="#calibre_link-1110">Balancer</a></dt><dd class="calibre8"></dd><dt class="calibre57">audit logging and, <a class="indexterm" href="#calibre_link-1111">Audit Logging</a></dt><dd class="calibre8"></dd><dt class="calibre57">balancing, <a class="indexterm" href="#calibre_link-1112">Keeping an HDFS Cluster Balanced</a></dt><dd class="calibre8"></dd><dt class="calibre57">benchmarking, <a class="indexterm" href="#calibre_link-1113">Benchmarking a Hadoop Cluster</a>–<a class="indexterm" href="#calibre_link-1114">User Jobs</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hadoop configuration</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">additional properties, <a class="indexterm" href="#calibre_link-1115">Other Hadoop Properties</a></dt><dd class="calibre8"></dd><dt class="calibre57">configuration files, <a class="indexterm" href="#calibre_link-1116">Hadoop Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">configuration management, <a class="indexterm" href="#calibre_link-1117">Configuration Management</a></dt><dd class="calibre8"></dd><dt class="calibre57">daemon addresses and ports, <a class="indexterm" href="#calibre_link-1118">Hadoop Daemon Addresses and Ports</a>–<a class="indexterm" href="#calibre_link-1119">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">daemon properties, <a class="indexterm" href="#calibre_link-1120">Important Hadoop Daemon Properties</a>–<a class="indexterm" href="#calibre_link-1121">CPU settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">environment variables, <a class="indexterm" href="#calibre_link-1122">Hadoop Configuration</a>, <a class="indexterm" href="#calibre_link-1123">Environment Settings</a>–<a class="indexterm" href="#calibre_link-1124">SSH settings</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">maintenance considerations, <a class="indexterm" href="#calibre_link-1125">Metadata backups</a>–<a class="indexterm" href="#calibre_link-1126">Finalize the upgrade (optional)</a></dt><dd class="calibre8"></dd><dt class="calibre57">monitoring, <a class="indexterm" href="#calibre_link-1127">Monitoring</a>–<a class="indexterm" href="#calibre_link-1128">Metrics and JMX</a></dt><dd class="calibre8"></dd><dt class="calibre57">network topology, <a class="indexterm" href="#calibre_link-1129">Network Topology</a>–<a class="indexterm" href="#calibre_link-1130">Rack awareness</a></dt><dd class="calibre8"></dd><dt class="calibre57">persistent data structures and, <a class="indexterm" href="#calibre_link-1131">Persistent Data Structures</a>–<a class="indexterm" href="#calibre_link-1132">Datanode directory structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">running MapReduce applications on, <a class="indexterm" href="#calibre_link-1133">Running on a Cluster</a>–<a class="indexterm" href="#calibre_link-1134">Remote Debugging</a></dt><dd class="calibre8"></dd><dt class="calibre57">safe mode and, <a class="indexterm" href="#calibre_link-1135">Safe Mode</a>–<a class="indexterm" href="#calibre_link-1136">Entering and leaving safe mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">security considerations, <a class="indexterm" href="#calibre_link-1137">Security</a>–<a class="indexterm" href="#calibre_link-1138">Other Security Enhancements</a></dt><dd class="calibre8"></dd><dt class="calibre57">setup and installation</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">configuring Hadoop, <a class="indexterm" href="#calibre_link-1139">Configuring Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">configuring SSH, <a class="indexterm" href="#calibre_link-1140">Configuring SSH</a></dt><dd class="calibre8"></dd><dt class="calibre57">creating Unix user accounts, <a class="indexterm" href="#calibre_link-1141">Creating Unix User Accounts</a></dt><dd class="calibre8"></dd><dt class="calibre57">creating user directories, <a class="indexterm" href="#calibre_link-1142">Creating User Directories</a></dt><dd class="calibre8"></dd><dt class="calibre57">formatting HDFS filesystem, <a class="indexterm" href="#calibre_link-1143">Formatting the HDFS Filesystem</a></dt><dd class="calibre8"></dd><dt class="calibre57">installation options, <a class="indexterm" href="#calibre_link-1144">Setting Up a Hadoop Cluster</a></dt><dd class="calibre8"></dd><dt class="calibre57">installing Hadoop, <a class="indexterm" href="#calibre_link-1145">Installing Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">installing Java, <a class="indexterm" href="#calibre_link-1146">Installing Java</a></dt><dd class="calibre8"></dd><dt class="calibre57">starting and stopping daemons, <a class="indexterm" href="#calibre_link-1147">Starting and Stopping the Daemons</a>–<a class="indexterm" href="#calibre_link-1148">Starting and Stopping the Daemons</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">sizing, <a class="indexterm" href="#calibre_link-1149">Cluster Sizing</a>–<a class="indexterm" href="#calibre_link-1150">Master node scenarios</a></dt><dd class="calibre8"></dd><dt class="calibre57">specifications for, <a class="indexterm" href="#calibre_link-1151">Cluster Specification</a>–<a class="indexterm" href="#calibre_link-1152">Rack awareness</a></dt><dd class="calibre8"></dd><dt class="calibre57">testing in miniclusters, <a class="indexterm" href="#calibre_link-1153">Testing the Driver</a></dt><dd class="calibre8"></dd><dt class="calibre57">upgrading, <a class="indexterm" href="#calibre_link-1154">Upgrades</a>–<a class="indexterm" href="#calibre_link-1155">Finalize the upgrade (optional)</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">CodecPool class, <a class="indexterm" href="#calibre_link-1156">CodecPool</a></dt><dd class="calibre8"></dd><dt class="calibre57">codecs</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1157">Codecs</a></dt><dd class="calibre8"></dd><dt class="calibre57">compressing streams, <a class="indexterm" href="#calibre_link-1158">Compressing and decompressing streams with
        CompressionCodec</a></dt><dd class="calibre8"></dd><dt class="calibre57">decompressing streams, <a class="indexterm" href="#calibre_link-1159">Compressing and decompressing streams with
        CompressionCodec</a></dt><dd class="calibre8"></dd><dt class="calibre57">inferring from file extensions, <a class="indexterm" href="#calibre_link-1160">Inferring CompressionCodecs using
        CompressionCodecFactory</a></dt><dd class="calibre8"></dd><dt class="calibre57">list of supported compression formats, <a class="indexterm" href="#calibre_link-1161">Codecs</a></dt><dd class="calibre8"></dd><dt class="calibre57">native libraries, <a class="indexterm" href="#calibre_link-1162">Native libraries</a>–<a class="indexterm" href="#calibre_link-1163">CodecPool</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">codegen tool, <a class="indexterm" href="#calibre_link-1164">Generated Code</a>, <a class="indexterm" href="#calibre_link-1165">Exports and SequenceFiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">Cogroup class, <a class="indexterm" href="#calibre_link-1166">Crunch Libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">COGROUP statement (Pig Latin), <a class="indexterm" href="#calibre_link-1167">Statements</a>, <a class="indexterm" href="#calibre_link-1168">COGROUP</a>–<a class="indexterm" href="#calibre_link-1169">COGROUP</a></dt><dd class="calibre8"></dd><dt class="calibre57">coherency models (filesystems), <a class="indexterm" href="#calibre_link-1170">Coherency Model</a>–<a class="indexterm" href="#calibre_link-1171">Consequences for application design</a></dt><dd class="calibre8"></dd><dt class="calibre57">Collection interface (Java), <a class="indexterm" href="#calibre_link-1172">PObject</a></dt><dd class="calibre8"></dd><dt class="calibre57">column-oriented storage</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1173">Other File Formats and Column-Oriented Formats</a>–<a class="indexterm" href="#calibre_link-1174">Other File Formats and Column-Oriented Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">Parquet and, <a class="indexterm" href="#calibre_link-1175">Parquet</a>–<a class="indexterm" href="#calibre_link-1176">Parquet MapReduce</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">com.sun.management.jmxremote.port property, <a class="indexterm" href="#calibre_link-1177">Metrics and JMX</a></dt><dd class="calibre8"></dd><dt class="calibre57">CombineFileInputFormat class, <a class="indexterm" href="#calibre_link-1178">Small files and CombineFileInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">combiner functions</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1179">Combiner Functions</a>–<a class="indexterm" href="#calibre_link-1180">Specifying a combiner function</a></dt><dd class="calibre8"></dd><dt class="calibre57">general form, <a class="indexterm" href="#calibre_link-1181">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">shuffle and sort, <a class="indexterm" href="#calibre_link-1182">The Reduce Side</a></dt><dd class="calibre8"></dd><dt class="calibre57">tuning checklist, <a class="indexterm" href="#calibre_link-1183">Tuning a Job</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">COMBINE_INPUT_RECORDS counter, <a class="indexterm" href="#calibre_link-1184">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">COMBINE_OUTPUT_RECORDS counter, <a class="indexterm" href="#calibre_link-1185">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">command-line interface</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1186">The Command-Line Interface</a>–<a class="indexterm" href="#calibre_link-1187">Basic Filesystem Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">displaying SequenceFile with, <a class="indexterm" href="#calibre_link-1188">Displaying a SequenceFile with the command-line
        interface</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive support, <a class="indexterm" href="#calibre_link-1189">Hive Services</a>–<a class="indexterm" href="#calibre_link-1190">Hive clients</a></dt><dd class="calibre8"></dd><dt class="calibre57">Pig Latin support, <a class="indexterm" href="#calibre_link-1191">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">running MapReduce jobs from, <a class="indexterm" href="#calibre_link-1192">GenericOptionsParser, Tool, and ToolRunner</a>–<a class="indexterm" href="#calibre_link-1193">GenericOptionsParser, Tool, and ToolRunner</a></dt><dd class="calibre8"></dd><dt class="calibre57">running miniclusters from, <a class="indexterm" href="#calibre_link-1194">Testing the Driver</a></dt><dd class="calibre8"></dd><dt class="calibre57">ZooKeeper support, <a class="indexterm" href="#calibre_link-1195">ZooKeeper command-line tools</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">comments (Pig Latin), <a class="indexterm" href="#calibre_link-1196">Structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">commissioning nodes, <a class="indexterm" href="#calibre_link-1197">Commissioning and Decommissioning Nodes</a>–<a class="indexterm" href="#calibre_link-1198">Commissioning new nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">COMMITTED_HEAP_BYTES counter, <a class="indexterm" href="#calibre_link-1199">Task counters</a>, <a class="indexterm" href="#calibre_link-1200">Memory settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">Comparable interface (Java), <a class="indexterm" href="#calibre_link-1201">WritableComparable and comparators</a></dt><dd class="calibre8"></dd><dt class="calibre57">Comparator interface (Java), <a class="indexterm" href="#calibre_link-1202">WritableComparable and comparators</a></dt><dd class="calibre8"></dd><dt class="calibre57">compatibility, upgrades and, <a class="indexterm" href="#calibre_link-1203">Upgrades</a></dt><dd class="calibre8"></dd><dt class="calibre57">CompositeInputFormat class, <a class="indexterm" href="#calibre_link-1204">Map-Side Joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">compression</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1205">Compression</a></dt><dd class="calibre8"></dd><dt class="calibre57">codecs and, <a class="indexterm" href="#calibre_link-1206">Codecs</a>–<a class="indexterm" href="#calibre_link-1207">CodecPool</a></dt><dd class="calibre8"></dd><dt class="calibre57">input splits and, <a class="indexterm" href="#calibre_link-1208">Compression and Input Splits</a></dt><dd class="calibre8"></dd><dt class="calibre57">list of supported formats, <a class="indexterm" href="#calibre_link-1209">Compression</a></dt><dd class="calibre8"></dd><dt class="calibre57">map output and, <a class="indexterm" href="#calibre_link-1210">The Map Side</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapReduce and, <a class="indexterm" href="#calibre_link-1211">Compression and Input Splits</a>–<a class="indexterm" href="#calibre_link-1212">Compressing map output</a></dt><dd class="calibre8"></dd><dt class="calibre57">Parquet and, <a class="indexterm" href="#calibre_link-1213">Parquet File Format</a></dt><dd class="calibre8"></dd><dt class="calibre57">selecting format to use, <a class="indexterm" href="#calibre_link-1214">Compression and Input Splits</a></dt><dd class="calibre8"></dd><dt class="calibre57">SequenceFiles, <a class="indexterm" href="#calibre_link-1215">The SequenceFile format</a></dt><dd class="calibre8"></dd><dt class="calibre57">tuning checklist, <a class="indexterm" href="#calibre_link-1216">Tuning a Job</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">CompressionCodec interface</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1217">Codecs</a></dt><dd class="calibre8"></dd><dt class="calibre57">compressing streams, <a class="indexterm" href="#calibre_link-1218">Compressing and decompressing streams with
        CompressionCodec</a></dt><dd class="calibre8"></dd><dt class="calibre57">decompressing streams, <a class="indexterm" href="#calibre_link-1219">Compressing and decompressing streams with
        CompressionCodec</a></dt><dd class="calibre8"></dd><dt class="calibre57">inferring codecs, <a class="indexterm" href="#calibre_link-1220">Inferring CompressionCodecs using
        CompressionCodecFactory</a>–<a class="indexterm" href="#calibre_link-1221">Inferring CompressionCodecs using
        CompressionCodecFactory</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">CompressionCodecFactory class, <a class="indexterm" href="#calibre_link-1222">Inferring CompressionCodecs using
        CompressionCodecFactory</a></dt><dd class="calibre8"></dd><dt class="calibre57">CompressionInputStream class, <a class="indexterm" href="#calibre_link-1223">Compressing and decompressing streams with
        CompressionCodec</a></dt><dd class="calibre8"></dd><dt class="calibre57">CompressionOutputStream class, <a class="indexterm" href="#calibre_link-1224">Compressing and decompressing streams with
        CompressionCodec</a></dt><dd class="calibre8"></dd><dt class="calibre57">CONCAT function (Pig Latin), <a class="indexterm" href="#calibre_link-1225">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">conf command (ZooKeeper), <a class="indexterm" href="#calibre_link-1226">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">Configurable interface, <a class="indexterm" href="#calibre_link-1227">GenericOptionsParser, Tool, and ToolRunner</a></dt><dd class="calibre8"></dd><dt class="calibre57">Configuration class</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1228">Reading Data Using the FileSystem API</a>, <a class="indexterm" href="#calibre_link-1229">The Configuration API</a>–<a class="indexterm" href="#calibre_link-1230">The Configuration API</a>, <a class="indexterm" href="#calibre_link-1231">GenericOptionsParser, Tool, and ToolRunner</a></dt><dd class="calibre8"></dd><dt class="calibre57">combining resources, <a class="indexterm" href="#calibre_link-1232">Combining Resources</a></dt><dd class="calibre8"></dd><dt class="calibre57">side data distribution, <a class="indexterm" href="#calibre_link-1233">Using the Job Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">variable expansion, <a class="indexterm" href="#calibre_link-1234">Variable Expansion</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">configuration files, listed, <a class="indexterm" href="#calibre_link-1235">Hadoop Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">Configured class, <a class="indexterm" href="#calibre_link-1236">GenericOptionsParser, Tool, and ToolRunner</a></dt><dd class="calibre8"></dd><dt class="calibre57">CONNECTED state (ZooKeeper), <a class="indexterm" href="#calibre_link-1237">States</a></dt><dd class="calibre8"></dd><dt class="calibre57">CONNECTING state (ZooKeeper), <a class="indexterm" href="#calibre_link-1238">States</a></dt><dd class="calibre8"></dd><dt class="calibre57">Connection interface, <a class="indexterm" href="#calibre_link-1239">Java</a></dt><dd class="calibre8"></dd><dt class="calibre57">ConnectionDriverName class, <a class="indexterm" href="#calibre_link-1240">The Metastore</a></dt><dd class="calibre8"></dd><dt class="calibre57">ConnectionFactory class, <a class="indexterm" href="#calibre_link-1241">Java</a></dt><dd class="calibre8"></dd><dt class="calibre57">ConnectionPassword class, <a class="indexterm" href="#calibre_link-1242">The Metastore</a></dt><dd class="calibre8"></dd><dt class="calibre57">ConnectionURL class, <a class="indexterm" href="#calibre_link-1243">The Metastore</a></dt><dd class="calibre8"></dd><dt class="calibre57">ConnectionUserName class, <a class="indexterm" href="#calibre_link-1244">The Metastore</a></dt><dd class="calibre8"></dd><dt class="calibre57">connectors (Sqoop), <a class="indexterm" href="#calibre_link-1245">Sqoop Connectors</a></dt><dd class="calibre8"></dd><dt class="calibre57">ConnectTimeout SSH setting, <a class="indexterm" href="#calibre_link-1246">SSH settings</a></dt><dd class="calibre8"></dd><dt class="calibre57">cons command (ZooKeeper), <a class="indexterm" href="#calibre_link-1247">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">containers</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1248">Anatomy of a YARN Application Run</a></dt><dd class="calibre8"></dd><dt class="calibre57">jobtrackers and, <a class="indexterm" href="#calibre_link-1249">YARN Compared to MapReduce 1</a></dt><dd class="calibre8"></dd><dt class="calibre57">virtual memory constraints, <a class="indexterm" href="#calibre_link-1250">Memory settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">control flow statements, <a class="indexterm" href="#calibre_link-1251">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">control-flow nodes, <a class="indexterm" href="#calibre_link-1252">Apache Oozie</a>, <a class="indexterm" href="#calibre_link-1253">Defining an Oozie workflow</a></dt><dd class="calibre8"></dd><dt class="calibre57">converting Hive data types, <a class="indexterm" href="#calibre_link-1254">Conversions</a></dt><dd class="calibre8"></dd><dt class="calibre57">coordinator engines, <a class="indexterm" href="#calibre_link-1255">Apache Oozie</a></dt><dd class="calibre8"></dd><dt class="calibre57">copyFromLocal command, <a class="indexterm" href="#calibre_link-1256">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">copyToLocal command, <a class="indexterm" href="#calibre_link-1257">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">core-site.xml file, <a class="indexterm" href="#calibre_link-1258">Hadoop Configuration</a>, <a class="indexterm" href="#calibre_link-1259">Important Hadoop Daemon Properties</a>, <a class="indexterm" href="#calibre_link-1260">An example</a></dt><dd class="calibre8"></dd><dt class="calibre57">COUNT function (Pig Latin), <a class="indexterm" href="#calibre_link-1261">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">counters</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1262">Counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">built-in, <a class="indexterm" href="#calibre_link-1263">Built-in Counters</a>–<a class="indexterm" href="#calibre_link-1264">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">Crunch support, <a class="indexterm" href="#calibre_link-1265">Running a Pipeline</a></dt><dd class="calibre8"></dd><dt class="calibre57">dynamic, <a class="indexterm" href="#calibre_link-1266">Dynamic counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">HBase and, <a class="indexterm" href="#calibre_link-1267">Counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">metrics and, <a class="indexterm" href="#calibre_link-1268">Metrics and JMX</a></dt><dd class="calibre8"></dd><dt class="calibre57">retrieving, <a class="indexterm" href="#calibre_link-1269">Retrieving counters</a>–<a class="indexterm" href="#calibre_link-1270">Retrieving counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">user-defined Java, <a class="indexterm" href="#calibre_link-1271">User-Defined Java Counters</a>–<a class="indexterm" href="#calibre_link-1272">Retrieving counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">user-defined Streaming, <a class="indexterm" href="#calibre_link-1273">User-Defined Streaming Counters</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Counters class, <a class="indexterm" href="#calibre_link-1274">Retrieving counters</a>, <a class="indexterm" href="#calibre_link-1275">MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">COUNT_STAR function (Pig Latin), <a class="indexterm" href="#calibre_link-1276">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">cp command, <a class="indexterm" href="#calibre_link-1277">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">CPU_MILLISECONDS counter, <a class="indexterm" href="#calibre_link-1278">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">CRC-32 (cyclic redundancy check), <a class="indexterm" href="#calibre_link-1279">Data Integrity</a></dt><dd class="calibre8"></dd><dt class="calibre57">CREATE DATABASE statement (Hive), <a class="indexterm" href="#calibre_link-1280">Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">CREATE FUNCTION statement (Hive), <a class="indexterm" href="#calibre_link-1281">Writing a UDF</a></dt><dd class="calibre8"></dd><dt class="calibre57">create operation (ZooKeeper), <a class="indexterm" href="#calibre_link-1282">Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">CREATE permission (ACL), <a class="indexterm" href="#calibre_link-1283">ACLs</a></dt><dd class="calibre8"></dd><dt class="calibre57">CREATE TABLE statement (Hive), <a class="indexterm" href="#calibre_link-1284">An Example</a>, <a class="indexterm" href="#calibre_link-1285">Managed Tables and External Tables</a>, <a class="indexterm" href="#calibre_link-1286">Binary storage formats: Sequence files, Avro datafiles, Parquet
        files, RCFiles, and ORCFiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">CREATE TABLE...AS SELECT statement (Hive), <a class="indexterm" href="#calibre_link-1287">CREATE TABLE...AS SELECT</a></dt><dd class="calibre8"></dd><dt class="calibre57">Crick, Francis, <a class="indexterm" href="#calibre_link-1288">The Structure of DNA</a></dt><dd class="calibre8"></dd><dt class="calibre57">CROSS statement (Pig Latin), <a class="indexterm" href="#calibre_link-1289">Statements</a>, <a class="indexterm" href="#calibre_link-1290">CROSS</a></dt><dd class="calibre8"></dd><dt class="calibre57">crst command (ZooKeeper), <a class="indexterm" href="#calibre_link-1291">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">Crunch</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1292">Crunch</a></dt><dd class="calibre8"></dd><dt class="calibre57">Cerner case study and, <a class="indexterm" href="#calibre_link-1293">Enter Apache Crunch</a></dt><dd class="calibre8"></dd><dt class="calibre57">functions, <a class="indexterm" href="#calibre_link-1294">Functions</a>–<a class="indexterm" href="#calibre_link-1295">Object reuse</a></dt><dd class="calibre8"></dd><dt class="calibre57">libraries supported, <a class="indexterm" href="#calibre_link-1296">Crunch Libraries</a>–<a class="indexterm" href="#calibre_link-1297">Crunch Libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">materialization process, <a class="indexterm" href="#calibre_link-1298">Materialization</a>–<a class="indexterm" href="#calibre_link-1299">PObject</a></dt><dd class="calibre8"></dd><dt class="calibre57">pipeline execution, <a class="indexterm" href="#calibre_link-1300">Pipeline Execution</a>–<a class="indexterm" href="#calibre_link-1301">Inspecting a Crunch Plan</a></dt><dd class="calibre8"></dd><dt class="calibre57">primitive operations, <a class="indexterm" href="#calibre_link-1302">Primitive Operations</a>–<a class="indexterm" href="#calibre_link-1303">combineValues()</a></dt><dd class="calibre8"></dd><dt class="calibre57">records and tuples, <a class="indexterm" href="#calibre_link-1304">Records and tuples</a>–<a class="indexterm" href="#calibre_link-1305">Records and tuples</a></dt><dd class="calibre8"></dd><dt class="calibre57">sorting data, <a class="indexterm" href="#calibre_link-1306">Total Sort</a></dt><dd class="calibre8"></dd><dt class="calibre57">sources and targets, <a class="indexterm" href="#calibre_link-1307">Sources and Targets</a>–<a class="indexterm" href="#calibre_link-1308">Combined sources and targets</a></dt><dd class="calibre8"></dd><dt class="calibre57">types supported, <a class="indexterm" href="#calibre_link-1309">Types</a>–<a class="indexterm" href="#calibre_link-1310">Records and tuples</a></dt><dd class="calibre8"></dd><dt class="calibre57">weather dataset example, <a class="indexterm" href="#calibre_link-1311">An Example</a>–<a class="indexterm" href="#calibre_link-1312">An Example</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">crunch.log.job.progress property, <a class="indexterm" href="#calibre_link-1313">Debugging</a></dt><dd class="calibre8"></dd><dt class="calibre57">cTime property, <a class="indexterm" href="#calibre_link-1314">Namenode directory structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">CUBE statement (Pig Latin), <a class="indexterm" href="#calibre_link-1315">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">Cutting, Doug, <a class="indexterm" href="#calibre_link-1316">A Brief History of Apache Hadoop</a>–<a class="indexterm" href="#calibre_link-1317">A Brief History of Apache Hadoop</a>, <a class="indexterm" href="#calibre_link-1318">Avro</a></dt><dd class="calibre8"></dd><dt class="calibre57">cyclic redundancy check (CRC-32), <a class="indexterm" href="#calibre_link-1319">Data Integrity</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">D</h3><dl class="book"><dt class="calibre57">daemons</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">addresses and ports, <a class="indexterm" href="#calibre_link-1320">Hadoop Daemon Addresses and Ports</a>–<a class="indexterm" href="#calibre_link-1321">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">balancer tool, <a class="indexterm" href="#calibre_link-1322">Balancer</a></dt><dd class="calibre8"></dd><dt class="calibre57">configuration properties, <a class="indexterm" href="#calibre_link-1323">Important Hadoop Daemon Properties</a>–<a class="indexterm" href="#calibre_link-1324">CPU settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">logfile support, <a class="indexterm" href="#calibre_link-1325">Hadoop Logs</a>, <a class="indexterm" href="#calibre_link-1326">System logfiles</a>, <a class="indexterm" href="#calibre_link-1327">Logging</a></dt><dd class="calibre8"></dd><dt class="calibre57">memory requirements, <a class="indexterm" href="#calibre_link-1328">Memory heap size</a></dt><dd class="calibre8"></dd><dt class="calibre57">starting and stopping, <a class="indexterm" href="#calibre_link-1329">Starting and Stopping the Daemons</a>–<a class="indexterm" href="#calibre_link-1330">Starting and Stopping the Daemons</a>, <a class="indexterm" href="#calibre_link-1331">Starting and stopping the daemons</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">DAGs (directed acyclic graphs), <a class="indexterm" href="#calibre_link-1332">JobControl</a>, <a class="indexterm" href="#calibre_link-1333">Pipeline Execution</a>, <a class="indexterm" href="#calibre_link-1334">DAG Construction</a>–<a class="indexterm" href="#calibre_link-1335">DAG Construction</a></dt><dd class="calibre8"></dd><dt class="calibre57">data integrity</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1336">Data Integrity</a></dt><dd class="calibre8"></dd><dt class="calibre57">ChecksumFileSystem class, <a class="indexterm" href="#calibre_link-1337">ChecksumFileSystem</a></dt><dd class="calibre8"></dd><dt class="calibre57">HDFs support, <a class="indexterm" href="#calibre_link-1338">Data Integrity in HDFS</a></dt><dd class="calibre8"></dd><dt class="calibre57">LocalFileSystem class, <a class="indexterm" href="#calibre_link-1339">LocalFileSystem</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">data local tasks, <a class="indexterm" href="#calibre_link-1340">Task Assignment</a></dt><dd class="calibre8"></dd><dt class="calibre57">data locality, <a class="indexterm" href="#calibre_link-1341">Grid Computing</a></dt><dd class="calibre8"></dd><dt class="calibre57">data locality optimization, <a class="indexterm" href="#calibre_link-1342">Data Flow</a></dt><dd class="calibre8"></dd><dt class="calibre57">data queue, <a class="indexterm" href="#calibre_link-1343">Anatomy of a File Write</a></dt><dd class="calibre8"></dd><dt class="calibre57">data storage and analysis</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1344">Data Storage and Analysis</a></dt><dd class="calibre8"></dd><dt class="calibre57">analyzing data with Hadoop, <a class="indexterm" href="#calibre_link-1345">Analyzing the Data with Hadoop</a>–<a class="indexterm" href="#calibre_link-1346">A test run</a></dt><dd class="calibre8"></dd><dt class="calibre57">analyzing data with Unix tools, <a class="indexterm" href="#calibre_link-1347">Analyzing the Data with Unix Tools</a>–<a class="indexterm" href="#calibre_link-1348">Analyzing the Data with Unix Tools</a></dt><dd class="calibre8"></dd><dt class="calibre57">column-oriented formats, <a class="indexterm" href="#calibre_link-1349">Other File Formats and Column-Oriented Formats</a>–<a class="indexterm" href="#calibre_link-1350">Other File Formats and Column-Oriented Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">HDFS blocks and, <a class="indexterm" href="#calibre_link-1351">Blocks</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive tables, <a class="indexterm" href="#calibre_link-1352">Storage Formats</a>–<a class="indexterm" href="#calibre_link-1353">Storage handlers</a></dt><dd class="calibre8"></dd><dt class="calibre57">scaling out, <a class="indexterm" href="#calibre_link-1354">Scaling Out</a>–<a class="indexterm" href="#calibre_link-1355">Running a Distributed MapReduce Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">system comparisons, <a class="indexterm" href="#calibre_link-1356">Comparison with Other Systems</a>–<a class="indexterm" href="#calibre_link-1357">Volunteer Computing</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">data structures</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">additional formats, <a class="indexterm" href="#calibre_link-1358">Other File Formats and Column-Oriented Formats</a>–<a class="indexterm" href="#calibre_link-1359">Other File Formats and Column-Oriented Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapFile class, <a class="indexterm" href="#calibre_link-1360">MapFile</a></dt><dd class="calibre8"></dd><dt class="calibre57">persistent, <a class="indexterm" href="#calibre_link-1361">Persistent Data Structures</a>–<a class="indexterm" href="#calibre_link-1362">Datanode directory structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">SequenceFile class, <a class="indexterm" href="#calibre_link-1363">SequenceFile</a>–<a class="indexterm" href="#calibre_link-1364">The SequenceFile format</a></dt><dd class="calibre8"></dd><dt class="calibre57">ZooKeeper and, <a class="indexterm" href="#calibre_link-1365">More Distributed Data Structures and Protocols</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">database formats, <a class="indexterm" href="#calibre_link-1366">Database Input (and Output)</a></dt><dd class="calibre8"></dd><dt class="calibre57">DataBlockScanner class, <a class="indexterm" href="#calibre_link-1367">Data Integrity in HDFS</a></dt><dd class="calibre8"></dd><dt class="calibre57">dataDir property, <a class="indexterm" href="#calibre_link-1368">Resilience and Performance</a></dt><dd class="calibre8"></dd><dt class="calibre57">DataDrivenDBInputFormat class, <a class="indexterm" href="#calibre_link-1369">Imports: A Deeper Look</a></dt><dd class="calibre8"></dd><dt class="calibre57">DataFileReader class, <a class="indexterm" href="#calibre_link-1370">Avro Datafiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">DataFileStream class, <a class="indexterm" href="#calibre_link-1371">Avro Datafiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">DataFileWriter class, <a class="indexterm" href="#calibre_link-1372">Avro Datafiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">DataInput interface (Java), <a class="indexterm" href="#calibre_link-1373">The Writable Interface</a></dt><dd class="calibre8"></dd><dt class="calibre57">dataLogDir property, <a class="indexterm" href="#calibre_link-1374">Resilience and Performance</a></dt><dd class="calibre8"></dd><dt class="calibre57">DataNodeProtocol interface, <a class="indexterm" href="#calibre_link-1375">Other Security Enhancements</a></dt><dd class="calibre8"></dd><dt class="calibre57">datanodes</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">balancer tool and, <a class="indexterm" href="#calibre_link-1376">Balancer</a></dt><dd class="calibre8"></dd><dt class="calibre57">block scanners and, <a class="indexterm" href="#calibre_link-1377">Datanode block scanner</a></dt><dd class="calibre8"></dd><dt class="calibre57">cluster setup and installation, <a class="indexterm" href="#calibre_link-1378">Formatting the HDFS Filesystem</a></dt><dd class="calibre8"></dd><dt class="calibre57">commissioning nodes, <a class="indexterm" href="#calibre_link-1379">Commissioning new nodes</a>–<a class="indexterm" href="#calibre_link-1380">Commissioning new nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">data integrity and, <a class="indexterm" href="#calibre_link-1381">Data Integrity in HDFS</a></dt><dd class="calibre8"></dd><dt class="calibre57">DataStreamer class and, <a class="indexterm" href="#calibre_link-1382">Anatomy of a File Write</a></dt><dd class="calibre8"></dd><dt class="calibre57">decommissioning nodes, <a class="indexterm" href="#calibre_link-1383">Decommissioning old nodes</a>–<a class="indexterm" href="#calibre_link-1384">Decommissioning old nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">DFSInputStream class and, <a class="indexterm" href="#calibre_link-1385">Anatomy of a File Read</a></dt><dd class="calibre8"></dd><dt class="calibre57">directory structure, <a class="indexterm" href="#calibre_link-1386">Datanode directory structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">HBase and, <a class="indexterm" href="#calibre_link-1387">HDFS</a></dt><dd class="calibre8"></dd><dt class="calibre57">master−worker pattern, <a class="indexterm" href="#calibre_link-1388">Namenodes and Datanodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">RAID storage and, <a class="indexterm" href="#calibre_link-1389">Cluster Specification</a></dt><dd class="calibre8"></dd><dt class="calibre57">replica placement, <a class="indexterm" href="#calibre_link-1390">Anatomy of a File Write</a></dt><dd class="calibre8"></dd><dt class="calibre57">starting, <a class="indexterm" href="#calibre_link-1391">Starting and Stopping the Daemons</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">DataOutput interface (Java), <a class="indexterm" href="#calibre_link-1392">The Writable Interface</a></dt><dd class="calibre8"></dd><dt class="calibre57">DataOutputStream class (Java), <a class="indexterm" href="#calibre_link-1393">The Writable Interface</a></dt><dd class="calibre8"></dd><dt class="calibre57">DataStreamer class, <a class="indexterm" href="#calibre_link-1394">Anatomy of a File Write</a>–<a class="indexterm" href="#calibre_link-1395">Anatomy of a File Write</a></dt><dd class="calibre8"></dd><dt class="calibre57">DATA_LOCAL_MAPS counter, <a class="indexterm" href="#calibre_link-1396">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">DatumWriter interface, <a class="indexterm" href="#calibre_link-1397">In-Memory Serialization and Deserialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">DBInputFormat class, <a class="indexterm" href="#calibre_link-1398">Input Splits and Records</a></dt><dd class="calibre8"></dd><dt class="calibre57">DBOutputFormat class, <a class="indexterm" href="#calibre_link-1399">Database Input (and Output)</a></dt><dd class="calibre8"></dd><dt class="calibre57">DBWritable interface, <a class="indexterm" href="#calibre_link-1400">Imports: A Deeper Look</a></dt><dd class="calibre8"></dd><dt class="calibre57">debugging problems</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1401">Debugging a Job</a>–<a class="indexterm" href="#calibre_link-1402">Debugging a Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">Crunch and, <a class="indexterm" href="#calibre_link-1403">Debugging</a></dt><dd class="calibre8"></dd><dt class="calibre57">handling malformed data, <a class="indexterm" href="#calibre_link-1404">Handling malformed data</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapReduce task attempts page, <a class="indexterm" href="#calibre_link-1405">The tasks and task attempts pages</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapReduce tasks page, <a class="indexterm" href="#calibre_link-1406">The tasks and task attempts pages</a></dt><dd class="calibre8"></dd><dt class="calibre57">remotely, <a class="indexterm" href="#calibre_link-1407">Remote Debugging</a></dt><dd class="calibre8"></dd><dt class="calibre57">setting log levels, <a class="indexterm" href="#calibre_link-1408">Setting log levels</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">decommissioning nodes, <a class="indexterm" href="#calibre_link-1409">Decommissioning old nodes</a>–<a class="indexterm" href="#calibre_link-1410">Decommissioning old nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">DefaultCodec class, <a class="indexterm" href="#calibre_link-1411">Codecs</a></dt><dd class="calibre8"></dd><dt class="calibre57">DefaultStringifier class, <a class="indexterm" href="#calibre_link-1412">Using the Job Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">DEFINE statement (Pig Latin), <a class="indexterm" href="#calibre_link-1413">Statements</a>, <a class="indexterm" href="#calibre_link-1414">A Filter UDF</a>, <a class="indexterm" href="#calibre_link-1415">STREAM</a></dt><dd class="calibre8"></dd><dt class="calibre57">DEFLATE compression, <a class="indexterm" href="#calibre_link-1416">Compression</a>–<a class="indexterm" href="#calibre_link-1417">Codecs</a>, <a class="indexterm" href="#calibre_link-1418">Native libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">DeflaterInputStream class (Java), <a class="indexterm" href="#calibre_link-1419">Compressing and decompressing streams with
        CompressionCodec</a></dt><dd class="calibre8"></dd><dt class="calibre57">DeflaterOutputStream class (Java), <a class="indexterm" href="#calibre_link-1420">Compressing and decompressing streams with
        CompressionCodec</a></dt><dd class="calibre8"></dd><dt class="calibre57">delay scheduling, <a class="indexterm" href="#calibre_link-1421">Delay Scheduling</a></dt><dd class="calibre8"></dd><dt class="calibre57">delegation tokens, <a class="indexterm" href="#calibre_link-1422">Delegation Tokens</a></dt><dd class="calibre8"></dd><dt class="calibre57">delete operation (ZooKeeper), <a class="indexterm" href="#calibre_link-1423">Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">DELETE permission (ACL), <a class="indexterm" href="#calibre_link-1424">ACLs</a></dt><dd class="calibre8"></dd><dt class="calibre57">DELETE statement (Hive), <a class="indexterm" href="#calibre_link-1425">Updates, Transactions, and Indexes</a></dt><dd class="calibre8"></dd><dt class="calibre57">DELIMITED keyword (Hive), <a class="indexterm" href="#calibre_link-1426">Using a custom SerDe: RegexSerDe</a></dt><dd class="calibre8"></dd><dt class="calibre57">delimited text storage format, <a class="indexterm" href="#calibre_link-1427">The default storage format: Delimited text</a>–<a class="indexterm" href="#calibre_link-1428">The default storage format: Delimited text</a></dt><dd class="calibre8"></dd><dt class="calibre57">dependencies, job, <a class="indexterm" href="#calibre_link-1429">Packaging dependencies</a></dt><dd class="calibre8"></dd><dt class="calibre57">DESCRIBE operator (Pig Latin), <a class="indexterm" href="#calibre_link-1430">An Example</a>, <a class="indexterm" href="#calibre_link-1431">Comparison with Databases</a>, <a class="indexterm" href="#calibre_link-1432">Statements</a>, <a class="indexterm" href="#calibre_link-1433">Anonymous Relations</a></dt><dd class="calibre8"></dd><dt class="calibre57">DESCRIBE statement (Hive), <a class="indexterm" href="#calibre_link-1434">Operators and Functions</a>, <a class="indexterm" href="#calibre_link-1435">Views</a></dt><dd class="calibre8"></dd><dt class="calibre57">deserialization</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1436">Serialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">Avro support, <a class="indexterm" href="#calibre_link-1437">In-Memory Serialization and Deserialization</a>–<a class="indexterm" href="#calibre_link-1438">The Specific API</a></dt><dd class="calibre8"></dd><dt class="calibre57">column-oriented storage and, <a class="indexterm" href="#calibre_link-1439">Other File Formats and Column-Oriented Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">Text class and, <a class="indexterm" href="#calibre_link-1440">Implementing a Custom Writable</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Deserializer interface, <a class="indexterm" href="#calibre_link-1441">Serialization Frameworks</a></dt><dd class="calibre8"></dd><dt class="calibre57">development environment</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">managing configuration, <a class="indexterm" href="#calibre_link-1442">Managing Configuration</a>–<a class="indexterm" href="#calibre_link-1443">Managing Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">running jobs from command-line, <a class="indexterm" href="#calibre_link-1444">GenericOptionsParser, Tool, and ToolRunner</a>–<a class="indexterm" href="#calibre_link-1445">GenericOptionsParser, Tool, and ToolRunner</a></dt><dd class="calibre8"></dd><dt class="calibre57">setting up, <a class="indexterm" href="#calibre_link-1446">Setting Up the Development Environment</a>–<a class="indexterm" href="#calibre_link-1447">Setting Up the Development Environment</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">df tool, <a class="indexterm" href="#calibre_link-1448">Blocks</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.block.access.token.enable property, <a class="indexterm" href="#calibre_link-1449">Delegation Tokens</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.blocksize property, <a class="indexterm" href="#calibre_link-1450">FileInputFormat input splits</a>, <a class="indexterm" href="#calibre_link-1451">HDFS block size</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.bytes-per-checksum property, <a class="indexterm" href="#calibre_link-1452">Data Integrity in HDFS</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.client.read.shortcircuit property, <a class="indexterm" href="#calibre_link-1453">Short-circuit local reads</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.datanode.address property, <a class="indexterm" href="#calibre_link-1454">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.datanode.balance.bandwidthPerSec property, <a class="indexterm" href="#calibre_link-1455">Balancer</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.datanode.data.dir property, <a class="indexterm" href="#calibre_link-1456">HDFS</a>, <a class="indexterm" href="#calibre_link-1457">YARN</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.datanode.http.address property, <a class="indexterm" href="#calibre_link-1458">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.datanode.ipc.address property, <a class="indexterm" href="#calibre_link-1459">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.datanode.kerberos.principal property, <a class="indexterm" href="#calibre_link-1460">Other Security Enhancements</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.datanode.keytab.file property, <a class="indexterm" href="#calibre_link-1461">Other Security Enhancements</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.datanode.numblocks property, <a class="indexterm" href="#calibre_link-1462">Datanode directory structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.datanode.scan.period.hours property, <a class="indexterm" href="#calibre_link-1463">Datanode block scanner</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.domain.socket.path property, <a class="indexterm" href="#calibre_link-1464">Short-circuit local reads</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.encrypt.data.transfer property, <a class="indexterm" href="#calibre_link-1465">Other Security Enhancements</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.hosts property, <a class="indexterm" href="#calibre_link-1466">Cluster membership</a>, <a class="indexterm" href="#calibre_link-1467">Commissioning new nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.namenode.checkpoint.dir property, <a class="indexterm" href="#calibre_link-1468">HDFS</a>, <a class="indexterm" href="#calibre_link-1469">Secondary namenode directory structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.namenode.checkpoint.period property, <a class="indexterm" href="#calibre_link-1470">The filesystem image and edit log</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.namenode.checkpoint.txns property, <a class="indexterm" href="#calibre_link-1471">The filesystem image and edit log</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.namenode.http-address property, <a class="indexterm" href="#calibre_link-1472">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.namenode.http-bind-host property, <a class="indexterm" href="#calibre_link-1473">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.namenode.name.dir property, <a class="indexterm" href="#calibre_link-1474">HDFS</a>, <a class="indexterm" href="#calibre_link-1475">Namenode directory structure</a>, <a class="indexterm" href="#calibre_link-1476">Secondary namenode directory structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.namenode.replication.min property, <a class="indexterm" href="#calibre_link-1477">Safe Mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.namenode.rpc-bind-host property, <a class="indexterm" href="#calibre_link-1478">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.namenode.safemode.extension property, <a class="indexterm" href="#calibre_link-1479">Safe Mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.namenode.safemode.threshold-pct
                  property, <a class="indexterm" href="#calibre_link-1480">Safe Mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.namenode.secondary.http-address
                  property, <a class="indexterm" href="#calibre_link-1481">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.permissions.enabled property, <a class="indexterm" href="#calibre_link-1482">Basic Filesystem Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.replication property, <a class="indexterm" href="#calibre_link-1483">The Command-Line Interface</a>, <a class="indexterm" href="#calibre_link-1484">Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfs.webhdfs.enabled property, <a class="indexterm" href="#calibre_link-1485">HTTP</a></dt><dd class="calibre8"></dd><dt class="calibre57">dfsadmin tool, <a class="indexterm" href="#calibre_link-1486">Entering and leaving safe mode</a>–<a class="indexterm" href="#calibre_link-1487">dfsadmin</a>, <a class="indexterm" href="#calibre_link-1488">Metadata backups</a></dt><dd class="calibre8"></dd><dt class="calibre57">DFSInputStream class, <a class="indexterm" href="#calibre_link-1489">Anatomy of a File Read</a></dt><dd class="calibre8"></dd><dt class="calibre57">DFSOutputStream class, <a class="indexterm" href="#calibre_link-1490">Anatomy of a File Write</a>–<a class="indexterm" href="#calibre_link-1491">Anatomy of a File Write</a></dt><dd class="calibre8"></dd><dt class="calibre57">DIFF function (Pig Latin), <a class="indexterm" href="#calibre_link-1492">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">digital universe, <a class="indexterm" href="#calibre_link-1493">Data!</a>–<a class="indexterm" href="#calibre_link-1494">Data!</a></dt><dd class="calibre8"></dd><dt class="calibre57">direct-mode imports, <a class="indexterm" href="#calibre_link-1495">Direct-Mode Imports</a></dt><dd class="calibre8"></dd><dt class="calibre57">directed acyclic graphs (DAGs), <a class="indexterm" href="#calibre_link-1496">JobControl</a>, <a class="indexterm" href="#calibre_link-1497">Pipeline Execution</a>, <a class="indexterm" href="#calibre_link-1498">DAG Construction</a>–<a class="indexterm" href="#calibre_link-1499">DAG Construction</a></dt><dd class="calibre8"></dd><dt class="calibre57">directories</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">creating, <a class="indexterm" href="#calibre_link-1500">Directories</a>, <a class="indexterm" href="#calibre_link-1501">Creating User Directories</a>, <a class="indexterm" href="#calibre_link-1502">Creating a user directory</a></dt><dd class="calibre8"></dd><dt class="calibre57">datanode structure, <a class="indexterm" href="#calibre_link-1503">Datanode directory structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">file permissions, <a class="indexterm" href="#calibre_link-1504">Basic Filesystem Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">namenode memory requirements, <a class="indexterm" href="#calibre_link-1505">The Design of HDFS</a></dt><dd class="calibre8"></dd><dt class="calibre57">namenode structure, <a class="indexterm" href="#calibre_link-1506">Namenode directory structure</a>–<a class="indexterm" href="#calibre_link-1507">Namenode directory structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">querying, <a class="indexterm" href="#calibre_link-1508">File metadata: FileStatus</a>–<a class="indexterm" href="#calibre_link-1509">PathFilter</a></dt><dd class="calibre8"></dd><dt class="calibre57">reserved storage space, <a class="indexterm" href="#calibre_link-1510">Reserved storage space</a></dt><dd class="calibre8"></dd><dt class="calibre57">secondary namenode structure, <a class="indexterm" href="#calibre_link-1511">Secondary namenode directory structure</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">distcp program, <a class="indexterm" href="#calibre_link-1512">Parallel Copying with distcp</a>–<a class="indexterm" href="#calibre_link-1513">Keeping an HDFS Cluster Balanced</a>, <a class="indexterm" href="#calibre_link-1514">Data Integrity in HDFS</a></dt><dd class="calibre8"></dd><dt class="calibre57">Distinct class, <a class="indexterm" href="#calibre_link-1515">Crunch Libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">DISTINCT statement (Pig Latin), <a class="indexterm" href="#calibre_link-1516">Statements</a>, <a class="indexterm" href="#calibre_link-1517">FOREACH...GENERATE</a></dt><dd class="calibre8"></dd><dt class="calibre57">DISTRIBUTE BY clause (Hive), <a class="indexterm" href="#calibre_link-1518">Sorting and Aggregating</a></dt><dd class="calibre8"></dd><dt class="calibre57">distributed cache, <a class="indexterm" href="#calibre_link-1519">Distributed Cache</a>–<a class="indexterm" href="#calibre_link-1520">The distributed cache API</a></dt><dd class="calibre8"></dd><dt class="calibre57">DistributedCache class, <a class="indexterm" href="#calibre_link-1521">The distributed cache API</a></dt><dd class="calibre8"></dd><dt class="calibre57">DistributedFileSystem class</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1522">Hadoop Filesystems</a></dt><dd class="calibre8"></dd><dt class="calibre57">FileSystem class and, <a class="indexterm" href="#calibre_link-1523">The Java Interface</a></dt><dd class="calibre8"></dd><dt class="calibre57">HTTP support, <a class="indexterm" href="#calibre_link-1524">HTTP</a></dt><dd class="calibre8"></dd><dt class="calibre57">reading files, <a class="indexterm" href="#calibre_link-1525">Anatomy of a File Read</a></dt><dd class="calibre8"></dd><dt class="calibre57">writing files, <a class="indexterm" href="#calibre_link-1526">Anatomy of a File Write</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">DNA</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">ADAM platform, <a class="indexterm" href="#calibre_link-1527">ADAM, A Scalable Genome Analysis Platform</a>–<a class="indexterm" href="#calibre_link-1528">A simple example: k-mer counting using
        Spark and ADAM</a></dt><dd class="calibre8"></dd><dt class="calibre57">genetic code, <a class="indexterm" href="#calibre_link-1529">The Genetic Code: Turning DNA Letters into Proteins</a></dt><dd class="calibre8"></dd><dt class="calibre57">Human Genome Project, <a class="indexterm" href="#calibre_link-1530">The Human Genome Project and Reference Genomes</a></dt><dd class="calibre8"></dd><dt class="calibre57">sequencing and aligning, <a class="indexterm" href="#calibre_link-1531">Sequencing and Aligning DNA</a></dt><dd class="calibre8"></dd><dt class="calibre57">as source code, <a class="indexterm" href="#calibre_link-1532">Thinking of DNA as Source Code</a>–<a class="indexterm" href="#calibre_link-1533">Thinking of DNA as Source Code</a></dt><dd class="calibre8"></dd><dt class="calibre57">structure of, <a class="indexterm" href="#calibre_link-1534">The Structure of DNA</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">DNSToSwitchMapping interface, <a class="indexterm" href="#calibre_link-1535">Rack awareness</a></dt><dd class="calibre8"></dd><dt class="calibre57">DoFn class</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1536">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">increment() method, <a class="indexterm" href="#calibre_link-1537">Running a Pipeline</a></dt><dd class="calibre8"></dd><dt class="calibre57">scaleFactor() method, <a class="indexterm" href="#calibre_link-1538">parallelDo()</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Dominant Resource Fairness (DRF), <a class="indexterm" href="#calibre_link-1539">Dominant Resource Fairness</a></dt><dd class="calibre8"></dd><dt class="calibre57">dot tool, <a class="indexterm" href="#calibre_link-1540">Inspecting a Crunch Plan</a></dt><dd class="calibre8"></dd><dt class="calibre57">DoubleWritable class, <a class="indexterm" href="#calibre_link-1541">Writable wrappers for Java primitives</a></dt><dd class="calibre8"></dd><dt class="calibre57">Dreadnaught, <a class="indexterm" href="#calibre_link-1542">A Brief History of Apache Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">DRF (Dominant Resource Fairness), <a class="indexterm" href="#calibre_link-1543">Dominant Resource Fairness</a></dt><dd class="calibre8"></dd><dt class="calibre57">DROP DATABASE statement (Hive), <a class="indexterm" href="#calibre_link-1544">Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">DROP FUNCTION statement (Hive), <a class="indexterm" href="#calibre_link-1545">Writing a UDF</a></dt><dd class="calibre8"></dd><dt class="calibre57">DROP TABLE statement (Hive), <a class="indexterm" href="#calibre_link-1546">Managed Tables and External Tables</a>, <a class="indexterm" href="#calibre_link-1547">Dropping Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">dsh shell tool, <a class="indexterm" href="#calibre_link-1548">Configuration Management</a></dt><dd class="calibre8"></dd><dt class="calibre57">Dumbo module (Python), <a class="indexterm" href="#calibre_link-1549">Python</a></dt><dd class="calibre8"></dd><dt class="calibre57">dump command (ZooKeeper), <a class="indexterm" href="#calibre_link-1550">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">DUMP statement (Pig Latin), <a class="indexterm" href="#calibre_link-1551">Statements</a>, <a class="indexterm" href="#calibre_link-1552">Statements</a>, <a class="indexterm" href="#calibre_link-1553">Sorting Data</a>, <a class="indexterm" href="#calibre_link-1554">Anonymous Relations</a></dt><dd class="calibre8"></dd><dt class="calibre57">Dyer, Chris, <a class="indexterm" href="#calibre_link-1555">MapReduce Workflows</a></dt><dd class="calibre8"></dd><dt class="calibre57">dynamic counters, <a class="indexterm" href="#calibre_link-1556">Dynamic counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">dynamic-partition insert, <a class="indexterm" href="#calibre_link-1557">Inserts</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">E</h3><dl class="book"><dt class="calibre57">EC2 computer cloud, <a class="indexterm" href="#calibre_link-1558">A Brief History of Apache Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">edit log, <a class="indexterm" href="#calibre_link-1559">The filesystem image and edit log</a>–<a class="indexterm" href="#calibre_link-1560">The filesystem image and edit log</a></dt><dd class="calibre8"></dd><dt class="calibre57">edits files, <a class="indexterm" href="#calibre_link-1561">Namenode directory structure</a>–<a class="indexterm" href="#calibre_link-1562">The filesystem image and edit log</a>, <a class="indexterm" href="#calibre_link-1563">Metadata backups</a></dt><dd class="calibre8"></dd><dt class="calibre57">Elephant Bird project, <a class="indexterm" href="#calibre_link-1564">Serialization IDL</a></dt><dd class="calibre8"></dd><dt class="calibre57">embedded metastore, <a class="indexterm" href="#calibre_link-1565">The Metastore</a></dt><dd class="calibre8"></dd><dt class="calibre57">EmbeddedAgent class, <a class="indexterm" href="#calibre_link-1566">Integrating Flume with Applications</a></dt><dd class="calibre8"></dd><dt class="calibre57">Encoder class, <a class="indexterm" href="#calibre_link-1567">In-Memory Serialization and Deserialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">encoding, nested, <a class="indexterm" href="#calibre_link-1568">Nested Encoding</a></dt><dd class="calibre8"></dd><dt class="calibre57">EnumSetWritable class, <a class="indexterm" href="#calibre_link-1569">Writable collections</a></dt><dd class="calibre8"></dd><dt class="calibre57">envi command (ZooKeeper), <a class="indexterm" href="#calibre_link-1570">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">environment variables, <a class="indexterm" href="#calibre_link-1571">A test run</a>, <a class="indexterm" href="#calibre_link-1572">Hadoop Configuration</a>, <a class="indexterm" href="#calibre_link-1573">Environment Settings</a>–<a class="indexterm" href="#calibre_link-1574">SSH settings</a></dt><dd class="calibre8"></dd><dt class="calibre57">ephemeral znodes, <a class="indexterm" href="#calibre_link-1575">Ephemeral znodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">escape sequences, <a class="indexterm" href="#calibre_link-1576">Performing an Export</a></dt><dd class="calibre8"></dd><dt class="calibre57">eval functions (Pig Latin), <a class="indexterm" href="#calibre_link-1577">Functions</a>, <a class="indexterm" href="#calibre_link-1578">An Eval UDF</a>–<a class="indexterm" href="#calibre_link-1579">Dynamic invokers</a></dt><dd class="calibre8"></dd><dt class="calibre57">EvalFunc class, <a class="indexterm" href="#calibre_link-1580">A Filter UDF</a></dt><dd class="calibre8"></dd><dt class="calibre57">exceptions (ZooKeeper), <a class="indexterm" href="#calibre_link-1581">The Resilient ZooKeeper Application</a>–<a class="indexterm" href="#calibre_link-1582">A reliable configuration service</a>, <a class="indexterm" href="#calibre_link-1583">Recoverable exceptions</a></dt><dd class="calibre8"></dd><dt class="calibre57">exec command, <a class="indexterm" href="#calibre_link-1584">Statements</a>, <a class="indexterm" href="#calibre_link-1585">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">execute (x) permission, <a class="indexterm" href="#calibre_link-1586">Basic Filesystem Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">exists operation (ZooKeeper), <a class="indexterm" href="#calibre_link-1587">Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">EXPLAIN keyword (Hive), <a class="indexterm" href="#calibre_link-1588">Inner joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">EXPLAIN operator (Pig Latin), <a class="indexterm" href="#calibre_link-1589">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">export process (Sqoop)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1590">Performing an Export</a>–<a class="indexterm" href="#calibre_link-1591">Exports: A Deeper Look</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive and, <a class="indexterm" href="#calibre_link-1592">Performing an Export</a></dt><dd class="calibre8"></dd><dt class="calibre57">SequenceFile and, <a class="indexterm" href="#calibre_link-1593">Exports and SequenceFiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">transactionality and, <a class="indexterm" href="#calibre_link-1594">Exports and Transactionality</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">expressions (Pig Latin), <a class="indexterm" href="#calibre_link-1595">Expressions</a>–<a class="indexterm" href="#calibre_link-1596">Expressions</a></dt><dd class="calibre8"></dd><dt class="calibre57">EXTERNAL keyword (Hive), <a class="indexterm" href="#calibre_link-1597">Managed Tables and External Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">external tables (Hive), <a class="indexterm" href="#calibre_link-1598">Managed Tables and External Tables</a>–<a class="indexterm" href="#calibre_link-1599">Managed Tables and External Tables</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">F</h3><dl class="book"><dt class="calibre57">FAILED_SHUFFLE counter, <a class="indexterm" href="#calibre_link-1600">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">failover controllers, <a class="indexterm" href="#calibre_link-1601">Failover and fencing</a>, <a class="indexterm" href="#calibre_link-1602">Resource Manager Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">failovers</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">sink groups and, <a class="indexterm" href="#calibre_link-1603">Sink Groups</a>–<a class="indexterm" href="#calibre_link-1604">Integrating Flume with Applications</a></dt><dd class="calibre8"></dd><dt class="calibre57">ZooKeeper service and, <a class="indexterm" href="#calibre_link-1605">Sessions</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">failures</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1606">Failures</a></dt><dd class="calibre8"></dd><dt class="calibre57">application master, <a class="indexterm" href="#calibre_link-1607">Application Master Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">node manager, <a class="indexterm" href="#calibre_link-1608">Node Manager Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">resource manager, <a class="indexterm" href="#calibre_link-1609">Resource Manager Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">task, <a class="indexterm" href="#calibre_link-1610">Task Failure</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Fair Scheduler (YARN), <a class="indexterm" href="#calibre_link-1611">Fair Scheduler Configuration</a>–<a class="indexterm" href="#calibre_link-1612">Preemption</a></dt><dd class="calibre8"></dd><dt class="calibre57">fanning out</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1613">Fan Out</a></dt><dd class="calibre8"></dd><dt class="calibre57">delivery guarantees, <a class="indexterm" href="#calibre_link-1614">Delivery Guarantees</a></dt><dd class="calibre8"></dd><dt class="calibre57">replicating and multiplexing selectors, <a class="indexterm" href="#calibre_link-1615">Replicating and Multiplexing Selectors</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">FieldSelectionMapper class, <a class="indexterm" href="#calibre_link-1616">MapReduce Library Classes</a></dt><dd class="calibre8"></dd><dt class="calibre57">FieldSelectionMapReduce class, <a class="indexterm" href="#calibre_link-1617">MapReduce Library Classes</a></dt><dd class="calibre8"></dd><dt class="calibre57">FieldSelectionReducer class, <a class="indexterm" href="#calibre_link-1618">MapReduce Library Classes</a></dt><dd class="calibre8"></dd><dt class="calibre57">FIFO Scheduler (YARN), <a class="indexterm" href="#calibre_link-1619">Scheduler Options</a>–<a class="indexterm" href="#calibre_link-1620">Scheduler Options</a></dt><dd class="calibre8"></dd><dt class="calibre57">File class (Java), <a class="indexterm" href="#calibre_link-1621">Directories</a></dt><dd class="calibre8"></dd><dt class="calibre57">file management</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">compression, <a class="indexterm" href="#calibre_link-1622">Compression</a>–<a class="indexterm" href="#calibre_link-1623">Compressing map output</a>, <a class="indexterm" href="#calibre_link-1624">The SequenceFile format</a></dt><dd class="calibre8"></dd><dt class="calibre57">file patterns, <a class="indexterm" href="#calibre_link-1625">File patterns</a></dt><dd class="calibre8"></dd><dt class="calibre57">file permissions, <a class="indexterm" href="#calibre_link-1626">Basic Filesystem Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">file-based data structures, <a class="indexterm" href="#calibre_link-1627">File-Based Data Structures</a>–<a class="indexterm" href="#calibre_link-1628">Other File Formats and Column-Oriented Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">finding file checksum, <a class="indexterm" href="#calibre_link-1629">Data Integrity in HDFS</a></dt><dd class="calibre8"></dd><dt class="calibre57">listing files, <a class="indexterm" href="#calibre_link-1630">Listing files</a></dt><dd class="calibre8"></dd><dt class="calibre57">Parquet considerations, <a class="indexterm" href="#calibre_link-1631">Writing and Reading Parquet Files</a>–<a class="indexterm" href="#calibre_link-1632">Projection and read schemas</a></dt><dd class="calibre8"></dd><dt class="calibre57">processing files as records, <a class="indexterm" href="#calibre_link-1633">Processing a whole file as a record</a>–<a class="indexterm" href="#calibre_link-1634">Processing a whole file as a record</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">file.bytes-per-checksum property, <a class="indexterm" href="#calibre_link-1635">LocalFileSystem</a></dt><dd class="calibre8"></dd><dt class="calibre57">FileContext class, <a class="indexterm" href="#calibre_link-1636">The Java Interface</a></dt><dd class="calibre8"></dd><dt class="calibre57">FileFilter class (Java), <a class="indexterm" href="#calibre_link-1637">PathFilter</a></dt><dd class="calibre8"></dd><dt class="calibre57">FileInputFormat class, <a class="indexterm" href="#calibre_link-1638">FileInputFormat</a>–<a class="indexterm" href="#calibre_link-1639">FileInputFormat input splits</a>, <a class="indexterm" href="#calibre_link-1640">Built-in Counters</a>, <a class="indexterm" href="#calibre_link-1641">Reading from a source</a></dt><dd class="calibre8"></dd><dt class="calibre57">FileOutputCommitter class, <a class="indexterm" href="#calibre_link-1642">Job Initialization</a>, <a class="indexterm" href="#calibre_link-1643">Output Committers</a></dt><dd class="calibre8"></dd><dt class="calibre57">FileOutputFormat class, <a class="indexterm" href="#calibre_link-1644">Using Compression in MapReduce</a>, <a class="indexterm" href="#calibre_link-1645">Task side-effect files</a>, <a class="indexterm" href="#calibre_link-1646">Built-in Counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">FileSplit class, <a class="indexterm" href="#calibre_link-1647">Processing a whole file as a record</a></dt><dd class="calibre8"></dd><dt class="calibre57">FileStatus class, <a class="indexterm" href="#calibre_link-1648">File metadata: FileStatus</a>–<a class="indexterm" href="#calibre_link-1649">File patterns</a></dt><dd class="calibre8"></dd><dt class="calibre57">FileSystem class</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1650">Hadoop Filesystems</a>, <a class="indexterm" href="#calibre_link-1651">Interfaces</a>, <a class="indexterm" href="#calibre_link-1652">The Java Interface</a></dt><dd class="calibre8"></dd><dt class="calibre57">creating directories, <a class="indexterm" href="#calibre_link-1653">Directories</a></dt><dd class="calibre8"></dd><dt class="calibre57">deleting data, <a class="indexterm" href="#calibre_link-1654">Deleting Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">querying directories, <a class="indexterm" href="#calibre_link-1655">File metadata: FileStatus</a>–<a class="indexterm" href="#calibre_link-1656">PathFilter</a></dt><dd class="calibre8"></dd><dt class="calibre57">reading data, <a class="indexterm" href="#calibre_link-1657">Reading Data Using the FileSystem API</a>–<a class="indexterm" href="#calibre_link-1658">FSDataInputStream</a>, <a class="indexterm" href="#calibre_link-1659">Anatomy of a File Read</a></dt><dd class="calibre8"></dd><dt class="calibre57">verifying checksums, <a class="indexterm" href="#calibre_link-1660">Data Integrity in HDFS</a></dt><dd class="calibre8"></dd><dt class="calibre57">writing data, <a class="indexterm" href="#calibre_link-1661">Writing Data</a>–<a class="indexterm" href="#calibre_link-1662">FSDataOutputStream</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Filesystem in Userspace (FUSE), <a class="indexterm" href="#calibre_link-1663">FUSE</a></dt><dd class="calibre8"></dd><dt class="calibre57">filesystems</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">basic concepts, <a class="indexterm" href="#calibre_link-1664">Blocks</a>–<a class="indexterm" href="#calibre_link-1665">Failover and fencing</a></dt><dd class="calibre8"></dd><dt class="calibre57">basic operations, <a class="indexterm" href="#calibre_link-1666">Basic Filesystem Operations</a>–<a class="indexterm" href="#calibre_link-1667">Basic Filesystem Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">built-in counters, <a class="indexterm" href="#calibre_link-1668">Built-in Counters</a>, <a class="indexterm" href="#calibre_link-1669">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">checking blocks, <a class="indexterm" href="#calibre_link-1670">Filesystem check (fsck)</a>–<a class="indexterm" href="#calibre_link-1671">Finding the blocks for a file</a></dt><dd class="calibre8"></dd><dt class="calibre57">cluster sizing, <a class="indexterm" href="#calibre_link-1672">Master node scenarios</a></dt><dd class="calibre8"></dd><dt class="calibre57">coherency models, <a class="indexterm" href="#calibre_link-1673">Coherency Model</a>–<a class="indexterm" href="#calibre_link-1674">Consequences for application design</a></dt><dd class="calibre8"></dd><dt class="calibre57">formatting for HDFS, <a class="indexterm" href="#calibre_link-1675">Formatting the HDFS Filesystem</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hadoop supported, <a class="indexterm" href="#calibre_link-1676">Hadoop Filesystems</a>–<a class="indexterm" href="#calibre_link-1677">FUSE</a></dt><dd class="calibre8"></dd><dt class="calibre57">high availability, <a class="indexterm" href="#calibre_link-1678">HDFS High Availability</a>–<a class="indexterm" href="#calibre_link-1679">Failover and fencing</a></dt><dd class="calibre8"></dd><dt class="calibre57">Java interface, <a class="indexterm" href="#calibre_link-1680">The Java Interface</a>–<a class="indexterm" href="#calibre_link-1681">Deleting Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">metadata and, <a class="indexterm" href="#calibre_link-1682">The filesystem image and edit log</a>–<a class="indexterm" href="#calibre_link-1683">The filesystem image and edit log</a></dt><dd class="calibre8"></dd><dt class="calibre57">namenodes and, <a class="indexterm" href="#calibre_link-1684">The Design of HDFS</a>, <a class="indexterm" href="#calibre_link-1685">Namenodes and Datanodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">parallel copying with distcp, <a class="indexterm" href="#calibre_link-1686">Parallel Copying with distcp</a>–<a class="indexterm" href="#calibre_link-1687">Keeping an HDFS Cluster Balanced</a></dt><dd class="calibre8"></dd><dt class="calibre57">upgrade considerations, <a class="indexterm" href="#calibre_link-1688">Upgrades</a>–<a class="indexterm" href="#calibre_link-1689">Finalize the upgrade (optional)</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">FileUtil class, <a class="indexterm" href="#calibre_link-1690">Listing files</a></dt><dd class="calibre8"></dd><dt class="calibre57">filter functions (Pig Latin), <a class="indexterm" href="#calibre_link-1691">Functions</a>, <a class="indexterm" href="#calibre_link-1692">A Filter UDF</a>–<a class="indexterm" href="#calibre_link-1693">Leveraging types</a></dt><dd class="calibre8"></dd><dt class="calibre57">FILTER statement (Pig Latin), <a class="indexterm" href="#calibre_link-1694">Statements</a>, <a class="indexterm" href="#calibre_link-1695">FOREACH...GENERATE</a></dt><dd class="calibre8"></dd><dt class="calibre57">FilterFn class, <a class="indexterm" href="#calibre_link-1696">parallelDo()</a></dt><dd class="calibre8"></dd><dt class="calibre57">FilterFunc class, <a class="indexterm" href="#calibre_link-1697">A Filter UDF</a></dt><dd class="calibre8"></dd><dt class="calibre57">FirstKeyOnlyFilter class, <a class="indexterm" href="#calibre_link-1698">MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">FixedLengthInputFormat class, <a class="indexterm" href="#calibre_link-1699">FixedLengthInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">FloatWritable class, <a class="indexterm" href="#calibre_link-1700">Writable wrappers for Java primitives</a></dt><dd class="calibre8"></dd><dt class="calibre57">Flume</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1701">Flume</a>, <a class="indexterm" href="#calibre_link-1702">Crunch</a></dt><dd class="calibre8"></dd><dt class="calibre57">additional information, <a class="indexterm" href="#calibre_link-1703">Further Reading</a></dt><dd class="calibre8"></dd><dt class="calibre57">agent tiers, <a class="indexterm" href="#calibre_link-1704">Distribution: Agent Tiers</a>–<a class="indexterm" href="#calibre_link-1705">Delivery Guarantees</a></dt><dd class="calibre8"></dd><dt class="calibre57">Avro support, <a class="indexterm" href="#calibre_link-1706">File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">batch processing, <a class="indexterm" href="#calibre_link-1707">Batching</a></dt><dd class="calibre8"></dd><dt class="calibre57">component catalog, <a class="indexterm" href="#calibre_link-1708">Component Catalog</a>–<a class="indexterm" href="#calibre_link-1709">Component Catalog</a></dt><dd class="calibre8"></dd><dt class="calibre57">example of, <a class="indexterm" href="#calibre_link-1710">An Example</a>–<a class="indexterm" href="#calibre_link-1711">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">fanning out, <a class="indexterm" href="#calibre_link-1712">Fan Out</a>–<a class="indexterm" href="#calibre_link-1713">Delivery Guarantees</a></dt><dd class="calibre8"></dd><dt class="calibre57">HDFS sinks and, <a class="indexterm" href="#calibre_link-1714">The HDFS Sink</a>–<a class="indexterm" href="#calibre_link-1715">Delivery Guarantees</a></dt><dd class="calibre8"></dd><dt class="calibre57">installing, <a class="indexterm" href="#calibre_link-1716">Installing Flume</a></dt><dd class="calibre8"></dd><dt class="calibre57">integrating with applications, <a class="indexterm" href="#calibre_link-1717">Integrating Flume with Applications</a></dt><dd class="calibre8"></dd><dt class="calibre57">sink groups, <a class="indexterm" href="#calibre_link-1718">Sink Groups</a>–<a class="indexterm" href="#calibre_link-1719">Sink Groups</a></dt><dd class="calibre8"></dd><dt class="calibre57">transactions and reliability, <a class="indexterm" href="#calibre_link-1720">Transactions and Reliability</a>–<a class="indexterm" href="#calibre_link-1721">Batching</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">flume-ng command, <a class="indexterm" href="#calibre_link-1722">Installing Flume</a>, <a class="indexterm" href="#calibre_link-1723">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">FlumeJava library, <a class="indexterm" href="#calibre_link-1724">Crunch</a></dt><dd class="calibre8"></dd><dt class="calibre57">Folding@home project, <a class="indexterm" href="#calibre_link-1725">Volunteer Computing</a></dt><dd class="calibre8"></dd><dt class="calibre57">FOREACH...GENERATE statement (Pig Latin), <a class="indexterm" href="#calibre_link-1726">Statements</a>, <a class="indexterm" href="#calibre_link-1727">Statements</a>, <a class="indexterm" href="#calibre_link-1728">FOREACH...GENERATE</a></dt><dd class="calibre8"></dd><dt class="calibre57">From class, <a class="indexterm" href="#calibre_link-1729">Reading from a source</a></dt><dd class="calibre8"></dd><dt class="calibre57">FROM clause (Hive), <a class="indexterm" href="#calibre_link-1730">Inner joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">fs command, <a class="indexterm" href="#calibre_link-1731">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">fs.datanode.dns.interface property, <a class="indexterm" href="#calibre_link-1732">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">fs.datanode.du.reserved property, <a class="indexterm" href="#calibre_link-1733">Reserved storage space</a></dt><dd class="calibre8"></dd><dt class="calibre57">fs.defaultFS property</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1734">The Command-Line Interface</a></dt><dd class="calibre8"></dd><dt class="calibre57">finding namenode hostname, <a class="indexterm" href="#calibre_link-1735">Starting and Stopping the Daemons</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hadoop modes and, <a class="indexterm" href="#calibre_link-1736">Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive and, <a class="indexterm" href="#calibre_link-1737">Configuring Hive</a></dt><dd class="calibre8"></dd><dt class="calibre57">Pig and, <a class="indexterm" href="#calibre_link-1738">MapReduce mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">RPC servers, <a class="indexterm" href="#calibre_link-1739">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">setting filesystem, <a class="indexterm" href="#calibre_link-1740">Testing the Driver</a></dt><dd class="calibre8"></dd><dt class="calibre57">specifying default filesystem, <a class="indexterm" href="#calibre_link-1741">HDFS</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">fs.file.impl property, <a class="indexterm" href="#calibre_link-1742">LocalFileSystem</a></dt><dd class="calibre8"></dd><dt class="calibre57">fs.trash.interval property, <a class="indexterm" href="#calibre_link-1743">Trash</a></dt><dd class="calibre8"></dd><dt class="calibre57">fsck tool, <a class="indexterm" href="#calibre_link-1744">Blocks</a>–<a class="indexterm" href="#calibre_link-1745">Blocks</a>, <a class="indexterm" href="#calibre_link-1746">Filesystem check (fsck)</a>–<a class="indexterm" href="#calibre_link-1747">Finding the blocks for a file</a>, <a class="indexterm" href="#calibre_link-1748">Filesystem check (fsck)</a></dt><dd class="calibre8"></dd><dt class="calibre57">FSDataInputStream class, <a class="indexterm" href="#calibre_link-1749">FSDataInputStream</a>–<a class="indexterm" href="#calibre_link-1750">FSDataInputStream</a>, <a class="indexterm" href="#calibre_link-1751">Anatomy of a File Read</a></dt><dd class="calibre8"></dd><dt class="calibre57">FSDataOutputStream class, <a class="indexterm" href="#calibre_link-1752">FSDataOutputStream</a>, <a class="indexterm" href="#calibre_link-1753">Anatomy of a File Write</a>, <a class="indexterm" href="#calibre_link-1754">Coherency Model</a></dt><dd class="calibre8"></dd><dt class="calibre57">fsimage file, <a class="indexterm" href="#calibre_link-1755">Namenode directory structure</a>–<a class="indexterm" href="#calibre_link-1756">The filesystem image and edit log</a>, <a class="indexterm" href="#calibre_link-1757">Safe Mode</a>, <a class="indexterm" href="#calibre_link-1758">Metadata backups</a></dt><dd class="calibre8"></dd><dt class="calibre57">FsInput class, <a class="indexterm" href="#calibre_link-1759">Avro Datafiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">FTPFileSystem class, <a class="indexterm" href="#calibre_link-1760">Hadoop Filesystems</a></dt><dd class="calibre8"></dd><dt class="calibre57">fully distributed mode (Hadoop), <a class="indexterm" href="#calibre_link-1761">Fully Distributed Mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">FuncSpec class, <a class="indexterm" href="#calibre_link-1762">Leveraging types</a></dt><dd class="calibre8"></dd><dt class="calibre57">functions (Crunch), <a class="indexterm" href="#calibre_link-1763">Functions</a>–<a class="indexterm" href="#calibre_link-1764">Object reuse</a>, <a class="indexterm" href="#calibre_link-1765">Crunch Libraries</a>–<a class="indexterm" href="#calibre_link-1766">Crunch Libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">functions (Hive)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1767">Operators and Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">UDF types, <a class="indexterm" href="#calibre_link-1768">User-Defined Functions</a>–<a class="indexterm" href="#calibre_link-1769">User-Defined Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">writing UDAFs, <a class="indexterm" href="#calibre_link-1770">Writing a UDAF</a>–<a class="indexterm" href="#calibre_link-1771">A more complex UDAF</a></dt><dd class="calibre8"></dd><dt class="calibre57">writing UDFs, <a class="indexterm" href="#calibre_link-1772">Writing a UDF</a>–<a class="indexterm" href="#calibre_link-1773">Writing a UDF</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">functions (Pig Latin)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">built-in, <a class="indexterm" href="#calibre_link-1774">Types</a>, <a class="indexterm" href="#calibre_link-1775">Functions</a>–<a class="indexterm" href="#calibre_link-1776">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">types supported, <a class="indexterm" href="#calibre_link-1777">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">user-defined, <a class="indexterm" href="#calibre_link-1778">Other libraries</a>, <a class="indexterm" href="#calibre_link-1779">User-Defined Functions</a>–<a class="indexterm" href="#calibre_link-1780">Using a schema</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">functions (Spark), <a class="indexterm" href="#calibre_link-1781">Transformations and Actions</a>, <a class="indexterm" href="#calibre_link-1782">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">FUSE (Filesystem in Userspace), <a class="indexterm" href="#calibre_link-1783">FUSE</a></dt><dd class="calibre8"></dd><dt class="calibre57">Future interface (Java), <a class="indexterm" href="#calibre_link-1784">Asynchronous execution</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">G</h3><dl class="book"><dt class="calibre57">GC_TIME_MILLIS counter, <a class="indexterm" href="#calibre_link-1785">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">GenericDatumWriter class, <a class="indexterm" href="#calibre_link-1786">In-Memory Serialization and Deserialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">GenericOptionsParser class, <a class="indexterm" href="#calibre_link-1787">GenericOptionsParser, Tool, and ToolRunner</a>–<a class="indexterm" href="#calibre_link-1788">GenericOptionsParser, Tool, and ToolRunner</a>, <a class="indexterm" href="#calibre_link-1789">Usage</a></dt><dd class="calibre8"></dd><dt class="calibre57">GenericRecord interface, <a class="indexterm" href="#calibre_link-1790">In-Memory Serialization and Deserialization</a>, <a class="indexterm" href="#calibre_link-1791">Records and tuples</a></dt><dd class="calibre8"></dd><dt class="calibre57">GenericWritable class, <a class="indexterm" href="#calibre_link-1792">ObjectWritable and GenericWritable</a>, <a class="indexterm" href="#calibre_link-1793">Writable collections</a></dt><dd class="calibre8"></dd><dt class="calibre57">Genome Reference Consortium (GRC), <a class="indexterm" href="#calibre_link-1794">The Human Genome Project and Reference Genomes</a></dt><dd class="calibre8"></dd><dt class="calibre57">Get class, <a class="indexterm" href="#calibre_link-1795">Java</a></dt><dd class="calibre8"></dd><dt class="calibre57">getACL operation (ZooKeeper), <a class="indexterm" href="#calibre_link-1796">Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">getChildren operation (ZooKeeper), <a class="indexterm" href="#calibre_link-1797">Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">getData operation (ZooKeeper), <a class="indexterm" href="#calibre_link-1798">Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">GFS (Google), <a class="indexterm" href="#calibre_link-1799">A Brief History of Apache Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">globbing operation, <a class="indexterm" href="#calibre_link-1800">File patterns</a></dt><dd class="calibre8"></dd><dt class="calibre57">Google GFS, <a class="indexterm" href="#calibre_link-1801">A Brief History of Apache Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">Google Protocol Buffers, <a class="indexterm" href="#calibre_link-1802">Serialization IDL</a></dt><dd class="calibre8"></dd><dt class="calibre57">graceful failover, <a class="indexterm" href="#calibre_link-1803">Failover and fencing</a></dt><dd class="calibre8"></dd><dt class="calibre57">Gradle build tool, <a class="indexterm" href="#calibre_link-1804">Setting Up the Development Environment</a></dt><dd class="calibre8"></dd><dt class="calibre57">Gray, Jim, <a class="indexterm" href="#calibre_link-1805">Grid Computing</a></dt><dd class="calibre8"></dd><dt class="calibre57">Gray, Jonathan, <a class="indexterm" href="#calibre_link-1806">HBasics</a>–<a class="indexterm" href="#calibre_link-1807">Further Reading</a></dt><dd class="calibre8"></dd><dt class="calibre57">GRC (Genome Reference Consortium), <a class="indexterm" href="#calibre_link-1808">The Human Genome Project and Reference Genomes</a></dt><dd class="calibre8"></dd><dt class="calibre57">Great Internet Mersenne Prime Search project, <a class="indexterm" href="#calibre_link-1809">Volunteer Computing</a></dt><dd class="calibre8"></dd><dt class="calibre57">grid computing, <a class="indexterm" href="#calibre_link-1810">Grid Computing</a></dt><dd class="calibre8"></dd><dt class="calibre57">Gridmix benchmark suite, <a class="indexterm" href="#calibre_link-1811">Other benchmarks</a></dt><dd class="calibre8"></dd><dt class="calibre57">GROUP BY clause (Hive), <a class="indexterm" href="#calibre_link-1812">An Example</a>, <a class="indexterm" href="#calibre_link-1813">Views</a></dt><dd class="calibre8"></dd><dt class="calibre57">GROUP BY operator (Pig Latin), <a class="indexterm" href="#calibre_link-1814">Comparison with Databases</a></dt><dd class="calibre8"></dd><dt class="calibre57">GROUP statement (Pig Latin), <a class="indexterm" href="#calibre_link-1815">Structure</a>, <a class="indexterm" href="#calibre_link-1816">Statements</a>, <a class="indexterm" href="#calibre_link-1817">Statements</a>, <a class="indexterm" href="#calibre_link-1818">GROUP</a></dt><dd class="calibre8"></dd><dt class="calibre57">grouping data, <a class="indexterm" href="#calibre_link-1819">Grouping and Joining Data</a>–<a class="indexterm" href="#calibre_link-1820">GROUP</a></dt><dd class="calibre8"></dd><dt class="calibre57">groups (ZooKeeper)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1821">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">creating, <a class="indexterm" href="#calibre_link-1822">Creating the Group</a>–<a class="indexterm" href="#calibre_link-1823">Creating the Group</a></dt><dd class="calibre8"></dd><dt class="calibre57">deleting, <a class="indexterm" href="#calibre_link-1824">Deleting a Group</a></dt><dd class="calibre8"></dd><dt class="calibre57">group membership, <a class="indexterm" href="#calibre_link-1825">Group Membership in ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">joining, <a class="indexterm" href="#calibre_link-1826">Joining a Group</a></dt><dd class="calibre8"></dd><dt class="calibre57">listing members, <a class="indexterm" href="#calibre_link-1827">Listing Members in a Group</a>–<a class="indexterm" href="#calibre_link-1828">ZooKeeper command-line tools</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Grunt shell, <a class="indexterm" href="#calibre_link-1829">Running Pig Programs</a></dt><dd class="calibre8"></dd><dt class="calibre57">gzip compression, <a class="indexterm" href="#calibre_link-1830">Compression</a>–<a class="indexterm" href="#calibre_link-1831">Codecs</a>, <a class="indexterm" href="#calibre_link-1832">Native libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">GzipCodec class, <a class="indexterm" href="#calibre_link-1833">Codecs</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">H</h3><dl class="book"><dt class="calibre57">Hadoop</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1834">Beyond Batch</a></dt><dd class="calibre8"></dd><dt class="calibre57">history of, <a class="indexterm" href="#calibre_link-1835">A Brief History of Apache Hadoop</a>–<a class="indexterm" href="#calibre_link-1836">A Brief History of Apache Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">installing, <a class="indexterm" href="#calibre_link-1837">Installing Apache Hadoop</a>–<a class="indexterm" href="#calibre_link-1838">Fully Distributed Mode</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">hadoop command</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">basic filesystem operations, <a class="indexterm" href="#calibre_link-1839">Basic Filesystem Operations</a>–<a class="indexterm" href="#calibre_link-1840">Basic Filesystem Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">creating HAR files, <a class="indexterm" href="#calibre_link-1841">Hadoop Filesystems</a></dt><dd class="calibre8"></dd><dt class="calibre57">distcp program and, <a class="indexterm" href="#calibre_link-1842">Parallel Copying with distcp</a></dt><dd class="calibre8"></dd><dt class="calibre57">finding file checksum, <a class="indexterm" href="#calibre_link-1843">Data Integrity in HDFS</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hadoop Streaming and, <a class="indexterm" href="#calibre_link-1844">Ruby</a></dt><dd class="calibre8"></dd><dt class="calibre57">launching JVM, <a class="indexterm" href="#calibre_link-1845">A test run</a></dt><dd class="calibre8"></dd><dt class="calibre57">retrieving job results, <a class="indexterm" href="#calibre_link-1846">Retrieving the Results</a></dt><dd class="calibre8"></dd><dt class="calibre57">running miniclusters from, <a class="indexterm" href="#calibre_link-1847">Testing the Driver</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Hadoop Distributed Filesystem (see HDFS)</dt><dd class="calibre8"></dd><dt class="calibre57">Hadoop Streaming</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1848">Hadoop Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapReduce scripts and, <a class="indexterm" href="#calibre_link-1849">MapReduce Scripts</a></dt><dd class="calibre8"></dd><dt class="calibre57">Python example, <a class="indexterm" href="#calibre_link-1850">Python</a></dt><dd class="calibre8"></dd><dt class="calibre57">Ruby example, <a class="indexterm" href="#calibre_link-1851">Ruby</a>–<a class="indexterm" href="#calibre_link-1852">Ruby</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">hadoop-env.sh file, <a class="indexterm" href="#calibre_link-1853">Hadoop Configuration</a>, <a class="indexterm" href="#calibre_link-1854">Metrics and JMX</a></dt><dd class="calibre8"></dd><dt class="calibre57">hadoop-metrics2.properties file, <a class="indexterm" href="#calibre_link-1855">Hadoop Configuration</a>, <a class="indexterm" href="#calibre_link-1856">Metrics and JMX</a></dt><dd class="calibre8"></dd><dt class="calibre57">hadoop-policy.xml file, <a class="indexterm" href="#calibre_link-1857">Hadoop Configuration</a>, <a class="indexterm" href="#calibre_link-1858">An example</a></dt><dd class="calibre8"></dd><dt class="calibre57">hadoop.http.staticuser.user property, <a class="indexterm" href="#calibre_link-1859">Managing Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">hadoop.rpc.protection property, <a class="indexterm" href="#calibre_link-1860">Other Security Enhancements</a></dt><dd class="calibre8"></dd><dt class="calibre57">hadoop.security.authentication property, <a class="indexterm" href="#calibre_link-1861">An example</a></dt><dd class="calibre8"></dd><dt class="calibre57">hadoop.security.authorization property, <a class="indexterm" href="#calibre_link-1862">An example</a></dt><dd class="calibre8"></dd><dt class="calibre57">hadoop.ssl.enabled property, <a class="indexterm" href="#calibre_link-1863">Other Security Enhancements</a></dt><dd class="calibre8"></dd><dt class="calibre57">hadoop.user.group.static.mapping.overrides
            property, <a class="indexterm" href="#calibre_link-1864">Managing Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">HADOOP_CLASSPATH environment variable, <a class="indexterm" href="#calibre_link-1865">The client classpath</a>, <a class="indexterm" href="#calibre_link-1866">Launching a Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">HADOOP_CONF_DIR environment variable, <a class="indexterm" href="#calibre_link-1867">Managing Configuration</a>, <a class="indexterm" href="#calibre_link-1868">Hadoop Configuration</a>, <a class="indexterm" href="#calibre_link-1869">MapReduce mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">HADOOP_HEAPSIZE environment variable, <a class="indexterm" href="#calibre_link-1870">Memory heap size</a></dt><dd class="calibre8"></dd><dt class="calibre57">HADOOP_HOME environment variable, <a class="indexterm" href="#calibre_link-1871">MapReduce mode</a>, <a class="indexterm" href="#calibre_link-1872">Installing Hive</a>, <a class="indexterm" href="#calibre_link-1873">Installation</a></dt><dd class="calibre8"></dd><dt class="calibre57">HADOOP_IDENT_STRING environment variable, <a class="indexterm" href="#calibre_link-1874">System logfiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">HADOOP_LOG_DIR environment variable, <a class="indexterm" href="#calibre_link-1875">Hadoop Logs</a>, <a class="indexterm" href="#calibre_link-1876">System logfiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">HADOOP_NAMENODE_OPTS environment variable, <a class="indexterm" href="#calibre_link-1877">Memory heap size</a>, <a class="indexterm" href="#calibre_link-1878">Metrics and JMX</a></dt><dd class="calibre8"></dd><dt class="calibre57">HADOOP_OPTS environment variable, <a class="indexterm" href="#calibre_link-1879">GenericOptionsParser, Tool, and ToolRunner</a></dt><dd class="calibre8"></dd><dt class="calibre57">HADOOP_SSH_OPTS environment variable, <a class="indexterm" href="#calibre_link-1880">SSH settings</a></dt><dd class="calibre8"></dd><dt class="calibre57">HADOOP_USER_CLASSPATH_FIRST environment
            variable, <a class="indexterm" href="#calibre_link-1881">Task classpath precedence</a></dt><dd class="calibre8"></dd><dt class="calibre57">HADOOP_USER_NAME environment variable, <a class="indexterm" href="#calibre_link-1882">Managing Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hammerbacher, Jeff, <a class="indexterm" href="#calibre_link-1883">Hive</a></dt><dd class="calibre8"></dd><dt class="calibre57">HAR files, <a class="indexterm" href="#calibre_link-1884">Hadoop Filesystems</a></dt><dd class="calibre8"></dd><dt class="calibre57">HarFileSystem class, <a class="indexterm" href="#calibre_link-1885">Hadoop Filesystems</a></dt><dd class="calibre8"></dd><dt class="calibre57">HashPartitioner class, <a class="indexterm" href="#calibre_link-1886">Implementing a Custom Writable</a>, <a class="indexterm" href="#calibre_link-1887">The Default MapReduce Job</a>, <a class="indexterm" href="#calibre_link-1888">An example: Partitioning data</a></dt><dd class="calibre8"></dd><dt class="calibre57">HBase</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1889">Beyond Batch</a>, <a class="indexterm" href="#calibre_link-1890">The Design of HDFS</a>, <a class="indexterm" href="#calibre_link-1891">HBasics</a>, <a class="indexterm" href="#calibre_link-1892">HBase</a></dt><dd class="calibre8"></dd><dt class="calibre57">additional information, <a class="indexterm" href="#calibre_link-1893">Further Reading</a></dt><dd class="calibre8"></dd><dt class="calibre57">building online query application</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1894">Building an Online Query Application</a></dt><dd class="calibre8"></dd><dt class="calibre57">loading data, <a class="indexterm" href="#calibre_link-1895">Loading Data</a>–<a class="indexterm" href="#calibre_link-1896">Bulk load</a></dt><dd class="calibre8"></dd><dt class="calibre57">online queries, <a class="indexterm" href="#calibre_link-1897">Online Queries</a>–<a class="indexterm" href="#calibre_link-1898">Observation queries</a></dt><dd class="calibre8"></dd><dt class="calibre57">schema design, <a class="indexterm" href="#calibre_link-1899">Schema Design</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">client options, <a class="indexterm" href="#calibre_link-1900">Clients</a>–<a class="indexterm" href="#calibre_link-1901">REST and Thrift</a></dt><dd class="calibre8"></dd><dt class="calibre57">common issues, <a class="indexterm" href="#calibre_link-1902">Praxis</a>–<a class="indexterm" href="#calibre_link-1903">Counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">data model, <a class="indexterm" href="#calibre_link-1904">Whirlwind Tour of the Data Model</a>–<a class="indexterm" href="#calibre_link-1905">Locking</a></dt><dd class="calibre8"></dd><dt class="calibre57">database input and output, <a class="indexterm" href="#calibre_link-1906">Database Input (and Output)</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive and, <a class="indexterm" href="#calibre_link-1907">Storage handlers</a></dt><dd class="calibre8"></dd><dt class="calibre57">implementing, <a class="indexterm" href="#calibre_link-1908">Implementation</a>–<a class="indexterm" href="#calibre_link-1909">HBase in operation</a></dt><dd class="calibre8"></dd><dt class="calibre57">installing, <a class="indexterm" href="#calibre_link-1910">Installation</a>–<a class="indexterm" href="#calibre_link-1911">Test Drive</a></dt><dd class="calibre8"></dd><dt class="calibre57">RDBMS comparison, <a class="indexterm" href="#calibre_link-1912">HBase Versus RDBMS</a>–<a class="indexterm" href="#calibre_link-1913">HBase</a></dt><dd class="calibre8"></dd><dt class="calibre57">test drive, <a class="indexterm" href="#calibre_link-1914">Test Drive</a>–<a class="indexterm" href="#calibre_link-1915">Test Drive</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">hbase.client.scanner.caching property, <a class="indexterm" href="#calibre_link-1916">Java</a></dt><dd class="calibre8"></dd><dt class="calibre57">hbase.client.scanner.timeout.period property, <a class="indexterm" href="#calibre_link-1917">Java</a></dt><dd class="calibre8"></dd><dt class="calibre57">hbase:meta table, <a class="indexterm" href="#calibre_link-1918">HBase in operation</a></dt><dd class="calibre8"></dd><dt class="calibre57">HBaseConfiguration class, <a class="indexterm" href="#calibre_link-1919">Java</a></dt><dd class="calibre8"></dd><dt class="calibre57">HBaseStorage function (Pig Latin), <a class="indexterm" href="#calibre_link-1920">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">HCatalog (Hive), <a class="indexterm" href="#calibre_link-1921">Using Hive tables with HCatalog</a></dt><dd class="calibre8"></dd><dt class="calibre57">HDFS (Hadoop Distributed Filesystem)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1922">Data Storage and Analysis</a>, <a class="indexterm" href="#calibre_link-1923">The Hadoop Distributed Filesystem</a></dt><dd class="calibre8"></dd><dt class="calibre57">audit logs, <a class="indexterm" href="#calibre_link-1924">Hadoop Logs</a>, <a class="indexterm" href="#calibre_link-1925">Audit Logging</a></dt><dd class="calibre8"></dd><dt class="calibre57">basic concepts, <a class="indexterm" href="#calibre_link-1926">Blocks</a>–<a class="indexterm" href="#calibre_link-1927">Failover and fencing</a></dt><dd class="calibre8"></dd><dt class="calibre57">basic operations, <a class="indexterm" href="#calibre_link-1928">Basic Filesystem Operations</a>–<a class="indexterm" href="#calibre_link-1929">Basic Filesystem Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">benchmarking, <a class="indexterm" href="#calibre_link-1930">Benchmarking MapReduce with TeraSort</a></dt><dd class="calibre8"></dd><dt class="calibre57">cluster balancing, <a class="indexterm" href="#calibre_link-1931">Keeping an HDFS Cluster Balanced</a></dt><dd class="calibre8"></dd><dt class="calibre57">cluster setup and installation, <a class="indexterm" href="#calibre_link-1932">Creating Unix User Accounts</a></dt><dd class="calibre8"></dd><dt class="calibre57">cluster sizing, <a class="indexterm" href="#calibre_link-1933">Master node scenarios</a></dt><dd class="calibre8"></dd><dt class="calibre57">coherency model, <a class="indexterm" href="#calibre_link-1934">Coherency Model</a>–<a class="indexterm" href="#calibre_link-1935">Consequences for application design</a></dt><dd class="calibre8"></dd><dt class="calibre57">command-line interface, <a class="indexterm" href="#calibre_link-1936">The Command-Line Interface</a>–<a class="indexterm" href="#calibre_link-1937">Basic Filesystem Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">daemon properties, <a class="indexterm" href="#calibre_link-1938">HDFS</a>–<a class="indexterm" href="#calibre_link-1939">HDFS</a></dt><dd class="calibre8"></dd><dt class="calibre57">design overview, <a class="indexterm" href="#calibre_link-1940">The Design of HDFS</a>–<a class="indexterm" href="#calibre_link-1941">The Design of HDFS</a></dt><dd class="calibre8"></dd><dt class="calibre57">file permissions, <a class="indexterm" href="#calibre_link-1942">Basic Filesystem Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">formatting filesystem, <a class="indexterm" href="#calibre_link-1943">Formatting the HDFS Filesystem</a>, <a class="indexterm" href="#calibre_link-1944">Formatting the HDFS filesystem</a></dt><dd class="calibre8"></dd><dt class="calibre57">HBase and, <a class="indexterm" href="#calibre_link-1945">HDFS</a></dt><dd class="calibre8"></dd><dt class="calibre57">high availability, <a class="indexterm" href="#calibre_link-1946">HDFS High Availability</a>–<a class="indexterm" href="#calibre_link-1947">Failover and fencing</a></dt><dd class="calibre8"></dd><dt class="calibre57">Java interface, <a class="indexterm" href="#calibre_link-1948">The Java Interface</a>–<a class="indexterm" href="#calibre_link-1949">Deleting Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">parallel copying with distcp, <a class="indexterm" href="#calibre_link-1950">Parallel Copying with distcp</a>–<a class="indexterm" href="#calibre_link-1951">Keeping an HDFS Cluster Balanced</a></dt><dd class="calibre8"></dd><dt class="calibre57">persistent data structures, <a class="indexterm" href="#calibre_link-1952">Persistent Data Structures</a>–<a class="indexterm" href="#calibre_link-1953">Datanode directory structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">reading files, <a class="indexterm" href="#calibre_link-1954">Anatomy of a File Read</a>–<a class="indexterm" href="#calibre_link-1955">Anatomy of a File Read</a></dt><dd class="calibre8"></dd><dt class="calibre57">safe mode, <a class="indexterm" href="#calibre_link-1956">Safe Mode</a>–<a class="indexterm" href="#calibre_link-1957">Entering and leaving safe mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">scaling out data, <a class="indexterm" href="#calibre_link-1958">Scaling Out</a>–<a class="indexterm" href="#calibre_link-1959">Data Flow</a></dt><dd class="calibre8"></dd><dt class="calibre57">starting and stopping daemons, <a class="indexterm" href="#calibre_link-1960">Starting and Stopping the Daemons</a></dt><dd class="calibre8"></dd><dt class="calibre57">tool support, <a class="indexterm" href="#calibre_link-1961">dfsadmin</a>–<a class="indexterm" href="#calibre_link-1962">Balancer</a></dt><dd class="calibre8"></dd><dt class="calibre57">upgrade considerations, <a class="indexterm" href="#calibre_link-1963">HDFS data and metadata upgrades</a>–<a class="indexterm" href="#calibre_link-1964">Finalize the upgrade (optional)</a></dt><dd class="calibre8"></dd><dt class="calibre57">writing files, <a class="indexterm" href="#calibre_link-1965">Anatomy of a File Write</a>–<a class="indexterm" href="#calibre_link-1966">Anatomy of a File Write</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">HDFS Federation, <a class="indexterm" href="#calibre_link-1967">HDFS Federation</a></dt><dd class="calibre8"></dd><dt class="calibre57">HDFS sinks</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1968">The HDFS Sink</a>–<a class="indexterm" href="#calibre_link-1969">The HDFS Sink</a></dt><dd class="calibre8"></dd><dt class="calibre57">fanning out, <a class="indexterm" href="#calibre_link-1970">Fan Out</a>–<a class="indexterm" href="#calibre_link-1971">Delivery Guarantees</a></dt><dd class="calibre8"></dd><dt class="calibre57">file formats, <a class="indexterm" href="#calibre_link-1972">File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">indexing events and, <a class="indexterm" href="#calibre_link-1973">Delivery Guarantees</a></dt><dd class="calibre8"></dd><dt class="calibre57">partitioning and interceptors, <a class="indexterm" href="#calibre_link-1974">Partitioning and Interceptors</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">hdfs-site.xml file, <a class="indexterm" href="#calibre_link-1975">Hadoop Configuration</a>, <a class="indexterm" href="#calibre_link-1976">Important Hadoop Daemon Properties</a>, <a class="indexterm" href="#calibre_link-1977">Commissioning new nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">hdfs.fileType property, <a class="indexterm" href="#calibre_link-1978">File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">hdfs.inUsePrefix property, <a class="indexterm" href="#calibre_link-1979">The HDFS Sink</a></dt><dd class="calibre8"></dd><dt class="calibre57">hdfs.path property, <a class="indexterm" href="#calibre_link-1980">The HDFS Sink</a>, <a class="indexterm" href="#calibre_link-1981">Partitioning and Interceptors</a></dt><dd class="calibre8"></dd><dt class="calibre57">hdfs.proxyUser property, <a class="indexterm" href="#calibre_link-1982">The HDFS Sink</a></dt><dd class="calibre8"></dd><dt class="calibre57">hdfs.rollInterval property, <a class="indexterm" href="#calibre_link-1983">The HDFS Sink</a></dt><dd class="calibre8"></dd><dt class="calibre57">hdfs.rollSize property, <a class="indexterm" href="#calibre_link-1984">The HDFS Sink</a></dt><dd class="calibre8"></dd><dt class="calibre57">hdfs.useLocalTimeStamp property, <a class="indexterm" href="#calibre_link-1985">Partitioning and Interceptors</a></dt><dd class="calibre8"></dd><dt class="calibre57">hdfs.writeFormat property, <a class="indexterm" href="#calibre_link-1986">File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hedwig system, <a class="indexterm" href="#calibre_link-1987">BookKeeper and Hedwig</a></dt><dd class="calibre8"></dd><dt class="calibre57">help command, <a class="indexterm" href="#calibre_link-1988">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">herd effect, <a class="indexterm" href="#calibre_link-1989">The herd effect</a></dt><dd class="calibre8"></dd><dt class="calibre57">hexdump tool, <a class="indexterm" href="#calibre_link-1990">The default storage format: Delimited text</a></dt><dd class="calibre8"></dd><dt class="calibre57">HFileOutputFormat2 class, <a class="indexterm" href="#calibre_link-1991">Bulk load</a></dt><dd class="calibre8"></dd><dt class="calibre57">high availability</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">HDFS and, <a class="indexterm" href="#calibre_link-1992">HDFS High Availability</a>–<a class="indexterm" href="#calibre_link-1993">Failover and fencing</a></dt><dd class="calibre8"></dd><dt class="calibre57">resource managers and, <a class="indexterm" href="#calibre_link-1994">Resource Manager Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">YARN and, <a class="indexterm" href="#calibre_link-1995">YARN Compared to MapReduce 1</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">high-performance computing (HPC), <a class="indexterm" href="#calibre_link-1996">Grid Computing</a></dt><dd class="calibre8"></dd><dt class="calibre57">history command, <a class="indexterm" href="#calibre_link-1997">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-1998">Hive</a></dt><dd class="calibre8"></dd><dt class="calibre57">additional information, <a class="indexterm" href="#calibre_link-1999">Further Reading</a></dt><dd class="calibre8"></dd><dt class="calibre57">Avro support, <a class="indexterm" href="#calibre_link-2000">Text and Binary File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">column-oriented format, <a class="indexterm" href="#calibre_link-2001">Other File Formats and Column-Oriented Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">configuring Hive, <a class="indexterm" href="#calibre_link-2002">Configuring Hive</a>–<a class="indexterm" href="#calibre_link-2003">Logging</a></dt><dd class="calibre8"></dd><dt class="calibre57">database comparison, <a class="indexterm" href="#calibre_link-2004">Comparison with Traditional Databases</a>–<a class="indexterm" href="#calibre_link-2005">SQL-on-Hadoop Alternatives</a></dt><dd class="calibre8"></dd><dt class="calibre57">execution engines, <a class="indexterm" href="#calibre_link-2006">Execution engines</a></dt><dd class="calibre8"></dd><dt class="calibre57">HCatalog support, <a class="indexterm" href="#calibre_link-2007">Using Hive tables with HCatalog</a></dt><dd class="calibre8"></dd><dt class="calibre57">installing, <a class="indexterm" href="#calibre_link-2008">Installing Hive</a>–<a class="indexterm" href="#calibre_link-2009">The Hive Shell</a></dt><dd class="calibre8"></dd><dt class="calibre57">metastore, <a class="indexterm" href="#calibre_link-2010">The Metastore</a>–<a class="indexterm" href="#calibre_link-2011">The Metastore</a></dt><dd class="calibre8"></dd><dt class="calibre57">ORCFile and, <a class="indexterm" href="#calibre_link-2012">Parquet</a></dt><dd class="calibre8"></dd><dt class="calibre57">Parquet support, <a class="indexterm" href="#calibre_link-2013">Text and Binary File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">querying data, <a class="indexterm" href="#calibre_link-2014">Querying Data</a>–<a class="indexterm" href="#calibre_link-2015">Views</a></dt><dd class="calibre8"></dd><dt class="calibre57">services supported, <a class="indexterm" href="#calibre_link-2016">Hive Services</a>–<a class="indexterm" href="#calibre_link-2017">Hive clients</a></dt><dd class="calibre8"></dd><dt class="calibre57">SQL dialect, <a class="indexterm" href="#calibre_link-2018">HiveQL</a>–<a class="indexterm" href="#calibre_link-2019">Conversions</a></dt><dd class="calibre8"></dd><dt class="calibre57">Sqoop exports and, <a class="indexterm" href="#calibre_link-2020">Performing an Export</a></dt><dd class="calibre8"></dd><dt class="calibre57">Squoop imports and, <a class="indexterm" href="#calibre_link-2021">Imported Data and Hive</a>–<a class="indexterm" href="#calibre_link-2022">Imported Data and Hive</a></dt><dd class="calibre8"></dd><dt class="calibre57">tables</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2023">Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">altering, <a class="indexterm" href="#calibre_link-2024">Altering Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">buckets and, <a class="indexterm" href="#calibre_link-2025">Partitions and Buckets</a>, <a class="indexterm" href="#calibre_link-2026">Buckets</a>–<a class="indexterm" href="#calibre_link-2027">Buckets</a></dt><dd class="calibre8"></dd><dt class="calibre57">dropping, <a class="indexterm" href="#calibre_link-2028">Dropping Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">external tables, <a class="indexterm" href="#calibre_link-2029">Managed Tables and External Tables</a>–<a class="indexterm" href="#calibre_link-2030">Managed Tables and External Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">importing data, <a class="indexterm" href="#calibre_link-2031">Importing Data</a>–<a class="indexterm" href="#calibre_link-2032">CREATE TABLE...AS SELECT</a></dt><dd class="calibre8"></dd><dt class="calibre57">managed tables, <a class="indexterm" href="#calibre_link-2033">Managed Tables and External Tables</a>–<a class="indexterm" href="#calibre_link-2034">Managed Tables and External Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">partitions and, <a class="indexterm" href="#calibre_link-2035">Partitions and Buckets</a>–<a class="indexterm" href="#calibre_link-2036">Partitions</a></dt><dd class="calibre8"></dd><dt class="calibre57">storage formats, <a class="indexterm" href="#calibre_link-2037">Storage Formats</a>–<a class="indexterm" href="#calibre_link-2038">Storage handlers</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">version considerations, <a class="indexterm" href="#calibre_link-2039">Installing Hive</a></dt><dd class="calibre8"></dd><dt class="calibre57">weather dataset example, <a class="indexterm" href="#calibre_link-2040">An Example</a>–<a class="indexterm" href="#calibre_link-2041">An Example</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Hive Web Interface (HWI), <a class="indexterm" href="#calibre_link-2042">Hive Services</a></dt><dd class="calibre8"></dd><dt class="calibre57">hive.execution.engine property, <a class="indexterm" href="#calibre_link-2043">Execution engines</a></dt><dd class="calibre8"></dd><dt class="calibre57">hive.metastore.uris property, <a class="indexterm" href="#calibre_link-2044">The Metastore</a></dt><dd class="calibre8"></dd><dt class="calibre57">hive.metastore.warehouse.dir property, <a class="indexterm" href="#calibre_link-2045">The Metastore</a></dt><dd class="calibre8"></dd><dt class="calibre57">hive.server2.thrift.port property, <a class="indexterm" href="#calibre_link-2046">Hive Services</a></dt><dd class="calibre8"></dd><dt class="calibre57">HiveDriver class, <a class="indexterm" href="#calibre_link-2047">Hive clients</a></dt><dd class="calibre8"></dd><dt class="calibre57">HiveQL</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2048">The Hive Shell</a></dt><dd class="calibre8"></dd><dt class="calibre57">data types, <a class="indexterm" href="#calibre_link-2049">Data Types</a>–<a class="indexterm" href="#calibre_link-2050">Complex types</a></dt><dd class="calibre8"></dd><dt class="calibre57">operators and functions, <a class="indexterm" href="#calibre_link-2051">Operators and Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">SQL comparison, <a class="indexterm" href="#calibre_link-2052">HiveQL</a>–<a class="indexterm" href="#calibre_link-2053">HiveQL</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">HiveServer2, <a class="indexterm" href="#calibre_link-2054">Hive Services</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hopper, Grace, <a class="indexterm" href="#calibre_link-2055">Data!</a></dt><dd class="calibre8"></dd><dt class="calibre57">HPC (high-performance computing), <a class="indexterm" href="#calibre_link-2056">Grid Computing</a></dt><dd class="calibre8"></dd><dt class="calibre57">HPROF profiling tool, <a class="indexterm" href="#calibre_link-2057">The HPROF profiler</a></dt><dd class="calibre8"></dd><dt class="calibre57">HTableDescriptor class, <a class="indexterm" href="#calibre_link-2058">Java</a></dt><dd class="calibre8"></dd><dt class="calibre57">HTTP REST API, <a class="indexterm" href="#calibre_link-2059">HTTP</a></dt><dd class="calibre8"></dd><dt class="calibre57">HTTP server properties, <a class="indexterm" href="#calibre_link-2060">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">HttpFS proxy, <a class="indexterm" href="#calibre_link-2061">HTTP</a>, <a class="indexterm" href="#calibre_link-2062">Parallel Copying with distcp</a></dt><dd class="calibre8"></dd><dt class="calibre57">Human Genome Project, <a class="indexterm" href="#calibre_link-2063">The Human Genome Project and Reference Genomes</a></dt><dd class="calibre8"></dd><dt class="calibre57">HWI (Hive Web Interface), <a class="indexterm" href="#calibre_link-2064">Hive Services</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">I</h3><dl class="book"><dt class="calibre57">I/O (input/output)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">compression, <a class="indexterm" href="#calibre_link-2065">Compression</a>–<a class="indexterm" href="#calibre_link-2066">Compressing map output</a></dt><dd class="calibre8"></dd><dt class="calibre57">data integrity, <a class="indexterm" href="#calibre_link-2067">Data Integrity</a>–<a class="indexterm" href="#calibre_link-2068">ChecksumFileSystem</a></dt><dd class="calibre8"></dd><dt class="calibre57">file-based data structures, <a class="indexterm" href="#calibre_link-2069">File-Based Data Structures</a>–<a class="indexterm" href="#calibre_link-2070">Other File Formats and Column-Oriented Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">serialization, <a class="indexterm" href="#calibre_link-2071">Serialization</a>–<a class="indexterm" href="#calibre_link-2072">Serialization IDL</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">IDL (interface description language), <a class="indexterm" href="#calibre_link-2073">Serialization IDL</a></dt><dd class="calibre8"></dd><dt class="calibre57">ILLUSTRATE operator (Pig Latin), <a class="indexterm" href="#calibre_link-2074">Generating Examples</a>, <a class="indexterm" href="#calibre_link-2075">Statements</a>, <a class="indexterm" href="#calibre_link-2076">Sorting Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">ImmutableBytesWritable class, <a class="indexterm" href="#calibre_link-2077">MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">Impala query engine, <a class="indexterm" href="#calibre_link-2078">Application Lifespan</a>, <a class="indexterm" href="#calibre_link-2079">SQL-on-Hadoop Alternatives</a></dt><dd class="calibre8"></dd><dt class="calibre57">import process (Hive tables), <a class="indexterm" href="#calibre_link-2080">Importing Data</a>–<a class="indexterm" href="#calibre_link-2081">CREATE TABLE...AS SELECT</a></dt><dd class="calibre8"></dd><dt class="calibre57">import process (Sqoop)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">consistency and, <a class="indexterm" href="#calibre_link-2082">Imports and Consistency</a></dt><dd class="calibre8"></dd><dt class="calibre57">controlling, <a class="indexterm" href="#calibre_link-2083">Controlling the Import</a></dt><dd class="calibre8"></dd><dt class="calibre57">direct-mode imports, <a class="indexterm" href="#calibre_link-2084">Direct-Mode Imports</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive and, <a class="indexterm" href="#calibre_link-2085">Imported Data and Hive</a>–<a class="indexterm" href="#calibre_link-2086">Imported Data and Hive</a></dt><dd class="calibre8"></dd><dt class="calibre57">importing large objects, <a class="indexterm" href="#calibre_link-2087">Importing Large Objects</a>–<a class="indexterm" href="#calibre_link-2088">Importing Large Objects</a></dt><dd class="calibre8"></dd><dt class="calibre57">incremental imports, <a class="indexterm" href="#calibre_link-2089">Incremental Imports</a></dt><dd class="calibre8"></dd><dt class="calibre57">overview, <a class="indexterm" href="#calibre_link-2090">Imports: A Deeper Look</a>–<a class="indexterm" href="#calibre_link-2091">Imports: A Deeper Look</a></dt><dd class="calibre8"></dd><dt class="calibre57">tool support, <a class="indexterm" href="#calibre_link-2092">A Sample Import</a></dt><dd class="calibre8"></dd><dt class="calibre57">working with imported data, <a class="indexterm" href="#calibre_link-2093">Working with Imported Data</a>–<a class="indexterm" href="#calibre_link-2094">Working with Imported Data</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">IMPORT statement (Pig Latin), <a class="indexterm" href="#calibre_link-2095">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">indexes (Hive), <a class="indexterm" href="#calibre_link-2096">Updates, Transactions, and Indexes</a></dt><dd class="calibre8"></dd><dt class="calibre57">Infochimps.org, <a class="indexterm" href="#calibre_link-2097">Data!</a></dt><dd class="calibre8"></dd><dt class="calibre57">information commons, <a class="indexterm" href="#calibre_link-2098">Data!</a></dt><dd class="calibre8"></dd><dt class="calibre57">initLimit property, <a class="indexterm" href="#calibre_link-2099">Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">inner joins, <a class="indexterm" href="#calibre_link-2100">Inner joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">input formats</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">binary input, <a class="indexterm" href="#calibre_link-2101">Binary Input</a></dt><dd class="calibre8"></dd><dt class="calibre57">database input, <a class="indexterm" href="#calibre_link-2102">Database Input (and Output)</a></dt><dd class="calibre8"></dd><dt class="calibre57">input splits and records, <a class="indexterm" href="#calibre_link-2103">Data Flow</a>, <a class="indexterm" href="#calibre_link-2104">Compression and Input Splits</a>, <a class="indexterm" href="#calibre_link-2105">Input Splits and Records</a>–<a class="indexterm" href="#calibre_link-2106">Processing a whole file as a record</a></dt><dd class="calibre8"></dd><dt class="calibre57">multiple inputs, <a class="indexterm" href="#calibre_link-2107">Multiple Inputs</a></dt><dd class="calibre8"></dd><dt class="calibre57">text input, <a class="indexterm" href="#calibre_link-2108">Text Input</a>–<a class="indexterm" href="#calibre_link-2109">XML</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">input splits</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2110">Data Flow</a>, <a class="indexterm" href="#calibre_link-2111">Input Splits and Records</a>–<a class="indexterm" href="#calibre_link-2112">Input Splits and Records</a></dt><dd class="calibre8"></dd><dt class="calibre57">block and, <a class="indexterm" href="#calibre_link-2113">TextInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">CombineFileInputFormat class, <a class="indexterm" href="#calibre_link-2114">Small files and CombineFileInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">compression and, <a class="indexterm" href="#calibre_link-2115">Compression and Input Splits</a></dt><dd class="calibre8"></dd><dt class="calibre57">controlling split size, <a class="indexterm" href="#calibre_link-2116">FileInputFormat input splits</a></dt><dd class="calibre8"></dd><dt class="calibre57">FileInputFormat class, <a class="indexterm" href="#calibre_link-2117">FileInputFormat</a>–<a class="indexterm" href="#calibre_link-2118">FileInputFormat input splits</a></dt><dd class="calibre8"></dd><dt class="calibre57">finding information on, <a class="indexterm" href="#calibre_link-2119">File information in the mapper</a></dt><dd class="calibre8"></dd><dt class="calibre57">preventing, <a class="indexterm" href="#calibre_link-2120">Preventing splitting</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">InputFormat class, <a class="indexterm" href="#calibre_link-2121">Input Splits and Records</a>, <a class="indexterm" href="#calibre_link-2122">FileInputFormat</a>, <a class="indexterm" href="#calibre_link-2123">Imports: A Deeper Look</a></dt><dd class="calibre8"></dd><dt class="calibre57">InputSampler class, <a class="indexterm" href="#calibre_link-2124">Total Sort</a></dt><dd class="calibre8"></dd><dt class="calibre57">InputSplit class (Java), <a class="indexterm" href="#calibre_link-2125">Input Splits and Records</a></dt><dd class="calibre8"></dd><dt class="calibre57">InputStream class (Java), <a class="indexterm" href="#calibre_link-2126">FSDataInputStream</a></dt><dd class="calibre8"></dd><dt class="calibre57">INSERT INTO TABLE statement (Hive), <a class="indexterm" href="#calibre_link-2127">Updates, Transactions, and Indexes</a>, <a class="indexterm" href="#calibre_link-2128">Inserts</a></dt><dd class="calibre8"></dd><dt class="calibre57">INSERT OVERWRITE DIRECTORY statement (Hive), <a class="indexterm" href="#calibre_link-2129">Managed Tables and External Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">INSERT OVERWRITE TABLE statement (Hive), <a class="indexterm" href="#calibre_link-2130">Buckets</a>, <a class="indexterm" href="#calibre_link-2131">Inserts</a></dt><dd class="calibre8"></dd><dt class="calibre57">interactive SQL, <a class="indexterm" href="#calibre_link-2132">Beyond Batch</a></dt><dd class="calibre8"></dd><dt class="calibre57">interface description language (IDL), <a class="indexterm" href="#calibre_link-2133">Serialization IDL</a></dt><dd class="calibre8"></dd><dt class="calibre57">interfaces (Hadoop), <a class="indexterm" href="#calibre_link-2134">Interfaces</a>–<a class="indexterm" href="#calibre_link-2135">FUSE</a></dt><dd class="calibre8"></dd><dt class="calibre57">IntSumReducer class, <a class="indexterm" href="#calibre_link-2136">MapReduce Library Classes</a></dt><dd class="calibre8"></dd><dt class="calibre57">IntWritable class, <a class="indexterm" href="#calibre_link-2137">Java MapReduce</a>, <a class="indexterm" href="#calibre_link-2138">The Writable Interface</a>, <a class="indexterm" href="#calibre_link-2139">Writable wrappers for Java primitives</a></dt><dd class="calibre8"></dd><dt class="calibre57">InverseMapper class, <a class="indexterm" href="#calibre_link-2140">MapReduce Library Classes</a></dt><dd class="calibre8"></dd><dt class="calibre57">InvokeForDouble class, <a class="indexterm" href="#calibre_link-2141">Dynamic invokers</a></dt><dd class="calibre8"></dd><dt class="calibre57">InvokeForFloat class, <a class="indexterm" href="#calibre_link-2142">Dynamic invokers</a></dt><dd class="calibre8"></dd><dt class="calibre57">InvokeForInt class, <a class="indexterm" href="#calibre_link-2143">Dynamic invokers</a></dt><dd class="calibre8"></dd><dt class="calibre57">InvokeForLong class, <a class="indexterm" href="#calibre_link-2144">Dynamic invokers</a></dt><dd class="calibre8"></dd><dt class="calibre57">InvokeForString class, <a class="indexterm" href="#calibre_link-2145">Dynamic invokers</a></dt><dd class="calibre8"></dd><dt class="calibre57">io.compression.codecs property, <a class="indexterm" href="#calibre_link-2146">Inferring CompressionCodecs using
        CompressionCodecFactory</a></dt><dd class="calibre8"></dd><dt class="calibre57">io.file.buffer.size property, <a class="indexterm" href="#calibre_link-2147">Buffer size</a></dt><dd class="calibre8"></dd><dt class="calibre57">io.native.lib.available property, <a class="indexterm" href="#calibre_link-2148">Native libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">io.serializations property, <a class="indexterm" href="#calibre_link-2149">Serialization Frameworks</a></dt><dd class="calibre8"></dd><dt class="calibre57">IOUtils class, <a class="indexterm" href="#calibre_link-2150">Compressing and decompressing streams with
        CompressionCodec</a>, <a class="indexterm" href="#calibre_link-2151">Processing a whole file as a record</a></dt><dd class="calibre8"></dd><dt class="calibre57">is null operator (Hive), <a class="indexterm" href="#calibre_link-2152">Validation and nulls</a></dt><dd class="calibre8"></dd><dt class="calibre57">IsEmpty function (Pig Latin), <a class="indexterm" href="#calibre_link-2153">Functions</a>, <a class="indexterm" href="#calibre_link-2154">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">isro command (ZooKeeper), <a class="indexterm" href="#calibre_link-2155">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">iterative processing, <a class="indexterm" href="#calibre_link-2156">Beyond Batch</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">J</h3><dl class="book"><dt class="calibre57">jar service (Hive), <a class="indexterm" href="#calibre_link-2157">Hive Services</a></dt><dd class="calibre8"></dd><dt class="calibre57">Java Database Connectivity (JDBC), <a class="indexterm" href="#calibre_link-2158">Imports: A Deeper Look</a>, <a class="indexterm" href="#calibre_link-2159">Exports: A Deeper Look</a></dt><dd class="calibre8"></dd><dt class="calibre57">Java language</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">creating directories, <a class="indexterm" href="#calibre_link-2160">Directories</a></dt><dd class="calibre8"></dd><dt class="calibre57">deleting data, <a class="indexterm" href="#calibre_link-2161">Deleting Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">environment variables, <a class="indexterm" href="#calibre_link-2162">Java</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hadoop Streaming and, <a class="indexterm" href="#calibre_link-2163">Ruby</a></dt><dd class="calibre8"></dd><dt class="calibre57">HBase and, <a class="indexterm" href="#calibre_link-2164">Java</a>–<a class="indexterm" href="#calibre_link-2165">Java</a></dt><dd class="calibre8"></dd><dt class="calibre57">installing, <a class="indexterm" href="#calibre_link-2166">Installing Java</a></dt><dd class="calibre8"></dd><dt class="calibre57">Pig and, <a class="indexterm" href="#calibre_link-2167">Running Pig Programs</a></dt><dd class="calibre8"></dd><dt class="calibre57">querying FileSystem, <a class="indexterm" href="#calibre_link-2168">File metadata: FileStatus</a>–<a class="indexterm" href="#calibre_link-2169">PathFilter</a></dt><dd class="calibre8"></dd><dt class="calibre57">reading data from Hadoop URL, <a class="indexterm" href="#calibre_link-2170">Reading Data from a Hadoop URL</a></dt><dd class="calibre8"></dd><dt class="calibre57">reading data using FileSystem API, <a class="indexterm" href="#calibre_link-2171">Reading Data Using the FileSystem API</a>–<a class="indexterm" href="#calibre_link-2172">FSDataInputStream</a></dt><dd class="calibre8"></dd><dt class="calibre57">secondary sort, <a class="indexterm" href="#calibre_link-2173">Java code</a>–<a class="indexterm" href="#calibre_link-2174">Java code</a></dt><dd class="calibre8"></dd><dt class="calibre57">Spark example, <a class="indexterm" href="#calibre_link-2175">A Java Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">syslog file, <a class="indexterm" href="#calibre_link-2176">Hadoop Logs</a></dt><dd class="calibre8"></dd><dt class="calibre57">user-defined counters, <a class="indexterm" href="#calibre_link-2177">User-Defined Java Counters</a>–<a class="indexterm" href="#calibre_link-2178">Retrieving counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">WAR files, <a class="indexterm" href="#calibre_link-2179">Packaging a Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">weather dataset example, <a class="indexterm" href="#calibre_link-2180">Java MapReduce</a>–<a class="indexterm" href="#calibre_link-2181">A test run</a></dt><dd class="calibre8"></dd><dt class="calibre57">Writable wrappers for Java primitives, <a class="indexterm" href="#calibre_link-2182">Writable wrappers for Java primitives</a>–<a class="indexterm" href="#calibre_link-2183">Writable collections</a></dt><dd class="calibre8"></dd><dt class="calibre57">writing data, <a class="indexterm" href="#calibre_link-2184">Writing Data</a>–<a class="indexterm" href="#calibre_link-2185">FSDataOutputStream</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Java Management Extensions (JMX), <a class="indexterm" href="#calibre_link-2186">Metrics and JMX</a>, <a class="indexterm" href="#calibre_link-2187">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">Java Native Interface (JNI), <a class="indexterm" href="#calibre_link-2188">C</a></dt><dd class="calibre8"></dd><dt class="calibre57">Java Object Serialization, <a class="indexterm" href="#calibre_link-2189">Serialization Frameworks</a></dt><dd class="calibre8"></dd><dt class="calibre57">Java virtual machine (JVM), <a class="indexterm" href="#calibre_link-2190">A test run</a>, <a class="indexterm" href="#calibre_link-2191">Remote Debugging</a>, <a class="indexterm" href="#calibre_link-2192">Task Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">java.library.path property, <a class="indexterm" href="#calibre_link-2193">Native libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">java.util.concurrent package, <a class="indexterm" href="#calibre_link-2194">Asynchronous execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">JavaPairRDD class, <a class="indexterm" href="#calibre_link-2195">A Java Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">JavaRDD class, <a class="indexterm" href="#calibre_link-2196">A Java Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">JavaRDDLike interface, <a class="indexterm" href="#calibre_link-2197">A Java Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">JavaSerialization class, <a class="indexterm" href="#calibre_link-2198">Serialization Frameworks</a></dt><dd class="calibre8"></dd><dt class="calibre57">JAVA_HOME environment variable, <a class="indexterm" href="#calibre_link-2199">Java</a>, <a class="indexterm" href="#calibre_link-2200">Installing and Running Pig</a>, <a class="indexterm" href="#calibre_link-2201">Installation</a></dt><dd class="calibre8"></dd><dt class="calibre57">JBOD (just a bunch of disks), <a class="indexterm" href="#calibre_link-2202">Cluster Specification</a></dt><dd class="calibre8"></dd><dt class="calibre57">JDBC (Java Database Connectivity), <a class="indexterm" href="#calibre_link-2203">Imports: A Deeper Look</a>, <a class="indexterm" href="#calibre_link-2204">Exports: A Deeper Look</a></dt><dd class="calibre8"></dd><dt class="calibre57">JDBC drivers, <a class="indexterm" href="#calibre_link-2205">Hive clients</a></dt><dd class="calibre8"></dd><dt class="calibre57">JMX (Java Management Extensions), <a class="indexterm" href="#calibre_link-2206">Metrics and JMX</a>, <a class="indexterm" href="#calibre_link-2207">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">JNI (Java Native Interface), <a class="indexterm" href="#calibre_link-2208">C</a></dt><dd class="calibre8"></dd><dt class="calibre57">Job class</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">distributed cache options, <a class="indexterm" href="#calibre_link-2209">The distributed cache API</a></dt><dd class="calibre8"></dd><dt class="calibre57">progress and status updates, <a class="indexterm" href="#calibre_link-2210">Progress and Status Updates</a></dt><dd class="calibre8"></dd><dt class="calibre57">setting explicit JAR files, <a class="indexterm" href="#calibre_link-2211">Packaging a Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">setting input paths, <a class="indexterm" href="#calibre_link-2212">FileInputFormat input paths</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">job counters, <a class="indexterm" href="#calibre_link-2213">Built-in Counters</a>, <a class="indexterm" href="#calibre_link-2214">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">job history, <a class="indexterm" href="#calibre_link-2215">The resource manager page</a></dt><dd class="calibre8"></dd><dt class="calibre57">job history logs (MapReduce), <a class="indexterm" href="#calibre_link-2216">Hadoop Logs</a></dt><dd class="calibre8"></dd><dt class="calibre57">job IDs, <a class="indexterm" href="#calibre_link-2217">Launching a Job</a>, <a class="indexterm" href="#calibre_link-2218">The Task Execution Environment</a></dt><dd class="calibre8"></dd><dt class="calibre57">job JAR files</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2219">Packaging a Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">client classpath, <a class="indexterm" href="#calibre_link-2220">The client classpath</a></dt><dd class="calibre8"></dd><dt class="calibre57">packaging dependencies, <a class="indexterm" href="#calibre_link-2221">Packaging dependencies</a></dt><dd class="calibre8"></dd><dt class="calibre57">task classpath, <a class="indexterm" href="#calibre_link-2222">The task classpath</a></dt><dd class="calibre8"></dd><dt class="calibre57">task classpath precedence, <a class="indexterm" href="#calibre_link-2223">Task classpath precedence</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">job page (MapReduce), <a class="indexterm" href="#calibre_link-2224">The MapReduce job page</a></dt><dd class="calibre8"></dd><dt class="calibre57">JobBuilder class, <a class="indexterm" href="#calibre_link-2225">The Default MapReduce Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">JobClient class, <a class="indexterm" href="#calibre_link-2226">Anatomy of a MapReduce Job Run</a></dt><dd class="calibre8"></dd><dt class="calibre57">JobConf class, <a class="indexterm" href="#calibre_link-2227">Compressing map output</a>, <a class="indexterm" href="#calibre_link-2228">Packaging a Job</a>, <a class="indexterm" href="#calibre_link-2229">The HPROF profiler</a></dt><dd class="calibre8"></dd><dt class="calibre57">JobControl class, <a class="indexterm" href="#calibre_link-2230">JobControl</a></dt><dd class="calibre8"></dd><dt class="calibre57">jobs</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">anatomy of MapReduce job runs, <a class="indexterm" href="#calibre_link-2231">Anatomy of a MapReduce Job Run</a>–<a class="indexterm" href="#calibre_link-2232">Job Completion</a></dt><dd class="calibre8"></dd><dt class="calibre57">anatomy of Spark job runs, <a class="indexterm" href="#calibre_link-2233">Anatomy of a Spark Job Run</a>–<a class="indexterm" href="#calibre_link-2234">Task Execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">completion process, <a class="indexterm" href="#calibre_link-2235">Job Completion</a></dt><dd class="calibre8"></dd><dt class="calibre57">DAG construction, <a class="indexterm" href="#calibre_link-2236">DAG Construction</a>–<a class="indexterm" href="#calibre_link-2237">DAG Construction</a></dt><dd class="calibre8"></dd><dt class="calibre57">debugging, <a class="indexterm" href="#calibre_link-2238">Debugging a Job</a>–<a class="indexterm" href="#calibre_link-2239">Handling malformed data</a>, <a class="indexterm" href="#calibre_link-2240">Remote Debugging</a></dt><dd class="calibre8"></dd><dt class="calibre57">decomposing problems into, <a class="indexterm" href="#calibre_link-2241">Decomposing a Problem into MapReduce Jobs</a>–<a class="indexterm" href="#calibre_link-2242">Decomposing a Problem into MapReduce Jobs</a></dt><dd class="calibre8"></dd><dt class="calibre57">default MapReduce, <a class="indexterm" href="#calibre_link-2243">The Default MapReduce Job</a>–<a class="indexterm" href="#calibre_link-2244">Keys and values in Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">initialization process, <a class="indexterm" href="#calibre_link-2245">Job Initialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">launching, <a class="indexterm" href="#calibre_link-2246">Launching a Job</a>–<a class="indexterm" href="#calibre_link-2247">Launching a Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">logging, <a class="indexterm" href="#calibre_link-2248">Hadoop Logs</a>–<a class="indexterm" href="#calibre_link-2249">Hadoop Logs</a></dt><dd class="calibre8"></dd><dt class="calibre57">packaging, <a class="indexterm" href="#calibre_link-2250">Packaging a Job</a>–<a class="indexterm" href="#calibre_link-2251">Task classpath precedence</a></dt><dd class="calibre8"></dd><dt class="calibre57">progress and status updates, <a class="indexterm" href="#calibre_link-2252">Progress and Status Updates</a></dt><dd class="calibre8"></dd><dt class="calibre57">retrieving results, <a class="indexterm" href="#calibre_link-2253">Retrieving the Results</a></dt><dd class="calibre8"></dd><dt class="calibre57">running as benchmarks, <a class="indexterm" href="#calibre_link-2254">User Jobs</a></dt><dd class="calibre8"></dd><dt class="calibre57">running locally, <a class="indexterm" href="#calibre_link-2255">Running a Job in a Local Job Runner</a>–<a class="indexterm" href="#calibre_link-2256">Running a Job in a Local Job Runner</a></dt><dd class="calibre8"></dd><dt class="calibre57">running Oozie workflow jobs, <a class="indexterm" href="#calibre_link-2257">Running an Oozie workflow job</a></dt><dd class="calibre8"></dd><dt class="calibre57">scheduling, <a class="indexterm" href="#calibre_link-2258">Job scheduler</a>, <a class="indexterm" href="#calibre_link-2259">Task Scheduling</a></dt><dd class="calibre8"></dd><dt class="calibre57">Spark support, <a class="indexterm" href="#calibre_link-2260">Spark Applications, Jobs, Stages, and Tasks</a></dt><dd class="calibre8"></dd><dt class="calibre57">submission process, <a class="indexterm" href="#calibre_link-2261">Job Submission</a>, <a class="indexterm" href="#calibre_link-2262">Job Submission</a></dt><dd class="calibre8"></dd><dt class="calibre57">task execution, <a class="indexterm" href="#calibre_link-2263">Task Execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">testing job drivers, <a class="indexterm" href="#calibre_link-2264">Testing the Driver</a>–<a class="indexterm" href="#calibre_link-2265">Testing the Driver</a></dt><dd class="calibre8"></dd><dt class="calibre57">tuning, <a class="indexterm" href="#calibre_link-2266">Tuning a Job</a>–<a class="indexterm" href="#calibre_link-2267">The HPROF profiler</a></dt><dd class="calibre8"></dd><dt class="calibre57">viewing information about, <a class="indexterm" href="#calibre_link-2268">The MapReduce Web UI</a>–<a class="indexterm" href="#calibre_link-2269">The MapReduce job page</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">JobSubmitter class, <a class="indexterm" href="#calibre_link-2270">Job Submission</a></dt><dd class="calibre8"></dd><dt class="calibre57">jobtrackers, <a class="indexterm" href="#calibre_link-2271">YARN Compared to MapReduce 1</a></dt><dd class="calibre8"></dd><dt class="calibre57">Join class, <a class="indexterm" href="#calibre_link-2272">Crunch Libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">JOIN clause (Hive), <a class="indexterm" href="#calibre_link-2273">Inner joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">JOIN statement (Pig Latin), <a class="indexterm" href="#calibre_link-2274">Statements</a>, <a class="indexterm" href="#calibre_link-2275">JOIN</a></dt><dd class="calibre8"></dd><dt class="calibre57">joins</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2276">Joins</a>, <a class="indexterm" href="#calibre_link-2277">Joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">inner, <a class="indexterm" href="#calibre_link-2278">Inner joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">map-side, <a class="indexterm" href="#calibre_link-2279">Map-Side Joins</a>–<a class="indexterm" href="#calibre_link-2280">Map-Side Joins</a>, <a class="indexterm" href="#calibre_link-2281">Map joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">outer, <a class="indexterm" href="#calibre_link-2282">Outer joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">Pig operators and, <a class="indexterm" href="#calibre_link-2283">Grouping and Joining Data</a>–<a class="indexterm" href="#calibre_link-2284">GROUP</a></dt><dd class="calibre8"></dd><dt class="calibre57">reduce-side, <a class="indexterm" href="#calibre_link-2285">Reduce-Side Joins</a>–<a class="indexterm" href="#calibre_link-2286">Reduce-Side Joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">semi, <a class="indexterm" href="#calibre_link-2287">Semi joins</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">journal nodes, <a class="indexterm" href="#calibre_link-2288">HDFS High Availability</a></dt><dd class="calibre8"></dd><dt class="calibre57">JsonLoader function (Pig Latin), <a class="indexterm" href="#calibre_link-2289">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">JsonStorage function (Pig Latin), <a class="indexterm" href="#calibre_link-2290">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">JSP Expression Language, <a class="indexterm" href="#calibre_link-2291">Defining an Oozie workflow</a></dt><dd class="calibre8"></dd><dt class="calibre57">JUnit framework, <a class="indexterm" href="#calibre_link-2292">Setting Up the Development Environment</a></dt><dd class="calibre8"></dd><dt class="calibre57">just a bunch of disks (JBOD), <a class="indexterm" href="#calibre_link-2293">Cluster Specification</a></dt><dd class="calibre8"></dd><dt class="calibre57">JVM (Java virtual machine), <a class="indexterm" href="#calibre_link-2294">A test run</a>, <a class="indexterm" href="#calibre_link-2295">Remote Debugging</a>, <a class="indexterm" href="#calibre_link-2296">Task Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">JVMFLAGS environment variable, <a class="indexterm" href="#calibre_link-2297">Resilience and Performance</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">K</h3><dl class="book"><dt class="calibre57">KDC (Key Distribution Center), <a class="indexterm" href="#calibre_link-2298">Kerberos and Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">kdestroy command, <a class="indexterm" href="#calibre_link-2299">An example</a></dt><dd class="calibre8"></dd><dt class="calibre57">Kellerman, Jim, <a class="indexterm" href="#calibre_link-2300">Backdrop</a></dt><dd class="calibre8"></dd><dt class="calibre57">Kerberos authentication, <a class="indexterm" href="#calibre_link-2301">Kerberos and Hadoop</a>–<a class="indexterm" href="#calibre_link-2302">An example</a>, <a class="indexterm" href="#calibre_link-2303">ACLs</a></dt><dd class="calibre8"></dd><dt class="calibre57">Key Distribution Center (KDC), <a class="indexterm" href="#calibre_link-2304">Kerberos and Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">KeyValueTextInputFormat class, <a class="indexterm" href="#calibre_link-2305">KeyValueTextInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">keywords (Pig Latin), <a class="indexterm" href="#calibre_link-2306">Structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">kill command, <a class="indexterm" href="#calibre_link-2307">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">Kimball, Aaron, <a class="indexterm" href="#calibre_link-2308">Sqoop</a>–<a class="indexterm" href="#calibre_link-2309">Further Reading</a></dt><dd class="calibre8"></dd><dt class="calibre57">kinit command, <a class="indexterm" href="#calibre_link-2310">Kerberos and Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">klist command, <a class="indexterm" href="#calibre_link-2311">An example</a></dt><dd class="calibre8"></dd><dt class="calibre57">ktutil command, <a class="indexterm" href="#calibre_link-2312">Kerberos and Hadoop</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">L</h3><dl class="book"><dt class="calibre57">LARGE_READ_OPS counter, <a class="indexterm" href="#calibre_link-2313">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">LATERAL VIEW statement (Hive), <a class="indexterm" href="#calibre_link-2314">User-Defined Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">LazyOutputFormat class, <a class="indexterm" href="#calibre_link-2315">Lazy Output</a></dt><dd class="calibre8"></dd><dt class="calibre57">leader election process, <a class="indexterm" href="#calibre_link-2316">Implementation</a>, <a class="indexterm" href="#calibre_link-2317">A Lock Service</a></dt><dd class="calibre8"></dd><dt class="calibre57">leaderServes property, <a class="indexterm" href="#calibre_link-2318">Consistency</a></dt><dd class="calibre8"></dd><dt class="calibre57">LEFT OUTER JOIN statement (Hive), <a class="indexterm" href="#calibre_link-2319">Outer joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">LEFT SEMI JOIN statement (Hive), <a class="indexterm" href="#calibre_link-2320">Semi joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">libhdfs (C library), <a class="indexterm" href="#calibre_link-2321">C</a></dt><dd class="calibre8"></dd><dt class="calibre57">LIMIT statement (Pig Latin), <a class="indexterm" href="#calibre_link-2322">Statements</a>, <a class="indexterm" href="#calibre_link-2323">Sorting Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">Lin, Jimmy, <a class="indexterm" href="#calibre_link-2324">MapReduce Workflows</a></dt><dd class="calibre8"></dd><dt class="calibre57">line length, maximum, <a class="indexterm" href="#calibre_link-2325">Controlling the maximum line length</a></dt><dd class="calibre8"></dd><dt class="calibre57">LinuxContainerExecutor class, <a class="indexterm" href="#calibre_link-2326">CPU settings in YARN and MapReduce</a>, <a class="indexterm" href="#calibre_link-2327">Other Security Enhancements</a></dt><dd class="calibre8"></dd><dt class="calibre57">LinuxTaskController class, <a class="indexterm" href="#calibre_link-2328">Other Security Enhancements</a></dt><dd class="calibre8"></dd><dt class="calibre57">list command (HBase), <a class="indexterm" href="#calibre_link-2329">Test Drive</a></dt><dd class="calibre8"></dd><dt class="calibre57">listing files, <a class="indexterm" href="#calibre_link-2330">Listing files</a></dt><dd class="calibre8"></dd><dt class="calibre57">Llama project, <a class="indexterm" href="#calibre_link-2331">Application Lifespan</a></dt><dd class="calibre8"></dd><dt class="calibre57">load balancing, sink groups and, <a class="indexterm" href="#calibre_link-2332">Sink Groups</a>–<a class="indexterm" href="#calibre_link-2333">Integrating Flume with Applications</a></dt><dd class="calibre8"></dd><dt class="calibre57">LOAD DATA statement (Hive), <a class="indexterm" href="#calibre_link-2334">An Example</a>, <a class="indexterm" href="#calibre_link-2335">Managed Tables and External Tables</a>, <a class="indexterm" href="#calibre_link-2336">Using a custom SerDe: RegexSerDe</a></dt><dd class="calibre8"></dd><dt class="calibre57">load functions (Pig Latin), <a class="indexterm" href="#calibre_link-2337">Functions</a>, <a class="indexterm" href="#calibre_link-2338">A Load UDF</a>–<a class="indexterm" href="#calibre_link-2339">Using a schema</a></dt><dd class="calibre8"></dd><dt class="calibre57">LOAD statement (Pig Latin), <a class="indexterm" href="#calibre_link-2340">Statements</a>, <a class="indexterm" href="#calibre_link-2341">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">LoadFunc class, <a class="indexterm" href="#calibre_link-2342">A Load UDF</a></dt><dd class="calibre8"></dd><dt class="calibre57">local mode (Hadoop), <a class="indexterm" href="#calibre_link-2343">Standalone Mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">local mode (Pig), <a class="indexterm" href="#calibre_link-2344">Local mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">local mode (Spark), <a class="indexterm" href="#calibre_link-2345">Executors and Cluster Managers</a></dt><dd class="calibre8"></dd><dt class="calibre57">LocalFileSystem class, <a class="indexterm" href="#calibre_link-2346">Hadoop Filesystems</a>, <a class="indexterm" href="#calibre_link-2347">LocalFileSystem</a></dt><dd class="calibre8"></dd><dt class="calibre57">locality constraints, <a class="indexterm" href="#calibre_link-2348">Resource Requests</a></dt><dd class="calibre8"></dd><dt class="calibre57">lock service (ZooKeeper), <a class="indexterm" href="#calibre_link-2349">A Lock Service</a>–<a class="indexterm" href="#calibre_link-2350">Implementation</a></dt><dd class="calibre8"></dd><dt class="calibre57">locking HBase tables, <a class="indexterm" href="#calibre_link-2351">Locking</a></dt><dd class="calibre8"></dd><dt class="calibre57">log aggregation, <a class="indexterm" href="#calibre_link-2352">Hadoop Logs</a></dt><dd class="calibre8"></dd><dt class="calibre57">log4j.properties file, <a class="indexterm" href="#calibre_link-2353">Hadoop Configuration</a>, <a class="indexterm" href="#calibre_link-2354">Setting log levels</a></dt><dd class="calibre8"></dd><dt class="calibre57">logfiles</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">edit logs, <a class="indexterm" href="#calibre_link-2355">The filesystem image and edit log</a>–<a class="indexterm" href="#calibre_link-2356">The filesystem image and edit log</a></dt><dd class="calibre8"></dd><dt class="calibre57">file-based data structures and, <a class="indexterm" href="#calibre_link-2357">SequenceFile</a>–<a class="indexterm" href="#calibre_link-2358">The SequenceFile format</a></dt><dd class="calibre8"></dd><dt class="calibre57">Flume support, <a class="indexterm" href="#calibre_link-2359">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive support, <a class="indexterm" href="#calibre_link-2360">Logging</a></dt><dd class="calibre8"></dd><dt class="calibre57">monitoring support, <a class="indexterm" href="#calibre_link-2361">Logging</a></dt><dd class="calibre8"></dd><dt class="calibre57">types supported, <a class="indexterm" href="#calibre_link-2362">Hadoop Logs</a>–<a class="indexterm" href="#calibre_link-2363">Hadoop Logs</a>, <a class="indexterm" href="#calibre_link-2364">System logfiles</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">LongSumReducer class, <a class="indexterm" href="#calibre_link-2365">MapReduce Library Classes</a></dt><dd class="calibre8"></dd><dt class="calibre57">LongWritable class, <a class="indexterm" href="#calibre_link-2366">Java MapReduce</a>, <a class="indexterm" href="#calibre_link-2367">Writable wrappers for Java primitives</a>, <a class="indexterm" href="#calibre_link-2368">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">ls command, <a class="indexterm" href="#calibre_link-2369">Structure</a>, <a class="indexterm" href="#calibre_link-2370">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">LZ4 compression, <a class="indexterm" href="#calibre_link-2371">Compression</a>–<a class="indexterm" href="#calibre_link-2372">Codecs</a>, <a class="indexterm" href="#calibre_link-2373">Native libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">Lz4Codec class, <a class="indexterm" href="#calibre_link-2374">Codecs</a></dt><dd class="calibre8"></dd><dt class="calibre57">LZO compression, <a class="indexterm" href="#calibre_link-2375">Compression</a>–<a class="indexterm" href="#calibre_link-2376">Codecs</a>, <a class="indexterm" href="#calibre_link-2377">Native libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">LzoCodec class, <a class="indexterm" href="#calibre_link-2378">Codecs</a></dt><dd class="calibre8"></dd><dt class="calibre57">lzop tool, <a class="indexterm" href="#calibre_link-2379">Codecs</a></dt><dd class="calibre8"></dd><dt class="calibre57">LzopCodec class, <a class="indexterm" href="#calibre_link-2380">Codecs</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">M</h3><dl class="book"><dt class="calibre57">macros (Pig Latin), <a class="indexterm" href="#calibre_link-2381">Macros</a>–<a class="indexterm" href="#calibre_link-2382">Macros</a></dt><dd class="calibre8"></dd><dt class="calibre57">MailTrust, <a class="indexterm" href="#calibre_link-2383">Querying All Your Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">maintenance (see system administration)</dt><dd class="calibre8"></dd><dt class="calibre57">managed tables (Hive), <a class="indexterm" href="#calibre_link-2384">Managed Tables and External Tables</a>–<a class="indexterm" href="#calibre_link-2385">Managed Tables and External Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">MAP clause (Hive), <a class="indexterm" href="#calibre_link-2386">MapReduce Scripts</a></dt><dd class="calibre8"></dd><dt class="calibre57">map functions (MapReduce)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2387">Map and Reduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">compressing output, <a class="indexterm" href="#calibre_link-2388">Compressing map output</a></dt><dd class="calibre8"></dd><dt class="calibre57">data flow tasks, <a class="indexterm" href="#calibre_link-2389">Data Flow</a>–<a class="indexterm" href="#calibre_link-2390">Specifying a combiner function</a></dt><dd class="calibre8"></dd><dt class="calibre57">general form, <a class="indexterm" href="#calibre_link-2391">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hadoop Streaming, <a class="indexterm" href="#calibre_link-2392">Hadoop Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">HDFS blocks and, <a class="indexterm" href="#calibre_link-2393">Blocks</a></dt><dd class="calibre8"></dd><dt class="calibre57">Java example, <a class="indexterm" href="#calibre_link-2394">Java MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">joining data, <a class="indexterm" href="#calibre_link-2395">Map-Side Joins</a>–<a class="indexterm" href="#calibre_link-2396">Map-Side Joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">progress and status updates, <a class="indexterm" href="#calibre_link-2397">Progress and Status Updates</a></dt><dd class="calibre8"></dd><dt class="calibre57">shuffle and sort, <a class="indexterm" href="#calibre_link-2398">The Map Side</a>–<a class="indexterm" href="#calibre_link-2399">The Map Side</a></dt><dd class="calibre8"></dd><dt class="calibre57">Spark and, <a class="indexterm" href="#calibre_link-2400">DAG Construction</a></dt><dd class="calibre8"></dd><dt class="calibre57">task assignments, <a class="indexterm" href="#calibre_link-2401">Task Assignment</a></dt><dd class="calibre8"></dd><dt class="calibre57">task execution, <a class="indexterm" href="#calibre_link-2402">Task Execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">task failures, <a class="indexterm" href="#calibre_link-2403">Task Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">testing with MRUnit, <a class="indexterm" href="#calibre_link-2404">Mapper</a>–<a class="indexterm" href="#calibre_link-2405">Mapper</a></dt><dd class="calibre8"></dd><dt class="calibre57">tuning checklist, <a class="indexterm" href="#calibre_link-2406">Tuning a Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">tuning properties, <a class="indexterm" href="#calibre_link-2407">Configuration Tuning</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">MapDriver class, <a class="indexterm" href="#calibre_link-2408">Mapper</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapFile class, <a class="indexterm" href="#calibre_link-2409">MapFile</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapFileOutputFormat class, <a class="indexterm" href="#calibre_link-2410">MapFileOutputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapFn class, <a class="indexterm" href="#calibre_link-2411">parallelDo()</a>, <a class="indexterm" href="#calibre_link-2412">combineValues()</a></dt><dd class="calibre8"></dd><dt class="calibre57">Mapper interface</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2413">Mapper</a>–<a class="indexterm" href="#calibre_link-2414">Mapper</a></dt><dd class="calibre8"></dd><dt class="calibre57">finding information on input splits, <a class="indexterm" href="#calibre_link-2415">File information in the mapper</a></dt><dd class="calibre8"></dd><dt class="calibre57">task execution, <a class="indexterm" href="#calibre_link-2416">The Task Execution Environment</a></dt><dd class="calibre8"></dd><dt class="calibre57">type parameters, <a class="indexterm" href="#calibre_link-2417">MapReduce Types</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Mapred class, <a class="indexterm" href="#calibre_link-2418">Crunch Libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapred-env.sh file, <a class="indexterm" href="#calibre_link-2419">Hadoop Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapred-site.xml file, <a class="indexterm" href="#calibre_link-2420">Hadoop Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapred.child.java.opts property, <a class="indexterm" href="#calibre_link-2421">Remote Debugging</a>, <a class="indexterm" href="#calibre_link-2422">Configuration Tuning</a>, <a class="indexterm" href="#calibre_link-2423">Memory settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapred.combiner.class property, <a class="indexterm" href="#calibre_link-2424">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapred.input.format.class property, <a class="indexterm" href="#calibre_link-2425">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapred.job.tracker property, <a class="indexterm" href="#calibre_link-2426">Running a Job in a Local Job Runner</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapred.map.runner.class property, <a class="indexterm" href="#calibre_link-2427">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapred.mapoutput.key.class property, <a class="indexterm" href="#calibre_link-2428">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapred.mapoutput.value.class property, <a class="indexterm" href="#calibre_link-2429">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapred.mapper.class property, <a class="indexterm" href="#calibre_link-2430">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapred.output.format.class property, <a class="indexterm" href="#calibre_link-2431">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapred.output.key.class property, <a class="indexterm" href="#calibre_link-2432">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapred.output.key.comparator.class property, <a class="indexterm" href="#calibre_link-2433">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapred.output.value.class property, <a class="indexterm" href="#calibre_link-2434">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapred.output.value.groupfn.class property, <a class="indexterm" href="#calibre_link-2435">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapred.partitioner.class property, <a class="indexterm" href="#calibre_link-2436">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapred.reducer.class property, <a class="indexterm" href="#calibre_link-2437">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapReduce</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2438">Data Storage and Analysis</a>, <a class="indexterm" href="#calibre_link-2439">MapReduce</a>, <a class="indexterm" href="#calibre_link-2440">MapReduce Workflows</a></dt><dd class="calibre8"></dd><dt class="calibre57">anatomy of job runs, <a class="indexterm" href="#calibre_link-2441">Anatomy of a MapReduce Job Run</a>–<a class="indexterm" href="#calibre_link-2442">Job Completion</a></dt><dd class="calibre8"></dd><dt class="calibre57">Avro support, <a class="indexterm" href="#calibre_link-2443">Avro MapReduce</a>–<a class="indexterm" href="#calibre_link-2444">Sorting Using Avro MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">batch processing, <a class="indexterm" href="#calibre_link-2445">Beyond Batch</a></dt><dd class="calibre8"></dd><dt class="calibre57">benchmarking with TeraSort, <a class="indexterm" href="#calibre_link-2446">Benchmarking MapReduce with TeraSort</a></dt><dd class="calibre8"></dd><dt class="calibre57">cluster setup and installation, <a class="indexterm" href="#calibre_link-2447">Creating Unix User Accounts</a></dt><dd class="calibre8"></dd><dt class="calibre57">compression and, <a class="indexterm" href="#calibre_link-2448">Compression and Input Splits</a>–<a class="indexterm" href="#calibre_link-2449">Compressing map output</a></dt><dd class="calibre8"></dd><dt class="calibre57">counters, <a class="indexterm" href="#calibre_link-2450">Counters</a>–<a class="indexterm" href="#calibre_link-2451">User-Defined Streaming Counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">Crunch and, <a class="indexterm" href="#calibre_link-2452">Crunch</a></dt><dd class="calibre8"></dd><dt class="calibre57">daemon properties, <a class="indexterm" href="#calibre_link-2453">Memory settings in YARN and MapReduce</a>–<a class="indexterm" href="#calibre_link-2454">CPU settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">decomposing problems into jobs, <a class="indexterm" href="#calibre_link-2455">Decomposing a Problem into MapReduce Jobs</a>–<a class="indexterm" href="#calibre_link-2456">Decomposing a Problem into MapReduce Jobs</a></dt><dd class="calibre8"></dd><dt class="calibre57">default job, <a class="indexterm" href="#calibre_link-2457">The Default MapReduce Job</a>–<a class="indexterm" href="#calibre_link-2458">Keys and values in Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">developing applications</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2459">Developing a MapReduce Application</a></dt><dd class="calibre8"></dd><dt class="calibre57">Configuration API, <a class="indexterm" href="#calibre_link-2460">The Configuration API</a>–<a class="indexterm" href="#calibre_link-2461">Variable Expansion</a></dt><dd class="calibre8"></dd><dt class="calibre57">running locally on test data, <a class="indexterm" href="#calibre_link-2462">Running Locally on Test Data</a>–<a class="indexterm" href="#calibre_link-2463">Testing the Driver</a></dt><dd class="calibre8"></dd><dt class="calibre57">running on clusters, <a class="indexterm" href="#calibre_link-2464">Running on a Cluster</a>–<a class="indexterm" href="#calibre_link-2465">Remote Debugging</a></dt><dd class="calibre8"></dd><dt class="calibre57">setting up development environment, <a class="indexterm" href="#calibre_link-2466">Setting Up the Development Environment</a>–<a class="indexterm" href="#calibre_link-2467">GenericOptionsParser, Tool, and ToolRunner</a></dt><dd class="calibre8"></dd><dt class="calibre57">tuning jobs, <a class="indexterm" href="#calibre_link-2468">Tuning a Job</a>–<a class="indexterm" href="#calibre_link-2469">The HPROF profiler</a></dt><dd class="calibre8"></dd><dt class="calibre57">workflows, <a class="indexterm" href="#calibre_link-2470">MapReduce Workflows</a>–<a class="indexterm" href="#calibre_link-2471">Running an Oozie workflow job</a></dt><dd class="calibre8"></dd><dt class="calibre57">writing unit tests, <a class="indexterm" href="#calibre_link-2472">Writing a Unit Test with MRUnit</a>–<a class="indexterm" href="#calibre_link-2473">Reducer</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">failure considerations, <a class="indexterm" href="#calibre_link-2474">Failures</a>–<a class="indexterm" href="#calibre_link-2475">Resource Manager Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hadoop Streaming, <a class="indexterm" href="#calibre_link-2476">Hadoop Streaming</a>–<a class="indexterm" href="#calibre_link-2477">Python</a></dt><dd class="calibre8"></dd><dt class="calibre57">HBase and, <a class="indexterm" href="#calibre_link-2478">MapReduce</a>–<a class="indexterm" href="#calibre_link-2479">MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive and, <a class="indexterm" href="#calibre_link-2480">Execution engines</a></dt><dd class="calibre8"></dd><dt class="calibre57">input formats, <a class="indexterm" href="#calibre_link-2481">Input Formats</a>–<a class="indexterm" href="#calibre_link-2482">Database Input (and Output)</a></dt><dd class="calibre8"></dd><dt class="calibre57">joining data, <a class="indexterm" href="#calibre_link-2483">Joins</a>–<a class="indexterm" href="#calibre_link-2484">Reduce-Side Joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">library classes supported, <a class="indexterm" href="#calibre_link-2485">MapReduce Library Classes</a></dt><dd class="calibre8"></dd><dt class="calibre57">old and new API comparison, <a class="indexterm" href="#calibre_link-2486">The Old and New Java MapReduce APIs</a>–<a class="indexterm" href="#calibre_link-2487">The Old and New Java MapReduce APIs</a></dt><dd class="calibre8"></dd><dt class="calibre57">old API signatures, <a class="indexterm" href="#calibre_link-2488">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">output formats, <a class="indexterm" href="#calibre_link-2489">Output Formats</a>–<a class="indexterm" href="#calibre_link-2490">Database Output</a></dt><dd class="calibre8"></dd><dt class="calibre57">Parquet support, <a class="indexterm" href="#calibre_link-2491">Parquet MapReduce</a>–<a class="indexterm" href="#calibre_link-2492">Parquet MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">progress reporting in, <a class="indexterm" href="#calibre_link-2493">Progress and Status Updates</a></dt><dd class="calibre8"></dd><dt class="calibre57">querying data, <a class="indexterm" href="#calibre_link-2494">Querying All Your Data</a>, <a class="indexterm" href="#calibre_link-2495">MapReduce Scripts</a></dt><dd class="calibre8"></dd><dt class="calibre57">resource requests, <a class="indexterm" href="#calibre_link-2496">Resource Requests</a></dt><dd class="calibre8"></dd><dt class="calibre57">shuffle and sort, <a class="indexterm" href="#calibre_link-2497">Shuffle and Sort</a>–<a class="indexterm" href="#calibre_link-2498">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">side data distribution, <a class="indexterm" href="#calibre_link-2499">Side Data Distribution</a>–<a class="indexterm" href="#calibre_link-2500">The distributed cache API</a></dt><dd class="calibre8"></dd><dt class="calibre57">sorting data, <a class="indexterm" href="#calibre_link-2501">Sorting</a>–<a class="indexterm" href="#calibre_link-2502">Streaming</a>, <a class="indexterm" href="#calibre_link-2503">Sorting Using Avro MapReduce</a>–<a class="indexterm" href="#calibre_link-2504">Sorting Using Avro MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">Spark and, <a class="indexterm" href="#calibre_link-2505">Transformations and Actions</a></dt><dd class="calibre8"></dd><dt class="calibre57">Sqoop support, <a class="indexterm" href="#calibre_link-2506">A Sample Import</a>, <a class="indexterm" href="#calibre_link-2507">Imports: A Deeper Look</a>, <a class="indexterm" href="#calibre_link-2508">Exports: A Deeper Look</a></dt><dd class="calibre8"></dd><dt class="calibre57">starting and stopping daemons, <a class="indexterm" href="#calibre_link-2509">Starting and Stopping the Daemons</a></dt><dd class="calibre8"></dd><dt class="calibre57">system comparison, <a class="indexterm" href="#calibre_link-2510">Relational Database Management Systems</a>–<a class="indexterm" href="#calibre_link-2511">Volunteer Computing</a></dt><dd class="calibre8"></dd><dt class="calibre57">task execution, <a class="indexterm" href="#calibre_link-2512">Task Execution</a>, <a class="indexterm" href="#calibre_link-2513">Task Execution</a>–<a class="indexterm" href="#calibre_link-2514">Task side-effect files</a></dt><dd class="calibre8"></dd><dt class="calibre57">types supported, <a class="indexterm" href="#calibre_link-2515">MapReduce Types</a>–<a class="indexterm" href="#calibre_link-2516">The Default MapReduce Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">weather dataset example, <a class="indexterm" href="#calibre_link-2517">A Weather Dataset</a>–<a class="indexterm" href="#calibre_link-2518">Running a Distributed MapReduce Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">YARN comparison, <a class="indexterm" href="#calibre_link-2519">YARN Compared to MapReduce 1</a>–<a class="indexterm" href="#calibre_link-2520">YARN Compared to MapReduce 1</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Mapreduce class, <a class="indexterm" href="#calibre_link-2521">Crunch Libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapReduce mode (Pig), <a class="indexterm" href="#calibre_link-2522">MapReduce mode</a>, <a class="indexterm" href="#calibre_link-2523">Parallelism</a></dt><dd class="calibre8"></dd><dt class="calibre57">MAPREDUCE statement (Pig Latin), <a class="indexterm" href="#calibre_link-2524">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.am.max-attempts property, <a class="indexterm" href="#calibre_link-2525">Application Master Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.client.progressmonitor.pollinterval
          property, <a class="indexterm" href="#calibre_link-2526">Progress and Status Updates</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.client.submit.file.replication
              property, <a class="indexterm" href="#calibre_link-2527">Job Submission</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.cluster.acls.enabled property, <a class="indexterm" href="#calibre_link-2528">Other Security Enhancements</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.cluster.local.dir property, <a class="indexterm" href="#calibre_link-2529">Remote Debugging</a>, <a class="indexterm" href="#calibre_link-2530">The Map Side</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.framework.name property, <a class="indexterm" href="#calibre_link-2531">Running a Job in a Local Job Runner</a>, <a class="indexterm" href="#calibre_link-2532">Testing the Driver</a>, <a class="indexterm" href="#calibre_link-2533">MapReduce mode</a>, <a class="indexterm" href="#calibre_link-2534">Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.input.fileinputformat.input.dir.recursive
              property, <a class="indexterm" href="#calibre_link-2535">FileInputFormat input paths</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.input.fileinputformat.inputdir
                    property, <a class="indexterm" href="#calibre_link-2536">FileInputFormat input paths</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.input.fileinputformat.split.maxsize
                    property, <a class="indexterm" href="#calibre_link-2537">FileInputFormat input splits</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.input.fileinputformat.split.minsize
                    property, <a class="indexterm" href="#calibre_link-2538">FileInputFormat input splits</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.input.keyvaluelinerecordreader.key.value.separator
            property, <a class="indexterm" href="#calibre_link-2539">KeyValueTextInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.input.lineinputformat.linespermap
            property, <a class="indexterm" href="#calibre_link-2540">NLineInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.input.linerecordreader.line.maxlength
              property, <a class="indexterm" href="#calibre_link-2541">Controlling the maximum line length</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.input.pathFilter.class
                    property, <a class="indexterm" href="#calibre_link-2542">FileInputFormat input paths</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.acl-modify-job property, <a class="indexterm" href="#calibre_link-2543">Other Security Enhancements</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.acl-view-job property, <a class="indexterm" href="#calibre_link-2544">Other Security Enhancements</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.combine.class property, <a class="indexterm" href="#calibre_link-2545">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.end-notification.url property, <a class="indexterm" href="#calibre_link-2546">Job Completion</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.hdfs-servers property, <a class="indexterm" href="#calibre_link-2547">Delegation Tokens</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.id property, <a class="indexterm" href="#calibre_link-2548">The Task Execution Environment</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.inputformat.class property, <a class="indexterm" href="#calibre_link-2549">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.map.class property, <a class="indexterm" href="#calibre_link-2550">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.maxtaskfailures.per.tracker
          property, <a class="indexterm" href="#calibre_link-2551">Node Manager Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.output.group.comparator.class
                property, <a class="indexterm" href="#calibre_link-2552">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.output.key.class property, <a class="indexterm" href="#calibre_link-2553">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.output.key.comparator.class
                property, <a class="indexterm" href="#calibre_link-2554">MapReduce Types</a>, <a class="indexterm" href="#calibre_link-2555">Partial Sort</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.output.value.class property, <a class="indexterm" href="#calibre_link-2556">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.outputformat.class property, <a class="indexterm" href="#calibre_link-2557">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.partitioner.class property, <a class="indexterm" href="#calibre_link-2558">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.queuename property, <a class="indexterm" href="#calibre_link-2559">Queue placement</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.reduce.class property, <a class="indexterm" href="#calibre_link-2560">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.reduce.slowstart.completedmaps
            property, <a class="indexterm" href="#calibre_link-2561">Reduce slow start</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.reduces property, <a class="indexterm" href="#calibre_link-2562">Job Initialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.ubertask.enable property, <a class="indexterm" href="#calibre_link-2563">Job Initialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.ubertask.maxbytes property, <a class="indexterm" href="#calibre_link-2564">Job Initialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.ubertask.maxmaps property, <a class="indexterm" href="#calibre_link-2565">Job Initialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.ubertask.maxreduces property, <a class="indexterm" href="#calibre_link-2566">Job Initialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.job.user.classpath.first property, <a class="indexterm" href="#calibre_link-2567">Task classpath precedence</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.jobhistory.address property, <a class="indexterm" href="#calibre_link-2568">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.jobhistory.bind-host property, <a class="indexterm" href="#calibre_link-2569">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.jobhistory.webapp.address
                  property, <a class="indexterm" href="#calibre_link-2570">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.map.combine.minspills property, <a class="indexterm" href="#calibre_link-2571">The Map Side</a>, <a class="indexterm" href="#calibre_link-2572">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.map.cpu.vcores property, <a class="indexterm" href="#calibre_link-2573">Task Assignment</a>, <a class="indexterm" href="#calibre_link-2574">CPU settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.map.failures.maxpercent property, <a class="indexterm" href="#calibre_link-2575">Task Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.map.input.file property, <a class="indexterm" href="#calibre_link-2576">File information in the mapper</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.map.input.length property, <a class="indexterm" href="#calibre_link-2577">File information in the mapper</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.map.input.start property, <a class="indexterm" href="#calibre_link-2578">File information in the mapper</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.map.java.opts property, <a class="indexterm" href="#calibre_link-2579">Memory settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.map.log.level property, <a class="indexterm" href="#calibre_link-2580">Hadoop Logs</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.map.maxattempts property, <a class="indexterm" href="#calibre_link-2581">Task Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.map.memory.mb property, <a class="indexterm" href="#calibre_link-2582">Task Assignment</a>, <a class="indexterm" href="#calibre_link-2583">Memory settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.map.output.compress property, <a class="indexterm" href="#calibre_link-2584">Compressing map output</a>, <a class="indexterm" href="#calibre_link-2585">The Map Side</a>, <a class="indexterm" href="#calibre_link-2586">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.map.output.compress.codec property, <a class="indexterm" href="#calibre_link-2587">Compressing map output</a>, <a class="indexterm" href="#calibre_link-2588">The Map Side</a>, <a class="indexterm" href="#calibre_link-2589">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.map.output.key.class property, <a class="indexterm" href="#calibre_link-2590">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.map.output.value.class property, <a class="indexterm" href="#calibre_link-2591">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.map.sort.spill.percent property, <a class="indexterm" href="#calibre_link-2592">The Map Side</a>, <a class="indexterm" href="#calibre_link-2593">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.map.speculative property, <a class="indexterm" href="#calibre_link-2594">Speculative Execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.mapper.multithreadedmapper.threads
          property, <a class="indexterm" href="#calibre_link-2595">Input Splits and Records</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.output.fileoutputformat.compress
          property, <a class="indexterm" href="#calibre_link-2596">Using Compression in MapReduce</a>, <a class="indexterm" href="#calibre_link-2597">Parquet Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.output.fileoutputformat.compress.codec
          property, <a class="indexterm" href="#calibre_link-2598">Using Compression in MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.output.fileoutputformat.compress.type
          property, <a class="indexterm" href="#calibre_link-2599">Using Compression in MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.output.textoutputformat.separator
          property, <a class="indexterm" href="#calibre_link-2600">Text Output</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.reduce.cpu.vcores property, <a class="indexterm" href="#calibre_link-2601">Task Assignment</a>, <a class="indexterm" href="#calibre_link-2602">CPU settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.reduce.failures.maxpercent property, <a class="indexterm" href="#calibre_link-2603">Task Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.reduce.input.buffer.percent property, <a class="indexterm" href="#calibre_link-2604">Configuration Tuning</a>, <a class="indexterm" href="#calibre_link-2605">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.reduce.java.opts property, <a class="indexterm" href="#calibre_link-2606">Memory settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.reduce.log.level property, <a class="indexterm" href="#calibre_link-2607">Hadoop Logs</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.reduce.maxattempts property, <a class="indexterm" href="#calibre_link-2608">Task Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.reduce.memory.mb property, <a class="indexterm" href="#calibre_link-2609">Task Assignment</a>, <a class="indexterm" href="#calibre_link-2610">Memory settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.reduce.merge.inmem.threshold property, <a class="indexterm" href="#calibre_link-2611">The Reduce Side</a>, <a class="indexterm" href="#calibre_link-2612">Configuration Tuning</a>, <a class="indexterm" href="#calibre_link-2613">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.reduce.shuffle.input.buffer.percent
          property, <a class="indexterm" href="#calibre_link-2614">The Reduce Side</a>, <a class="indexterm" href="#calibre_link-2615">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.reduce.shuffle.maxfetchfailures
                  property, <a class="indexterm" href="#calibre_link-2616">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.reduce.shuffle.merge.percent property, <a class="indexterm" href="#calibre_link-2617">The Reduce Side</a>, <a class="indexterm" href="#calibre_link-2618">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.reduce.shuffle.parallelcopies property, <a class="indexterm" href="#calibre_link-2619">The Reduce Side</a>, <a class="indexterm" href="#calibre_link-2620">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.reduce.speculative property, <a class="indexterm" href="#calibre_link-2621">Speculative Execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.shuffle.max.threads property, <a class="indexterm" href="#calibre_link-2622">The Map Side</a>, <a class="indexterm" href="#calibre_link-2623">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.shuffle.port property, <a class="indexterm" href="#calibre_link-2624">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.shuffle.ssl.enabled property, <a class="indexterm" href="#calibre_link-2625">Other Security Enhancements</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.task.attempt.id property, <a class="indexterm" href="#calibre_link-2626">The Task Execution Environment</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.task.files.preserve.failedtasks
          property, <a class="indexterm" href="#calibre_link-2627">Remote Debugging</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.task.files.preserve.filepattern
          property, <a class="indexterm" href="#calibre_link-2628">Remote Debugging</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.task.id property, <a class="indexterm" href="#calibre_link-2629">The Task Execution Environment</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.task.io.sort.factor property, <a class="indexterm" href="#calibre_link-2630">The Map Side</a>, <a class="indexterm" href="#calibre_link-2631">The Reduce Side</a>, <a class="indexterm" href="#calibre_link-2632">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.task.io.sort.mb property, <a class="indexterm" href="#calibre_link-2633">The Map Side</a>, <a class="indexterm" href="#calibre_link-2634">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.task.ismap property, <a class="indexterm" href="#calibre_link-2635">The Task Execution Environment</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.task.output.dir property, <a class="indexterm" href="#calibre_link-2636">Task side-effect files</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.task.partition property, <a class="indexterm" href="#calibre_link-2637">The Task Execution Environment</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.task.profile property, <a class="indexterm" href="#calibre_link-2638">The HPROF profiler</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.task.profile.maps property, <a class="indexterm" href="#calibre_link-2639">The HPROF profiler</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.task.profile.reduces property, <a class="indexterm" href="#calibre_link-2640">The HPROF profiler</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.task.timeout property, <a class="indexterm" href="#calibre_link-2641">Task Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapreduce.task.userlog.limit.kb property, <a class="indexterm" href="#calibre_link-2642">Hadoop Logs</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapWritable class, <a class="indexterm" href="#calibre_link-2643">Writable collections</a></dt><dd class="calibre8"></dd><dt class="calibre57">MAP_INPUT_RECORDS counter, <a class="indexterm" href="#calibre_link-2644">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">MAP_OUTPUT_BYTES counter, <a class="indexterm" href="#calibre_link-2645">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">MAP_OUTPUT_MATERIALIZED_BYTES counter, <a class="indexterm" href="#calibre_link-2646">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">MAP_OUTPUT_RECORDS counter, <a class="indexterm" href="#calibre_link-2647">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">mashups, <a class="indexterm" href="#calibre_link-2648">Data!</a></dt><dd class="calibre8"></dd><dt class="calibre57">Massie, Matt, <a class="indexterm" href="#calibre_link-2649">Biological Data Science: Saving Lives with Software</a></dt><dd class="calibre8"></dd><dt class="calibre57">master nodes (HBase), <a class="indexterm" href="#calibre_link-2650">Implementation</a></dt><dd class="calibre8"></dd><dt class="calibre57">master−worker pattern (namenodes), <a class="indexterm" href="#calibre_link-2651">Namenodes and Datanodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">materialization process (Crunch), <a class="indexterm" href="#calibre_link-2652">Materialization</a>–<a class="indexterm" href="#calibre_link-2653">PObject</a></dt><dd class="calibre8"></dd><dt class="calibre57">Maven POM (Project Object Model), <a class="indexterm" href="#calibre_link-2654">Setting Up the Development Environment</a>–<a class="indexterm" href="#calibre_link-2655">Setting Up the Development Environment</a>, <a class="indexterm" href="#calibre_link-2656">Packaging a Job</a>, <a class="indexterm" href="#calibre_link-2657">The Specific API</a></dt><dd class="calibre8"></dd><dt class="calibre57">MAX function (Pig Latin), <a class="indexterm" href="#calibre_link-2658">Validation and nulls</a>, <a class="indexterm" href="#calibre_link-2659">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">MB_MILLIS_MAPS counter, <a class="indexterm" href="#calibre_link-2660">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">MB_MILLIS_REDUCES counter, <a class="indexterm" href="#calibre_link-2661">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">memory management</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">buffering writes, <a class="indexterm" href="#calibre_link-2662">The Map Side</a></dt><dd class="calibre8"></dd><dt class="calibre57">container virtual memory constraints, <a class="indexterm" href="#calibre_link-2663">Memory settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">daemons, <a class="indexterm" href="#calibre_link-2664">Memory heap size</a></dt><dd class="calibre8"></dd><dt class="calibre57">memory heap size, <a class="indexterm" href="#calibre_link-2665">Memory heap size</a></dt><dd class="calibre8"></dd><dt class="calibre57">namenodes, <a class="indexterm" href="#calibre_link-2666">Master node scenarios</a>, <a class="indexterm" href="#calibre_link-2667">Memory heap size</a></dt><dd class="calibre8"></dd><dt class="calibre57">Spark and, <a class="indexterm" href="#calibre_link-2668">Spark</a></dt><dd class="calibre8"></dd><dt class="calibre57">task assignments, <a class="indexterm" href="#calibre_link-2669">Task Assignment</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">MemPipeline class, <a class="indexterm" href="#calibre_link-2670">union()</a></dt><dd class="calibre8"></dd><dt class="calibre57">MERGED_MAP_OUTPUTS counter, <a class="indexterm" href="#calibre_link-2671">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">Message Passing Interface (MPI), <a class="indexterm" href="#calibre_link-2672">Grid Computing</a></dt><dd class="calibre8"></dd><dt class="calibre57">metadata</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">backups of, <a class="indexterm" href="#calibre_link-2673">Metadata backups</a></dt><dd class="calibre8"></dd><dt class="calibre57">block sizes and, <a class="indexterm" href="#calibre_link-2674">Blocks</a></dt><dd class="calibre8"></dd><dt class="calibre57">filesystems and, <a class="indexterm" href="#calibre_link-2675">The filesystem image and edit log</a>–<a class="indexterm" href="#calibre_link-2676">The filesystem image and edit log</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive metastore, <a class="indexterm" href="#calibre_link-2677">Installing Hive</a>, <a class="indexterm" href="#calibre_link-2678">Hive Services</a>, <a class="indexterm" href="#calibre_link-2679">The Metastore</a>–<a class="indexterm" href="#calibre_link-2680">The Metastore</a></dt><dd class="calibre8"></dd><dt class="calibre57">namenode memory requirements, <a class="indexterm" href="#calibre_link-2681">The Design of HDFS</a></dt><dd class="calibre8"></dd><dt class="calibre57">Parquet considerations, <a class="indexterm" href="#calibre_link-2682">Parquet File Format</a></dt><dd class="calibre8"></dd><dt class="calibre57">querying, <a class="indexterm" href="#calibre_link-2683">File metadata: FileStatus</a>–<a class="indexterm" href="#calibre_link-2684">File metadata: FileStatus</a></dt><dd class="calibre8"></dd><dt class="calibre57">upgrade considerations, <a class="indexterm" href="#calibre_link-2685">HDFS data and metadata upgrades</a>–<a class="indexterm" href="#calibre_link-2686">Finalize the upgrade (optional)</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">metastore (Hive), <a class="indexterm" href="#calibre_link-2687">Installing Hive</a>, <a class="indexterm" href="#calibre_link-2688">Hive Services</a>, <a class="indexterm" href="#calibre_link-2689">The Metastore</a>–<a class="indexterm" href="#calibre_link-2690">The Metastore</a></dt><dd class="calibre8"></dd><dt class="calibre57">METASTORE_PORT environment variable, <a class="indexterm" href="#calibre_link-2691">Hive Services</a></dt><dd class="calibre8"></dd><dt class="calibre57">metrics</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">counters and, <a class="indexterm" href="#calibre_link-2692">Metrics and JMX</a></dt><dd class="calibre8"></dd><dt class="calibre57">HBase and, <a class="indexterm" href="#calibre_link-2693">Metrics</a></dt><dd class="calibre8"></dd><dt class="calibre57">JMX and, <a class="indexterm" href="#calibre_link-2694">Metrics and JMX</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Microsoft Research MyLifeBits project, <a class="indexterm" href="#calibre_link-2695">Data!</a></dt><dd class="calibre8"></dd><dt class="calibre57">MILLIS_MAPS counter, <a class="indexterm" href="#calibre_link-2696">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">MILLIS_REDUCES counter, <a class="indexterm" href="#calibre_link-2697">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">MIN function (Pig Latin), <a class="indexterm" href="#calibre_link-2698">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">miniclusters, testing in, <a class="indexterm" href="#calibre_link-2699">Testing the Driver</a></dt><dd class="calibre8"></dd><dt class="calibre57">minimal replication condition, <a class="indexterm" href="#calibre_link-2700">Safe Mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">MIP (Message Passing Interface), <a class="indexterm" href="#calibre_link-2701">Grid Computing</a></dt><dd class="calibre8"></dd><dt class="calibre57">mkdir command, <a class="indexterm" href="#calibre_link-2702">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">mntr command (ZooKeeper), <a class="indexterm" href="#calibre_link-2703">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">monitoring clusters</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2704">Monitoring</a></dt><dd class="calibre8"></dd><dt class="calibre57">logging support, <a class="indexterm" href="#calibre_link-2705">Logging</a></dt><dd class="calibre8"></dd><dt class="calibre57">metrics and JMX, <a class="indexterm" href="#calibre_link-2706">Metrics and JMX</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">MorphlineSolrSink class, <a class="indexterm" href="#calibre_link-2707">Delivery Guarantees</a></dt><dd class="calibre8"></dd><dt class="calibre57">MRBench benchmark, <a class="indexterm" href="#calibre_link-2708">Other benchmarks</a></dt><dd class="calibre8"></dd><dt class="calibre57">MRUnit library</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2709">Setting Up the Development Environment</a>, <a class="indexterm" href="#calibre_link-2710">Writing a Unit Test with MRUnit</a></dt><dd class="calibre8"></dd><dt class="calibre57">testing map functions, <a class="indexterm" href="#calibre_link-2711">Mapper</a>–<a class="indexterm" href="#calibre_link-2712">Mapper</a></dt><dd class="calibre8"></dd><dt class="calibre57">testing reduce functions, <a class="indexterm" href="#calibre_link-2713">Reducer</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">multiple files</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">input formats, <a class="indexterm" href="#calibre_link-2714">Multiple Inputs</a></dt><dd class="calibre8"></dd><dt class="calibre57">MultipleOutputs class, <a class="indexterm" href="#calibre_link-2715">MultipleOutputs</a>–<a class="indexterm" href="#calibre_link-2716">MultipleOutputs</a></dt><dd class="calibre8"></dd><dt class="calibre57">output formats, <a class="indexterm" href="#calibre_link-2717">Multiple Outputs</a>–<a class="indexterm" href="#calibre_link-2718">MultipleOutputs</a></dt><dd class="calibre8"></dd><dt class="calibre57">partitioning data, <a class="indexterm" href="#calibre_link-2719">An example: Partitioning data</a>–<a class="indexterm" href="#calibre_link-2720">An example: Partitioning data</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">MultipleInputs class, <a class="indexterm" href="#calibre_link-2721">Multiple Inputs</a>, <a class="indexterm" href="#calibre_link-2722">Reduce-Side Joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">MultipleOutputFormat class, <a class="indexterm" href="#calibre_link-2723">Multiple Outputs</a></dt><dd class="calibre8"></dd><dt class="calibre57">MultipleOutputs class, <a class="indexterm" href="#calibre_link-2724">MultipleOutputs</a>–<a class="indexterm" href="#calibre_link-2725">MultipleOutputs</a></dt><dd class="calibre8"></dd><dt class="calibre57">multiplexing selectors, <a class="indexterm" href="#calibre_link-2726">Replicating and Multiplexing Selectors</a></dt><dd class="calibre8"></dd><dt class="calibre57">multiquery execution, <a class="indexterm" href="#calibre_link-2727">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">multitable insert, <a class="indexterm" href="#calibre_link-2728">Multitable insert</a></dt><dd class="calibre8"></dd><dt class="calibre57">MultithreadedMapper class, <a class="indexterm" href="#calibre_link-2729">Input Splits and Records</a>, <a class="indexterm" href="#calibre_link-2730">MapReduce Library Classes</a></dt><dd class="calibre8"></dd><dt class="calibre57">MultithreadedMapRunner class, <a class="indexterm" href="#calibre_link-2731">MapReduce Library Classes</a></dt><dd class="calibre8"></dd><dt class="calibre57">mv command, <a class="indexterm" href="#calibre_link-2732">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">MyLifeBits project, <a class="indexterm" href="#calibre_link-2733">Data!</a></dt><dd class="calibre8"></dd><dt class="calibre57">MySQL</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">creating database schemas, <a class="indexterm" href="#calibre_link-2734">A Sample Import</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive and, <a class="indexterm" href="#calibre_link-2735">The Metastore</a></dt><dd class="calibre8"></dd><dt class="calibre57">HiveQL and, <a class="indexterm" href="#calibre_link-2736">The Hive Shell</a></dt><dd class="calibre8"></dd><dt class="calibre57">installing and configuring, <a class="indexterm" href="#calibre_link-2737">A Sample Import</a></dt><dd class="calibre8"></dd><dt class="calibre57">populating database, <a class="indexterm" href="#calibre_link-2738">A Sample Import</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">mysqlimport utility, <a class="indexterm" href="#calibre_link-2739">Exports: A Deeper Look</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">N</h3><dl class="book"><dt class="calibre57">namenodes</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2740">A Brief History of Apache Hadoop</a>, <a class="indexterm" href="#calibre_link-2741">Namenodes and Datanodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">block caching, <a class="indexterm" href="#calibre_link-2742">Block Caching</a></dt><dd class="calibre8"></dd><dt class="calibre57">checkpointing process, <a class="indexterm" href="#calibre_link-2743">The filesystem image and edit log</a></dt><dd class="calibre8"></dd><dt class="calibre57">cluster setup and installation, <a class="indexterm" href="#calibre_link-2744">Formatting the HDFS Filesystem</a></dt><dd class="calibre8"></dd><dt class="calibre57">cluster sizing, <a class="indexterm" href="#calibre_link-2745">Master node scenarios</a></dt><dd class="calibre8"></dd><dt class="calibre57">commissioning nodes, <a class="indexterm" href="#calibre_link-2746">Commissioning new nodes</a>–<a class="indexterm" href="#calibre_link-2747">Commissioning new nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">data integrity and, <a class="indexterm" href="#calibre_link-2748">Data Integrity in HDFS</a></dt><dd class="calibre8"></dd><dt class="calibre57">DataStreamer class and, <a class="indexterm" href="#calibre_link-2749">Anatomy of a File Write</a></dt><dd class="calibre8"></dd><dt class="calibre57">decommissioning nodes, <a class="indexterm" href="#calibre_link-2750">Decommissioning old nodes</a>–<a class="indexterm" href="#calibre_link-2751">Decommissioning old nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">DFSInputStream class and, <a class="indexterm" href="#calibre_link-2752">Anatomy of a File Read</a></dt><dd class="calibre8"></dd><dt class="calibre57">directory structure, <a class="indexterm" href="#calibre_link-2753">Namenode directory structure</a>–<a class="indexterm" href="#calibre_link-2754">Namenode directory structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">failover controllers and, <a class="indexterm" href="#calibre_link-2755">Failover and fencing</a></dt><dd class="calibre8"></dd><dt class="calibre57">filesystem metadata and, <a class="indexterm" href="#calibre_link-2756">The Design of HDFS</a></dt><dd class="calibre8"></dd><dt class="calibre57">HDFS federation, <a class="indexterm" href="#calibre_link-2757">HDFS Federation</a></dt><dd class="calibre8"></dd><dt class="calibre57">memory considerations, <a class="indexterm" href="#calibre_link-2758">Master node scenarios</a>, <a class="indexterm" href="#calibre_link-2759">Memory heap size</a></dt><dd class="calibre8"></dd><dt class="calibre57">replica placement, <a class="indexterm" href="#calibre_link-2760">Anatomy of a File Write</a></dt><dd class="calibre8"></dd><dt class="calibre57">safe mode, <a class="indexterm" href="#calibre_link-2761">Safe Mode</a>–<a class="indexterm" href="#calibre_link-2762">Entering and leaving safe mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">secondary, <a class="indexterm" href="#calibre_link-2763">Namenodes and Datanodes</a>, <a class="indexterm" href="#calibre_link-2764">Starting and Stopping the Daemons</a>, <a class="indexterm" href="#calibre_link-2765">Secondary namenode directory structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">single points of failure, <a class="indexterm" href="#calibre_link-2766">HDFS High Availability</a></dt><dd class="calibre8"></dd><dt class="calibre57">starting, <a class="indexterm" href="#calibre_link-2767">Starting and Stopping the Daemons</a>, <a class="indexterm" href="#calibre_link-2768">The filesystem image and edit log</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">namespaceID identifier, <a class="indexterm" href="#calibre_link-2769">Namenode directory structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">National Climatic Data Center (NCDC)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">data format, <a class="indexterm" href="#calibre_link-2770">Data Format</a></dt><dd class="calibre8"></dd><dt class="calibre57">encapsulating parsing logic, <a class="indexterm" href="#calibre_link-2771">Mapper</a></dt><dd class="calibre8"></dd><dt class="calibre57">multiple inputs, <a class="indexterm" href="#calibre_link-2772">Multiple Inputs</a></dt><dd class="calibre8"></dd><dt class="calibre57">preparing weather datafiles, <a class="indexterm" href="#calibre_link-2773">Preparing the NCDC Weather Data</a>–<a class="indexterm" href="#calibre_link-2774">Preparing the NCDC Weather Data</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">NativeAzureFileSystem class, <a class="indexterm" href="#calibre_link-2775">Hadoop Filesystems</a></dt><dd class="calibre8"></dd><dt class="calibre57">NCDC (National Climatic Data Center)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">data format, <a class="indexterm" href="#calibre_link-2776">Data Format</a></dt><dd class="calibre8"></dd><dt class="calibre57">encapsulating parsing logic, <a class="indexterm" href="#calibre_link-2777">Mapper</a></dt><dd class="calibre8"></dd><dt class="calibre57">multiple inputs, <a class="indexterm" href="#calibre_link-2778">Multiple Inputs</a></dt><dd class="calibre8"></dd><dt class="calibre57">preparing weather datafiles, <a class="indexterm" href="#calibre_link-2779">Preparing the NCDC Weather Data</a>–<a class="indexterm" href="#calibre_link-2780">Preparing the NCDC Weather Data</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">NDFS (Nutch Distributed Filesystem), <a class="indexterm" href="#calibre_link-2781">A Brief History of Apache Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">nested encoding, <a class="indexterm" href="#calibre_link-2782">Nested Encoding</a></dt><dd class="calibre8"></dd><dt class="calibre57">net.topology.node.switch.mapping.impl property, <a class="indexterm" href="#calibre_link-2783">Rack awareness</a></dt><dd class="calibre8"></dd><dt class="calibre57">net.topology.script.file.name property, <a class="indexterm" href="#calibre_link-2784">Rack awareness</a></dt><dd class="calibre8"></dd><dt class="calibre57">network topology, <a class="indexterm" href="#calibre_link-2785">Anatomy of a File Read</a>, <a class="indexterm" href="#calibre_link-2786">Anatomy of a File Write</a>, <a class="indexterm" href="#calibre_link-2787">Network Topology</a>–<a class="indexterm" href="#calibre_link-2788">Rack awareness</a></dt><dd class="calibre8"></dd><dt class="calibre57">NFS gateway, <a class="indexterm" href="#calibre_link-2789">NFS</a></dt><dd class="calibre8"></dd><dt class="calibre57">NLineInputFormat class, <a class="indexterm" href="#calibre_link-2790">Task side-effect files</a>, <a class="indexterm" href="#calibre_link-2791">NLineInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">NNBench benchmark, <a class="indexterm" href="#calibre_link-2792">Other benchmarks</a></dt><dd class="calibre8"></dd><dt class="calibre57">node managers</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2793">Anatomy of a YARN Application Run</a></dt><dd class="calibre8"></dd><dt class="calibre57">blacklisting, <a class="indexterm" href="#calibre_link-2794">Node Manager Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">commissioning nodes, <a class="indexterm" href="#calibre_link-2795">Commissioning new nodes</a>–<a class="indexterm" href="#calibre_link-2796">Commissioning new nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">decommissioning nodes, <a class="indexterm" href="#calibre_link-2797">Decommissioning old nodes</a>–<a class="indexterm" href="#calibre_link-2798">Decommissioning old nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">failure considerations, <a class="indexterm" href="#calibre_link-2799">Node Manager Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">heartbeat requests, <a class="indexterm" href="#calibre_link-2800">Delay Scheduling</a></dt><dd class="calibre8"></dd><dt class="calibre57">job initialization process, <a class="indexterm" href="#calibre_link-2801">Job Initialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">resource manager failure, <a class="indexterm" href="#calibre_link-2802">Resource Manager Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">starting, <a class="indexterm" href="#calibre_link-2803">Starting and Stopping the Daemons</a></dt><dd class="calibre8"></dd><dt class="calibre57">streaming tasks, <a class="indexterm" href="#calibre_link-2804">Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">task execution, <a class="indexterm" href="#calibre_link-2805">Task Execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">task failure, <a class="indexterm" href="#calibre_link-2806">Task Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">tasktrackers and, <a class="indexterm" href="#calibre_link-2807">YARN Compared to MapReduce 1</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">normalization (data), <a class="indexterm" href="#calibre_link-2808">Relational Database Management Systems</a></dt><dd class="calibre8"></dd><dt class="calibre57">NullOutputFormat class, <a class="indexterm" href="#calibre_link-2809">Text Output</a></dt><dd class="calibre8"></dd><dt class="calibre57">NullWritable class, <a class="indexterm" href="#calibre_link-2810">Mutability</a>, <a class="indexterm" href="#calibre_link-2811">NullWritable</a>, <a class="indexterm" href="#calibre_link-2812">Text Output</a></dt><dd class="calibre8"></dd><dt class="calibre57">NUM_FAILED_MAPS counter, <a class="indexterm" href="#calibre_link-2813">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">NUM_FAILED_REDUCES counter, <a class="indexterm" href="#calibre_link-2814">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">NUM_FAILED_UBERTASKS counter, <a class="indexterm" href="#calibre_link-2815">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">NUM_KILLED_MAPS counter, <a class="indexterm" href="#calibre_link-2816">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">NUM_KILLED_REDUCES counter, <a class="indexterm" href="#calibre_link-2817">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">NUM_UBER_SUBMAPS counter, <a class="indexterm" href="#calibre_link-2818">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">NUM_UBER_SUBREDUCES counter, <a class="indexterm" href="#calibre_link-2819">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">Nutch Distributed Filesystem (NDFS), <a class="indexterm" href="#calibre_link-2820">A Brief History of Apache Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">Nutch search engine, <a class="indexterm" href="#calibre_link-2821">A Brief History of Apache Hadoop</a>–<a class="indexterm" href="#calibre_link-2822">A Brief History of Apache Hadoop</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">O</h3><dl class="book"><dt class="calibre57">Object class (Java), <a class="indexterm" href="#calibre_link-2823">Implementing a Custom Writable</a></dt><dd class="calibre8"></dd><dt class="calibre57">object properties, printing, <a class="indexterm" href="#calibre_link-2824">GenericOptionsParser, Tool, and ToolRunner</a>–<a class="indexterm" href="#calibre_link-2825">GenericOptionsParser, Tool, and ToolRunner</a></dt><dd class="calibre8"></dd><dt class="calibre57">ObjectWritable class, <a class="indexterm" href="#calibre_link-2826">ObjectWritable and GenericWritable</a></dt><dd class="calibre8"></dd><dt class="calibre57">ODBC drivers, <a class="indexterm" href="#calibre_link-2827">Hive clients</a></dt><dd class="calibre8"></dd><dt class="calibre57">oozie command-line tool, <a class="indexterm" href="#calibre_link-2828">Running an Oozie workflow job</a></dt><dd class="calibre8"></dd><dt class="calibre57">oozie.wf.application.path property, <a class="indexterm" href="#calibre_link-2829">Running an Oozie workflow job</a></dt><dd class="calibre8"></dd><dt class="calibre57">OOZIE_URL environment variable, <a class="indexterm" href="#calibre_link-2830">Running an Oozie workflow job</a></dt><dd class="calibre8"></dd><dt class="calibre57">operations (ZooKeeper)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">exceptions supported, <a class="indexterm" href="#calibre_link-2831">The Resilient ZooKeeper Application</a>–<a class="indexterm" href="#calibre_link-2832">A reliable configuration service</a>, <a class="indexterm" href="#calibre_link-2833">Recoverable exceptions</a></dt><dd class="calibre8"></dd><dt class="calibre57">language bindings, <a class="indexterm" href="#calibre_link-2834">APIs</a></dt><dd class="calibre8"></dd><dt class="calibre57">multiupdate, <a class="indexterm" href="#calibre_link-2835">Multiupdate</a></dt><dd class="calibre8"></dd><dt class="calibre57">watch triggers, <a class="indexterm" href="#calibre_link-2836">Watch triggers</a></dt><dd class="calibre8"></dd><dt class="calibre57">znode supported, <a class="indexterm" href="#calibre_link-2837">Operations</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">operators (HiveQL), <a class="indexterm" href="#calibre_link-2838">Operators and Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">operators (Pig)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">combining and splitting data, <a class="indexterm" href="#calibre_link-2839">Combining and Splitting Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">filtering data, <a class="indexterm" href="#calibre_link-2840">Filtering Data</a>–<a class="indexterm" href="#calibre_link-2841">STREAM</a></dt><dd class="calibre8"></dd><dt class="calibre57">grouping and joining data, <a class="indexterm" href="#calibre_link-2842">Grouping and Joining Data</a>–<a class="indexterm" href="#calibre_link-2843">GROUP</a></dt><dd class="calibre8"></dd><dt class="calibre57">loading and storing data, <a class="indexterm" href="#calibre_link-2844">Loading and Storing Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">sorting data, <a class="indexterm" href="#calibre_link-2845">Sorting Data</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Optimized Record Columnar File (ORCFile), <a class="indexterm" href="#calibre_link-2846">Parquet</a>, <a class="indexterm" href="#calibre_link-2847">Binary storage formats: Sequence files, Avro datafiles, Parquet
        files, RCFiles, and ORCFiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">ORCFile (Optimized Record Columnar File), <a class="indexterm" href="#calibre_link-2848">Parquet</a>, <a class="indexterm" href="#calibre_link-2849">Binary storage formats: Sequence files, Avro datafiles, Parquet
        files, RCFiles, and ORCFiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">OrcStorage function (Pig Latin), <a class="indexterm" href="#calibre_link-2850">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">ORDER BY clause (Hive), <a class="indexterm" href="#calibre_link-2851">Sorting and Aggregating</a></dt><dd class="calibre8"></dd><dt class="calibre57">ORDER statement (Pig Latin), <a class="indexterm" href="#calibre_link-2852">Statements</a>, <a class="indexterm" href="#calibre_link-2853">Sorting Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">org.apache.avro.mapreduce package, <a class="indexterm" href="#calibre_link-2854">Avro MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">org.apache.crunch.io package, <a class="indexterm" href="#calibre_link-2855">Reading from a source</a></dt><dd class="calibre8"></dd><dt class="calibre57">org.apache.crunch.lib package, <a class="indexterm" href="#calibre_link-2856">Crunch Libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">org.apache.flume.serialization package, <a class="indexterm" href="#calibre_link-2857">File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">org.apache.hadoop.classification package, <a class="indexterm" href="#calibre_link-2858">Upgrades</a></dt><dd class="calibre8"></dd><dt class="calibre57">org.apache.hadoop.conf package, <a class="indexterm" href="#calibre_link-2859">The Configuration API</a></dt><dd class="calibre8"></dd><dt class="calibre57">org.apache.hadoop.hbase package, <a class="indexterm" href="#calibre_link-2860">Java</a></dt><dd class="calibre8"></dd><dt class="calibre57">org.apache.hadoop.hbase.mapreduce package, <a class="indexterm" href="#calibre_link-2861">MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">org.apache.hadoop.hbase.util package, <a class="indexterm" href="#calibre_link-2862">Java</a></dt><dd class="calibre8"></dd><dt class="calibre57">org.apache.hadoop.io package, <a class="indexterm" href="#calibre_link-2863">Java MapReduce</a>, <a class="indexterm" href="#calibre_link-2864">Writable Classes</a></dt><dd class="calibre8"></dd><dt class="calibre57">org.apache.hadoop.io.serializer package, <a class="indexterm" href="#calibre_link-2865">Serialization Frameworks</a></dt><dd class="calibre8"></dd><dt class="calibre57">org.apache.hadoop.mapreduce package, <a class="indexterm" href="#calibre_link-2866">Input Splits and Records</a></dt><dd class="calibre8"></dd><dt class="calibre57">org.apache.hadoop.mapreduce.jobcontrol package, <a class="indexterm" href="#calibre_link-2867">JobControl</a></dt><dd class="calibre8"></dd><dt class="calibre57">org.apache.hadoop.mapreduce.join package, <a class="indexterm" href="#calibre_link-2868">Map-Side Joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">org.apache.hadoop.streaming.mapreduce package, <a class="indexterm" href="#calibre_link-2869">XML</a></dt><dd class="calibre8"></dd><dt class="calibre57">org.apache.pig.builtin package, <a class="indexterm" href="#calibre_link-2870">A Filter UDF</a></dt><dd class="calibre8"></dd><dt class="calibre57">org.apache.spark.rdd package, <a class="indexterm" href="#calibre_link-2871">Transformations and Actions</a></dt><dd class="calibre8"></dd><dt class="calibre57">OTHER_LOCAL_MAPS counter, <a class="indexterm" href="#calibre_link-2872">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">outer joins, <a class="indexterm" href="#calibre_link-2873">Outer joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">output formats</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">binary output, <a class="indexterm" href="#calibre_link-2874">SequenceFileOutputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">database output, <a class="indexterm" href="#calibre_link-2875">Database Input (and Output)</a></dt><dd class="calibre8"></dd><dt class="calibre57">lazy output, <a class="indexterm" href="#calibre_link-2876">Lazy Output</a></dt><dd class="calibre8"></dd><dt class="calibre57">multiple outputs, <a class="indexterm" href="#calibre_link-2877">Multiple Outputs</a>–<a class="indexterm" href="#calibre_link-2878">MultipleOutputs</a></dt><dd class="calibre8"></dd><dt class="calibre57">text output, <a class="indexterm" href="#calibre_link-2879">Text Output</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">OutputCollector interface, <a class="indexterm" href="#calibre_link-2880">Task side-effect files</a></dt><dd class="calibre8"></dd><dt class="calibre57">OutputCommitter class, <a class="indexterm" href="#calibre_link-2881">Job Initialization</a>, <a class="indexterm" href="#calibre_link-2882">Task Execution</a>, <a class="indexterm" href="#calibre_link-2883">Output Committers</a>–<a class="indexterm" href="#calibre_link-2884">Task side-effect files</a></dt><dd class="calibre8"></dd><dt class="calibre57">OutputFormat interface, <a class="indexterm" href="#calibre_link-2885">Output Committers</a>, <a class="indexterm" href="#calibre_link-2886">Output Formats</a>–<a class="indexterm" href="#calibre_link-2887">Database Output</a></dt><dd class="calibre8"></dd><dt class="calibre57">OVERWRITE keyword (Hive), <a class="indexterm" href="#calibre_link-2888">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">OVERWRITE write mode, <a class="indexterm" href="#calibre_link-2889">Existing outputs</a></dt><dd class="calibre8"></dd><dt class="calibre57">O’Malley, Owen, <a class="indexterm" href="#calibre_link-2890">A Brief History of Apache Hadoop</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">P</h3><dl class="book"><dt class="calibre57">packaging jobs</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2891">Packaging a Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">client classpath, <a class="indexterm" href="#calibre_link-2892">The client classpath</a></dt><dd class="calibre8"></dd><dt class="calibre57">packaging dependencies, <a class="indexterm" href="#calibre_link-2893">Packaging dependencies</a></dt><dd class="calibre8"></dd><dt class="calibre57">task classpath, <a class="indexterm" href="#calibre_link-2894">The task classpath</a></dt><dd class="calibre8"></dd><dt class="calibre57">task classpath precedence, <a class="indexterm" href="#calibre_link-2895">Task classpath precedence</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">packaging Oozie workflow applications, <a class="indexterm" href="#calibre_link-2896">Packaging and deploying an Oozie workflow application</a></dt><dd class="calibre8"></dd><dt class="calibre57">PageRank algorithm, <a class="indexterm" href="#calibre_link-2897">Iterative Algorithms</a></dt><dd class="calibre8"></dd><dt class="calibre57">Pair class, <a class="indexterm" href="#calibre_link-2898">parallelDo()</a>, <a class="indexterm" href="#calibre_link-2899">combineValues()</a></dt><dd class="calibre8"></dd><dt class="calibre57">PairRDDFunctions class, <a class="indexterm" href="#calibre_link-2900">A Scala Standalone Application</a></dt><dd class="calibre8"></dd><dt class="calibre57">PARALLEL keyword (Pig Latin), <a class="indexterm" href="#calibre_link-2901">FOREACH...GENERATE</a>, <a class="indexterm" href="#calibre_link-2902">Parallelism</a></dt><dd class="calibre8"></dd><dt class="calibre57">parallel processing, <a class="indexterm" href="#calibre_link-2903">Parallel Copying with distcp</a>–<a class="indexterm" href="#calibre_link-2904">Keeping an HDFS Cluster Balanced</a></dt><dd class="calibre8"></dd><dt class="calibre57">ParallelDo fusion, <a class="indexterm" href="#calibre_link-2905">Inspecting a Crunch Plan</a></dt><dd class="calibre8"></dd><dt class="calibre57">parameter substitution (Pig), <a class="indexterm" href="#calibre_link-2906">Parameter Substitution</a>–<a class="indexterm" href="#calibre_link-2907">Parameter substitution processing</a></dt><dd class="calibre8"></dd><dt class="calibre57">Parquet</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2908">Other File Formats and Column-Oriented Formats</a>, <a class="indexterm" href="#calibre_link-2909">Parquet</a></dt><dd class="calibre8"></dd><dt class="calibre57">Avro and, <a class="indexterm" href="#calibre_link-2910">Avro, Protocol Buffers, and Thrift</a>–<a class="indexterm" href="#calibre_link-2911">Projection and read schemas</a></dt><dd class="calibre8"></dd><dt class="calibre57">binary storage format and, <a class="indexterm" href="#calibre_link-2912">Binary storage formats: Sequence files, Avro datafiles, Parquet
        files, RCFiles, and ORCFiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">configuring, <a class="indexterm" href="#calibre_link-2913">Parquet Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">data model, <a class="indexterm" href="#calibre_link-2914">Data Model</a>–<a class="indexterm" href="#calibre_link-2915">Nested Encoding</a></dt><dd class="calibre8"></dd><dt class="calibre57">file format, <a class="indexterm" href="#calibre_link-2916">Parquet File Format</a>–<a class="indexterm" href="#calibre_link-2917">Parquet File Format</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive support, <a class="indexterm" href="#calibre_link-2918">Text and Binary File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapReduce support, <a class="indexterm" href="#calibre_link-2919">Parquet MapReduce</a>–<a class="indexterm" href="#calibre_link-2920">Parquet MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">nested encoding, <a class="indexterm" href="#calibre_link-2921">Nested Encoding</a></dt><dd class="calibre8"></dd><dt class="calibre57">Protocol Buffers and, <a class="indexterm" href="#calibre_link-2922">Avro, Protocol Buffers, and Thrift</a>–<a class="indexterm" href="#calibre_link-2923">Projection and read schemas</a></dt><dd class="calibre8"></dd><dt class="calibre57">Sqoop support, <a class="indexterm" href="#calibre_link-2924">Text and Binary File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">Thrift and, <a class="indexterm" href="#calibre_link-2925">Avro, Protocol Buffers, and Thrift</a>–<a class="indexterm" href="#calibre_link-2926">Projection and read schemas</a></dt><dd class="calibre8"></dd><dt class="calibre57">tool support, <a class="indexterm" href="#calibre_link-2927">Parquet</a></dt><dd class="calibre8"></dd><dt class="calibre57">writing and reading files, <a class="indexterm" href="#calibre_link-2928">Writing and Reading Parquet Files</a>–<a class="indexterm" href="#calibre_link-2929">Projection and read schemas</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">parquet.block.size property, <a class="indexterm" href="#calibre_link-2930">Parquet Configuration</a>, <a class="indexterm" href="#calibre_link-2931">Parquet MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">parquet.compression property, <a class="indexterm" href="#calibre_link-2932">Parquet Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">parquet.dictionary.page.size property, <a class="indexterm" href="#calibre_link-2933">Parquet Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">parquet.enable.dictionary property, <a class="indexterm" href="#calibre_link-2934">Parquet Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">parquet.example.data package, <a class="indexterm" href="#calibre_link-2935">Writing and Reading Parquet Files</a></dt><dd class="calibre8"></dd><dt class="calibre57">parquet.example.data.simple package, <a class="indexterm" href="#calibre_link-2936">Writing and Reading Parquet Files</a></dt><dd class="calibre8"></dd><dt class="calibre57">parquet.page.size property, <a class="indexterm" href="#calibre_link-2937">Parquet Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">ParquetLoader function (Pig Latin), <a class="indexterm" href="#calibre_link-2938">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">ParquetReader class, <a class="indexterm" href="#calibre_link-2939">Writing and Reading Parquet Files</a></dt><dd class="calibre8"></dd><dt class="calibre57">ParquetStorer function (Pig Latin), <a class="indexterm" href="#calibre_link-2940">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">ParquetWriter class, <a class="indexterm" href="#calibre_link-2941">Writing and Reading Parquet Files</a></dt><dd class="calibre8"></dd><dt class="calibre57">partial sort, <a class="indexterm" href="#calibre_link-2942">Partial Sort</a>–<a class="indexterm" href="#calibre_link-2943">Partial Sort</a></dt><dd class="calibre8"></dd><dt class="calibre57">PARTITION clause (Hive), <a class="indexterm" href="#calibre_link-2944">Inserts</a></dt><dd class="calibre8"></dd><dt class="calibre57">PARTITIONED BY clause (Hive), <a class="indexterm" href="#calibre_link-2945">Partitions</a></dt><dd class="calibre8"></dd><dt class="calibre57">partitioned data</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2946">Relational Database Management Systems</a></dt><dd class="calibre8"></dd><dt class="calibre57">HDFS sinks and, <a class="indexterm" href="#calibre_link-2947">Partitioning and Interceptors</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive tables and, <a class="indexterm" href="#calibre_link-2948">Partitions and Buckets</a>–<a class="indexterm" href="#calibre_link-2949">Partitions</a></dt><dd class="calibre8"></dd><dt class="calibre57">weather dataset example, <a class="indexterm" href="#calibre_link-2950">An example: Partitioning data</a>–<a class="indexterm" href="#calibre_link-2951">An example: Partitioning data</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Partitioner interface, <a class="indexterm" href="#calibre_link-2952">MapReduce Types</a>, <a class="indexterm" href="#calibre_link-2953">Reduce-Side Joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">Path class, <a class="indexterm" href="#calibre_link-2954">Reading Data Using the FileSystem API</a>, <a class="indexterm" href="#calibre_link-2955">Writing Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">PATH environment variable, <a class="indexterm" href="#calibre_link-2956">HDFS data and metadata upgrades</a></dt><dd class="calibre8"></dd><dt class="calibre57">PathFilter interface, <a class="indexterm" href="#calibre_link-2957">Listing files</a>–<a class="indexterm" href="#calibre_link-2958">PathFilter</a></dt><dd class="calibre8"></dd><dt class="calibre57">Paxos algorithm, <a class="indexterm" href="#calibre_link-2959">Implementation</a></dt><dd class="calibre8"></dd><dt class="calibre57">PCollection interface</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2960">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">asCollection() method, <a class="indexterm" href="#calibre_link-2961">PObject</a></dt><dd class="calibre8"></dd><dt class="calibre57">checkpointing pipelines, <a class="indexterm" href="#calibre_link-2962">Checkpointing a Pipeline</a></dt><dd class="calibre8"></dd><dt class="calibre57">materialize() method, <a class="indexterm" href="#calibre_link-2963">Materialization</a>–<a class="indexterm" href="#calibre_link-2964">PObject</a></dt><dd class="calibre8"></dd><dt class="calibre57">parallelDo() method, <a class="indexterm" href="#calibre_link-2965">An Example</a>, <a class="indexterm" href="#calibre_link-2966">parallelDo()</a>–<a class="indexterm" href="#calibre_link-2967">parallelDo()</a>, <a class="indexterm" href="#calibre_link-2968">Inspecting a Crunch Plan</a></dt><dd class="calibre8"></dd><dt class="calibre57">pipeline execution, <a class="indexterm" href="#calibre_link-2969">Pipeline Execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">reading files, <a class="indexterm" href="#calibre_link-2970">Reading from a source</a></dt><dd class="calibre8"></dd><dt class="calibre57">types supported, <a class="indexterm" href="#calibre_link-2971">Types</a>–<a class="indexterm" href="#calibre_link-2972">Records and tuples</a></dt><dd class="calibre8"></dd><dt class="calibre57">union() method, <a class="indexterm" href="#calibre_link-2973">union()</a></dt><dd class="calibre8"></dd><dt class="calibre57">writing files, <a class="indexterm" href="#calibre_link-2974">Writing to a target</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">permissions</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">ACL, <a class="indexterm" href="#calibre_link-2975">ACLs</a></dt><dd class="calibre8"></dd><dt class="calibre57">HDFS considerations, <a class="indexterm" href="#calibre_link-2976">Basic Filesystem Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">storing, <a class="indexterm" href="#calibre_link-2977">Blocks</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">persistence, RDD, <a class="indexterm" href="#calibre_link-2978">Persistence</a>–<a class="indexterm" href="#calibre_link-2979">Persistence levels</a></dt><dd class="calibre8"></dd><dt class="calibre57">persistent data structures, <a class="indexterm" href="#calibre_link-2980">Persistent Data Structures</a></dt><dd class="calibre8"></dd><dt class="calibre57">persistent znodes, <a class="indexterm" href="#calibre_link-2981">Ephemeral znodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">PGroupedTable interface</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2982">An Example</a>, <a class="indexterm" href="#calibre_link-2983">groupByKey()</a></dt><dd class="calibre8"></dd><dt class="calibre57">combineValues() method, <a class="indexterm" href="#calibre_link-2984">combineValues()</a>–<a class="indexterm" href="#calibre_link-2985">combineValues()</a>, <a class="indexterm" href="#calibre_link-2986">Object reuse</a></dt><dd class="calibre8"></dd><dt class="calibre57">mapValues() method, <a class="indexterm" href="#calibre_link-2987">Object reuse</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">PHYSICAL_MEMORY_BYTES counter, <a class="indexterm" href="#calibre_link-2988">Task counters</a>, <a class="indexterm" href="#calibre_link-2989">Memory settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">Pig</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-2990">Pig</a></dt><dd class="calibre8"></dd><dt class="calibre57">additional information, <a class="indexterm" href="#calibre_link-2991">Further Reading</a></dt><dd class="calibre8"></dd><dt class="calibre57">anonymous relations and, <a class="indexterm" href="#calibre_link-2992">Anonymous Relations</a></dt><dd class="calibre8"></dd><dt class="calibre57">comparison with databases, <a class="indexterm" href="#calibre_link-2993">Comparison with Databases</a>–<a class="indexterm" href="#calibre_link-2994">Comparison with Databases</a></dt><dd class="calibre8"></dd><dt class="calibre57">Crunch and, <a class="indexterm" href="#calibre_link-2995">Crunch</a></dt><dd class="calibre8"></dd><dt class="calibre57">data processing operators, <a class="indexterm" href="#calibre_link-2996">Loading and Storing Data</a>–<a class="indexterm" href="#calibre_link-2997">Combining and Splitting Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">execution types, <a class="indexterm" href="#calibre_link-2998">Execution Types</a>–<a class="indexterm" href="#calibre_link-2999">MapReduce mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">installing and running, <a class="indexterm" href="#calibre_link-3000">Installing and Running Pig</a>–<a class="indexterm" href="#calibre_link-3001">Pig Latin Editors</a></dt><dd class="calibre8"></dd><dt class="calibre57">parallelism and, <a class="indexterm" href="#calibre_link-3002">Parallelism</a></dt><dd class="calibre8"></dd><dt class="calibre57">parameter substitution and, <a class="indexterm" href="#calibre_link-3003">Parameter Substitution</a>–<a class="indexterm" href="#calibre_link-3004">Parameter substitution processing</a></dt><dd class="calibre8"></dd><dt class="calibre57">practical techniques, <a class="indexterm" href="#calibre_link-3005">Pig in Practice</a>–<a class="indexterm" href="#calibre_link-3006">Parameter substitution processing</a></dt><dd class="calibre8"></dd><dt class="calibre57">sorting data, <a class="indexterm" href="#calibre_link-3007">Total Sort</a></dt><dd class="calibre8"></dd><dt class="calibre57">user-defined functions, <a class="indexterm" href="#calibre_link-3008">User-Defined Functions</a>–<a class="indexterm" href="#calibre_link-3009">Using a schema</a></dt><dd class="calibre8"></dd><dt class="calibre57">weather dataset example, <a class="indexterm" href="#calibre_link-3010">An Example</a>–<a class="indexterm" href="#calibre_link-3011">Generating Examples</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Pig Latin</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3012">Pig</a>, <a class="indexterm" href="#calibre_link-3013">Pig Latin</a></dt><dd class="calibre8"></dd><dt class="calibre57">built-in types, <a class="indexterm" href="#calibre_link-3014">Types</a>–<a class="indexterm" href="#calibre_link-3015">Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">commands supported, <a class="indexterm" href="#calibre_link-3016">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">editor support, <a class="indexterm" href="#calibre_link-3017">Pig Latin Editors</a></dt><dd class="calibre8"></dd><dt class="calibre57">expressions, <a class="indexterm" href="#calibre_link-3018">Expressions</a>–<a class="indexterm" href="#calibre_link-3019">Expressions</a></dt><dd class="calibre8"></dd><dt class="calibre57">functions, <a class="indexterm" href="#calibre_link-3020">Types</a>, <a class="indexterm" href="#calibre_link-3021">Functions</a>–<a class="indexterm" href="#calibre_link-3022">Other libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">macros, <a class="indexterm" href="#calibre_link-3023">Macros</a>–<a class="indexterm" href="#calibre_link-3024">Macros</a></dt><dd class="calibre8"></dd><dt class="calibre57">schemas, <a class="indexterm" href="#calibre_link-3025">Schemas</a>–<a class="indexterm" href="#calibre_link-3026">Schema merging</a></dt><dd class="calibre8"></dd><dt class="calibre57">statements, <a class="indexterm" href="#calibre_link-3027">Statements</a>–<a class="indexterm" href="#calibre_link-3028">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">structure, <a class="indexterm" href="#calibre_link-3029">Structure</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">pig.auto.local.enabled property, <a class="indexterm" href="#calibre_link-3030">MapReduce mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">pig.auto.local.input.maxbytes, <a class="indexterm" href="#calibre_link-3031">MapReduce mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">PigRunner class, <a class="indexterm" href="#calibre_link-3032">Running Pig Programs</a></dt><dd class="calibre8"></dd><dt class="calibre57">PigServer class, <a class="indexterm" href="#calibre_link-3033">Running Pig Programs</a></dt><dd class="calibre8"></dd><dt class="calibre57">PigStorage function (Pig Latin), <a class="indexterm" href="#calibre_link-3034">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">PIG_CONF_DIR environment variable, <a class="indexterm" href="#calibre_link-3035">MapReduce mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">pipeline execution (Crunch)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3036">Pipeline Execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">checkpointing pipelines, <a class="indexterm" href="#calibre_link-3037">Checkpointing a Pipeline</a></dt><dd class="calibre8"></dd><dt class="calibre57">inspecting plans, <a class="indexterm" href="#calibre_link-3038">Inspecting a Crunch Plan</a>–<a class="indexterm" href="#calibre_link-3039">Inspecting a Crunch Plan</a></dt><dd class="calibre8"></dd><dt class="calibre57">iterative algorithms, <a class="indexterm" href="#calibre_link-3040">Iterative Algorithms</a>–<a class="indexterm" href="#calibre_link-3041">Iterative Algorithms</a></dt><dd class="calibre8"></dd><dt class="calibre57">running pipelines, <a class="indexterm" href="#calibre_link-3042">Running a Pipeline</a>–<a class="indexterm" href="#calibre_link-3043">Debugging</a></dt><dd class="calibre8"></dd><dt class="calibre57">stopping pipelines, <a class="indexterm" href="#calibre_link-3044">Stopping a Pipeline</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Pipeline interface</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">done() method, <a class="indexterm" href="#calibre_link-3045">Stopping a Pipeline</a></dt><dd class="calibre8"></dd><dt class="calibre57">enableDebug() method, <a class="indexterm" href="#calibre_link-3046">Debugging</a></dt><dd class="calibre8"></dd><dt class="calibre57">read() method, <a class="indexterm" href="#calibre_link-3047">Reading from a source</a></dt><dd class="calibre8"></dd><dt class="calibre57">readTextFile() method, <a class="indexterm" href="#calibre_link-3048">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">run() method, <a class="indexterm" href="#calibre_link-3049">Running a Pipeline</a>–<a class="indexterm" href="#calibre_link-3050">Asynchronous execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">runAsync() method, <a class="indexterm" href="#calibre_link-3051">Asynchronous execution</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">PipelineExecution interface, <a class="indexterm" href="#calibre_link-3052">Stopping a Pipeline</a></dt><dd class="calibre8"></dd><dt class="calibre57">PipelineResult class, <a class="indexterm" href="#calibre_link-3053">An Example</a>, <a class="indexterm" href="#calibre_link-3054">Running a Pipeline</a></dt><dd class="calibre8"></dd><dt class="calibre57">PObject interface, <a class="indexterm" href="#calibre_link-3055">PObject</a>, <a class="indexterm" href="#calibre_link-3056">Iterative Algorithms</a></dt><dd class="calibre8"></dd><dt class="calibre57">PositionedReadable interface, <a class="indexterm" href="#calibre_link-3057">FSDataInputStream</a></dt><dd class="calibre8"></dd><dt class="calibre57">preemption, <a class="indexterm" href="#calibre_link-3058">Preemption</a></dt><dd class="calibre8"></dd><dt class="calibre57">PrimitiveEvalFunc class, <a class="indexterm" href="#calibre_link-3059">An Eval UDF</a></dt><dd class="calibre8"></dd><dt class="calibre57">printing object properties, <a class="indexterm" href="#calibre_link-3060">GenericOptionsParser, Tool, and ToolRunner</a>–<a class="indexterm" href="#calibre_link-3061">GenericOptionsParser, Tool, and ToolRunner</a></dt><dd class="calibre8"></dd><dt class="calibre57">profiling tasks, <a class="indexterm" href="#calibre_link-3062">Profiling Tasks</a>–<a class="indexterm" href="#calibre_link-3063">The HPROF profiler</a></dt><dd class="calibre8"></dd><dt class="calibre57">progress, tracking for tasks, <a class="indexterm" href="#calibre_link-3064">Progress and Status Updates</a></dt><dd class="calibre8"></dd><dt class="calibre57">Progressable interface, <a class="indexterm" href="#calibre_link-3065">Writing Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">properties</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">daemon, <a class="indexterm" href="#calibre_link-3066">Important Hadoop Daemon Properties</a>–<a class="indexterm" href="#calibre_link-3067">CPU settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">map-side tuning, <a class="indexterm" href="#calibre_link-3068">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">printing for objects, <a class="indexterm" href="#calibre_link-3069">GenericOptionsParser, Tool, and ToolRunner</a>–<a class="indexterm" href="#calibre_link-3070">GenericOptionsParser, Tool, and ToolRunner</a></dt><dd class="calibre8"></dd><dt class="calibre57">reduce-side tuning, <a class="indexterm" href="#calibre_link-3071">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">znodes, <a class="indexterm" href="#calibre_link-3072">Data Model</a>–<a class="indexterm" href="#calibre_link-3073">Watches</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Protocol Buffers, <a class="indexterm" href="#calibre_link-3074">Avro, Protocol Buffers, and Thrift</a>–<a class="indexterm" href="#calibre_link-3075">Projection and read schemas</a></dt><dd class="calibre8"></dd><dt class="calibre57">ProtoParquetWriter class, <a class="indexterm" href="#calibre_link-3076">Avro, Protocol Buffers, and Thrift</a></dt><dd class="calibre8"></dd><dt class="calibre57">psdsh shell tool, <a class="indexterm" href="#calibre_link-3077">Configuration Management</a></dt><dd class="calibre8"></dd><dt class="calibre57">pseudodistributed mode (Hadoop), <a class="indexterm" href="#calibre_link-3078">Pseudodistributed Mode</a>–<a class="indexterm" href="#calibre_link-3079">Creating a user directory</a></dt><dd class="calibre8"></dd><dt class="calibre57">PTable interface</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3080">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">asMap() method, <a class="indexterm" href="#calibre_link-3081">PObject</a></dt><dd class="calibre8"></dd><dt class="calibre57">creating instance, <a class="indexterm" href="#calibre_link-3082">parallelDo()</a></dt><dd class="calibre8"></dd><dt class="calibre57">finding set of unique values for keys, <a class="indexterm" href="#calibre_link-3083">Object reuse</a></dt><dd class="calibre8"></dd><dt class="calibre57">groupByKey() method, <a class="indexterm" href="#calibre_link-3084">groupByKey()</a></dt><dd class="calibre8"></dd><dt class="calibre57">materializeToMap() method, <a class="indexterm" href="#calibre_link-3085">Materialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">reading text files, <a class="indexterm" href="#calibre_link-3086">Reading from a source</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">PTables class, <a class="indexterm" href="#calibre_link-3087">Crunch Libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">PTableType interface, <a class="indexterm" href="#calibre_link-3088">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">PType interface, <a class="indexterm" href="#calibre_link-3089">parallelDo()</a>, <a class="indexterm" href="#calibre_link-3090">Types</a>–<a class="indexterm" href="#calibre_link-3091">Records and tuples</a>, <a class="indexterm" href="#calibre_link-3092">Object reuse</a></dt><dd class="calibre8"></dd><dt class="calibre57">Public Data Sets, <a class="indexterm" href="#calibre_link-3093">Data!</a></dt><dd class="calibre8"></dd><dt class="calibre57">pwd command, <a class="indexterm" href="#calibre_link-3094">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">PySpark API, <a class="indexterm" href="#calibre_link-3095">A Python Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">pyspark command, <a class="indexterm" href="#calibre_link-3096">A Python Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">Python language</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">Avro and, <a class="indexterm" href="#calibre_link-3097">Python API</a></dt><dd class="calibre8"></dd><dt class="calibre57">incrementing counters, <a class="indexterm" href="#calibre_link-3098">User-Defined Streaming Counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">querying data, <a class="indexterm" href="#calibre_link-3099">MapReduce Scripts</a></dt><dd class="calibre8"></dd><dt class="calibre57">Spark example, <a class="indexterm" href="#calibre_link-3100">A Python Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">weather dataset example, <a class="indexterm" href="#calibre_link-3101">Python</a></dt><dd class="calibre8"></dd></dl></dd></dl></div><div class="book"><h3 class="author1">Q</h3><dl class="book"><dt class="calibre57">QJM (quorum journal manager), <a class="indexterm" href="#calibre_link-3102">HDFS High Availability</a></dt><dd class="calibre8"></dd><dt class="calibre57">querying data</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3103">Querying All Your Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">aggregating data, <a class="indexterm" href="#calibre_link-3104">Sorting and Aggregating</a></dt><dd class="calibre8"></dd><dt class="calibre57">batch processing, <a class="indexterm" href="#calibre_link-3105">Beyond Batch</a></dt><dd class="calibre8"></dd><dt class="calibre57">FileStatus class, <a class="indexterm" href="#calibre_link-3106">File metadata: FileStatus</a>–<a class="indexterm" href="#calibre_link-3107">File metadata: FileStatus</a></dt><dd class="calibre8"></dd><dt class="calibre57">FileSystem class, <a class="indexterm" href="#calibre_link-3108">File metadata: FileStatus</a>–<a class="indexterm" href="#calibre_link-3109">PathFilter</a></dt><dd class="calibre8"></dd><dt class="calibre57">HBase online query application, <a class="indexterm" href="#calibre_link-3110">Building an Online Query Application</a>–<a class="indexterm" href="#calibre_link-3111">Observation queries</a></dt><dd class="calibre8"></dd><dt class="calibre57">joining data, <a class="indexterm" href="#calibre_link-3112">Joins</a>–<a class="indexterm" href="#calibre_link-3113">Map joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapReduce scripts, <a class="indexterm" href="#calibre_link-3114">MapReduce Scripts</a></dt><dd class="calibre8"></dd><dt class="calibre57">sorting data, <a class="indexterm" href="#calibre_link-3115">Sorting and Aggregating</a></dt><dd class="calibre8"></dd><dt class="calibre57">subqueries, <a class="indexterm" href="#calibre_link-3116">Subqueries</a></dt><dd class="calibre8"></dd><dt class="calibre57">views, <a class="indexterm" href="#calibre_link-3117">Views</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">queue elasticity, <a class="indexterm" href="#calibre_link-3118">Capacity Scheduler Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">queues</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">Capacity Scheduler, <a class="indexterm" href="#calibre_link-3119">Capacity Scheduler Configuration</a>–<a class="indexterm" href="#calibre_link-3120">Queue placement</a></dt><dd class="calibre8"></dd><dt class="calibre57">Fair Scheduler, <a class="indexterm" href="#calibre_link-3121">Fair Scheduler Configuration</a>–<a class="indexterm" href="#calibre_link-3122">Preemption</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">quit command, <a class="indexterm" href="#calibre_link-3123">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">quorum journal manager (QJM), <a class="indexterm" href="#calibre_link-3124">HDFS High Availability</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">R</h3><dl class="book"><dt class="calibre57">r (read) permission, <a class="indexterm" href="#calibre_link-3125">Basic Filesystem Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">rack local tasks, <a class="indexterm" href="#calibre_link-3126">Task Assignment</a></dt><dd class="calibre8"></dd><dt class="calibre57">rack topology, <a class="indexterm" href="#calibre_link-3127">Rack awareness</a>–<a class="indexterm" href="#calibre_link-3128">Rack awareness</a></dt><dd class="calibre8"></dd><dt class="calibre57">Rackspace MailTrust, <a class="indexterm" href="#calibre_link-3129">Querying All Your Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">RACK_LOCAL_MAPS counter, <a class="indexterm" href="#calibre_link-3130">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">RAID (redundant array of independent disks), <a class="indexterm" href="#calibre_link-3131">Cluster Specification</a></dt><dd class="calibre8"></dd><dt class="calibre57">Rajaraman, Anand, <a class="indexterm" href="#calibre_link-3132">Data!</a></dt><dd class="calibre8"></dd><dt class="calibre57">RANK statement (Pig Latin), <a class="indexterm" href="#calibre_link-3133">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">RawComparator interface, <a class="indexterm" href="#calibre_link-3134">WritableComparable and comparators</a>, <a class="indexterm" href="#calibre_link-3135">Implementing a RawComparator for speed</a>, <a class="indexterm" href="#calibre_link-3136">Partial Sort</a></dt><dd class="calibre8"></dd><dt class="calibre57">RawLocalFileSystem class, <a class="indexterm" href="#calibre_link-3137">Hadoop Filesystems</a>, <a class="indexterm" href="#calibre_link-3138">LocalFileSystem</a></dt><dd class="calibre8"></dd><dt class="calibre57">RDBMSs (Relational Database Management Systems)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3139">Relational Database Management Systems</a>–<a class="indexterm" href="#calibre_link-3140">Relational Database Management Systems</a></dt><dd class="calibre8"></dd><dt class="calibre57">HBase comparison, <a class="indexterm" href="#calibre_link-3141">HBase Versus RDBMS</a>–<a class="indexterm" href="#calibre_link-3142">HBase</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive metadata and, <a class="indexterm" href="#calibre_link-3143">Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">Pig comparison, <a class="indexterm" href="#calibre_link-3144">Comparison with Databases</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">RDD class</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">filter() method, <a class="indexterm" href="#calibre_link-3145">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">map() method, <a class="indexterm" href="#calibre_link-3146">An Example</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">RDDs (Resilient Distributed Datasets)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3147">An Example</a>, <a class="indexterm" href="#calibre_link-3148">Resilient Distributed Datasets</a></dt><dd class="calibre8"></dd><dt class="calibre57">creating, <a class="indexterm" href="#calibre_link-3149">Creation</a></dt><dd class="calibre8"></dd><dt class="calibre57">Java and, <a class="indexterm" href="#calibre_link-3150">A Java Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">operations on, <a class="indexterm" href="#calibre_link-3151">Transformations and Actions</a>–<a class="indexterm" href="#calibre_link-3152">Aggregation transformations</a></dt><dd class="calibre8"></dd><dt class="calibre57">persistence and, <a class="indexterm" href="#calibre_link-3153">Persistence</a>–<a class="indexterm" href="#calibre_link-3154">Persistence levels</a></dt><dd class="calibre8"></dd><dt class="calibre57">serialization, <a class="indexterm" href="#calibre_link-3155">Serialization</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">read (r) permission, <a class="indexterm" href="#calibre_link-3156">Basic Filesystem Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">READ permission (ACL), <a class="indexterm" href="#calibre_link-3157">ACLs</a></dt><dd class="calibre8"></dd><dt class="calibre57">reading data</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">Crunch support, <a class="indexterm" href="#calibre_link-3158">Reading from a source</a></dt><dd class="calibre8"></dd><dt class="calibre57">FileSystem class and, <a class="indexterm" href="#calibre_link-3159">Reading Data Using the FileSystem API</a>–<a class="indexterm" href="#calibre_link-3160">FSDataInputStream</a>, <a class="indexterm" href="#calibre_link-3161">Anatomy of a File Read</a></dt><dd class="calibre8"></dd><dt class="calibre57">from Hadoop URL, <a class="indexterm" href="#calibre_link-3162">Reading Data from a Hadoop URL</a></dt><dd class="calibre8"></dd><dt class="calibre57">HDFS data flow, <a class="indexterm" href="#calibre_link-3163">Anatomy of a File Read</a>–<a class="indexterm" href="#calibre_link-3164">Anatomy of a File Read</a></dt><dd class="calibre8"></dd><dt class="calibre57">Parquet and, <a class="indexterm" href="#calibre_link-3165">Writing and Reading Parquet Files</a>–<a class="indexterm" href="#calibre_link-3166">Projection and read schemas</a></dt><dd class="calibre8"></dd><dt class="calibre57">SequenceFile class, <a class="indexterm" href="#calibre_link-3167">Reading a SequenceFile</a>–<a class="indexterm" href="#calibre_link-3168">Reading a SequenceFile</a></dt><dd class="calibre8"></dd><dt class="calibre57">short-circuiting local reads, <a class="indexterm" href="#calibre_link-3169">Short-circuit local reads</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">ReadSupport class, <a class="indexterm" href="#calibre_link-3170">Writing and Reading Parquet Files</a></dt><dd class="calibre8"></dd><dt class="calibre57">READ_OPS counter, <a class="indexterm" href="#calibre_link-3171">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">RecordReader class, <a class="indexterm" href="#calibre_link-3172">Input Splits and Records</a>, <a class="indexterm" href="#calibre_link-3173">Processing a whole file as a record</a></dt><dd class="calibre8"></dd><dt class="calibre57">records, processing files as, <a class="indexterm" href="#calibre_link-3174">Processing a whole file as a record</a>–<a class="indexterm" href="#calibre_link-3175">Processing a whole file as a record</a></dt><dd class="calibre8"></dd><dt class="calibre57">REDUCE clause (Hive), <a class="indexterm" href="#calibre_link-3176">MapReduce Scripts</a></dt><dd class="calibre8"></dd><dt class="calibre57">reduce functions (MapReduce)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3177">Map and Reduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">data flow tasks, <a class="indexterm" href="#calibre_link-3178">Data Flow</a>–<a class="indexterm" href="#calibre_link-3179">Specifying a combiner function</a></dt><dd class="calibre8"></dd><dt class="calibre57">general form, <a class="indexterm" href="#calibre_link-3180">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hadoop Streaming, <a class="indexterm" href="#calibre_link-3181">Hadoop Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">Java example, <a class="indexterm" href="#calibre_link-3182">Java MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">joining data, <a class="indexterm" href="#calibre_link-3183">Reduce-Side Joins</a>–<a class="indexterm" href="#calibre_link-3184">Reduce-Side Joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">progress and status updates, <a class="indexterm" href="#calibre_link-3185">Progress and Status Updates</a></dt><dd class="calibre8"></dd><dt class="calibre57">shuffle and sort, <a class="indexterm" href="#calibre_link-3186">The Reduce Side</a>–<a class="indexterm" href="#calibre_link-3187">The Reduce Side</a></dt><dd class="calibre8"></dd><dt class="calibre57">Spark and, <a class="indexterm" href="#calibre_link-3188">DAG Construction</a></dt><dd class="calibre8"></dd><dt class="calibre57">task assignments, <a class="indexterm" href="#calibre_link-3189">Task Assignment</a></dt><dd class="calibre8"></dd><dt class="calibre57">task execution, <a class="indexterm" href="#calibre_link-3190">Task Execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">task failures, <a class="indexterm" href="#calibre_link-3191">Task Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">testing with MRUnit, <a class="indexterm" href="#calibre_link-3192">Reducer</a></dt><dd class="calibre8"></dd><dt class="calibre57">tuning checklist, <a class="indexterm" href="#calibre_link-3193">Tuning a Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">tuning properties, <a class="indexterm" href="#calibre_link-3194">Configuration Tuning</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">ReduceDriver class, <a class="indexterm" href="#calibre_link-3195">Reducer</a></dt><dd class="calibre8"></dd><dt class="calibre57">Reducer interface, <a class="indexterm" href="#calibre_link-3196">The Task Execution Environment</a>, <a class="indexterm" href="#calibre_link-3197">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">REDUCE_INPUT_GROUPS counter, <a class="indexterm" href="#calibre_link-3198">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">REDUCE_INPUT_RECORDS counter, <a class="indexterm" href="#calibre_link-3199">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">REDUCE_OUTPUT_RECORDS counter, <a class="indexterm" href="#calibre_link-3200">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">REDUCE_SHUFFLE_BYTES counter, <a class="indexterm" href="#calibre_link-3201">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">redundant array of independent disks (RAID), <a class="indexterm" href="#calibre_link-3202">Cluster Specification</a></dt><dd class="calibre8"></dd><dt class="calibre57">reference genomes, <a class="indexterm" href="#calibre_link-3203">The Human Genome Project and Reference Genomes</a></dt><dd class="calibre8"></dd><dt class="calibre57">ReflectionUtils class, <a class="indexterm" href="#calibre_link-3204">Compressing and decompressing streams with
        CompressionCodec</a>, <a class="indexterm" href="#calibre_link-3205">Reading a SequenceFile</a></dt><dd class="calibre8"></dd><dt class="calibre57">RegexMapper class, <a class="indexterm" href="#calibre_link-3206">MapReduce Library Classes</a></dt><dd class="calibre8"></dd><dt class="calibre57">RegexSerDe class, <a class="indexterm" href="#calibre_link-3207">Using a custom SerDe: RegexSerDe</a></dt><dd class="calibre8"></dd><dt class="calibre57">regionservers (HBase), <a class="indexterm" href="#calibre_link-3208">Implementation</a></dt><dd class="calibre8"></dd><dt class="calibre57">REGISTER statement (Pig Latin), <a class="indexterm" href="#calibre_link-3209">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">regular expressions, <a class="indexterm" href="#calibre_link-3210">Using a custom SerDe: RegexSerDe</a></dt><dd class="calibre8"></dd><dt class="calibre57">Relational Database Management Systems (see RDBMSs)</dt><dd class="calibre8"></dd><dt class="calibre57">remote debugging, <a class="indexterm" href="#calibre_link-3211">Remote Debugging</a></dt><dd class="calibre8"></dd><dt class="calibre57">remote procedure calls (RPCs), <a class="indexterm" href="#calibre_link-3212">Serialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">replicated mode (ZooKeeper), <a class="indexterm" href="#calibre_link-3213">Implementation</a>, <a class="indexterm" href="#calibre_link-3214">Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">Reporter interface, <a class="indexterm" href="#calibre_link-3215">Progress and Status Updates</a></dt><dd class="calibre8"></dd><dt class="calibre57">reserved storage space, <a class="indexterm" href="#calibre_link-3216">Reserved storage space</a></dt><dd class="calibre8"></dd><dt class="calibre57">Resilient Distributed Datasets (see RDDs)</dt><dd class="calibre8"></dd><dt class="calibre57">resource manager page, <a class="indexterm" href="#calibre_link-3217">The resource manager page</a></dt><dd class="calibre8"></dd><dt class="calibre57">resource managers</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3218">Anatomy of a YARN Application Run</a></dt><dd class="calibre8"></dd><dt class="calibre57">application master failure, <a class="indexterm" href="#calibre_link-3219">Application Master Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">cluster sizing, <a class="indexterm" href="#calibre_link-3220">Master node scenarios</a></dt><dd class="calibre8"></dd><dt class="calibre57">commissioning nodes, <a class="indexterm" href="#calibre_link-3221">Commissioning new nodes</a>–<a class="indexterm" href="#calibre_link-3222">Commissioning new nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">decommissioning nodes, <a class="indexterm" href="#calibre_link-3223">Decommissioning old nodes</a>–<a class="indexterm" href="#calibre_link-3224">Decommissioning old nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">failure considerations, <a class="indexterm" href="#calibre_link-3225">Resource Manager Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">heartbeat requests, <a class="indexterm" href="#calibre_link-3226">Delay Scheduling</a></dt><dd class="calibre8"></dd><dt class="calibre57">job initialization process, <a class="indexterm" href="#calibre_link-3227">Job Initialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">job submission process, <a class="indexterm" href="#calibre_link-3228">Job Submission</a></dt><dd class="calibre8"></dd><dt class="calibre57">jobtrackers and, <a class="indexterm" href="#calibre_link-3229">YARN Compared to MapReduce 1</a></dt><dd class="calibre8"></dd><dt class="calibre57">node manager failure, <a class="indexterm" href="#calibre_link-3230">Node Manager Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">progress and status updates, <a class="indexterm" href="#calibre_link-3231">Progress and Status Updates</a></dt><dd class="calibre8"></dd><dt class="calibre57">starting, <a class="indexterm" href="#calibre_link-3232">Starting and Stopping the Daemons</a></dt><dd class="calibre8"></dd><dt class="calibre57">task assignments, <a class="indexterm" href="#calibre_link-3233">Task Assignment</a></dt><dd class="calibre8"></dd><dt class="calibre57">task execution, <a class="indexterm" href="#calibre_link-3234">Task Execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">thread dumps, <a class="indexterm" href="#calibre_link-3235">Getting stack traces</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">resource requests, <a class="indexterm" href="#calibre_link-3236">Resource Requests</a></dt><dd class="calibre8"></dd><dt class="calibre57">REST, HBase and, <a class="indexterm" href="#calibre_link-3237">REST and Thrift</a></dt><dd class="calibre8"></dd><dt class="calibre57">Result class, <a class="indexterm" href="#calibre_link-3238">Java</a></dt><dd class="calibre8"></dd><dt class="calibre57">ResultScanner interface, <a class="indexterm" href="#calibre_link-3239">Java</a></dt><dd class="calibre8"></dd><dt class="calibre57">ResultSet interface, <a class="indexterm" href="#calibre_link-3240">Imports: A Deeper Look</a></dt><dd class="calibre8"></dd><dt class="calibre57">rg.apache.hadoop.hbase.client package, <a class="indexterm" href="#calibre_link-3241">Java</a></dt><dd class="calibre8"></dd><dt class="calibre57">rm command, <a class="indexterm" href="#calibre_link-3242">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">rmf command, <a class="indexterm" href="#calibre_link-3243">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">ROW FORMAT clause (Hive), <a class="indexterm" href="#calibre_link-3244">An Example</a>, <a class="indexterm" href="#calibre_link-3245">Using a custom SerDe: RegexSerDe</a>, <a class="indexterm" href="#calibre_link-3246">User-Defined Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">RowCounter class, <a class="indexterm" href="#calibre_link-3247">MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">RPC server properties, <a class="indexterm" href="#calibre_link-3248">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">RpcClient class (Java), <a class="indexterm" href="#calibre_link-3249">Integrating Flume with Applications</a></dt><dd class="calibre8"></dd><dt class="calibre57">RPCs (remote procedure calls), <a class="indexterm" href="#calibre_link-3250">Serialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">Ruby language, <a class="indexterm" href="#calibre_link-3251">Ruby</a>–<a class="indexterm" href="#calibre_link-3252">Ruby</a></dt><dd class="calibre8"></dd><dt class="calibre57">run command, <a class="indexterm" href="#calibre_link-3253">Statements</a>, <a class="indexterm" href="#calibre_link-3254">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">Runnable interface (Java), <a class="indexterm" href="#calibre_link-3255">Building YARN Applications</a></dt><dd class="calibre8"></dd><dt class="calibre57">ruok command (ZooKeeper), <a class="indexterm" href="#calibre_link-3256">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">S</h3><dl class="book"><dt class="calibre57">S3AFileSystem class, <a class="indexterm" href="#calibre_link-3257">Hadoop Filesystems</a></dt><dd class="calibre8"></dd><dt class="calibre57">safe mode, <a class="indexterm" href="#calibre_link-3258">Safe Mode</a>–<a class="indexterm" href="#calibre_link-3259">Entering and leaving safe mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">Sammer, Eric, <a class="indexterm" href="#calibre_link-3260">Setting Up a Hadoop Cluster</a></dt><dd class="calibre8"></dd><dt class="calibre57">Sample class, <a class="indexterm" href="#calibre_link-3261">Crunch Libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">SAMPLE statement (Pig Latin), <a class="indexterm" href="#calibre_link-3262">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">Scala application example, <a class="indexterm" href="#calibre_link-3263">A Scala Standalone Application</a>–<a class="indexterm" href="#calibre_link-3264">A Scala Standalone Application</a></dt><dd class="calibre8"></dd><dt class="calibre57">scaling out (data)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3265">Scaling Out</a></dt><dd class="calibre8"></dd><dt class="calibre57">combiner functions, <a class="indexterm" href="#calibre_link-3266">Combiner Functions</a>–<a class="indexterm" href="#calibre_link-3267">Specifying a combiner function</a></dt><dd class="calibre8"></dd><dt class="calibre57">data flow, <a class="indexterm" href="#calibre_link-3268">Data Flow</a>–<a class="indexterm" href="#calibre_link-3269">Data Flow</a></dt><dd class="calibre8"></dd><dt class="calibre57">running distributed jobs, <a class="indexterm" href="#calibre_link-3270">Running a Distributed MapReduce Job</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Scan class, <a class="indexterm" href="#calibre_link-3271">Java</a></dt><dd class="calibre8"></dd><dt class="calibre57">scheduling in YARN</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3272">Scheduling in YARN</a></dt><dd class="calibre8"></dd><dt class="calibre57">Capacity Scheduler, <a class="indexterm" href="#calibre_link-3273">Capacity Scheduler Configuration</a>–<a class="indexterm" href="#calibre_link-3274">Queue placement</a></dt><dd class="calibre8"></dd><dt class="calibre57">delay scheduling, <a class="indexterm" href="#calibre_link-3275">Delay Scheduling</a></dt><dd class="calibre8"></dd><dt class="calibre57">Dominant Resource Fairness, <a class="indexterm" href="#calibre_link-3276">Dominant Resource Fairness</a></dt><dd class="calibre8"></dd><dt class="calibre57">Fair Scheduler, <a class="indexterm" href="#calibre_link-3277">Fair Scheduler Configuration</a>–<a class="indexterm" href="#calibre_link-3278">Preemption</a></dt><dd class="calibre8"></dd><dt class="calibre57">FIFO Scheduler, <a class="indexterm" href="#calibre_link-3279">Scheduler Options</a></dt><dd class="calibre8"></dd><dt class="calibre57">jobs, <a class="indexterm" href="#calibre_link-3280">Job scheduler</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">scheduling tasks in Spark, <a class="indexterm" href="#calibre_link-3281">Task Scheduling</a></dt><dd class="calibre8"></dd><dt class="calibre57">schema-on-read, <a class="indexterm" href="#calibre_link-3282">Relational Database Management Systems</a>, <a class="indexterm" href="#calibre_link-3283">Schema on Read Versus Schema on Write</a></dt><dd class="calibre8"></dd><dt class="calibre57">schema-on-write, <a class="indexterm" href="#calibre_link-3284">Schema on Read Versus Schema on Write</a></dt><dd class="calibre8"></dd><dt class="calibre57">schemas</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">Avro, <a class="indexterm" href="#calibre_link-3285">Avro Data Types and Schemas</a>–<a class="indexterm" href="#calibre_link-3286">Avro Data Types and Schemas</a>, <a class="indexterm" href="#calibre_link-3287">Avro, Protocol Buffers, and Thrift</a></dt><dd class="calibre8"></dd><dt class="calibre57">HBase online query application, <a class="indexterm" href="#calibre_link-3288">Schema Design</a></dt><dd class="calibre8"></dd><dt class="calibre57">MySQL, <a class="indexterm" href="#calibre_link-3289">A Sample Import</a></dt><dd class="calibre8"></dd><dt class="calibre57">Parquet, <a class="indexterm" href="#calibre_link-3290">Writing and Reading Parquet Files</a></dt><dd class="calibre8"></dd><dt class="calibre57">Pig Latin, <a class="indexterm" href="#calibre_link-3291">Schemas</a>–<a class="indexterm" href="#calibre_link-3292">Schema merging</a>, <a class="indexterm" href="#calibre_link-3293">Using a schema</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">ScriptBasedMapping class, <a class="indexterm" href="#calibre_link-3294">Rack awareness</a></dt><dd class="calibre8"></dd><dt class="calibre57">scripts</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">MapReduce, <a class="indexterm" href="#calibre_link-3295">MapReduce Scripts</a></dt><dd class="calibre8"></dd><dt class="calibre57">Pig, <a class="indexterm" href="#calibre_link-3296">Running Pig Programs</a></dt><dd class="calibre8"></dd><dt class="calibre57">Python, <a class="indexterm" href="#calibre_link-3297">MapReduce Scripts</a></dt><dd class="calibre8"></dd><dt class="calibre57">ZooKeeper, <a class="indexterm" href="#calibre_link-3298">Resilience and Performance</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">search platforms, <a class="indexterm" href="#calibre_link-3299">Beyond Batch</a></dt><dd class="calibre8"></dd><dt class="calibre57">secondary namenodes</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3300">Namenodes and Datanodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">checkpointing process, <a class="indexterm" href="#calibre_link-3301">The filesystem image and edit log</a></dt><dd class="calibre8"></dd><dt class="calibre57">directory structure, <a class="indexterm" href="#calibre_link-3302">Secondary namenode directory structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">starting, <a class="indexterm" href="#calibre_link-3303">Starting and Stopping the Daemons</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">secondary sort, <a class="indexterm" href="#calibre_link-3304">Secondary Sort</a>–<a class="indexterm" href="#calibre_link-3305">Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">SecondarySort class, <a class="indexterm" href="#calibre_link-3306">Crunch Libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">security</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3307">Security</a></dt><dd class="calibre8"></dd><dt class="calibre57">additional enhancements, <a class="indexterm" href="#calibre_link-3308">Other Security Enhancements</a>–<a class="indexterm" href="#calibre_link-3309">Other Security Enhancements</a></dt><dd class="calibre8"></dd><dt class="calibre57">delegation tokens, <a class="indexterm" href="#calibre_link-3310">Delegation Tokens</a></dt><dd class="calibre8"></dd><dt class="calibre57">Kerberos and, <a class="indexterm" href="#calibre_link-3311">Kerberos and Hadoop</a>–<a class="indexterm" href="#calibre_link-3312">An example</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">security.datanode.protocol.acl property, <a class="indexterm" href="#calibre_link-3313">Other Security Enhancements</a></dt><dd class="calibre8"></dd><dt class="calibre57">seek time, <a class="indexterm" href="#calibre_link-3314">Relational Database Management Systems</a></dt><dd class="calibre8"></dd><dt class="calibre57">Seekable interface, <a class="indexterm" href="#calibre_link-3315">FSDataInputStream</a></dt><dd class="calibre8"></dd><dt class="calibre57">SELECT statement (Hive)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">grouping rows, <a class="indexterm" href="#calibre_link-3316">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">index support, <a class="indexterm" href="#calibre_link-3317">Updates, Transactions, and Indexes</a></dt><dd class="calibre8"></dd><dt class="calibre57">partitioned data and, <a class="indexterm" href="#calibre_link-3318">Inserts</a></dt><dd class="calibre8"></dd><dt class="calibre57">subqueries and, <a class="indexterm" href="#calibre_link-3319">Subqueries</a></dt><dd class="calibre8"></dd><dt class="calibre57">views and, <a class="indexterm" href="#calibre_link-3320">Views</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">SELECT TRANSFORM statement (Hive), <a class="indexterm" href="#calibre_link-3321">User-Defined Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">selectors, replicating and multiplexing, <a class="indexterm" href="#calibre_link-3322">Replicating and Multiplexing Selectors</a></dt><dd class="calibre8"></dd><dt class="calibre57">semi joins, <a class="indexterm" href="#calibre_link-3323">Semi joins</a></dt><dd class="calibre8"></dd><dt class="calibre57">semi-structured data, <a class="indexterm" href="#calibre_link-3324">Relational Database Management Systems</a></dt><dd class="calibre8"></dd><dt class="calibre57">semicolons, <a class="indexterm" href="#calibre_link-3325">Structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">SequenceFile class</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3326">SequenceFile</a></dt><dd class="calibre8"></dd><dt class="calibre57">compressing streams, <a class="indexterm" href="#calibre_link-3327">Compressing and decompressing streams with
        CompressionCodec</a></dt><dd class="calibre8"></dd><dt class="calibre57">converting tar files, <a class="indexterm" href="#calibre_link-3328">SequenceFile</a></dt><dd class="calibre8"></dd><dt class="calibre57">displaying with command-line interface, <a class="indexterm" href="#calibre_link-3329">Displaying a SequenceFile with the command-line
        interface</a></dt><dd class="calibre8"></dd><dt class="calibre57">exports and, <a class="indexterm" href="#calibre_link-3330">Exports and SequenceFiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">format overview, <a class="indexterm" href="#calibre_link-3331">The SequenceFile format</a>–<a class="indexterm" href="#calibre_link-3332">The SequenceFile format</a></dt><dd class="calibre8"></dd><dt class="calibre57">NullWritable class and, <a class="indexterm" href="#calibre_link-3333">NullWritable</a></dt><dd class="calibre8"></dd><dt class="calibre57">ObjectWritable class and, <a class="indexterm" href="#calibre_link-3334">ObjectWritable and GenericWritable</a></dt><dd class="calibre8"></dd><dt class="calibre57">reading, <a class="indexterm" href="#calibre_link-3335">Reading a SequenceFile</a>–<a class="indexterm" href="#calibre_link-3336">Reading a SequenceFile</a></dt><dd class="calibre8"></dd><dt class="calibre57">sorting and merging, <a class="indexterm" href="#calibre_link-3337">Sorting and merging SequenceFiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">Sqoop support, <a class="indexterm" href="#calibre_link-3338">Text and Binary File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">writing, <a class="indexterm" href="#calibre_link-3339">Writing a SequenceFile</a>–<a class="indexterm" href="#calibre_link-3340">Writing a SequenceFile</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">SequenceFileAsBinaryInputFormat class, <a class="indexterm" href="#calibre_link-3341">SequenceFileAsBinaryInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">SequenceFileAsBinaryOutputFormat class, <a class="indexterm" href="#calibre_link-3342">SequenceFileAsBinaryOutputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">SequenceFileAsTextInputFormat class, <a class="indexterm" href="#calibre_link-3343">SequenceFileAsTextInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">SequenceFileInputFormat class, <a class="indexterm" href="#calibre_link-3344">SequenceFileInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">SequenceFileOutputFormat class, <a class="indexterm" href="#calibre_link-3345">Using Compression in MapReduce</a>, <a class="indexterm" href="#calibre_link-3346">Processing a whole file as a record</a>, <a class="indexterm" href="#calibre_link-3347">SequenceFileOutputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">sequential znodes, <a class="indexterm" href="#calibre_link-3348">Sequence numbers</a></dt><dd class="calibre8"></dd><dt class="calibre57">SerDe (Serializer-Deserializer), <a class="indexterm" href="#calibre_link-3349">Storage Formats</a>–<a class="indexterm" href="#calibre_link-3350">Using a custom SerDe: RegexSerDe</a></dt><dd class="calibre8"></dd><dt class="calibre57">SERDE keyword (Hive), <a class="indexterm" href="#calibre_link-3351">Using a custom SerDe: RegexSerDe</a></dt><dd class="calibre8"></dd><dt class="calibre57">Serializable interface (Java), <a class="indexterm" href="#calibre_link-3352">Serialization of functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">serialization</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3353">Serialization</a>–<a class="indexterm" href="#calibre_link-3354">Serialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">Avro support, <a class="indexterm" href="#calibre_link-3355">In-Memory Serialization and Deserialization</a>–<a class="indexterm" href="#calibre_link-3356">The Specific API</a></dt><dd class="calibre8"></dd><dt class="calibre57">DefaultStringifier class, <a class="indexterm" href="#calibre_link-3357">Using the Job Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">of functions, <a class="indexterm" href="#calibre_link-3358">Serialization of functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">IDL support, <a class="indexterm" href="#calibre_link-3359">Serialization IDL</a></dt><dd class="calibre8"></dd><dt class="calibre57">implementing custom Writable, <a class="indexterm" href="#calibre_link-3360">Implementing a Custom Writable</a>–<a class="indexterm" href="#calibre_link-3361">Custom comparators</a></dt><dd class="calibre8"></dd><dt class="calibre57">pluggable frameworks, <a class="indexterm" href="#calibre_link-3362">Serialization Frameworks</a>–<a class="indexterm" href="#calibre_link-3363">Serialization IDL</a></dt><dd class="calibre8"></dd><dt class="calibre57">RDD, <a class="indexterm" href="#calibre_link-3364">Serialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">Sqoop support, <a class="indexterm" href="#calibre_link-3365">Additional Serialization Systems</a></dt><dd class="calibre8"></dd><dt class="calibre57">tuning checklist, <a class="indexterm" href="#calibre_link-3366">Tuning a Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">Writable class hierarchy, <a class="indexterm" href="#calibre_link-3367">Writable Classes</a>–<a class="indexterm" href="#calibre_link-3368">Writable collections</a></dt><dd class="calibre8"></dd><dt class="calibre57">Writable interface, <a class="indexterm" href="#calibre_link-3369">Serialization</a>–<a class="indexterm" href="#calibre_link-3370">WritableComparable and comparators</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Serialization interface, <a class="indexterm" href="#calibre_link-3371">Serialization Frameworks</a></dt><dd class="calibre8"></dd><dt class="calibre57">Serializer interface, <a class="indexterm" href="#calibre_link-3372">Serialization Frameworks</a></dt><dd class="calibre8"></dd><dt class="calibre57">serializer property, <a class="indexterm" href="#calibre_link-3373">File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">Serializer-Deserializer (SerDe), <a class="indexterm" href="#calibre_link-3374">Storage Formats</a>–<a class="indexterm" href="#calibre_link-3375">Using a custom SerDe: RegexSerDe</a></dt><dd class="calibre8"></dd><dt class="calibre57">service requests, <a class="indexterm" href="#calibre_link-3376">Kerberos and Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">Set class, <a class="indexterm" href="#calibre_link-3377">Crunch Libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">SET command (Hive), <a class="indexterm" href="#calibre_link-3378">Configuring Hive</a></dt><dd class="calibre8"></dd><dt class="calibre57">set command (Pig), <a class="indexterm" href="#calibre_link-3379">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">setACL operation (ZooKeeper), <a class="indexterm" href="#calibre_link-3380">Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">setData operation (ZooKeeper), <a class="indexterm" href="#calibre_link-3381">Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">SetFile class, <a class="indexterm" href="#calibre_link-3382">MapFile variants</a></dt><dd class="calibre8"></dd><dt class="calibre57">SETI@home project, <a class="indexterm" href="#calibre_link-3383">Volunteer Computing</a></dt><dd class="calibre8"></dd><dt class="calibre57">sh command, <a class="indexterm" href="#calibre_link-3384">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">Shard class, <a class="indexterm" href="#calibre_link-3385">Crunch Libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">shared-nothing architecture, <a class="indexterm" href="#calibre_link-3386">Grid Computing</a></dt><dd class="calibre8"></dd><dt class="calibre57">ShareThis sharing network, <a class="indexterm" href="#calibre_link-3387">Hadoop and Cascading at ShareThis</a>–<a class="indexterm" href="#calibre_link-3388">Hadoop and Cascading at ShareThis</a></dt><dd class="calibre8"></dd><dt class="calibre57">short-circuit local reads, <a class="indexterm" href="#calibre_link-3389">Short-circuit local reads</a></dt><dd class="calibre8"></dd><dt class="calibre57">ShortWritable class, <a class="indexterm" href="#calibre_link-3390">Writable wrappers for Java primitives</a></dt><dd class="calibre8"></dd><dt class="calibre57">SHOW FUNCTIONS statement (Hive), <a class="indexterm" href="#calibre_link-3391">Operators and Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">SHOW LOCKS statement (Hive), <a class="indexterm" href="#calibre_link-3392">Updates, Transactions, and Indexes</a></dt><dd class="calibre8"></dd><dt class="calibre57">SHOW PARTITIONS statement (Hive), <a class="indexterm" href="#calibre_link-3393">Partitions</a></dt><dd class="calibre8"></dd><dt class="calibre57">SHOW TABLES statement (Hive), <a class="indexterm" href="#calibre_link-3394">Views</a></dt><dd class="calibre8"></dd><dt class="calibre57">shuffle process</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3395">Shuffle and Sort</a></dt><dd class="calibre8"></dd><dt class="calibre57">configuration tuning, <a class="indexterm" href="#calibre_link-3396">Configuration Tuning</a>–<a class="indexterm" href="#calibre_link-3397">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">map side, <a class="indexterm" href="#calibre_link-3398">The Map Side</a>–<a class="indexterm" href="#calibre_link-3399">The Map Side</a></dt><dd class="calibre8"></dd><dt class="calibre57">reduce side, <a class="indexterm" href="#calibre_link-3400">The Reduce Side</a>–<a class="indexterm" href="#calibre_link-3401">The Reduce Side</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">SHUFFLED_MAPS counter, <a class="indexterm" href="#calibre_link-3402">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">side data distribution</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3403">Side Data Distribution</a></dt><dd class="calibre8"></dd><dt class="calibre57">distributed cache, <a class="indexterm" href="#calibre_link-3404">Distributed Cache</a>–<a class="indexterm" href="#calibre_link-3405">The distributed cache API</a></dt><dd class="calibre8"></dd><dt class="calibre57">job configuration, <a class="indexterm" href="#calibre_link-3406">Using the Job Configuration</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Sierra, Stuart, <a class="indexterm" href="#calibre_link-3407">SequenceFile</a></dt><dd class="calibre8"></dd><dt class="calibre57">single point of failure (SPOF), <a class="indexterm" href="#calibre_link-3408">HDFS High Availability</a></dt><dd class="calibre8"></dd><dt class="calibre57">single sign-ons, <a class="indexterm" href="#calibre_link-3409">Kerberos and Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">sink groups (Flume), <a class="indexterm" href="#calibre_link-3410">Sink Groups</a>–<a class="indexterm" href="#calibre_link-3411">Sink Groups</a></dt><dd class="calibre8"></dd><dt class="calibre57">sinkgroups property, <a class="indexterm" href="#calibre_link-3412">Sink Groups</a></dt><dd class="calibre8"></dd><dt class="calibre57">SIZE function (Pig Latin), <a class="indexterm" href="#calibre_link-3413">Validation and nulls</a>, <a class="indexterm" href="#calibre_link-3414">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">slaves file, <a class="indexterm" href="#calibre_link-3415">Starting and Stopping the Daemons</a>, <a class="indexterm" href="#calibre_link-3416">Hadoop Configuration</a>, <a class="indexterm" href="#calibre_link-3417">Commissioning new nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">Snappy compression, <a class="indexterm" href="#calibre_link-3418">Compression</a>–<a class="indexterm" href="#calibre_link-3419">Codecs</a>, <a class="indexterm" href="#calibre_link-3420">Native libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">SnappyCodec class, <a class="indexterm" href="#calibre_link-3421">Codecs</a></dt><dd class="calibre8"></dd><dt class="calibre57">SORT BY clause (Hive), <a class="indexterm" href="#calibre_link-3422">Sorting and Aggregating</a></dt><dd class="calibre8"></dd><dt class="calibre57">Sort class, <a class="indexterm" href="#calibre_link-3423">Crunch</a>, <a class="indexterm" href="#calibre_link-3424">Crunch Libraries</a></dt><dd class="calibre8"></dd><dt class="calibre57">SortedMapWritable class, <a class="indexterm" href="#calibre_link-3425">Writable collections</a></dt><dd class="calibre8"></dd><dt class="calibre57">sorting data</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3426">Sorting</a></dt><dd class="calibre8"></dd><dt class="calibre57">Avro and, <a class="indexterm" href="#calibre_link-3427">Sort Order</a>, <a class="indexterm" href="#calibre_link-3428">Sorting Using Avro MapReduce</a>–<a class="indexterm" href="#calibre_link-3429">Sorting Using Avro MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">controlling sort order, <a class="indexterm" href="#calibre_link-3430">Partial Sort</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive tables, <a class="indexterm" href="#calibre_link-3431">Sorting and Aggregating</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapReduce and, <a class="indexterm" href="#calibre_link-3432">Sorting</a>–<a class="indexterm" href="#calibre_link-3433">Streaming</a>, <a class="indexterm" href="#calibre_link-3434">Sorting Using Avro MapReduce</a>–<a class="indexterm" href="#calibre_link-3435">Sorting Using Avro MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">partial sort, <a class="indexterm" href="#calibre_link-3436">Partial Sort</a>–<a class="indexterm" href="#calibre_link-3437">Partial Sort</a></dt><dd class="calibre8"></dd><dt class="calibre57">Pig operators and, <a class="indexterm" href="#calibre_link-3438">Sorting Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">preparation overview, <a class="indexterm" href="#calibre_link-3439">Preparation</a></dt><dd class="calibre8"></dd><dt class="calibre57">secondary sort, <a class="indexterm" href="#calibre_link-3440">Secondary Sort</a>–<a class="indexterm" href="#calibre_link-3441">Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">shuffle process and, <a class="indexterm" href="#calibre_link-3442">Shuffle and Sort</a>–<a class="indexterm" href="#calibre_link-3443">Configuration Tuning</a></dt><dd class="calibre8"></dd><dt class="calibre57">total sort, <a class="indexterm" href="#calibre_link-3444">Total Sort</a>–<a class="indexterm" href="#calibre_link-3445">Total Sort</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Source interface, <a class="indexterm" href="#calibre_link-3446">Reading from a source</a></dt><dd class="calibre8"></dd><dt class="calibre57">SourceTarget interface, <a class="indexterm" href="#calibre_link-3447">Combined sources and targets</a></dt><dd class="calibre8"></dd><dt class="calibre57">Spark</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3448">Spark</a></dt><dd class="calibre8"></dd><dt class="calibre57">additonal information, <a class="indexterm" href="#calibre_link-3449">Further Reading</a></dt><dd class="calibre8"></dd><dt class="calibre57">anatomy of job runs, <a class="indexterm" href="#calibre_link-3450">Anatomy of a Spark Job Run</a>–<a class="indexterm" href="#calibre_link-3451">Task Execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">cluster managers and, <a class="indexterm" href="#calibre_link-3452">Executors and Cluster Managers</a></dt><dd class="calibre8"></dd><dt class="calibre57">example of, <a class="indexterm" href="#calibre_link-3453">An Example</a>–<a class="indexterm" href="#calibre_link-3454">A Python Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">executors and, <a class="indexterm" href="#calibre_link-3455">Executors and Cluster Managers</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive and, <a class="indexterm" href="#calibre_link-3456">Execution engines</a></dt><dd class="calibre8"></dd><dt class="calibre57">installing, <a class="indexterm" href="#calibre_link-3457">Installing Spark</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapReduce and, <a class="indexterm" href="#calibre_link-3458">Transformations and Actions</a></dt><dd class="calibre8"></dd><dt class="calibre57">RDDs and, <a class="indexterm" href="#calibre_link-3459">Resilient Distributed Datasets</a>–<a class="indexterm" href="#calibre_link-3460">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">resource requests, <a class="indexterm" href="#calibre_link-3461">Resource Requests</a></dt><dd class="calibre8"></dd><dt class="calibre57">shared variables, <a class="indexterm" href="#calibre_link-3462">Shared Variables</a>–<a class="indexterm" href="#calibre_link-3463">Accumulators</a></dt><dd class="calibre8"></dd><dt class="calibre57">sorting data, <a class="indexterm" href="#calibre_link-3464">Total Sort</a></dt><dd class="calibre8"></dd><dt class="calibre57">YARN and, <a class="indexterm" href="#calibre_link-3465">Spark on YARN</a>–<a class="indexterm" href="#calibre_link-3466">YARN cluster mode</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">spark-shell command, <a class="indexterm" href="#calibre_link-3467">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">spark-submit command, <a class="indexterm" href="#calibre_link-3468">A Scala Standalone Application</a>, <a class="indexterm" href="#calibre_link-3469">YARN cluster mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">spark.kryo.registrator property, <a class="indexterm" href="#calibre_link-3470">Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">SparkConf class, <a class="indexterm" href="#calibre_link-3471">A Scala Standalone Application</a></dt><dd class="calibre8"></dd><dt class="calibre57">SparkContext class, <a class="indexterm" href="#calibre_link-3472">An Example</a>, <a class="indexterm" href="#calibre_link-3473">YARN client mode</a>–<a class="indexterm" href="#calibre_link-3474">YARN cluster mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">SpecificDatumReader class, <a class="indexterm" href="#calibre_link-3475">The Specific API</a></dt><dd class="calibre8"></dd><dt class="calibre57">speculative execution of tasks, <a class="indexterm" href="#calibre_link-3476">Speculative Execution</a>–<a class="indexterm" href="#calibre_link-3477">Speculative Execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">SPILLED_RECORDS counter, <a class="indexterm" href="#calibre_link-3478">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">SPLIT statement (Pig Latin), <a class="indexterm" href="#calibre_link-3479">Statements</a>, <a class="indexterm" href="#calibre_link-3480">Combining and Splitting Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">splits (input data) (see input splits)</dt><dd class="calibre8"></dd><dt class="calibre57">SPLIT_RAW_BYTES counter, <a class="indexterm" href="#calibre_link-3481">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">SPOF (single point of failure), <a class="indexterm" href="#calibre_link-3482">HDFS High Availability</a></dt><dd class="calibre8"></dd><dt class="calibre57">Sqoop</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3483">Sqoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">additional information, <a class="indexterm" href="#calibre_link-3484">Further Reading</a></dt><dd class="calibre8"></dd><dt class="calibre57">Avro support, <a class="indexterm" href="#calibre_link-3485">Text and Binary File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">connectors and, <a class="indexterm" href="#calibre_link-3486">Sqoop Connectors</a></dt><dd class="calibre8"></dd><dt class="calibre57">escape sequences supported, <a class="indexterm" href="#calibre_link-3487">Performing an Export</a></dt><dd class="calibre8"></dd><dt class="calibre57">export process, <a class="indexterm" href="#calibre_link-3488">Performing an Export</a>–<a class="indexterm" href="#calibre_link-3489">Exports and SequenceFiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">file formats, <a class="indexterm" href="#calibre_link-3490">Text and Binary File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">generated code, <a class="indexterm" href="#calibre_link-3491">Generated Code</a></dt><dd class="calibre8"></dd><dt class="calibre57">getting, <a class="indexterm" href="#calibre_link-3492">Getting Sqoop</a>–<a class="indexterm" href="#calibre_link-3493">Getting Sqoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">import process, <a class="indexterm" href="#calibre_link-3494">Imports: A Deeper Look</a>–<a class="indexterm" href="#calibre_link-3495">Direct-Mode Imports</a></dt><dd class="calibre8"></dd><dt class="calibre57">importing large objects, <a class="indexterm" href="#calibre_link-3496">Importing Large Objects</a>–<a class="indexterm" href="#calibre_link-3497">Importing Large Objects</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapReduce support, <a class="indexterm" href="#calibre_link-3498">A Sample Import</a>, <a class="indexterm" href="#calibre_link-3499">Imports: A Deeper Look</a>, <a class="indexterm" href="#calibre_link-3500">Exports: A Deeper Look</a></dt><dd class="calibre8"></dd><dt class="calibre57">Parquet support, <a class="indexterm" href="#calibre_link-3501">Text and Binary File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">sample import, <a class="indexterm" href="#calibre_link-3502">A Sample Import</a>–<a class="indexterm" href="#calibre_link-3503">Text and Binary File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">SequenceFile class and, <a class="indexterm" href="#calibre_link-3504">Text and Binary File Formats</a></dt><dd class="calibre8"></dd><dt class="calibre57">serialization and, <a class="indexterm" href="#calibre_link-3505">Additional Serialization Systems</a></dt><dd class="calibre8"></dd><dt class="calibre57">tool support, <a class="indexterm" href="#calibre_link-3506">Getting Sqoop</a>, <a class="indexterm" href="#calibre_link-3507">Generated Code</a></dt><dd class="calibre8"></dd><dt class="calibre57">working with imported data, <a class="indexterm" href="#calibre_link-3508">Working with Imported Data</a>–<a class="indexterm" href="#calibre_link-3509">Imported Data and Hive</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">srst command (ZooKeeper), <a class="indexterm" href="#calibre_link-3510">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">srvr command (ZooKeeper), <a class="indexterm" href="#calibre_link-3511">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">SSH, configuring, <a class="indexterm" href="#calibre_link-3512">Configuring SSH</a>, <a class="indexterm" href="#calibre_link-3513">SSH settings</a>, <a class="indexterm" href="#calibre_link-3514">Configuring SSH</a></dt><dd class="calibre8"></dd><dt class="calibre57">stack traces, <a class="indexterm" href="#calibre_link-3515">Getting stack traces</a></dt><dd class="calibre8"></dd><dt class="calibre57">Stack, Michael, <a class="indexterm" href="#calibre_link-3516">HBasics</a>–<a class="indexterm" href="#calibre_link-3517">Further Reading</a></dt><dd class="calibre8"></dd><dt class="calibre57">standalone mode (Hadoop), <a class="indexterm" href="#calibre_link-3518">Standalone Mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">stat command (ZooKeeper), <a class="indexterm" href="#calibre_link-3519">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">statements (Pig Latin)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3520">Statements</a>–<a class="indexterm" href="#calibre_link-3521">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">control flow, <a class="indexterm" href="#calibre_link-3522">Statements</a></dt><dd class="calibre8"></dd><dt class="calibre57">expressions and, <a class="indexterm" href="#calibre_link-3523">Expressions</a>–<a class="indexterm" href="#calibre_link-3524">Expressions</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">states (ZooKeeper), <a class="indexterm" href="#calibre_link-3525">States</a>–<a class="indexterm" href="#calibre_link-3526">States</a>, <a class="indexterm" href="#calibre_link-3527">State exceptions</a></dt><dd class="calibre8"></dd><dt class="calibre57">status updates for tasks, <a class="indexterm" href="#calibre_link-3528">Progress and Status Updates</a></dt><dd class="calibre8"></dd><dt class="calibre57">storage handlers, <a class="indexterm" href="#calibre_link-3529">Storage handlers</a></dt><dd class="calibre8"></dd><dt class="calibre57">store functions (Pig Latin), <a class="indexterm" href="#calibre_link-3530">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">STORE statement (Pig Latin), <a class="indexterm" href="#calibre_link-3531">Statements</a>, <a class="indexterm" href="#calibre_link-3532">Statements</a>, <a class="indexterm" href="#calibre_link-3533">Sorting Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">STORED AS clause (Hive), <a class="indexterm" href="#calibre_link-3534">Binary storage formats: Sequence files, Avro datafiles, Parquet
        files, RCFiles, and ORCFiles</a></dt><dd class="calibre8"></dd><dt class="calibre57">STORED BY clause (Hive), <a class="indexterm" href="#calibre_link-3535">Storage handlers</a></dt><dd class="calibre8"></dd><dt class="calibre57">STREAM statement (Pig Latin), <a class="indexterm" href="#calibre_link-3536">Statements</a>, <a class="indexterm" href="#calibre_link-3537">STREAM</a></dt><dd class="calibre8"></dd><dt class="calibre57">stream.map.input.field.separator
                    property, <a class="indexterm" href="#calibre_link-3538">Keys and values in Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">stream.map.input.ignoreKey property, <a class="indexterm" href="#calibre_link-3539">The default Streaming job</a></dt><dd class="calibre8"></dd><dt class="calibre57">stream.map.output.field.separator
                    property, <a class="indexterm" href="#calibre_link-3540">Keys and values in Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">stream.non.zero.exit.is.failure property, <a class="indexterm" href="#calibre_link-3541">Task Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">stream.num.map.output.key.fields property, <a class="indexterm" href="#calibre_link-3542">Keys and values in Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">stream.num.reduce.output.key.fields property, <a class="indexterm" href="#calibre_link-3543">Keys and values in Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">stream.recordreader.class property, <a class="indexterm" href="#calibre_link-3544">XML</a></dt><dd class="calibre8"></dd><dt class="calibre57">stream.reduce.input.field.separator
                    property, <a class="indexterm" href="#calibre_link-3545">Keys and values in Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">stream.reduce.output.field.separator property, <a class="indexterm" href="#calibre_link-3546">Keys and values in Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">Streaming programs</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3547">Beyond Batch</a></dt><dd class="calibre8"></dd><dt class="calibre57">default job, <a class="indexterm" href="#calibre_link-3548">The default Streaming job</a>–<a class="indexterm" href="#calibre_link-3549">Keys and values in Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">secondary sort, <a class="indexterm" href="#calibre_link-3550">Streaming</a>–<a class="indexterm" href="#calibre_link-3551">Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">task execution, <a class="indexterm" href="#calibre_link-3552">Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">user-defined counters, <a class="indexterm" href="#calibre_link-3553">User-Defined Streaming Counters</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">StreamXmlRecordReader class, <a class="indexterm" href="#calibre_link-3554">XML</a></dt><dd class="calibre8"></dd><dt class="calibre57">StrictHostKeyChecking SSH setting, <a class="indexterm" href="#calibre_link-3555">SSH settings</a></dt><dd class="calibre8"></dd><dt class="calibre57">String class (Java), <a class="indexterm" href="#calibre_link-3556">Text</a>–<a class="indexterm" href="#calibre_link-3557">Resorting to String</a>, <a class="indexterm" href="#calibre_link-3558">Avro Data Types and Schemas</a></dt><dd class="calibre8"></dd><dt class="calibre57">StringTokenizer class (Java), <a class="indexterm" href="#calibre_link-3559">MapReduce Library Classes</a></dt><dd class="calibre8"></dd><dt class="calibre57">StringUtils class, <a class="indexterm" href="#calibre_link-3560">The Writable Interface</a>, <a class="indexterm" href="#calibre_link-3561">Dynamic invokers</a></dt><dd class="calibre8"></dd><dt class="calibre57">structured data, <a class="indexterm" href="#calibre_link-3562">Relational Database Management Systems</a></dt><dd class="calibre8"></dd><dt class="calibre57">subqueries, <a class="indexterm" href="#calibre_link-3563">Subqueries</a></dt><dd class="calibre8"></dd><dt class="calibre57">SUM function (Pig Latin), <a class="indexterm" href="#calibre_link-3564">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">SWebHdfsFileSystem class, <a class="indexterm" href="#calibre_link-3565">Hadoop Filesystems</a></dt><dd class="calibre8"></dd><dt class="calibre57">SwiftNativeFileSystem class, <a class="indexterm" href="#calibre_link-3566">Hadoop Filesystems</a></dt><dd class="calibre8"></dd><dt class="calibre57">SWIM repository, <a class="indexterm" href="#calibre_link-3567">Other benchmarks</a></dt><dd class="calibre8"></dd><dt class="calibre57">sync operation (ZooKeeper), <a class="indexterm" href="#calibre_link-3568">Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">syncLimit property, <a class="indexterm" href="#calibre_link-3569">Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">syslog file (Java), <a class="indexterm" href="#calibre_link-3570">Hadoop Logs</a></dt><dd class="calibre8"></dd><dt class="calibre57">system administration</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">commissioning nodes, <a class="indexterm" href="#calibre_link-3571">Commissioning and Decommissioning Nodes</a>–<a class="indexterm" href="#calibre_link-3572">Commissioning new nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">decommissioning nodes, <a class="indexterm" href="#calibre_link-3573">Decommissioning old nodes</a>–<a class="indexterm" href="#calibre_link-3574">Decommissioning old nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">HDFS support, <a class="indexterm" href="#calibre_link-3575">Persistent Data Structures</a>–<a class="indexterm" href="#calibre_link-3576">Balancer</a></dt><dd class="calibre8"></dd><dt class="calibre57">monitoring, <a class="indexterm" href="#calibre_link-3577">Monitoring</a>–<a class="indexterm" href="#calibre_link-3578">Metrics and JMX</a></dt><dd class="calibre8"></dd><dt class="calibre57">routine procedures, <a class="indexterm" href="#calibre_link-3579">Metadata backups</a>–<a class="indexterm" href="#calibre_link-3580">Filesystem balancer</a></dt><dd class="calibre8"></dd><dt class="calibre57">upgrading clusters, <a class="indexterm" href="#calibre_link-3581">Upgrades</a>–<a class="indexterm" href="#calibre_link-3582">Finalize the upgrade (optional)</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">System class (Java), <a class="indexterm" href="#calibre_link-3583">GenericOptionsParser, Tool, and ToolRunner</a></dt><dd class="calibre8"></dd><dt class="calibre57">system logfiles, <a class="indexterm" href="#calibre_link-3584">Hadoop Logs</a>, <a class="indexterm" href="#calibre_link-3585">System logfiles</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">T</h3><dl class="book"><dt class="calibre57">TableInputFormat class, <a class="indexterm" href="#calibre_link-3586">Database Input (and Output)</a>, <a class="indexterm" href="#calibre_link-3587">MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">TableMapper class, <a class="indexterm" href="#calibre_link-3588">MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">TableMapReduceUtil class, <a class="indexterm" href="#calibre_link-3589">MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">TableOutputFormat class, <a class="indexterm" href="#calibre_link-3590">Database Input (and Output)</a>, <a class="indexterm" href="#calibre_link-3591">MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">tables (HBase)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3592">Whirlwind Tour of the Data Model</a>–<a class="indexterm" href="#calibre_link-3593">Whirlwind Tour of the Data Model</a></dt><dd class="calibre8"></dd><dt class="calibre57">creating, <a class="indexterm" href="#calibre_link-3594">Test Drive</a></dt><dd class="calibre8"></dd><dt class="calibre57">inserting data into, <a class="indexterm" href="#calibre_link-3595">Test Drive</a></dt><dd class="calibre8"></dd><dt class="calibre57">locking, <a class="indexterm" href="#calibre_link-3596">Locking</a></dt><dd class="calibre8"></dd><dt class="calibre57">regions, <a class="indexterm" href="#calibre_link-3597">Regions</a></dt><dd class="calibre8"></dd><dt class="calibre57">removing, <a class="indexterm" href="#calibre_link-3598">Test Drive</a></dt><dd class="calibre8"></dd><dt class="calibre57">wide tables, <a class="indexterm" href="#calibre_link-3599">Schema Design</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">tables (Hive)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3600">Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">altering, <a class="indexterm" href="#calibre_link-3601">Altering Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">buckets and, <a class="indexterm" href="#calibre_link-3602">Partitions and Buckets</a>, <a class="indexterm" href="#calibre_link-3603">Buckets</a>–<a class="indexterm" href="#calibre_link-3604">Buckets</a></dt><dd class="calibre8"></dd><dt class="calibre57">dropping, <a class="indexterm" href="#calibre_link-3605">Dropping Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">external tables, <a class="indexterm" href="#calibre_link-3606">Managed Tables and External Tables</a>–<a class="indexterm" href="#calibre_link-3607">Managed Tables and External Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">importing data, <a class="indexterm" href="#calibre_link-3608">Importing Data</a>–<a class="indexterm" href="#calibre_link-3609">CREATE TABLE...AS SELECT</a></dt><dd class="calibre8"></dd><dt class="calibre57">managed tables, <a class="indexterm" href="#calibre_link-3610">Managed Tables and External Tables</a>–<a class="indexterm" href="#calibre_link-3611">Managed Tables and External Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">partitions and, <a class="indexterm" href="#calibre_link-3612">Partitions and Buckets</a>–<a class="indexterm" href="#calibre_link-3613">Partitions</a></dt><dd class="calibre8"></dd><dt class="calibre57">storage formats, <a class="indexterm" href="#calibre_link-3614">Storage Formats</a>–<a class="indexterm" href="#calibre_link-3615">Storage handlers</a></dt><dd class="calibre8"></dd><dt class="calibre57">views, <a class="indexterm" href="#calibre_link-3616">Views</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">TABLESAMPLE clause (Hive), <a class="indexterm" href="#calibre_link-3617">Buckets</a></dt><dd class="calibre8"></dd><dt class="calibre57">TableSource interface, <a class="indexterm" href="#calibre_link-3618">Reading from a source</a></dt><dd class="calibre8"></dd><dt class="calibre57">Target interface, <a class="indexterm" href="#calibre_link-3619">Writing to a target</a></dt><dd class="calibre8"></dd><dt class="calibre57">task attempt IDs, <a class="indexterm" href="#calibre_link-3620">Launching a Job</a>, <a class="indexterm" href="#calibre_link-3621">The Task Execution Environment</a></dt><dd class="calibre8"></dd><dt class="calibre57">task attempts page (MapReduce), <a class="indexterm" href="#calibre_link-3622">The tasks and task attempts pages</a></dt><dd class="calibre8"></dd><dt class="calibre57">task counters, <a class="indexterm" href="#calibre_link-3623">Built-in Counters</a>–<a class="indexterm" href="#calibre_link-3624">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">task IDs, <a class="indexterm" href="#calibre_link-3625">Launching a Job</a>, <a class="indexterm" href="#calibre_link-3626">The Task Execution Environment</a></dt><dd class="calibre8"></dd><dt class="calibre57">task logs (MapReduce), <a class="indexterm" href="#calibre_link-3627">Hadoop Logs</a></dt><dd class="calibre8"></dd><dt class="calibre57">TaskAttemptContext interface, <a class="indexterm" href="#calibre_link-3628">Progress and Status Updates</a></dt><dd class="calibre8"></dd><dt class="calibre57">tasks</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">executing, <a class="indexterm" href="#calibre_link-3629">Task Execution</a>, <a class="indexterm" href="#calibre_link-3630">Task Execution</a>–<a class="indexterm" href="#calibre_link-3631">Task side-effect files</a>, <a class="indexterm" href="#calibre_link-3632">Task Execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">failure considerations, <a class="indexterm" href="#calibre_link-3633">Task Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">profiling, <a class="indexterm" href="#calibre_link-3634">Profiling Tasks</a>–<a class="indexterm" href="#calibre_link-3635">The HPROF profiler</a></dt><dd class="calibre8"></dd><dt class="calibre57">progress and status updates, <a class="indexterm" href="#calibre_link-3636">Progress and Status Updates</a></dt><dd class="calibre8"></dd><dt class="calibre57">scheduling in Spark, <a class="indexterm" href="#calibre_link-3637">Task Scheduling</a></dt><dd class="calibre8"></dd><dt class="calibre57">Spark support, <a class="indexterm" href="#calibre_link-3638">Spark Applications, Jobs, Stages, and Tasks</a></dt><dd class="calibre8"></dd><dt class="calibre57">speculative execution, <a class="indexterm" href="#calibre_link-3639">Speculative Execution</a>–<a class="indexterm" href="#calibre_link-3640">Speculative Execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">streaming, <a class="indexterm" href="#calibre_link-3641">Streaming</a></dt><dd class="calibre8"></dd><dt class="calibre57">task assignments, <a class="indexterm" href="#calibre_link-3642">Task Assignment</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">tasks page (MapReduce), <a class="indexterm" href="#calibre_link-3643">The tasks and task attempts pages</a></dt><dd class="calibre8"></dd><dt class="calibre57">tasktrackers, <a class="indexterm" href="#calibre_link-3644">YARN Compared to MapReduce 1</a></dt><dd class="calibre8"></dd><dt class="calibre57">TEMPORARY keyword (Hive), <a class="indexterm" href="#calibre_link-3645">Writing a UDF</a></dt><dd class="calibre8"></dd><dt class="calibre57">teragen program, <a class="indexterm" href="#calibre_link-3646">Benchmarking MapReduce with TeraSort</a></dt><dd class="calibre8"></dd><dt class="calibre57">TeraSort program, <a class="indexterm" href="#calibre_link-3647">Benchmarking MapReduce with TeraSort</a></dt><dd class="calibre8"></dd><dt class="calibre57">TestDFSIO benchmark, <a class="indexterm" href="#calibre_link-3648">Other benchmarks</a></dt><dd class="calibre8"></dd><dt class="calibre57">testing</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">HBase installation, <a class="indexterm" href="#calibre_link-3649">Test Drive</a>–<a class="indexterm" href="#calibre_link-3650">Test Drive</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive considerations, <a class="indexterm" href="#calibre_link-3651">The Hive Shell</a></dt><dd class="calibre8"></dd><dt class="calibre57">job drivers, <a class="indexterm" href="#calibre_link-3652">Testing the Driver</a>–<a class="indexterm" href="#calibre_link-3653">Testing the Driver</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapReduce test runs, <a class="indexterm" href="#calibre_link-3654">A test run</a>–<a class="indexterm" href="#calibre_link-3655">A test run</a></dt><dd class="calibre8"></dd><dt class="calibre57">in miniclusters, <a class="indexterm" href="#calibre_link-3656">Testing the Driver</a></dt><dd class="calibre8"></dd><dt class="calibre57">running jobs locally on test data, <a class="indexterm" href="#calibre_link-3657">Running Locally on Test Data</a>–<a class="indexterm" href="#calibre_link-3658">Testing the Driver</a></dt><dd class="calibre8"></dd><dt class="calibre57">running jobs on clusters, <a class="indexterm" href="#calibre_link-3659">Running on a Cluster</a>–<a class="indexterm" href="#calibre_link-3660">Remote Debugging</a></dt><dd class="calibre8"></dd><dt class="calibre57">writing unit tests with MRUnit, <a class="indexterm" href="#calibre_link-3661">Writing a Unit Test with MRUnit</a>–<a class="indexterm" href="#calibre_link-3662">Reducer</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Text class, <a class="indexterm" href="#calibre_link-3663">Text</a>–<a class="indexterm" href="#calibre_link-3664">Resorting to String</a>, <a class="indexterm" href="#calibre_link-3665">Implementing a Custom Writable</a>–<a class="indexterm" href="#calibre_link-3666">Implementing a RawComparator for speed</a>, <a class="indexterm" href="#calibre_link-3667">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">text formats</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">controlling maximum line length, <a class="indexterm" href="#calibre_link-3668">Controlling the maximum line length</a></dt><dd class="calibre8"></dd><dt class="calibre57">KeyValueTextInputFormat class, <a class="indexterm" href="#calibre_link-3669">KeyValueTextInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">NLineInputFormat class, <a class="indexterm" href="#calibre_link-3670">NLineInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">NullOutputFormat class, <a class="indexterm" href="#calibre_link-3671">Text Output</a></dt><dd class="calibre8"></dd><dt class="calibre57">TextInputFormat class, <a class="indexterm" href="#calibre_link-3672">TextInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">TextOutputFormat class, <a class="indexterm" href="#calibre_link-3673">Text Output</a></dt><dd class="calibre8"></dd><dt class="calibre57">XML documents and, <a class="indexterm" href="#calibre_link-3674">XML</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">TextInputFormat class</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3675">TextInputFormat</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapReduce types and, <a class="indexterm" href="#calibre_link-3676">Running a Job in a Local Job Runner</a>, <a class="indexterm" href="#calibre_link-3677">MapReduce Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">Sqoop imports and, <a class="indexterm" href="#calibre_link-3678">Working with Imported Data</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">TextLoader function (Pig Latin), <a class="indexterm" href="#calibre_link-3679">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">TextOutputFormat class, <a class="indexterm" href="#calibre_link-3680">Implementing a Custom Writable</a>, <a class="indexterm" href="#calibre_link-3681">Text Output</a>, <a class="indexterm" href="#calibre_link-3682">An Example</a></dt><dd class="calibre8"></dd><dt class="calibre57">TGT (Ticket-Granting Ticket), <a class="indexterm" href="#calibre_link-3683">Kerberos and Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">thread dumps, <a class="indexterm" href="#calibre_link-3684">Getting stack traces</a></dt><dd class="calibre8"></dd><dt class="calibre57">Thrift</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">HBase and, <a class="indexterm" href="#calibre_link-3685">REST and Thrift</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive and, <a class="indexterm" href="#calibre_link-3686">Hive clients</a></dt><dd class="calibre8"></dd><dt class="calibre57">Parquet and, <a class="indexterm" href="#calibre_link-3687">Avro, Protocol Buffers, and Thrift</a>–<a class="indexterm" href="#calibre_link-3688">Projection and read schemas</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">ThriftParquetWriter class, <a class="indexterm" href="#calibre_link-3689">Avro, Protocol Buffers, and Thrift</a></dt><dd class="calibre8"></dd><dt class="calibre57">tick time (ZooKeeper), <a class="indexterm" href="#calibre_link-3690">Time</a></dt><dd class="calibre8"></dd><dt class="calibre57">Ticket-Granting Ticket (TGT), <a class="indexterm" href="#calibre_link-3691">Kerberos and Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">timeline servers, <a class="indexterm" href="#calibre_link-3692">YARN Compared to MapReduce 1</a></dt><dd class="calibre8"></dd><dt class="calibre57">TOBAG function (Pig Latin), <a class="indexterm" href="#calibre_link-3693">Types</a>, <a class="indexterm" href="#calibre_link-3694">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">tojson command, <a class="indexterm" href="#calibre_link-3695">Avro Tools</a></dt><dd class="calibre8"></dd><dt class="calibre57">TokenCounterMapper class, <a class="indexterm" href="#calibre_link-3696">MapReduce Library Classes</a></dt><dd class="calibre8"></dd><dt class="calibre57">TOKENSIZE function (Pig Latin), <a class="indexterm" href="#calibre_link-3697">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">ToLowerFn function, <a class="indexterm" href="#calibre_link-3698">Materialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">TOMAP function (Pig Latin), <a class="indexterm" href="#calibre_link-3699">Types</a>, <a class="indexterm" href="#calibre_link-3700">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">Tool interface, <a class="indexterm" href="#calibre_link-3701">GenericOptionsParser, Tool, and ToolRunner</a>–<a class="indexterm" href="#calibre_link-3702">GenericOptionsParser, Tool, and ToolRunner</a></dt><dd class="calibre8"></dd><dt class="calibre57">ToolRunner class, <a class="indexterm" href="#calibre_link-3703">GenericOptionsParser, Tool, and ToolRunner</a>–<a class="indexterm" href="#calibre_link-3704">GenericOptionsParser, Tool, and ToolRunner</a></dt><dd class="calibre8"></dd><dt class="calibre57">TOP function (Pig Latin), <a class="indexterm" href="#calibre_link-3705">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">TotalOrderPartitioner class, <a class="indexterm" href="#calibre_link-3706">Total Sort</a></dt><dd class="calibre8"></dd><dt class="calibre57">TOTAL_LAUNCHED_MAPS counter, <a class="indexterm" href="#calibre_link-3707">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">TOTAL_LAUNCHED_REDUCES counter, <a class="indexterm" href="#calibre_link-3708">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">TOTAL_LAUNCHED_UBERTASKS counter, <a class="indexterm" href="#calibre_link-3709">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">TOTUPLE function (Pig Latin), <a class="indexterm" href="#calibre_link-3710">Types</a>, <a class="indexterm" href="#calibre_link-3711">Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">TPCx-HS benchmark, <a class="indexterm" href="#calibre_link-3712">Other benchmarks</a></dt><dd class="calibre8"></dd><dt class="calibre57">transfer rate, <a class="indexterm" href="#calibre_link-3713">Relational Database Management Systems</a></dt><dd class="calibre8"></dd><dt class="calibre57">TRANSFORM clause (Hive), <a class="indexterm" href="#calibre_link-3714">MapReduce Scripts</a></dt><dd class="calibre8"></dd><dt class="calibre57">transformations, RDD, <a class="indexterm" href="#calibre_link-3715">Transformations and Actions</a>–<a class="indexterm" href="#calibre_link-3716">Aggregation transformations</a></dt><dd class="calibre8"></dd><dt class="calibre57">Trash class, <a class="indexterm" href="#calibre_link-3717">Trash</a></dt><dd class="calibre8"></dd><dt class="calibre57">trash facility, <a class="indexterm" href="#calibre_link-3718">Trash</a></dt><dd class="calibre8"></dd><dt class="calibre57">TRUNCATE TABLE statement (Hive), <a class="indexterm" href="#calibre_link-3719">Dropping Tables</a></dt><dd class="calibre8"></dd><dt class="calibre57">tuning jobs, <a class="indexterm" href="#calibre_link-3720">Tuning a Job</a>–<a class="indexterm" href="#calibre_link-3721">The HPROF profiler</a></dt><dd class="calibre8"></dd><dt class="calibre57">TwoDArrayWritable class, <a class="indexterm" href="#calibre_link-3722">Writable collections</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">U</h3><dl class="book"><dt class="calibre57">uber tasks, <a class="indexterm" href="#calibre_link-3723">Job Initialization</a></dt><dd class="calibre8"></dd><dt class="calibre57">UDAF class, <a class="indexterm" href="#calibre_link-3724">Writing a UDAF</a></dt><dd class="calibre8"></dd><dt class="calibre57">UDAFEvaluator interface, <a class="indexterm" href="#calibre_link-3725">Writing a UDAF</a></dt><dd class="calibre8"></dd><dt class="calibre57">UDAFs (user-defined aggregate functions), <a class="indexterm" href="#calibre_link-3726">User-Defined Functions</a>, <a class="indexterm" href="#calibre_link-3727">Writing a UDAF</a>–<a class="indexterm" href="#calibre_link-3728">A more complex UDAF</a></dt><dd class="calibre8"></dd><dt class="calibre57">UDF class, <a class="indexterm" href="#calibre_link-3729">Writing a UDF</a></dt><dd class="calibre8"></dd><dt class="calibre57">UDFs (user-defined functions)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">Hive and, <a class="indexterm" href="#calibre_link-3730">User-Defined Functions</a>–<a class="indexterm" href="#calibre_link-3731">A more complex UDAF</a></dt><dd class="calibre8"></dd><dt class="calibre57">Pig and, <a class="indexterm" href="#calibre_link-3732">Pig</a>, <a class="indexterm" href="#calibre_link-3733">Other libraries</a>, <a class="indexterm" href="#calibre_link-3734">User-Defined Functions</a>–<a class="indexterm" href="#calibre_link-3735">Using a schema</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">UDTFs (user-defined table-generating functions), <a class="indexterm" href="#calibre_link-3736">User-Defined Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">Unicode characters, <a class="indexterm" href="#calibre_link-3737">Unicode</a>–<a class="indexterm" href="#calibre_link-3738">Unicode</a></dt><dd class="calibre8"></dd><dt class="calibre57">UNION statement (Pig Latin), <a class="indexterm" href="#calibre_link-3739">Statements</a>, <a class="indexterm" href="#calibre_link-3740">Combining and Splitting Data</a></dt><dd class="calibre8"></dd><dt class="calibre57">unit tests with MRUnit, <a class="indexterm" href="#calibre_link-3741">Setting Up the Development Environment</a>, <a class="indexterm" href="#calibre_link-3742">Writing a Unit Test with MRUnit</a>–<a class="indexterm" href="#calibre_link-3743">Reducer</a></dt><dd class="calibre8"></dd><dt class="calibre57">Unix user accounts, <a class="indexterm" href="#calibre_link-3744">Creating Unix User Accounts</a></dt><dd class="calibre8"></dd><dt class="calibre57">unmanaged application masters, <a class="indexterm" href="#calibre_link-3745">Anatomy of a YARN Application Run</a></dt><dd class="calibre8"></dd><dt class="calibre57">unstructured data, <a class="indexterm" href="#calibre_link-3746">Relational Database Management Systems</a></dt><dd class="calibre8"></dd><dt class="calibre57">UPDATE statement (Hive), <a class="indexterm" href="#calibre_link-3747">Updates, Transactions, and Indexes</a></dt><dd class="calibre8"></dd><dt class="calibre57">upgrading clusters, <a class="indexterm" href="#calibre_link-3748">Upgrades</a>–<a class="indexterm" href="#calibre_link-3749">Finalize the upgrade (optional)</a></dt><dd class="calibre8"></dd><dt class="calibre57">URL class (Java), <a class="indexterm" href="#calibre_link-3750">Reading Data from a Hadoop URL</a></dt><dd class="calibre8"></dd><dt class="calibre57">user accounts, Unix, <a class="indexterm" href="#calibre_link-3751">Creating Unix User Accounts</a></dt><dd class="calibre8"></dd><dt class="calibre57">user identity, <a class="indexterm" href="#calibre_link-3752">Managing Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">user-defined aggregate functions (UDAFs), <a class="indexterm" href="#calibre_link-3753">User-Defined Functions</a>, <a class="indexterm" href="#calibre_link-3754">Writing a UDAF</a>–<a class="indexterm" href="#calibre_link-3755">A more complex UDAF</a></dt><dd class="calibre8"></dd><dt class="calibre57">user-defined functions (see UDFs)</dt><dd class="calibre8"></dd><dt class="calibre57">user-defined table-generating functions (UDTFs), <a class="indexterm" href="#calibre_link-3756">User-Defined Functions</a></dt><dd class="calibre8"></dd><dt class="calibre57">USING JAR clause (Hive), <a class="indexterm" href="#calibre_link-3757">Writing a UDF</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">V</h3><dl class="book"><dt class="calibre57">VCORES_MILLIS_MAPS counter, <a class="indexterm" href="#calibre_link-3758">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">VCORES_MILLIS_REDUCES counter, <a class="indexterm" href="#calibre_link-3759">Job counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">VERSION file, <a class="indexterm" href="#calibre_link-3760">Namenode directory structure</a></dt><dd class="calibre8"></dd><dt class="calibre57">versions (Hive), <a class="indexterm" href="#calibre_link-3761">Installing Hive</a></dt><dd class="calibre8"></dd><dt class="calibre57">ViewFileSystem class, <a class="indexterm" href="#calibre_link-3762">HDFS Federation</a>, <a class="indexterm" href="#calibre_link-3763">Hadoop Filesystems</a></dt><dd class="calibre8"></dd><dt class="calibre57">views (virtual tables), <a class="indexterm" href="#calibre_link-3764">Views</a></dt><dd class="calibre8"></dd><dt class="calibre57">VIntWritable class, <a class="indexterm" href="#calibre_link-3765">Writable wrappers for Java primitives</a></dt><dd class="calibre8"></dd><dt class="calibre57">VIRTUAL_MEMORY_BYTES counter, <a class="indexterm" href="#calibre_link-3766">Task counters</a>, <a class="indexterm" href="#calibre_link-3767">Memory settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">VLongWritable class, <a class="indexterm" href="#calibre_link-3768">Writable wrappers for Java primitives</a></dt><dd class="calibre8"></dd><dt class="calibre57">volunteer computing, <a class="indexterm" href="#calibre_link-3769">Volunteer Computing</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">W</h3><dl class="book"><dt class="calibre57">w (write) permission, <a class="indexterm" href="#calibre_link-3770">Basic Filesystem Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">Walters, Chad, <a class="indexterm" href="#calibre_link-3771">Backdrop</a></dt><dd class="calibre8"></dd><dt class="calibre57">WAR (Web application archive) files, <a class="indexterm" href="#calibre_link-3772">Packaging a Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">watches (ZooKeeper), <a class="indexterm" href="#calibre_link-3773">Watches</a>, <a class="indexterm" href="#calibre_link-3774">Watch triggers</a></dt><dd class="calibre8"></dd><dt class="calibre57">Watson, James D., <a class="indexterm" href="#calibre_link-3775">The Structure of DNA</a></dt><dd class="calibre8"></dd><dt class="calibre57">wchc command (ZooKeeper), <a class="indexterm" href="#calibre_link-3776">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">wchp command (ZooKeeper), <a class="indexterm" href="#calibre_link-3777">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">wchs command (ZooKeeper), <a class="indexterm" href="#calibre_link-3778">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">Web application archive (WAR) files, <a class="indexterm" href="#calibre_link-3779">Packaging a Job</a></dt><dd class="calibre8"></dd><dt class="calibre57">WebHDFS protocol, <a class="indexterm" href="#calibre_link-3780">HTTP</a></dt><dd class="calibre8"></dd><dt class="calibre57">WebHdfsFileSystem class, <a class="indexterm" href="#calibre_link-3781">Hadoop Filesystems</a></dt><dd class="calibre8"></dd><dt class="calibre57">webtables (HBase), <a class="indexterm" href="#calibre_link-3782">HBasics</a></dt><dd class="calibre8"></dd><dt class="calibre57">Wensel, Chris K., <a class="indexterm" href="#calibre_link-3783">Cascading</a></dt><dd class="calibre8"></dd><dt class="calibre57">Whitacre, Micah, <a class="indexterm" href="#calibre_link-3784">Composable Data at Cerner</a></dt><dd class="calibre8"></dd><dt class="calibre57">whoami command, <a class="indexterm" href="#calibre_link-3785">Managing Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">WITH SERDEPROPERTIES clause (Hive), <a class="indexterm" href="#calibre_link-3786">Using a custom SerDe: RegexSerDe</a></dt><dd class="calibre8"></dd><dt class="calibre57">work units, <a class="indexterm" href="#calibre_link-3787">Volunteer Computing</a>, <a class="indexterm" href="#calibre_link-3788">Data Flow</a></dt><dd class="calibre8"></dd><dt class="calibre57">workflow engines, <a class="indexterm" href="#calibre_link-3789">Apache Oozie</a></dt><dd class="calibre8"></dd><dt class="calibre57">workflows (MapReduce)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3790">MapReduce Workflows</a></dt><dd class="calibre8"></dd><dt class="calibre57">Apache Oozie system, <a class="indexterm" href="#calibre_link-3791">Apache Oozie</a>–<a class="indexterm" href="#calibre_link-3792">Running an Oozie workflow job</a></dt><dd class="calibre8"></dd><dt class="calibre57">decomposing problems into jobs, <a class="indexterm" href="#calibre_link-3793">Decomposing a Problem into MapReduce Jobs</a>–<a class="indexterm" href="#calibre_link-3794">Decomposing a Problem into MapReduce Jobs</a></dt><dd class="calibre8"></dd><dt class="calibre57">JobControl class, <a class="indexterm" href="#calibre_link-3795">JobControl</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">Writable interface</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3796">Serialization</a>–<a class="indexterm" href="#calibre_link-3797">WritableComparable and comparators</a></dt><dd class="calibre8"></dd><dt class="calibre57">class hierarchy, <a class="indexterm" href="#calibre_link-3798">Writable Classes</a>–<a class="indexterm" href="#calibre_link-3799">Writable collections</a></dt><dd class="calibre8"></dd><dt class="calibre57">Crunch and, <a class="indexterm" href="#calibre_link-3800">Types</a></dt><dd class="calibre8"></dd><dt class="calibre57">implementing custom, <a class="indexterm" href="#calibre_link-3801">Implementing a Custom Writable</a>–<a class="indexterm" href="#calibre_link-3802">Custom comparators</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">WritableComparable interface, <a class="indexterm" href="#calibre_link-3803">WritableComparable and comparators</a>, <a class="indexterm" href="#calibre_link-3804">Partial Sort</a></dt><dd class="calibre8"></dd><dt class="calibre57">WritableComparator class, <a class="indexterm" href="#calibre_link-3805">WritableComparable and comparators</a></dt><dd class="calibre8"></dd><dt class="calibre57">WritableSerialization class, <a class="indexterm" href="#calibre_link-3806">Serialization Frameworks</a></dt><dd class="calibre8"></dd><dt class="calibre57">WritableUtils class, <a class="indexterm" href="#calibre_link-3807">Custom comparators</a></dt><dd class="calibre8"></dd><dt class="calibre57">write (w) permission, <a class="indexterm" href="#calibre_link-3808">Basic Filesystem Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">WRITE permission (ACL), <a class="indexterm" href="#calibre_link-3809">ACLs</a></dt><dd class="calibre8"></dd><dt class="calibre57">WriteSupport class, <a class="indexterm" href="#calibre_link-3810">Writing and Reading Parquet Files</a></dt><dd class="calibre8"></dd><dt class="calibre57">WRITE_OPS counter, <a class="indexterm" href="#calibre_link-3811">Task counters</a></dt><dd class="calibre8"></dd><dt class="calibre57">writing data</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">Crunch support, <a class="indexterm" href="#calibre_link-3812">Writing to a target</a></dt><dd class="calibre8"></dd><dt class="calibre57">using FileSystem API, <a class="indexterm" href="#calibre_link-3813">Writing Data</a>–<a class="indexterm" href="#calibre_link-3814">FSDataOutputStream</a></dt><dd class="calibre8"></dd><dt class="calibre57">HDFS data flow, <a class="indexterm" href="#calibre_link-3815">Anatomy of a File Write</a>–<a class="indexterm" href="#calibre_link-3816">Anatomy of a File Write</a></dt><dd class="calibre8"></dd><dt class="calibre57">Parquet and, <a class="indexterm" href="#calibre_link-3817">Writing and Reading Parquet Files</a>–<a class="indexterm" href="#calibre_link-3818">Projection and read schemas</a></dt><dd class="calibre8"></dd><dt class="calibre57">SequenceFile class, <a class="indexterm" href="#calibre_link-3819">Writing a SequenceFile</a>–<a class="indexterm" href="#calibre_link-3820">Writing a SequenceFile</a></dt><dd class="calibre8"></dd></dl></dd></dl></div><div class="book"><h3 class="author1">X</h3><dl class="book"><dt class="calibre57">x (execute) permission, <a class="indexterm" href="#calibre_link-3821">Basic Filesystem Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">XML documents, <a class="indexterm" href="#calibre_link-3822">XML</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">Y</h3><dl class="book"><dt class="calibre57">Yahoo!, <a class="indexterm" href="#calibre_link-3823">A Brief History of Apache Hadoop</a></dt><dd class="calibre8"></dd><dt class="calibre57">YARN (Yet Another Resource Negotiator)</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3824">Beyond Batch</a>, <a class="indexterm" href="#calibre_link-3825">YARN</a>, <a class="indexterm" href="#calibre_link-3826">Further Reading</a></dt><dd class="calibre8"></dd><dt class="calibre57">anatomy of application run, <a class="indexterm" href="#calibre_link-3827">Anatomy of a YARN Application Run</a>–<a class="indexterm" href="#calibre_link-3828">Building YARN Applications</a></dt><dd class="calibre8"></dd><dt class="calibre57">application lifespan, <a class="indexterm" href="#calibre_link-3829">Application Lifespan</a></dt><dd class="calibre8"></dd><dt class="calibre57">application master failure, <a class="indexterm" href="#calibre_link-3830">Application Master Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">building applications, <a class="indexterm" href="#calibre_link-3831">Building YARN Applications</a></dt><dd class="calibre8"></dd><dt class="calibre57">cluster setup and installation, <a class="indexterm" href="#calibre_link-3832">Creating Unix User Accounts</a></dt><dd class="calibre8"></dd><dt class="calibre57">cluster sizing, <a class="indexterm" href="#calibre_link-3833">Master node scenarios</a></dt><dd class="calibre8"></dd><dt class="calibre57">daemon properties, <a class="indexterm" href="#calibre_link-3834">YARN</a>–<a class="indexterm" href="#calibre_link-3835">CPU settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">distributed shell, <a class="indexterm" href="#calibre_link-3836">Building YARN Applications</a></dt><dd class="calibre8"></dd><dt class="calibre57">log aggregation, <a class="indexterm" href="#calibre_link-3837">Hadoop Logs</a></dt><dd class="calibre8"></dd><dt class="calibre57">MapReduce comparison, <a class="indexterm" href="#calibre_link-3838">YARN Compared to MapReduce 1</a>–<a class="indexterm" href="#calibre_link-3839">YARN Compared to MapReduce 1</a></dt><dd class="calibre8"></dd><dt class="calibre57">scaling out data, <a class="indexterm" href="#calibre_link-3840">Scaling Out</a></dt><dd class="calibre8"></dd><dt class="calibre57">scheduling in, <a class="indexterm" href="#calibre_link-3841">Scheduling in YARN</a>–<a class="indexterm" href="#calibre_link-3842">Dominant Resource Fairness</a>, <a class="indexterm" href="#calibre_link-3843">Job scheduler</a></dt><dd class="calibre8"></dd><dt class="calibre57">Spark and, <a class="indexterm" href="#calibre_link-3844">Spark on YARN</a>–<a class="indexterm" href="#calibre_link-3845">YARN cluster mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">starting and stopping daemons, <a class="indexterm" href="#calibre_link-3846">Starting and Stopping the Daemons</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">YARN client mode (Spark), <a class="indexterm" href="#calibre_link-3847">Spark on YARN</a></dt><dd class="calibre8"></dd><dt class="calibre57">YARN cluster mode (Spark), <a class="indexterm" href="#calibre_link-3848">YARN cluster mode</a>–<a class="indexterm" href="#calibre_link-3849">YARN cluster mode</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn-env.sh file, <a class="indexterm" href="#calibre_link-3850">Hadoop Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn-site.xml file, <a class="indexterm" href="#calibre_link-3851">Hadoop Configuration</a>, <a class="indexterm" href="#calibre_link-3852">Important Hadoop Daemon Properties</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.app.mapreduce.am.job.recovery.enable
          property, <a class="indexterm" href="#calibre_link-3853">Application Master Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.app.mapreduce.am.job.speculator.class
                  property, <a class="indexterm" href="#calibre_link-3854">Speculative Execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.app.mapreduce.am.job.task.estimator.class
                  property, <a class="indexterm" href="#calibre_link-3855">Speculative Execution</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.log-aggregation-enable property, <a class="indexterm" href="#calibre_link-3856">Hadoop Logs</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.nodemanager.address property, <a class="indexterm" href="#calibre_link-3857">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.nodemanager.aux-services property, <a class="indexterm" href="#calibre_link-3858">YARN</a>, <a class="indexterm" href="#calibre_link-3859">Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.nodemanager.bind-host property, <a class="indexterm" href="#calibre_link-3860">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.nodemanager.container-executor.class
              property, <a class="indexterm" href="#calibre_link-3861">Task Failure</a>, <a class="indexterm" href="#calibre_link-3862">CPU settings in YARN and MapReduce</a>, <a class="indexterm" href="#calibre_link-3863">Other Security Enhancements</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.nodemanager.delete.debug-delay-sec property, <a class="indexterm" href="#calibre_link-3864">Remote Debugging</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.nodemanager.hostname property, <a class="indexterm" href="#calibre_link-3865">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.nodemanager.linux-container-executor
              property, <a class="indexterm" href="#calibre_link-3866">CPU settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.nodemanager.local-dirs property, <a class="indexterm" href="#calibre_link-3867">YARN</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.nodemanager.localizer.address
                  property, <a class="indexterm" href="#calibre_link-3868">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.nodemanager.log.retain-second property, <a class="indexterm" href="#calibre_link-3869">Hadoop Logs</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.nodemanager.resource.cpu-vcores
                    property, <a class="indexterm" href="#calibre_link-3870">YARN</a>, <a class="indexterm" href="#calibre_link-3871">CPU settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.nodemanager.resource.memory-mb property, <a class="indexterm" href="#calibre_link-3872">GenericOptionsParser, Tool, and ToolRunner</a>, <a class="indexterm" href="#calibre_link-3873">YARN</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.nodemanager.vmem-pmem-ratio
                    property, <a class="indexterm" href="#calibre_link-3874">YARN</a>, <a class="indexterm" href="#calibre_link-3875">Memory settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.nodemanager.webapp.address property, <a class="indexterm" href="#calibre_link-3876">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.resourcemanager.address property</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3877">YARN</a>, <a class="indexterm" href="#calibre_link-3878">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">Hive and, <a class="indexterm" href="#calibre_link-3879">Configuring Hive</a></dt><dd class="calibre8"></dd><dt class="calibre57">Pig and, <a class="indexterm" href="#calibre_link-3880">MapReduce mode</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">yarn.resourcemanager.admin.address
                  property, <a class="indexterm" href="#calibre_link-3881">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.resourcemanager.am.max-attempts property, <a class="indexterm" href="#calibre_link-3882">Application Master Failure</a>, <a class="indexterm" href="#calibre_link-3883">Resource Manager Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.resourcemanager.bind-host property, <a class="indexterm" href="#calibre_link-3884">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.resourcemanager.hostname property, <a class="indexterm" href="#calibre_link-3885">YARN</a>, <a class="indexterm" href="#calibre_link-3886">Hadoop Daemon Addresses and Ports</a>, <a class="indexterm" href="#calibre_link-3887">Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.resourcemanager.max-completed-applications
            property, <a class="indexterm" href="#calibre_link-3888">The resource manager page</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.resourcemanager.nm.liveness-monitor.expiry-interval-ms
          property, <a class="indexterm" href="#calibre_link-3889">Node Manager Failure</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.resourcemanager.nodes.exclude-path
            property, <a class="indexterm" href="#calibre_link-3890">Cluster membership</a>, <a class="indexterm" href="#calibre_link-3891">Decommissioning old nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.resourcemanager.nodes.include-path
            property, <a class="indexterm" href="#calibre_link-3892">Cluster membership</a>, <a class="indexterm" href="#calibre_link-3893">Commissioning new nodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.resourcemanager.resource-tracker.address
                  property, <a class="indexterm" href="#calibre_link-3894">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.resourcemanager.scheduler.address
                  property, <a class="indexterm" href="#calibre_link-3895">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.resourcemanager.scheduler.class property, <a class="indexterm" href="#calibre_link-3896">Enabling the Fair Scheduler</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.resourcemanager.webapp.address
                  property, <a class="indexterm" href="#calibre_link-3897">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.scheduler.capacity.node-locality-delay
          property, <a class="indexterm" href="#calibre_link-3898">Delay Scheduling</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.scheduler.fair.allocation.file property, <a class="indexterm" href="#calibre_link-3899">Queue configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.scheduler.fair.allow-undeclared-pools
            property, <a class="indexterm" href="#calibre_link-3900">Queue placement</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.scheduler.fair.locality.threshold.node
          property, <a class="indexterm" href="#calibre_link-3901">Delay Scheduling</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.scheduler.fair.locality.threshold.rack
          property, <a class="indexterm" href="#calibre_link-3902">Delay Scheduling</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.scheduler.fair.preemption property, <a class="indexterm" href="#calibre_link-3903">Preemption</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.scheduler.fair.user-as-default-queue
            property, <a class="indexterm" href="#calibre_link-3904">Queue placement</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.scheduler.maximum-allocation-mb property, <a class="indexterm" href="#calibre_link-3905">Memory settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.scheduler.minimum-allocation-mb property, <a class="indexterm" href="#calibre_link-3906">Memory settings in YARN and MapReduce</a></dt><dd class="calibre8"></dd><dt class="calibre57">yarn.web-proxy.address property, <a class="indexterm" href="#calibre_link-3907">Hadoop Daemon Addresses and Ports</a></dt><dd class="calibre8"></dd><dt class="calibre57">YARN_LOG_DIR environment variable, <a class="indexterm" href="#calibre_link-3908">Hadoop Logs</a></dt><dd class="calibre8"></dd><dt class="calibre57">YARN_RESOURCEMANAGER_HEAPSIZE environment
            variable, <a class="indexterm" href="#calibre_link-3909">Memory heap size</a></dt><dd class="calibre8"></dd></dl></div><div class="book"><h3 class="author1">Z</h3><dl class="book"><dt class="calibre57">Zab protocol, <a class="indexterm" href="#calibre_link-3910">Implementation</a></dt><dd class="calibre8"></dd><dt class="calibre57">zettabytes, <a class="indexterm" href="#calibre_link-3911">Data!</a></dt><dd class="calibre8"></dd><dt class="calibre57">znodes</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3912">Group Membership in ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">ACLs and, <a class="indexterm" href="#calibre_link-3913">ACLs</a></dt><dd class="calibre8"></dd><dt class="calibre57">creating, <a class="indexterm" href="#calibre_link-3914">Creating the Group</a>–<a class="indexterm" href="#calibre_link-3915">Creating the Group</a></dt><dd class="calibre8"></dd><dt class="calibre57">deleting, <a class="indexterm" href="#calibre_link-3916">Deleting a Group</a></dt><dd class="calibre8"></dd><dt class="calibre57">ephemeral, <a class="indexterm" href="#calibre_link-3917">Ephemeral znodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">joining groups, <a class="indexterm" href="#calibre_link-3918">Joining a Group</a></dt><dd class="calibre8"></dd><dt class="calibre57">listing , <a class="indexterm" href="#calibre_link-3919">Listing Members in a Group</a>–<a class="indexterm" href="#calibre_link-3920">ZooKeeper command-line tools</a></dt><dd class="calibre8"></dd><dt class="calibre57">operations supported, <a class="indexterm" href="#calibre_link-3921">Operations</a></dt><dd class="calibre8"></dd><dt class="calibre57">persistent, <a class="indexterm" href="#calibre_link-3922">Ephemeral znodes</a></dt><dd class="calibre8"></dd><dt class="calibre57">properties supported, <a class="indexterm" href="#calibre_link-3923">Data Model</a>–<a class="indexterm" href="#calibre_link-3924">Watches</a></dt><dd class="calibre8"></dd><dt class="calibre57">sequential, <a class="indexterm" href="#calibre_link-3925">Sequence numbers</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">ZOOCFGDIR environment variable, <a class="indexterm" href="#calibre_link-3926">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">ZooKeeper</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">about, <a class="indexterm" href="#calibre_link-3927">ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">additional information, <a class="indexterm" href="#calibre_link-3928">Further Reading</a></dt><dd class="calibre8"></dd><dt class="calibre57">building applications</dt><dd class="calibre8"><dl class="book"><dt class="calibre57">configuration service, <a class="indexterm" href="#calibre_link-3929">A Configuration Service</a>–<a class="indexterm" href="#calibre_link-3930">A Configuration Service</a>, <a class="indexterm" href="#calibre_link-3931">A Lock Service</a>–<a class="indexterm" href="#calibre_link-3932">Implementation</a></dt><dd class="calibre8"></dd><dt class="calibre57">distributed data structures and protocols, <a class="indexterm" href="#calibre_link-3933">More Distributed Data Structures and Protocols</a></dt><dd class="calibre8"></dd><dt class="calibre57">resilient, <a class="indexterm" href="#calibre_link-3934">The Resilient ZooKeeper Application</a>–<a class="indexterm" href="#calibre_link-3935">A reliable configuration service</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">consistency and, <a class="indexterm" href="#calibre_link-3936">Consistency</a>–<a class="indexterm" href="#calibre_link-3937">Consistency</a></dt><dd class="calibre8"></dd><dt class="calibre57">data model, <a class="indexterm" href="#calibre_link-3938">Data Model</a></dt><dd class="calibre8"></dd><dt class="calibre57">example of, <a class="indexterm" href="#calibre_link-3939">An Example</a>–<a class="indexterm" href="#calibre_link-3940">Deleting a Group</a></dt><dd class="calibre8"></dd><dt class="calibre57">failover controllers and, <a class="indexterm" href="#calibre_link-3941">Failover and fencing</a></dt><dd class="calibre8"></dd><dt class="calibre57">HBase and, <a class="indexterm" href="#calibre_link-3942">Implementation</a></dt><dd class="calibre8"></dd><dt class="calibre57">high availability and, <a class="indexterm" href="#calibre_link-3943">HDFS High Availability</a></dt><dd class="calibre8"></dd><dt class="calibre57">implementing, <a class="indexterm" href="#calibre_link-3944">Implementation</a></dt><dd class="calibre8"></dd><dt class="calibre57">installing and running, <a class="indexterm" href="#calibre_link-3945">Installing and Running ZooKeeper</a>–<a class="indexterm" href="#calibre_link-3946">Installing and Running ZooKeeper</a></dt><dd class="calibre8"></dd><dt class="calibre57">operations in, <a class="indexterm" href="#calibre_link-3947">Operations</a>–<a class="indexterm" href="#calibre_link-3948">ACLs</a></dt><dd class="calibre8"></dd><dt class="calibre57">production considerations, <a class="indexterm" href="#calibre_link-3949">ZooKeeper in Production</a>–<a class="indexterm" href="#calibre_link-3950">Configuration</a></dt><dd class="calibre8"></dd><dt class="calibre57">sessions and, <a class="indexterm" href="#calibre_link-3951">Sessions</a>–<a class="indexterm" href="#calibre_link-3952">Time</a></dt><dd class="calibre8"></dd><dt class="calibre57">states and, <a class="indexterm" href="#calibre_link-3953">States</a>–<a class="indexterm" href="#calibre_link-3954">States</a>, <a class="indexterm" href="#calibre_link-3955">State exceptions</a></dt><dd class="calibre8"></dd></dl></dd><dt class="calibre57">zxid, <a class="indexterm" href="#calibre_link-3956">Consistency</a></dt><dd class="calibre8"></dd></dl></div></div></div></div>

<div class="calibre1" id="calibre_link-87"><div class="book" type="colophon" id="calibre_link-4766" title="Colophon"><h2 class="title10">Colophon</h2><p class="calibre2">The animal on the cover of <span class="calibre"><em class="calibre10">Hadoop: The Definitive
  Guide</em></span> is an African elephant. These members of the genus
  <span class="calibre"><em class="calibre10">Loxodonta</em></span> are the largest land animals on Earth
  (slightly larger than their cousin, the Asian elephant) and can be
  identified by their ears, which have been said to look somewhat like the
  continent of Asia. Males stand 12 feet tall at the shoulder and weigh 12,000
  pounds, but they can get as big as 15,000 pounds, whereas females stand 10
  feet tall and weigh 8,000–11,000 pounds. Even young elephants are very
  large: at birth, they already weigh approximately 200 pounds and stand about
  3 feet tall.</p><p class="calibre2">African elephants live throughout sub-Saharan Africa. Most of the
  continent’s elephants live on savannas and in dry woodlands. In some
  regions, they can be found in desert areas; in others, they are found in
  mountains.</p><p class="calibre2">The species plays an important role in the forest and savanna
  ecosystems in which they live. Many plant species are dependent on passing
  through an elephant’s digestive tract before they can germinate; it is
  estimated that at least a third of tree species in west African forests rely
  on elephants in this way. Elephants grazing on vegetation also affect the
  structure of habitats and influence bush fire patterns. For example, under
  natural conditions, elephants make gaps through the rainforest, enabling the
  sunlight to enter, which allows the growth of various plant species. This,
  in turn, facilitates more abundance and more diversity of smaller animals.
  As a result of the influence elephants have over many plants and animals,
  they are often referred to as a <span class="calibre">keystone
  species</span> because they are vital to the long-term survival of the
  ecosystems in which they live.</p><p class="calibre2">Many of the animals on O’Reilly covers are endangered; all of them are important to the
    world. To learn more about how you can help, go to <a class="ulink" href="http://animals.oreilly.com" target="_top"><span class="calibre">animals.oreilly.com</span></a>.</p><p class="calibre2">The cover image is from the <span class="calibre"><em class="calibre10">Dover Pictorial
  Archive</em></span>. The cover fonts are URW Typewriter and Guardian Sans.
  The text font is Adobe Minion Pro; the heading font is Adobe Myriad
  Condensed; and the code font is Dalton Maag’s Ubuntu Mono.</p></div></div>

<div class="calibre1" id="calibre_link-454"><div class="book" type="colophon" id="calibre_link-4767"><h2 class="title10"></h2><div class="book"><h1 class="title11">Hadoop:&nbsp;The Definitive Guide</h1></div><div class="book"><div class="book"><h3 class="author3"><span class="firstname">Tom</span> <span class="firstname">White</span></h3></div></div><div class="book"><h4 class="calibre58">Editor</h4><h3 class="editor"><span class="calibre">Mike</span> <span class="calibre">Loukides</span></h3></div><div class="book"><h4 class="calibre58">Editor</h4><h3 class="editor"><span class="calibre">Meghan</span> <span class="calibre">Blanchette</span></h3></div><div class="book"><h4 class="calibre58">Editor</h4><h3 class="editor"><span class="calibre">Matthew</span> <span class="calibre">Hacker</span></h3></div><div class="book"><h4 class="calibre58">Editor</h4><h3 class="editor"><span class="calibre">Jasmine</span> <span class="calibre">Kwityn</span></h3></div><div class="book"><div class="book"><table class="calibre59"><tbody><tr class="calibre19"><td colspan="2" class="calibre60"><strong class="calibre24">Revision History</strong></td></tr><tr class="calibre26"><td class="calibre28">2015-03-19</td><td class="calibre28">First release</td></tr><tr class="calibre19"><td class="calibre28">2015-04-17</td><td class="calibre28">Second release</td></tr></tbody></table></div></div><div class="book"><p class="calibre2">Copyright © 2015 Tom White</p></div><div class="book"><div class="book" title="Legal Notice"><a id="calibre_link-4768" class="calibre"></a><p class="calibre61">O’Reilly books may be purchased for educational, business, or sales
    promotional use. Online editions are also available for most titles
    (<a class="ulink" href="http://safaribooksonline.com" target="_top">http://safaribooksonline.com</a>).
    For more information, contact our corporate/institutional sales
    department: 800-998-9938 or <span class="calibre"><a class="ulink" href="mailto:corporate@oreilly.com">corporate@oreilly.com</a></span>.</p></div></div><div class="book"><div class="book" title="Legal Notice"><a id="calibre_link-4769" class="calibre"></a><p class="calibre61">The O’Reilly logo is a registered trademark of O’Reilly Media, Inc.
    <span class="calibre"><em class="calibre10">Hadoop: The Definitive Guide</em></span>, the cover image of an
    African elephant, and related trade dress are trademarks of O’Reilly
    Media, Inc.</p><p class="calibre61">Many of the designations used by manufacturers and sellers to
    distinguish their products are claimed as trademarks. Where those
    designations appear in this book, and O’Reilly Media, Inc. was aware of a
    trademark claim, the designations have been printed in caps or initial
    caps.</p></div></div><div class="book"><div class="book" title="Legal Notice"><a id="calibre_link-4770" class="calibre"></a><p class="calibre61">While the publisher and the author have used good faith efforts to
    ensure that the information and instructions contained in this work are
    accurate, the publisher and the author disclaim all responsibility for
    errors or omissions, including without limitation responsibility for
    damages resulting from the use of or reliance on this work. Use of the
    information and instructions contained in this work is at your own risk.
    If any code samples or other technology this work contains or describes is
    subject to open source licenses or the intellectual property rights of
    others, it is your responsibility to ensure that your use thereof complies
    with such licenses and/or rights.</p></div></div><div class="book"><div class="book" title="Legal Notice"><a id="calibre_link-4771" class="calibre"></a><p class="calibre61"></p></div></div><div class="publisher"><span class="publishername">O’Reilly Media, Inc.<br class="calibre3"></span><div class="book"><p class="calibre62"><span class="calibre">1005 Gravenstein Highway North</span></p><p class="calibre62"><span class="calibre">Sebastopol</span>, <span class="calibre">CA</span> <span class="calibre">95472</span> </p></div></div><div class="book"><p class="calibre63">2015-04-28T19:37:25Z</p></div><p class="calibre2"></p></div></div>

</body></html>